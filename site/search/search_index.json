{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Actuarial Exam Notes Test","title":"**Actuarial Exam Notes**"},{"location":"#actuarial-exam-notes","text":"Test","title":"Actuarial Exam Notes"},{"location":"Placeholder/","text":"Work in progress!","title":"ASA-PA"},{"location":"Placeholder/#work-in-progress","text":"","title":"Work in progress!"},{"location":"2.%20Actuarial%20Mathematics/Overview/","text":"Overview of Actuarial Mathematics These set of exams cover the main concepts needed to evaluate insurance related risks. They are split into two main exams: Fundamentals of Actuarial Mathematics (FAM) Advanced Long Term or Short Term Actuarial Mathematics (ALTAM or ASTAM) This set of notes splits the FAM component into two seperate components for clarity, as the components are not related to one another: FAM-S (Short Term) FAM-L (Long Term) However, the actual FAM exam tests both components together in an MCQ format. It is regarded as one of the tougher examinations due to its wide breadth (due to the unrelated components).","title":"**Overview of Actuarial Mathematics**"},{"location":"2.%20Actuarial%20Mathematics/Overview/#overview-of-actuarial-mathematics","text":"These set of exams cover the main concepts needed to evaluate insurance related risks. They are split into two main exams: Fundamentals of Actuarial Mathematics (FAM) Advanced Long Term or Short Term Actuarial Mathematics (ALTAM or ASTAM) This set of notes splits the FAM component into two seperate components for clarity, as the components are not related to one another: FAM-S (Short Term) FAM-L (Long Term) However, the actual FAM exam tests both components together in an MCQ format. It is regarded as one of the tougher examinations due to its wide breadth (due to the unrelated components).","title":"Overview of Actuarial Mathematics"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/","text":"Survival Models Survival Models are probability distributions that measure the time to failure of an entity, or phrased another way, the future lifetime of an entity. In an Actuarial context, it measures the time to death of a person. Since survival models measure time , they are denoted by the continuous random variable \\(T_i\\) , where the subscript represents the age of the person being studied. Newborn Lifetime The base survival model measures the future lifetime of a person aged 0 (newborn), denoted by \\(T_0\\) . The CDF thus represents the probability that the newborn dies before age \\(t\\) : \\[ F_0(t) = P(T_0 \\le t) \\] The complement of the CDF is the probability that the newborn survives past a certain age \\(t\\) . From a different perspective, it can also be seen as the newborn dying after age \\(t\\) . This is known as the Survival Function . \\[ S_0(t) = P(T_0 \\ge t) = 1 - P(T_0 \\le t) = 1 - F_0(t) \\] Continuous Lifetime Similarly, we can generalize the above for a person aged \\(x\\) , denoted by \\(T_x\\) . However, the intepretation of the functions have a slightly different meaning. The CDF instead represents the probability that the person dies within a certain number of years \\(t\\) . Given how often this expression is used, it can be simplified to the notation \\(q\\) . \\[ F_x(t) = P(T_x \\le t) = {}_{t}q_{x} \\] It can also be written in terms of the newborn distribution , whereby the newborn dies by age \\(x+t\\) , given that it survives till age \\(x\\) : \\[ P(T_0 \\le x + t | T_0 > x) = \\frac{P(x < T_0 < x +t)}{P(T_0 > x)} = \\frac{P(T_0 < x +t)-P(T_0 < x)}{P(T_0 > x)} = \\frac{F_0(x+t) - F_0(x)}{S_0(x)} \\] Conversely, the survival function is the probability that the person survives another certain number of years \\(t\\) or dies after a certain number of years \\(t\\) . It can be simplified to the notation \\(p\\) . \\[ S_x(t) = P(T_x \\ge t) = {}_{t}p_{x} \\] It can also be written in terms of the newborn distribution, whereby the newborn survives till age \\(x+t\\) , given that it survives till age \\(x\\) . \\[ P(T_x \\ge t) = P(T_0 ge x + t | T_0 > x) = \\frac{P(T_0 > x + t)}{P(T_0 > x)} = \\frac{S_0(x+t)}{S_0(x)} \\] Note that since the surviving till age \\(x\\) is a subset of surviving till age \\(x+t\\) , the conditional probability simply uses \\(x+t\\) in the numerator. Recall that the two are complements of one another - the person will inevitably die. \\[ \\begin{aligned} F_x(t) + S_x(t) &= 1 \\\\ {}_{t}q_{x} + {}_{t}p_{x} &= 1 \\end{aligned} \\] Deferred Death The death of the person can also be \" deferred \", whereby the person survives \\(s\\) years then dies within the following \\(t\\) years . Following this definition: \\[ {}_{s|t}q_{x} = {}_{s}p_{x} * {}_{t}q_{x+s} \\] From another perspective, it calculates the probability of the person dying within a specified range of time in the future: \\[ {}_{s|t}q_{x} = {}_{s+t}q_{x} - {}_{s}q_{x} \\] Force of Mortality Loosely speaking, the rate of mortality (different from mortality rate) is the probability of death during a period per unit time . \\[ Rate~Of~Mortality = \\frac{P(T_x \\le h)}{h} = \\frac{{}_{h}q_{x}}{h} \\] If the period of time is gets smaller and smaller , then the result is the instantaneous change in probability, known as the Force of Mortality . \\[ \\mu_x = \\lim_{h \\to 0} \\frac{{}_{h}q_{x}}{h} \\] For very small periods of time , the force of mortality can be used to approximate the change in probability of death: \\[ {}_{h}q_{x} = {}_{0}q_{x} + h * \\mu_x = h * \\mu_x \\] Probability Density Function The PDF of \\(T_x\\) is the differential of the CDF: \\[ \\begin{aligned} f_x(t) &= \\frac{d}{dt} F_x(t) \\\\ &= \\frac{d}{dt} P(T_x \\le t) \\\\ &= \\lim_{h \\to 0} \\frac{P(T_x \\le t+h) - P(T_x \\le t)}{h} \\\\ &= \\lim_{h \\to 0} \\frac{P(x + t < T_0 < x + t +h)}{h * P(T_0 > t)} \\end{aligned} \\] The denominator of the limit can be adjusted to make it into a smoother conditional probability: \\[ \\begin{aligned} f_x(t) &= \\lim_{h \\to 0} \\frac{P(x + t < T_0 < x + t +h)}{h * P(T_0 > t)} \\\\ &= \\frac{P(T_0 > t)}{P(T_0 > x + t)} \\lim_{h \\to 0} \\frac{P(x + t < T_0 < x + t +h)}{h * P(T_0 > x + t)} \\\\ &= \\frac{P(T_0 > t)}{P(T_0 > x + t)} \\lim_{h \\to 0} \\frac{P(x + t < T_0 < x + t +h | T_0 > x + t)}{h} \\\\ &= {}_{t}p_{x} * \\mu_{x+t} \\end{aligned} \\] CDF and Survival Function The CDF can also be expressed as a function of the force of mortality, making use of the PDF derived above: \\[ \\begin{aligned} F_x(t) &= \\int_{0}^{t} f_x(t) \\\\ F_x(t) &= \\int_{0}^{t} {}_{t}p_{x} * \\mu_{x+t} \\\\ {}_{t}q_{x} &= \\int_{0}^{t} {}_{t}p_{x} * \\mu_{x+t} \\end{aligned} \\] Similarly for the Survival Function , \\[ \\begin{aligned} f_x(t) &= {}_{t}p_{x} * \\mu_{x + t} \\\\ \\mu_{x + t} &= \\frac{f_x(t)}{S_x(t)} \\end{aligned} \\] Consider another expression for the PDF: \\[ \\begin{aligned} f_x(t) &= \\frac{d}{dt} F_x(t) \\\\ f_x(t) &= \\frac{d}{dt} (1 - S_x(t)) \\\\ f_x(t) &= -S'_x(t) \\end{aligned} \\] Combining both the above, \\[ \\begin{aligned} \\mu_{x + t} &= \\frac{-S'_x(t)}{S_x(t)} \\\\ \\mu_{x + t} &= - \\frac{d}{dt} (\\ln S_x(t)) \\\\ \\ln S_x(t) &= - \\int_{0}^{t} \\mu_{x + t} \\\\ S_x(t) &= e ^ {- \\int_{0}^{t} \\mu_{x + t}} \\\\ {}_{t}p_{x} &= e ^ {- \\int_{0}^{t} \\mu_{x + t}} \\end{aligned} \\] Expectation and Variance The Expectation of the Future Lifetime is known as the Life Expectancy of the individual. \\[ \\begin{aligned} \\mathring{e}_x &= E(T_x) \\\\ &= \\int_{0}^{t} t * {}_{t}p_{x} * \\mu_{x+t} \\\\ &= \\int_{0}^{t} t * -S'_x(t) \\\\ &= [-t * {}_{t}p_{x}]^{\\infty}_0 + \\int_{0}^{t} {}_{t}p_{x} \\\\ &= \\int_{0}^{t} {}_{t}p_{x} \\end{aligned} \\] Note that \\([-t * {}_{t}p_{x}]^{\\infty}_0\\) is always 0 as the probability of living forever is 0. Unfortunately, there is no simplification to be made for the Variance of the future lifetime: \\[ \\begin{aligned} Var(T_x) &= E(T^2_x) - [E(T_x)]^2 \\\\ &= \\int_{0}^{t} t^2 * {}_{t}p_{x} * \\mu_{x+t} - [\\mathring{e}_x]^2 \\end{aligned} \\] Discrete Lifetime Life Tables","title":"Survival Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#survival-models","text":"Survival Models are probability distributions that measure the time to failure of an entity, or phrased another way, the future lifetime of an entity. In an Actuarial context, it measures the time to death of a person. Since survival models measure time , they are denoted by the continuous random variable \\(T_i\\) , where the subscript represents the age of the person being studied.","title":"Survival Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#newborn-lifetime","text":"The base survival model measures the future lifetime of a person aged 0 (newborn), denoted by \\(T_0\\) . The CDF thus represents the probability that the newborn dies before age \\(t\\) : \\[ F_0(t) = P(T_0 \\le t) \\] The complement of the CDF is the probability that the newborn survives past a certain age \\(t\\) . From a different perspective, it can also be seen as the newborn dying after age \\(t\\) . This is known as the Survival Function . \\[ S_0(t) = P(T_0 \\ge t) = 1 - P(T_0 \\le t) = 1 - F_0(t) \\]","title":"Newborn Lifetime"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#continuous-lifetime","text":"Similarly, we can generalize the above for a person aged \\(x\\) , denoted by \\(T_x\\) . However, the intepretation of the functions have a slightly different meaning. The CDF instead represents the probability that the person dies within a certain number of years \\(t\\) . Given how often this expression is used, it can be simplified to the notation \\(q\\) . \\[ F_x(t) = P(T_x \\le t) = {}_{t}q_{x} \\] It can also be written in terms of the newborn distribution , whereby the newborn dies by age \\(x+t\\) , given that it survives till age \\(x\\) : \\[ P(T_0 \\le x + t | T_0 > x) = \\frac{P(x < T_0 < x +t)}{P(T_0 > x)} = \\frac{P(T_0 < x +t)-P(T_0 < x)}{P(T_0 > x)} = \\frac{F_0(x+t) - F_0(x)}{S_0(x)} \\] Conversely, the survival function is the probability that the person survives another certain number of years \\(t\\) or dies after a certain number of years \\(t\\) . It can be simplified to the notation \\(p\\) . \\[ S_x(t) = P(T_x \\ge t) = {}_{t}p_{x} \\] It can also be written in terms of the newborn distribution, whereby the newborn survives till age \\(x+t\\) , given that it survives till age \\(x\\) . \\[ P(T_x \\ge t) = P(T_0 ge x + t | T_0 > x) = \\frac{P(T_0 > x + t)}{P(T_0 > x)} = \\frac{S_0(x+t)}{S_0(x)} \\] Note that since the surviving till age \\(x\\) is a subset of surviving till age \\(x+t\\) , the conditional probability simply uses \\(x+t\\) in the numerator. Recall that the two are complements of one another - the person will inevitably die. \\[ \\begin{aligned} F_x(t) + S_x(t) &= 1 \\\\ {}_{t}q_{x} + {}_{t}p_{x} &= 1 \\end{aligned} \\]","title":"Continuous Lifetime"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#deferred-death","text":"The death of the person can also be \" deferred \", whereby the person survives \\(s\\) years then dies within the following \\(t\\) years . Following this definition: \\[ {}_{s|t}q_{x} = {}_{s}p_{x} * {}_{t}q_{x+s} \\] From another perspective, it calculates the probability of the person dying within a specified range of time in the future: \\[ {}_{s|t}q_{x} = {}_{s+t}q_{x} - {}_{s}q_{x} \\]","title":"Deferred Death"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#force-of-mortality","text":"Loosely speaking, the rate of mortality (different from mortality rate) is the probability of death during a period per unit time . \\[ Rate~Of~Mortality = \\frac{P(T_x \\le h)}{h} = \\frac{{}_{h}q_{x}}{h} \\] If the period of time is gets smaller and smaller , then the result is the instantaneous change in probability, known as the Force of Mortality . \\[ \\mu_x = \\lim_{h \\to 0} \\frac{{}_{h}q_{x}}{h} \\] For very small periods of time , the force of mortality can be used to approximate the change in probability of death: \\[ {}_{h}q_{x} = {}_{0}q_{x} + h * \\mu_x = h * \\mu_x \\]","title":"Force of Mortality"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#probability-density-function","text":"The PDF of \\(T_x\\) is the differential of the CDF: \\[ \\begin{aligned} f_x(t) &= \\frac{d}{dt} F_x(t) \\\\ &= \\frac{d}{dt} P(T_x \\le t) \\\\ &= \\lim_{h \\to 0} \\frac{P(T_x \\le t+h) - P(T_x \\le t)}{h} \\\\ &= \\lim_{h \\to 0} \\frac{P(x + t < T_0 < x + t +h)}{h * P(T_0 > t)} \\end{aligned} \\] The denominator of the limit can be adjusted to make it into a smoother conditional probability: \\[ \\begin{aligned} f_x(t) &= \\lim_{h \\to 0} \\frac{P(x + t < T_0 < x + t +h)}{h * P(T_0 > t)} \\\\ &= \\frac{P(T_0 > t)}{P(T_0 > x + t)} \\lim_{h \\to 0} \\frac{P(x + t < T_0 < x + t +h)}{h * P(T_0 > x + t)} \\\\ &= \\frac{P(T_0 > t)}{P(T_0 > x + t)} \\lim_{h \\to 0} \\frac{P(x + t < T_0 < x + t +h | T_0 > x + t)}{h} \\\\ &= {}_{t}p_{x} * \\mu_{x+t} \\end{aligned} \\]","title":"Probability Density Function"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#cdf-and-survival-function","text":"The CDF can also be expressed as a function of the force of mortality, making use of the PDF derived above: \\[ \\begin{aligned} F_x(t) &= \\int_{0}^{t} f_x(t) \\\\ F_x(t) &= \\int_{0}^{t} {}_{t}p_{x} * \\mu_{x+t} \\\\ {}_{t}q_{x} &= \\int_{0}^{t} {}_{t}p_{x} * \\mu_{x+t} \\end{aligned} \\] Similarly for the Survival Function , \\[ \\begin{aligned} f_x(t) &= {}_{t}p_{x} * \\mu_{x + t} \\\\ \\mu_{x + t} &= \\frac{f_x(t)}{S_x(t)} \\end{aligned} \\] Consider another expression for the PDF: \\[ \\begin{aligned} f_x(t) &= \\frac{d}{dt} F_x(t) \\\\ f_x(t) &= \\frac{d}{dt} (1 - S_x(t)) \\\\ f_x(t) &= -S'_x(t) \\end{aligned} \\] Combining both the above, \\[ \\begin{aligned} \\mu_{x + t} &= \\frac{-S'_x(t)}{S_x(t)} \\\\ \\mu_{x + t} &= - \\frac{d}{dt} (\\ln S_x(t)) \\\\ \\ln S_x(t) &= - \\int_{0}^{t} \\mu_{x + t} \\\\ S_x(t) &= e ^ {- \\int_{0}^{t} \\mu_{x + t}} \\\\ {}_{t}p_{x} &= e ^ {- \\int_{0}^{t} \\mu_{x + t}} \\end{aligned} \\]","title":"CDF and Survival Function"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#expectation-and-variance","text":"The Expectation of the Future Lifetime is known as the Life Expectancy of the individual. \\[ \\begin{aligned} \\mathring{e}_x &= E(T_x) \\\\ &= \\int_{0}^{t} t * {}_{t}p_{x} * \\mu_{x+t} \\\\ &= \\int_{0}^{t} t * -S'_x(t) \\\\ &= [-t * {}_{t}p_{x}]^{\\infty}_0 + \\int_{0}^{t} {}_{t}p_{x} \\\\ &= \\int_{0}^{t} {}_{t}p_{x} \\end{aligned} \\] Note that \\([-t * {}_{t}p_{x}]^{\\infty}_0\\) is always 0 as the probability of living forever is 0. Unfortunately, there is no simplification to be made for the Variance of the future lifetime: \\[ \\begin{aligned} Var(T_x) &= E(T^2_x) - [E(T_x)]^2 \\\\ &= \\int_{0}^{t} t^2 * {}_{t}p_{x} * \\mu_{x+t} - [\\mathring{e}_x]^2 \\end{aligned} \\]","title":"Expectation and Variance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#discrete-lifetime","text":"","title":"Discrete Lifetime"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#life-tables","text":"","title":"Life Tables"},{"location":"3.%20Predictive%20Analytics/Exam%20Overview/","text":"Overview of Predictive Analytics Predictive Analytics is the usage of statistical models to analyze historical or current data to make predictions about the future or unknown events. Due to the growing relevance of Predictive Analytics , the SOA has added a significant amount of material on the topic into the credentialling process: Exam Statistics for Risk Modelling (SRM) Exam Predictive Analytics (PA) Exam Advanced Topics for Predictive Analytics (ATPA) Exam ATPA was added midway through 2022 to replace Exam IFM. Credit for IFM also counts towards credit for ATPA, thus this set of notes will NOT be covering ATPA. All of them share the same theme of working with statistical models: Constructing statistical models Intepreting their outputs Evaluating their performance Exam SRM covers the theory about the various types of models, tested in the typical MCQ format . Unlike the other exams where most of the questions are quantitative, most of SRM's questions are qualitative . Exam PA tests the same concepts in an applied manner , providing real data and a business problem to navigate through in a written format . Exam ATPA builds on both of these exams, covering more advanced concepts in a business context.","title":"Overview"},{"location":"3.%20Predictive%20Analytics/Exam%20Overview/#overview-of-predictive-analytics","text":"Predictive Analytics is the usage of statistical models to analyze historical or current data to make predictions about the future or unknown events. Due to the growing relevance of Predictive Analytics , the SOA has added a significant amount of material on the topic into the credentialling process: Exam Statistics for Risk Modelling (SRM) Exam Predictive Analytics (PA) Exam Advanced Topics for Predictive Analytics (ATPA) Exam ATPA was added midway through 2022 to replace Exam IFM. Credit for IFM also counts towards credit for ATPA, thus this set of notes will NOT be covering ATPA. All of them share the same theme of working with statistical models: Constructing statistical models Intepreting their outputs Evaluating their performance Exam SRM covers the theory about the various types of models, tested in the typical MCQ format . Unlike the other exams where most of the questions are quantitative, most of SRM's questions are qualitative . Exam PA tests the same concepts in an applied manner , providing real data and a business problem to navigate through in a written format . Exam ATPA builds on both of these exams, covering more advanced concepts in a business context.","title":"Overview of Predictive Analytics"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Review%20of%20Statistical%20Theory/","text":"Review of Statistical Theory This is meant to be a quick review of the key content of VEE: Mathematical Statistics that will be used for this exam. Overview of Statistics Statistics is a discipline revolving around data. A Population refers to the theoretical set of all possible data of the event of interest. The goal of statistics is to determine certain attributes that summarizes or describes the population, known as Parameters . However, it is impossible to study the entire population at once, thus a subset of the population is studied instead, known as the Sample . Attributes that summarize or describe the sample are known as Statistics . Ideally, the sample is representative of the population, which means that findings from the sample can be applied to the population as a whole. This means that the sample statistics can be used estimate population parameters. We distinguish between the two (when they have the same notation) through the Hat accent (^) - Population Parameters are their written without the hat ( \\(x\\) ) while their corresponding sample statistics are written with the hat ( \\(\\hat{x}\\) ). Common Sample Statistics The Mean is the average of the population. Population Mean Sample Mean \\(\\mu = \\sum\\limits_{i=i}^n x_i * p(x_i)\\) \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=i}^n x_i\\) The Variance measures the spread of values about the mean. However, the units of Variance are unintuitive, thus we consider the Standard Deviation for more practical purposes, which is the square root of the variance . Population Variance Sample Variance \\(\\sigma^2 = \\sum\\limits_{i=i}^n (x_i - \\mu)^2 * p(x_i)\\) \\(s^2 = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x})^2\\) Covariance is a measure of the linear relationship between two variables: Positive Covariance - Variables move in the same direction Negative Covariance - Variables move in opposite directions Population Covariance Sample Covariance \\(\\sigma_{x, y}\\) \\(\\hat{\\sigma}_{x, y} = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x}) * (y_i - \\bar{y})\\) However, there are two issues with Covariance - the units are unintuitive (similar to variance) and there is no benchmark as to what constitutes a strong/weak relationship. Thus, the Correlation is an adjusted measure of the relationship between -1 and 1 . Population Correlation Sample Correlation \\(\\rho = \\frac{\\sigma_{x,y}}{\\sigma_x * \\sigma_y}\\) \\(r = \\frac{s_{x,y}}{s_x * s_y}\\) Sampling Distribution Whenever a sample is drawn from a population and a statistic is calculated, it is known as a Point Estimate . If another sample were to be drawn, it would most likely result in a different point estimate as the underlying sample is likely to be different. If this process were to be repeated a large number times, the probability distribution of the resulting point estimates is known as the Sampling Distribution of the statistic. The standard deviation of this sampling distribution is known as the Standard Error . There is no rule surrounding the sampling distribution - it depends on the distribution of the population, statistic, sampling method etc. A special case is the Sample Mean . If the population is normally distributed, then it is normally distributed as well. However, regardless of the population distribution, it is approximately normally distributed through the Central Limit Theorem or Law of Large Numbers . \\[ \\bar{x} \\sim N(\\mu, \\frac{\\sigma^2}{n}) \\] Following that, the standard error is given by the following: \\[ \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}} \\] However, the population standard deviation is most likely unknown. Thus, we can calculate an estimate of the standard error by using the sample standard deviation instead: \\[ \\hat{\\sigma_{\\bar{x}}} = \\frac{s}{\\sqrt{n}} \\] Confidence Interval Given that there is only one true value for the population parameter and a whole distribution of estimators, it is unlikely that a point estimate will be equal to the population parameter. Thus, instead of a point estimate, a range of estimates is used, known as a Confidence Interval . The interval is made a chosen Confidence Level which represents the proportion of confidence intervals that will contain the true value . In other words, if a large number of confidence intervals constructed in the same manner were to be made, {confidence level}% of them would contain the true value. \\[ CI = \\hat{\\theta}~\\pm~Margin~of~Error \\] The Margin of Error represents the range of values on both sides of the point estimate that the true value could lie. The width of the confidence interval is thus the sum of the two margins. \\[ Margin~of~Error = Critical Value * SE(\\hat{\\theta}) \\] Hypothesis Testing Hypothesis Testing is a formal method of making inferences about the population parameters based on the sample statistic. It starts with a Hypothesis which is a conjecture about the population parameters: Null Hypothesis - What is currently believed to be true Alternative Hypothesis - What is to be proven A Test Statistic is then calculated, which would quantify the behaviour of population such that it would distinguish the null and alternative hypothesis. Assuming the Null Hypothesis is true , the sampling distribution of the test statistic is determined. From the sampling distribution, the p-value is calculated, which is the probability of observing the calculated sample statistic or more extreme . If the p-value is smaller than a pre-determined level of Statistical Significance ( \\(\\alpha\\) ), then the test is \"successful\" and the null hypothesis is rejected. If not, then the test \"fails\" and the null is not rejected. Note that it rejecting the null hypothesis does NOT mean that the alternative hypothesis is accepted - EG. Rejected then hypothesis of a large paw print being from a bear does not mean that it is from Bigfoot. Thus, the hypotheses are usually constructed such that the two hypothesis are complementary , such that rejecting the null allows acceptance of the alternative, leading to a definitive insight. In layman terms, a hypothesis test is a test of extremeness . Assuming the null hypothesis is true, how extreme (low probability) is the observed sample? If the p-value is sufficiently low, it means that the sample is rare - it could be due to pure chance or that it is actually not rare because the null is not true. We distinguish between the two mathematically through \\(\\alpha\\) . It is the probability of a False Positive - that the result obtained was by pure chance. It is typically set at 5%, which means that 5% of all statistics calculated are expected to be extreme under the null. Thus, if the p-value of the sample is smaller than \\(\\alpha\\) , then it is likely that the observation was not due to chance but instead because the null was false . Alternatively, instead of comparing p-value to \\(alpha\\) , the test-statistic and the corresponding value of \\(\\alpha\\) on the sampling distribution can be used. It is known as the Critical Value , which represents the boundary of Reject Null Do not Reject Null p-value smaller than \\(\\alpha\\) p-value smaller than \\(\\alpha\\) test-statistic larger than critical value test-statistic smaller than critical value Maximum Likelihood Estimation If the population distribution is known, there is an alternative method of estimating the parameters apart from calculating the corresponding sample statistics. We model a set of observations as a random sample from an unknown joint probability distribution which is expressed in terms of a set of parameters. The goal of maximum likelihood estimation is to determine the parameters for which the observed data have the highest joint probability. The goal of maximum likelihood estimation is to find the values of the model parameters that maximize the likelihood function over the parameter space","title":"Review of Statistical Theory"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Review%20of%20Statistical%20Theory/#review-of-statistical-theory","text":"This is meant to be a quick review of the key content of VEE: Mathematical Statistics that will be used for this exam.","title":"Review of Statistical Theory"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Review%20of%20Statistical%20Theory/#overview-of-statistics","text":"Statistics is a discipline revolving around data. A Population refers to the theoretical set of all possible data of the event of interest. The goal of statistics is to determine certain attributes that summarizes or describes the population, known as Parameters . However, it is impossible to study the entire population at once, thus a subset of the population is studied instead, known as the Sample . Attributes that summarize or describe the sample are known as Statistics . Ideally, the sample is representative of the population, which means that findings from the sample can be applied to the population as a whole. This means that the sample statistics can be used estimate population parameters. We distinguish between the two (when they have the same notation) through the Hat accent (^) - Population Parameters are their written without the hat ( \\(x\\) ) while their corresponding sample statistics are written with the hat ( \\(\\hat{x}\\) ).","title":"Overview of Statistics"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Review%20of%20Statistical%20Theory/#common-sample-statistics","text":"The Mean is the average of the population. Population Mean Sample Mean \\(\\mu = \\sum\\limits_{i=i}^n x_i * p(x_i)\\) \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=i}^n x_i\\) The Variance measures the spread of values about the mean. However, the units of Variance are unintuitive, thus we consider the Standard Deviation for more practical purposes, which is the square root of the variance . Population Variance Sample Variance \\(\\sigma^2 = \\sum\\limits_{i=i}^n (x_i - \\mu)^2 * p(x_i)\\) \\(s^2 = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x})^2\\) Covariance is a measure of the linear relationship between two variables: Positive Covariance - Variables move in the same direction Negative Covariance - Variables move in opposite directions Population Covariance Sample Covariance \\(\\sigma_{x, y}\\) \\(\\hat{\\sigma}_{x, y} = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x}) * (y_i - \\bar{y})\\) However, there are two issues with Covariance - the units are unintuitive (similar to variance) and there is no benchmark as to what constitutes a strong/weak relationship. Thus, the Correlation is an adjusted measure of the relationship between -1 and 1 . Population Correlation Sample Correlation \\(\\rho = \\frac{\\sigma_{x,y}}{\\sigma_x * \\sigma_y}\\) \\(r = \\frac{s_{x,y}}{s_x * s_y}\\)","title":"Common Sample Statistics"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Review%20of%20Statistical%20Theory/#sampling-distribution","text":"Whenever a sample is drawn from a population and a statistic is calculated, it is known as a Point Estimate . If another sample were to be drawn, it would most likely result in a different point estimate as the underlying sample is likely to be different. If this process were to be repeated a large number times, the probability distribution of the resulting point estimates is known as the Sampling Distribution of the statistic. The standard deviation of this sampling distribution is known as the Standard Error . There is no rule surrounding the sampling distribution - it depends on the distribution of the population, statistic, sampling method etc. A special case is the Sample Mean . If the population is normally distributed, then it is normally distributed as well. However, regardless of the population distribution, it is approximately normally distributed through the Central Limit Theorem or Law of Large Numbers . \\[ \\bar{x} \\sim N(\\mu, \\frac{\\sigma^2}{n}) \\] Following that, the standard error is given by the following: \\[ \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}} \\] However, the population standard deviation is most likely unknown. Thus, we can calculate an estimate of the standard error by using the sample standard deviation instead: \\[ \\hat{\\sigma_{\\bar{x}}} = \\frac{s}{\\sqrt{n}} \\]","title":"Sampling Distribution"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Review%20of%20Statistical%20Theory/#confidence-interval","text":"Given that there is only one true value for the population parameter and a whole distribution of estimators, it is unlikely that a point estimate will be equal to the population parameter. Thus, instead of a point estimate, a range of estimates is used, known as a Confidence Interval . The interval is made a chosen Confidence Level which represents the proportion of confidence intervals that will contain the true value . In other words, if a large number of confidence intervals constructed in the same manner were to be made, {confidence level}% of them would contain the true value. \\[ CI = \\hat{\\theta}~\\pm~Margin~of~Error \\] The Margin of Error represents the range of values on both sides of the point estimate that the true value could lie. The width of the confidence interval is thus the sum of the two margins. \\[ Margin~of~Error = Critical Value * SE(\\hat{\\theta}) \\]","title":"Confidence Interval"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Review%20of%20Statistical%20Theory/#hypothesis-testing","text":"Hypothesis Testing is a formal method of making inferences about the population parameters based on the sample statistic. It starts with a Hypothesis which is a conjecture about the population parameters: Null Hypothesis - What is currently believed to be true Alternative Hypothesis - What is to be proven A Test Statistic is then calculated, which would quantify the behaviour of population such that it would distinguish the null and alternative hypothesis. Assuming the Null Hypothesis is true , the sampling distribution of the test statistic is determined. From the sampling distribution, the p-value is calculated, which is the probability of observing the calculated sample statistic or more extreme . If the p-value is smaller than a pre-determined level of Statistical Significance ( \\(\\alpha\\) ), then the test is \"successful\" and the null hypothesis is rejected. If not, then the test \"fails\" and the null is not rejected. Note that it rejecting the null hypothesis does NOT mean that the alternative hypothesis is accepted - EG. Rejected then hypothesis of a large paw print being from a bear does not mean that it is from Bigfoot. Thus, the hypotheses are usually constructed such that the two hypothesis are complementary , such that rejecting the null allows acceptance of the alternative, leading to a definitive insight. In layman terms, a hypothesis test is a test of extremeness . Assuming the null hypothesis is true, how extreme (low probability) is the observed sample? If the p-value is sufficiently low, it means that the sample is rare - it could be due to pure chance or that it is actually not rare because the null is not true. We distinguish between the two mathematically through \\(\\alpha\\) . It is the probability of a False Positive - that the result obtained was by pure chance. It is typically set at 5%, which means that 5% of all statistics calculated are expected to be extreme under the null. Thus, if the p-value of the sample is smaller than \\(\\alpha\\) , then it is likely that the observation was not due to chance but instead because the null was false . Alternatively, instead of comparing p-value to \\(alpha\\) , the test-statistic and the corresponding value of \\(\\alpha\\) on the sampling distribution can be used. It is known as the Critical Value , which represents the boundary of Reject Null Do not Reject Null p-value smaller than \\(\\alpha\\) p-value smaller than \\(\\alpha\\) test-statistic larger than critical value test-statistic smaller than critical value","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Review%20of%20Statistical%20Theory/#maximum-likelihood-estimation","text":"If the population distribution is known, there is an alternative method of estimating the parameters apart from calculating the corresponding sample statistics. We model a set of observations as a random sample from an unknown joint probability distribution which is expressed in terms of a set of parameters. The goal of maximum likelihood estimation is to determine the parameters for which the observed data have the highest joint probability. The goal of maximum likelihood estimation is to find the values of the model parameters that maximize the likelihood function over the parameter space","title":"Maximum Likelihood Estimation"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Regression%20Introduction/","text":"Regression Introduction Overview Regression is a statistical model that relates a Dependent Variable to one or more Independent Variables . The dependent variable is regressed on to the independent variable. Different regression models assume different relationships between the Independent and Dependent variable. The goal is to use the model that best captures the relationship between the variables. Independent Variable(s) Dependent Variables Variable used to make predictions Variable being predicted Free to change the value Depends on the value of indepenent variable Deterministic Random Variable Denoted as \\(X\\) Denoted as \\(Y\\) To be precise, for every set of independent variables, the dependent variable has a Conditional Distribution dependent on the given independent variables. For instance, the the dependent variable could take any possible value (non-conditional distribution), but given these set of independent variables, the possible values can be narrowed down to a certain range (conditional distribution). \\[ Y|X \\sim Distribution \\] The regression model calculates the Expected Value of the conditional distribution, \\(E(Y|X)\\) . It is also commonly denoted as just \\(y\\) . The \"accuracy\" of the regression can be gauged by comparing the actual value to the model's predicted value. The difference between an observed value and the corresponding regression value is known as the Error of the model. It represents all other factors affecting the dependent variable that were not captured in the regression. \\[ \\varepsilon_i = y_i - y \\] Regressions are a function of the independent variables and several Regression Parameters ( \\(\\beta\\) ). There are several methods to determine the parameters, but the most common is the Least Squares Method , where the parameters are chosen such that they Minimize the Sum of the Squared Errors . This creates a regression model that best fits the observations - distance between regression model and all observations are minimized. \\[ y = f(X,\\beta) \\] Simple Linear Regression In practice sample model, estimates, residuals sum squared residuals In practice, the prediction of the Sample Model is an estimate of the expected value of the conditional distribution. The difference between an observed value and the sample model's is known as the Residual . It is an estimate of the model error. \\[ \\hat{\\varepsilon} = y_i - \\hat{y} \\] Simple Linear Regression Simple Linear Regression (SLR) assumes a Linear Relationship between a pair of Numeric independent and dependent variable. \\[ y = \\beta_0 + \\beta_1 x \\] fit the regression","title":"**Regression Introduction**"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Regression%20Introduction/#regression-introduction","text":"","title":"Regression Introduction"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Regression%20Introduction/#overview","text":"Regression is a statistical model that relates a Dependent Variable to one or more Independent Variables . The dependent variable is regressed on to the independent variable. Different regression models assume different relationships between the Independent and Dependent variable. The goal is to use the model that best captures the relationship between the variables. Independent Variable(s) Dependent Variables Variable used to make predictions Variable being predicted Free to change the value Depends on the value of indepenent variable Deterministic Random Variable Denoted as \\(X\\) Denoted as \\(Y\\) To be precise, for every set of independent variables, the dependent variable has a Conditional Distribution dependent on the given independent variables. For instance, the the dependent variable could take any possible value (non-conditional distribution), but given these set of independent variables, the possible values can be narrowed down to a certain range (conditional distribution). \\[ Y|X \\sim Distribution \\] The regression model calculates the Expected Value of the conditional distribution, \\(E(Y|X)\\) . It is also commonly denoted as just \\(y\\) . The \"accuracy\" of the regression can be gauged by comparing the actual value to the model's predicted value. The difference between an observed value and the corresponding regression value is known as the Error of the model. It represents all other factors affecting the dependent variable that were not captured in the regression. \\[ \\varepsilon_i = y_i - y \\] Regressions are a function of the independent variables and several Regression Parameters ( \\(\\beta\\) ). There are several methods to determine the parameters, but the most common is the Least Squares Method , where the parameters are chosen such that they Minimize the Sum of the Squared Errors . This creates a regression model that best fits the observations - distance between regression model and all observations are minimized. \\[ y = f(X,\\beta) \\]","title":"Overview"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Regression%20Introduction/#simple-linear-regression","text":"In practice sample model, estimates, residuals sum squared residuals In practice, the prediction of the Sample Model is an estimate of the expected value of the conditional distribution. The difference between an observed value and the sample model's is known as the Residual . It is an estimate of the model error. \\[ \\hat{\\varepsilon} = y_i - \\hat{y} \\]","title":"Simple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Regression%20Introduction/#simple-linear-regression_1","text":"Simple Linear Regression (SLR) assumes a Linear Relationship between a pair of Numeric independent and dependent variable. \\[ y = \\beta_0 + \\beta_1 x \\] fit the regression","title":"Simple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/","text":"Regression Fundamentals Population Regression Model Regression is a statistical model that relates a Dependent Variable to one or more Independent Variables . The dependent variable is regressed on to the independent variable. They are fundamentally a function of the independent variables and several Regression Parameters , \\(\\beta\\) . The functional form of the regression is based on the relationship between the variables. \\[ y = f(X, \\beta) \\] Different regression models assume different relationships between the Independent and Dependent variable. The goal is to use the model that best captures the relationship between the variables. Independent Variable(s) Dependent Variables Variable used to make predictions Variable being predicted Free to change the value Depends on the value of indepenent variable Deterministic Random Variable Denoted as \\(X\\) Denoted as \\(Y\\) To be precise, for every set of independent variables, the dependent variable has a Conditional Distribution dependent on the given independent variables. For instance, the the dependent variable could take any possible value (non-conditional distribution), but given these set of independent variables, the possible values can be narrowed down to a certain range (conditional distribution). \\[ \\displaylines{ Y \\sim Distribution \\\\ Y|X \\sim Conditional~Distribution} \\] The regression model calculates the Expected Value of the conditional distribution, \\(E(Y|X)\\) , for every possible \\(X\\) . It is also commonly denoted as just \\(y\\) . The \"accuracy\" of the regression can be gauged by comparing the actual value to the model's predicted value. The difference between an observed value and the corresponding regression value is known as the Error of the model. It represents all other factors affecting the dependent variable that were not captured in the regression. Note that the sign of the errors are significant - positive implies the actual value lies above the regression model while negative implies it lies below. \\[ \\varepsilon_i = y_i - f(X, \\beta) \\] Sample Regression Model Everything discussed up till this point was regarding the Population Regression Model . In practice, a Sample Regression Model is created from the data that seeks to estimate the population's. Fit the model In practice sample model, estimates, residuals sum squared residuals In practice, the prediction of the Sample Model is an estimate of the expected value of the conditional distribution. The difference between an observed value and the sample model's is known as the Residual . It is an estimate of the model error. \\[ \\hat{\\varepsilon} = y_i - \\hat{y} \\] There are several methods to determine the parameters, but the most common is the Least Squares Method , where the parameters are chosen such that they Minimize the Sum of the Squared Errors . The errors are squared to remove the sign of the errors. The model thus best fits the observations, such that the distance between the regression model and ALL observations are minimized. Simple Linear Regression Simple Linear Regression (SLR) assumes a Linear Relationship between a pair of Numeric independent and dependent variable. \\[ y = \\beta_0 + \\beta_1 x \\] fit the regression","title":"Simple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#regression-fundamentals","text":"","title":"Regression Fundamentals"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#population-regression-model","text":"Regression is a statistical model that relates a Dependent Variable to one or more Independent Variables . The dependent variable is regressed on to the independent variable. They are fundamentally a function of the independent variables and several Regression Parameters , \\(\\beta\\) . The functional form of the regression is based on the relationship between the variables. \\[ y = f(X, \\beta) \\] Different regression models assume different relationships between the Independent and Dependent variable. The goal is to use the model that best captures the relationship between the variables. Independent Variable(s) Dependent Variables Variable used to make predictions Variable being predicted Free to change the value Depends on the value of indepenent variable Deterministic Random Variable Denoted as \\(X\\) Denoted as \\(Y\\) To be precise, for every set of independent variables, the dependent variable has a Conditional Distribution dependent on the given independent variables. For instance, the the dependent variable could take any possible value (non-conditional distribution), but given these set of independent variables, the possible values can be narrowed down to a certain range (conditional distribution). \\[ \\displaylines{ Y \\sim Distribution \\\\ Y|X \\sim Conditional~Distribution} \\] The regression model calculates the Expected Value of the conditional distribution, \\(E(Y|X)\\) , for every possible \\(X\\) . It is also commonly denoted as just \\(y\\) . The \"accuracy\" of the regression can be gauged by comparing the actual value to the model's predicted value. The difference between an observed value and the corresponding regression value is known as the Error of the model. It represents all other factors affecting the dependent variable that were not captured in the regression. Note that the sign of the errors are significant - positive implies the actual value lies above the regression model while negative implies it lies below. \\[ \\varepsilon_i = y_i - f(X, \\beta) \\]","title":"Population Regression Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#sample-regression-model","text":"Everything discussed up till this point was regarding the Population Regression Model . In practice, a Sample Regression Model is created from the data that seeks to estimate the population's. Fit the model In practice sample model, estimates, residuals sum squared residuals In practice, the prediction of the Sample Model is an estimate of the expected value of the conditional distribution. The difference between an observed value and the sample model's is known as the Residual . It is an estimate of the model error. \\[ \\hat{\\varepsilon} = y_i - \\hat{y} \\] There are several methods to determine the parameters, but the most common is the Least Squares Method , where the parameters are chosen such that they Minimize the Sum of the Squared Errors . The errors are squared to remove the sign of the errors. The model thus best fits the observations, such that the distance between the regression model and ALL observations are minimized.","title":"Sample Regression Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#simple-linear-regression","text":"Simple Linear Regression (SLR) assumes a Linear Relationship between a pair of Numeric independent and dependent variable. \\[ y = \\beta_0 + \\beta_1 x \\] fit the regression","title":"Simple Linear Regression"}]}