{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Actuarial Exam Notes Test","title":"**Actuarial Exam Notes**"},{"location":"#actuarial-exam-notes","text":"Test","title":"Actuarial Exam Notes"},{"location":"Placeholder/","text":"Work in progress!","title":"ASA-PA"},{"location":"Placeholder/#work-in-progress","text":"","title":"Work in progress!"},{"location":"2.%20Actuarial%20Mathematics/Overview/","text":"Overview of Actuarial Mathematics These set of exams cover the main concepts needed to evaluate insurance related risks. They are split into two main exams: Fundamentals of Actuarial Mathematics (FAM) Advanced Long Term or Short Term Actuarial Mathematics (ALTAM or ASTAM) This set of notes splits the FAM component into two seperate components for clarity, as the components are not related to one another: FAM-S (Short Term) FAM-L (Long Term) However, the actual FAM exam tests both components together in an MCQ format. It is regarded as one of the tougher examinations due to its wide breadth (due to the unrelated components).","title":"**Overview of Actuarial Mathematics**"},{"location":"2.%20Actuarial%20Mathematics/Overview/#overview-of-actuarial-mathematics","text":"These set of exams cover the main concepts needed to evaluate insurance related risks. They are split into two main exams: Fundamentals of Actuarial Mathematics (FAM) Advanced Long Term or Short Term Actuarial Mathematics (ALTAM or ASTAM) This set of notes splits the FAM component into two seperate components for clarity, as the components are not related to one another: FAM-S (Short Term) FAM-L (Long Term) However, the actual FAM exam tests both components together in an MCQ format. It is regarded as one of the tougher examinations due to its wide breadth (due to the unrelated components).","title":"Overview of Actuarial Mathematics"},{"location":"3.%20Predictive%20Analytics/Exam%20Overview/","text":"Overview of Predictive Analytics Predictive Analytics is the usage of statistical models to analyze historical or current data to make predictions about the future or unknown events. Due to the growing relevance of Predictive Analytics , the SOA has added a significant amount of material on the topic into the credentialling process: Exam Statistics for Risk Modelling (SRM) Exam Predictive Analytics (PA) Exam Advanced Topics for Predictive Analytics (ATPA) Exam ATPA was added midway through 2022 to replace Exam IFM. Credit for IFM also counts towards credit for ATPA, thus this set of notes will NOT be covering ATPA. All of them share the same theme of working with statistical models: Constructing statistical models Intepreting their outputs Evaluating their performance Exam SRM covers the theory about the various types of models, tested in the typical MCQ format . Unlike the other exams where most of the questions are quantitative, most of SRM's questions are qualitative . Exam PA tests the same concepts in an applied manner , providing real data and a business problem to navigate through in a written format . Exam ATPA builds on both of these exams, covering more advanced concepts in a business context.","title":"Overview"},{"location":"3.%20Predictive%20Analytics/Exam%20Overview/#overview-of-predictive-analytics","text":"Predictive Analytics is the usage of statistical models to analyze historical or current data to make predictions about the future or unknown events. Due to the growing relevance of Predictive Analytics , the SOA has added a significant amount of material on the topic into the credentialling process: Exam Statistics for Risk Modelling (SRM) Exam Predictive Analytics (PA) Exam Advanced Topics for Predictive Analytics (ATPA) Exam ATPA was added midway through 2022 to replace Exam IFM. Credit for IFM also counts towards credit for ATPA, thus this set of notes will NOT be covering ATPA. All of them share the same theme of working with statistical models: Constructing statistical models Intepreting their outputs Evaluating their performance Exam SRM covers the theory about the various types of models, tested in the typical MCQ format . Unlike the other exams where most of the questions are quantitative, most of SRM's questions are qualitative . Exam PA tests the same concepts in an applied manner , providing real data and a business problem to navigate through in a written format . Exam ATPA builds on both of these exams, covering more advanced concepts in a business context.","title":"Overview of Predictive Analytics"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Review%20of%20Statistical%20Theory/","text":"Review of Statistical Theory Statistics is a discipline revolving around data. A Population refers to the theoretical set of all possible data of the event of interest. The goal of statistics is to determine certain attributes that summarizes or describes the population, known as Parameters . However, it is impossible to study the entire population at once, thus a subset of the population is studied instead, known as the Sample . Attributes that summarize or describe the sample are known as Statistics . Ideally, the sample is representative of the population, which means that findings from the sample can be applied to the population as a whole. This means that the sample statistics can be used estimate population parameters. We distinguish between the two (when they have the same notation) through the Hat accent (^) - Population Parameters are their written without the hat ( \\(x\\) ) while their corresponding sample statistics are written with the hat ( \\(\\hat{x}\\) ). Common Sample Statistics The Mean is the average of the population. Population Mean Sample Mean \\(\\mu = \\sum\\limits_{i=i}^n x_i * p(x_i)\\) \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=i}^n x_i\\) The Variance measures the spread of values about the mean. However, the units of Variance are unintuitive, thus we consider the Standard Deviation for more practical purposes, which is the square root of the variance . Population Variance Sample Variance \\(\\sigma^2 = \\sum\\limits_{i=i}^n (x_i - \\mu)^2 * p(x_i)\\) \\(s^2 = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x})^2\\) Covariance is a measure of the linear relationship between two variables: Positive Covariance - Variables move in the same direction Negative Covariance - Variables move in opposite directions Population Covariance Sample Covariance \\(\\sigma_{x, y}\\) \\(\\hat{\\sigma}_{x, y} = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x}) * (y_i - \\bar{y})\\) However, there are two issues with Covariance - the units are unintuitive (similar to variance) and there is no benchmark as to what constitutes a strong/weak relationship. Thus, the Correlation is an adjusted measure of the relationship between -1 and 1 . Population Correlation Sample Correlation \\(\\rho = \\frac{\\sigma_{x,y}}{\\sigma_x * \\sigma_y}\\) \\(r = \\frac{s_{x,y}}{s_x * s_y}\\) Sampling Distribution Whenever a sample is drawn from a population and a statistic is calculated, it is known as a Point Estimate . If another sample were to be drawn, it would most likely result in a different point estimate as the underlying sample is likely to be different. If this process were to be repeated a large number times, the probability distribution of the resulting point estimates is known as the Sampling Distribution of the statistic. The standard deviation of this sampling distribution is known as the Standard Error . There is no rule surrounding the sampling distribution - it depends on the distribution of the population, statistic, sampling method etc. A special case is the Sample Mean . If the population is normally distributed, then it is normally distributed as well. However, regardless of the population distribution, it is approximately normally distributed through the Central Limit Theorem or Law of Large Numbers . \\[ \\bar{x} \\sim N(\\mu, \\frac{\\sigma^2}{n}) \\] Following that, the standard error is given by the following: \\[ \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}} \\] However, the population standard deviation is most likely unknown. Thus, we can calculate an estimate of the standard error by using the sample standard deviation instead: \\[ \\hat{\\sigma_{\\bar{x}}} = \\frac{s}{\\sqrt{n}} \\] Confidence Interval Given that there is only one true value for the population parameter and a whole distribution of estimators, it is unlikely that a point estimate will be equal to the population parameter. Thus, instead of a point estimate, a range of estimates is used, known as a Confidence Interval . The interval is made a chosen Confidence Level which represents the proportion of confidence intervals that will contain the true value . In other words, if a large number of confidence intervals constructed in the same manner were to be made, {confidence level}% of them would contain the true value. \\[ CI = \\hat{\\theta}~\\pm~Margin~of~Error \\] The Margin of Error represents the range of values on both sides of the point estimate that the true value could lie. The width of the confidence interval is thus the sum of the two margins. \\[ Margin~of~Error = Z * SE(\\hat{\\theta}) \\] Hypothesis Testing Hypothesis Testing is a formal method of making inferences about the population parameters based on the sample statistic. It starts with a Hypothesis which is a conjecture about the population parameters: Null Hypothesis - What is currently believed to be true Alternative Hypothesis - What is to be proven A Test Statistic is then calculated, which would quantify the behaviour of population such that it would distinguish the null and alternative hypothesis. Assuming the Null Hypothesis is true , the sampling distribution of the test statistic is determined. From the sampling distribution, the p-value is calculated, which is the probability of observing the calculated sample statistic or more extreme . If the p-value is smaller than a pre-determined level of Statistical Significance ( \\(\\alpha\\) ), then the test is \"successful\" and the null hypothesis is rejected. If not, then the test \"fails\" and the null is not rejected. Note that it rejecting the null hypothesis does NOT mean that the alternative hypothesis is accepted - EG. Rejected then hypothesis of a large paw print being from a bear does not mean that it is from Bigfoot. Thus, the hypotheses are usually constructed such that the two hypothesis are complementary , such that rejecting the null allows acceptance of the alternative, leading to a definitive insight. In layman terms, a hypothesis test is a test of extremeness . Assuming the null hypothesis is true, how extreme (low probability) is the observed sample? If the p-value is sufficiently low, it means that the sample is rare - it could be due to pure chance or that it is actually not rare because the null is not true. We distinguish between the two mathematically through \\(\\alpha\\) . It is the probability of a False Positive - that the result obtained was by pure chance. It is typically set at 5%, which means that 5% of all statistics calculated are expected to be extreme under the null. Thus, if the p-value of the sample is smaller than \\(\\alpha\\) , then it is likely that the observation was not due to chance but instead because the null was false . confidence interval alternative intepration Maximum Likelihood Estimation If the population distribution is known, there is an alternative method of estimating the parameters apart from calculating the corresponding sample statistics.","title":"Review of Statistical Theory"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Review%20of%20Statistical%20Theory/#review-of-statistical-theory","text":"Statistics is a discipline revolving around data. A Population refers to the theoretical set of all possible data of the event of interest. The goal of statistics is to determine certain attributes that summarizes or describes the population, known as Parameters . However, it is impossible to study the entire population at once, thus a subset of the population is studied instead, known as the Sample . Attributes that summarize or describe the sample are known as Statistics . Ideally, the sample is representative of the population, which means that findings from the sample can be applied to the population as a whole. This means that the sample statistics can be used estimate population parameters. We distinguish between the two (when they have the same notation) through the Hat accent (^) - Population Parameters are their written without the hat ( \\(x\\) ) while their corresponding sample statistics are written with the hat ( \\(\\hat{x}\\) ).","title":"Review of Statistical Theory"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Review%20of%20Statistical%20Theory/#common-sample-statistics","text":"The Mean is the average of the population. Population Mean Sample Mean \\(\\mu = \\sum\\limits_{i=i}^n x_i * p(x_i)\\) \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=i}^n x_i\\) The Variance measures the spread of values about the mean. However, the units of Variance are unintuitive, thus we consider the Standard Deviation for more practical purposes, which is the square root of the variance . Population Variance Sample Variance \\(\\sigma^2 = \\sum\\limits_{i=i}^n (x_i - \\mu)^2 * p(x_i)\\) \\(s^2 = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x})^2\\) Covariance is a measure of the linear relationship between two variables: Positive Covariance - Variables move in the same direction Negative Covariance - Variables move in opposite directions Population Covariance Sample Covariance \\(\\sigma_{x, y}\\) \\(\\hat{\\sigma}_{x, y} = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x}) * (y_i - \\bar{y})\\) However, there are two issues with Covariance - the units are unintuitive (similar to variance) and there is no benchmark as to what constitutes a strong/weak relationship. Thus, the Correlation is an adjusted measure of the relationship between -1 and 1 . Population Correlation Sample Correlation \\(\\rho = \\frac{\\sigma_{x,y}}{\\sigma_x * \\sigma_y}\\) \\(r = \\frac{s_{x,y}}{s_x * s_y}\\)","title":"Common Sample Statistics"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Review%20of%20Statistical%20Theory/#sampling-distribution","text":"Whenever a sample is drawn from a population and a statistic is calculated, it is known as a Point Estimate . If another sample were to be drawn, it would most likely result in a different point estimate as the underlying sample is likely to be different. If this process were to be repeated a large number times, the probability distribution of the resulting point estimates is known as the Sampling Distribution of the statistic. The standard deviation of this sampling distribution is known as the Standard Error . There is no rule surrounding the sampling distribution - it depends on the distribution of the population, statistic, sampling method etc. A special case is the Sample Mean . If the population is normally distributed, then it is normally distributed as well. However, regardless of the population distribution, it is approximately normally distributed through the Central Limit Theorem or Law of Large Numbers . \\[ \\bar{x} \\sim N(\\mu, \\frac{\\sigma^2}{n}) \\] Following that, the standard error is given by the following: \\[ \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}} \\] However, the population standard deviation is most likely unknown. Thus, we can calculate an estimate of the standard error by using the sample standard deviation instead: \\[ \\hat{\\sigma_{\\bar{x}}} = \\frac{s}{\\sqrt{n}} \\]","title":"Sampling Distribution"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Review%20of%20Statistical%20Theory/#confidence-interval","text":"Given that there is only one true value for the population parameter and a whole distribution of estimators, it is unlikely that a point estimate will be equal to the population parameter. Thus, instead of a point estimate, a range of estimates is used, known as a Confidence Interval . The interval is made a chosen Confidence Level which represents the proportion of confidence intervals that will contain the true value . In other words, if a large number of confidence intervals constructed in the same manner were to be made, {confidence level}% of them would contain the true value. \\[ CI = \\hat{\\theta}~\\pm~Margin~of~Error \\] The Margin of Error represents the range of values on both sides of the point estimate that the true value could lie. The width of the confidence interval is thus the sum of the two margins. \\[ Margin~of~Error = Z * SE(\\hat{\\theta}) \\]","title":"Confidence Interval"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Review%20of%20Statistical%20Theory/#hypothesis-testing","text":"Hypothesis Testing is a formal method of making inferences about the population parameters based on the sample statistic. It starts with a Hypothesis which is a conjecture about the population parameters: Null Hypothesis - What is currently believed to be true Alternative Hypothesis - What is to be proven A Test Statistic is then calculated, which would quantify the behaviour of population such that it would distinguish the null and alternative hypothesis. Assuming the Null Hypothesis is true , the sampling distribution of the test statistic is determined. From the sampling distribution, the p-value is calculated, which is the probability of observing the calculated sample statistic or more extreme . If the p-value is smaller than a pre-determined level of Statistical Significance ( \\(\\alpha\\) ), then the test is \"successful\" and the null hypothesis is rejected. If not, then the test \"fails\" and the null is not rejected. Note that it rejecting the null hypothesis does NOT mean that the alternative hypothesis is accepted - EG. Rejected then hypothesis of a large paw print being from a bear does not mean that it is from Bigfoot. Thus, the hypotheses are usually constructed such that the two hypothesis are complementary , such that rejecting the null allows acceptance of the alternative, leading to a definitive insight. In layman terms, a hypothesis test is a test of extremeness . Assuming the null hypothesis is true, how extreme (low probability) is the observed sample? If the p-value is sufficiently low, it means that the sample is rare - it could be due to pure chance or that it is actually not rare because the null is not true. We distinguish between the two mathematically through \\(\\alpha\\) . It is the probability of a False Positive - that the result obtained was by pure chance. It is typically set at 5%, which means that 5% of all statistics calculated are expected to be extreme under the null. Thus, if the p-value of the sample is smaller than \\(\\alpha\\) , then it is likely that the observation was not due to chance but instead because the null was false . confidence interval alternative intepration","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Review%20of%20Statistical%20Theory/#maximum-likelihood-estimation","text":"If the population distribution is known, there is an alternative method of estimating the parameters apart from calculating the corresponding sample statistics.","title":"Maximum Likelihood Estimation"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/","text":"Simple Linear Regression Every statistical model has two components: Independent Variable(s) Dependent Variables Variable used to make predictions Variable being predicted Free to change the value Depends on the value of indepenent variable Denoted as \\(X\\) Denoted as \\(Y\\) Different statistical models assume different relationships between the Independent and Dependent variable - Linear, Non-Linear, Piecewise Constant etc. The goal is to use the model that best captures the true relationship between the variables. Simple Linear Regression (SLR) assumes a Linear Relationship between a pair of Numeric independent and dependent variable . It is the simplest of all forms of statistical models (hence the name), but it is well suited to demonstrate the fundamental concepts. Regression Fundamentals Every regression is a mathematical function that contains one or more required Parameters that define the relationship between the Independent and Dependent variables. It is the parameters of the model that distinguishes one model (of the same type) from another. These parameters are Estimated through the analysis of the historical/current data. This process is known as Training the model. If the entire population data is used to train the model, then the resulting model is the Population Model , which represents the theoretical relationship between the variables. If a sample is used to train the model, then the resulting model is a Sample Model , which aims to estimate the population one. Model Prediction Since the independent variables can be freely changed, they are Deterministic ; NOT random. Naturally, the dependent variable is a Random Variable that follows some unknown probability distribution . To be precise, for every set of independent variables, the dependent variable has a Conditional Probability Distribution dependent on the those independent variables. For instance, the the dependent variable could take any possible value (non-conditional distribution), but given these set of independent variables, the possible values can be narrowed down to a certain range (conditional distribution). \\[ Y|X \\sim Distribution \\] The corresponding prediction of the model based on those independent variables is the Expected Value of this conditional distribution, \\(E(Y|X)\\) . It is also commonly denoted as just \\(y\\) . Note that we refer to a random variable using Large Letters and values of the variable using Small Letters . We can gauge the \"accuracy\" of the model by comparing the actual value to the model's predicted value. The difference between an observed value and the model's is known as the Error of the model. It represents all other factors affecting the dependent variable that were not captured in the model. \\[ \\varepsilon = y_i - y \\] In practice, the prediction of the Sample Model is an estimate of the expected value of the conditional distribution. The difference between an observed value and the sample model's is known as the Residual . It is an estimate of the model error. \\[ \\hat{\\varepsilon} = y_i - \\hat{y} \\] \\[ y = \\hat{\\beta}_0 + \\hat{\\beta}_1 x \\] Note that Y and X are vectors which represent the collection of all values of the independent variable and their corresponding dependent variable. \\(\\beta_0\\) and \\(\\beta_1\\) are known as the Parameters of the model as they define the relationship between the variables. \\(\\varepsilon\\) is the Error term which is a catch-all for all other factors that the model does not capture.","title":"Simple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#simple-linear-regression","text":"Every statistical model has two components: Independent Variable(s) Dependent Variables Variable used to make predictions Variable being predicted Free to change the value Depends on the value of indepenent variable Denoted as \\(X\\) Denoted as \\(Y\\) Different statistical models assume different relationships between the Independent and Dependent variable - Linear, Non-Linear, Piecewise Constant etc. The goal is to use the model that best captures the true relationship between the variables. Simple Linear Regression (SLR) assumes a Linear Relationship between a pair of Numeric independent and dependent variable . It is the simplest of all forms of statistical models (hence the name), but it is well suited to demonstrate the fundamental concepts.","title":"Simple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#regression-fundamentals","text":"Every regression is a mathematical function that contains one or more required Parameters that define the relationship between the Independent and Dependent variables. It is the parameters of the model that distinguishes one model (of the same type) from another. These parameters are Estimated through the analysis of the historical/current data. This process is known as Training the model. If the entire population data is used to train the model, then the resulting model is the Population Model , which represents the theoretical relationship between the variables. If a sample is used to train the model, then the resulting model is a Sample Model , which aims to estimate the population one.","title":"Regression Fundamentals"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#model-prediction","text":"Since the independent variables can be freely changed, they are Deterministic ; NOT random. Naturally, the dependent variable is a Random Variable that follows some unknown probability distribution . To be precise, for every set of independent variables, the dependent variable has a Conditional Probability Distribution dependent on the those independent variables. For instance, the the dependent variable could take any possible value (non-conditional distribution), but given these set of independent variables, the possible values can be narrowed down to a certain range (conditional distribution). \\[ Y|X \\sim Distribution \\] The corresponding prediction of the model based on those independent variables is the Expected Value of this conditional distribution, \\(E(Y|X)\\) . It is also commonly denoted as just \\(y\\) . Note that we refer to a random variable using Large Letters and values of the variable using Small Letters . We can gauge the \"accuracy\" of the model by comparing the actual value to the model's predicted value. The difference between an observed value and the model's is known as the Error of the model. It represents all other factors affecting the dependent variable that were not captured in the model. \\[ \\varepsilon = y_i - y \\] In practice, the prediction of the Sample Model is an estimate of the expected value of the conditional distribution. The difference between an observed value and the sample model's is known as the Residual . It is an estimate of the model error. \\[ \\hat{\\varepsilon} = y_i - \\hat{y} \\] \\[ y = \\hat{\\beta}_0 + \\hat{\\beta}_1 x \\] Note that Y and X are vectors which represent the collection of all values of the independent variable and their corresponding dependent variable. \\(\\beta_0\\) and \\(\\beta_1\\) are known as the Parameters of the model as they define the relationship between the variables. \\(\\varepsilon\\) is the Error term which is a catch-all for all other factors that the model does not capture.","title":"Model Prediction"}]}