{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Actuarial Exam Notes Test","title":"**Actuarial Exam Notes**"},{"location":"#actuarial-exam-notes","text":"Test","title":"Actuarial Exam Notes"},{"location":"Placeholder/","text":"Work in progress!","title":"ASA-PA"},{"location":"Placeholder/#work-in-progress","text":"","title":"Work in progress!"},{"location":"2.%20Actuarial%20Mathematics/Overview/","text":"Overview of Actuarial Mathematics These set of exams cover the main concepts needed to evaluate insurance related risks. They are split into two main exams: Fundamentals of Actuarial Mathematics (FAM) Advanced Long Term or Short Term Actuarial Mathematics (ALTAM or ASTAM) This set of notes splits the FAM component into two seperate components for clarity, as the components are not related to one another: FAM-S (Short Term) FAM-L (Long Term) However, the actual FAM exam tests both components together in an MCQ format. It is regarded as one of the tougher examinations due to its wide breadth (due to the unrelated components).","title":"**Overview of Actuarial Mathematics**"},{"location":"2.%20Actuarial%20Mathematics/Overview/#overview-of-actuarial-mathematics","text":"These set of exams cover the main concepts needed to evaluate insurance related risks. They are split into two main exams: Fundamentals of Actuarial Mathematics (FAM) Advanced Long Term or Short Term Actuarial Mathematics (ALTAM or ASTAM) This set of notes splits the FAM component into two seperate components for clarity, as the components are not related to one another: FAM-S (Short Term) FAM-L (Long Term) However, the actual FAM exam tests both components together in an MCQ format. It is regarded as one of the tougher examinations due to its wide breadth (due to the unrelated components).","title":"Overview of Actuarial Mathematics"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/","text":"Survival Models Survival Models are probability distributions that measure the time to failure of an entity, or phrased another way, the future lifetime of an entity. In an Actuarial context, it measures the time to death of a person. Since survival models measure time , they are denoted by the continuous random variable \\(T_i\\) , where the subscript represents the age of the person being studied. Newborn Lifetime The base survival model measures the future lifetime of a person aged 0 (newborn), denoted by \\(T_0\\) . The CDF thus represents the probability that the newborn dies before age \\(t\\) : \\[ F_0(t) = P(T_0 \\le t) \\] The complement of the CDF is the probability that the newborn survives past a certain age \\(t\\) . From a different perspective, it can also be seen as the newborn dying after age \\(t\\) . This is known as the Survival Function . \\[ S_0(t) = P(T_0 \\ge t) = 1 - P(T_0 \\le t) = 1 - F_0(t) \\] Continuous Lifetime Similarly, we can generalize the above for a person aged \\(x\\) , denoted by \\(T_x\\) . However, the intepretation of the functions have a slightly different meaning. The CDF instead represents the probability that the person dies within a certain number of years \\(t\\) . Given how often this expression is used, it can be simplified to the notation \\(q\\) . \\[ F_x(t) = P(T_x \\le t) = {}_{t}q_{x} \\] It can also be written in terms of the newborn distribution , whereby the newborn dies by age \\(x+t\\) , given that it survives till age \\(x\\) : \\[ P(T_0 \\le x + t | T_0 > x) = \\frac{P(x < T_0 < x +t)}{P(T_0 > x)} = \\frac{P(T_0 < x +t)-P(T_0 < x)}{P(T_0 > x)} = \\frac{F_0(x+t) - F_0(x)}{S_0(x)} \\] Conversely, the survival function is the probability that the person survives another certain number of years \\(t\\) or dies after a certain number of years \\(t\\) . It can be simplified to the notation \\(p\\) . \\[ S_x(t) = P(T_x \\ge t) = {}_{t}p_{x} \\] It can also be written in terms of the newborn distribution, whereby the newborn survives till age \\(x+t\\) , given that it survives till age \\(x\\) . \\[ P(T_x \\ge t) = P(T_0 ge x + t | T_0 > x) = \\frac{P(T_0 > x + t)}{P(T_0 > x)} = \\frac{S_0(x+t)}{S_0(x)} \\] Note that since the surviving till age \\(x\\) is a subset of surviving till age \\(x+t\\) , the conditional probability simply uses \\(x+t\\) in the numerator. Recall that the two are complements of one another - the person will inevitably die. \\[ \\begin{aligned} F_x(t) + S_x(t) &= 1 \\\\ {}_{t}q_{x} + {}_{t}p_{x} &= 1 \\end{aligned} \\] Deferred Death The death of the person can also be \" deferred \", whereby the person survives \\(s\\) years then dies within the following \\(t\\) years . Following this definition: \\[ {}_{s|t}q_{x} = {}_{s}p_{x} * {}_{t}q_{x+s} \\] From another perspective, it calculates the probability of the person dying within a specified range of time in the future: \\[ {}_{s|t}q_{x} = {}_{s+t}q_{x} - {}_{s}q_{x} \\] Force of Mortality (TBC) Loosely speaking, the rate of mortality (different from mortality rate) is the probability of death during a period per unit time . \\[ Rate~Of~Mortality = \\frac{P(T_x \\le h)}{h} = \\frac{{}_{h}q_{x}}{h} \\] If the period of time is gets smaller and smaller , then the result is the instantaneous change in probability, known as the Force of Mortality . \\[ \\mu_x = \\lim_{h \\to 0} \\frac{{}_{h}q_{x}}{h} \\] For very small periods of time , the force of mortality can be used to approximate the change in probability of death: \\[ {}_{h}q_{x} = {}_{0}q_{x} + h * \\mu_x = h * \\mu_x \\] Probability Density Function The PDF of \\(T_x\\) is the differential of the CDF: \\[ \\begin{aligned} f_x(t) &= \\frac{d}{dt} F_x(t) \\\\ &= \\frac{d}{dt} P(T_x \\le t) \\\\ &= \\lim_{h \\to 0} \\frac{P(T_x \\le t+h) - P(T_x \\le t)}{h} \\\\ &= \\lim_{h \\to 0} \\frac{P(x + t < T_0 < x + t +h)}{h * P(T_0 > t)} \\end{aligned} \\] The denominator of the limit can be adjusted to make it into a smoother conditional probability: \\[ \\begin{aligned} f_x(t) &= \\lim_{h \\to 0} \\frac{P(x + t < T_0 < x + t +h)}{h * P(T_0 > t)} \\\\ &= \\frac{P(T_0 > t)}{P(T_0 > x + t)} \\lim_{h \\to 0} \\frac{P(x + t < T_0 < x + t +h)}{h * P(T_0 > x + t)} \\\\ &= \\frac{P(T_0 > t)}{P(T_0 > x + t)} \\lim_{h \\to 0} \\frac{P(x + t < T_0 < x + t +h | T_0 > x + t)}{h} \\\\ &= {}_{t}p_{x} * \\mu_{x+t} \\end{aligned} \\] CDF and Survival Function The CDF can also be expressed as a function of the force of mortality, making use of the PDF derived above: \\[ \\begin{aligned} F_x(t) &= \\int_{0}^{t} f_x(t) \\\\ F_x(t) &= \\int_{0}^{t} {}_{t}p_{x} * \\mu_{x+t} \\\\ {}_{t}q_{x} &= \\int_{0}^{t} {}_{t}p_{x} * \\mu_{x+t} \\end{aligned} \\] Similarly for the Survival Function , \\[ \\begin{aligned} f_x(t) &= {}_{t}p_{x} * \\mu_{x + t} \\\\ \\mu_{x + t} &= \\frac{f_x(t)}{S_x(t)} \\end{aligned} \\] Consider another expression for the PDF: \\[ \\begin{aligned} f_x(t) &= \\frac{d}{dt} F_x(t) \\\\ f_x(t) &= \\frac{d}{dt} (1 - S_x(t)) \\\\ f_x(t) &= -S'_x(t) \\end{aligned} \\] Combining both the above, \\[ \\begin{aligned} \\mu_{x + t} &= \\frac{-S'_x(t)}{S_x(t)} \\\\ \\mu_{x + t} &= - \\frac{d}{dt} (\\ln S_x(t)) \\\\ \\ln S_x(t) &= - \\int_{0}^{t} \\mu_{x + t} \\\\ S_x(t) &= e ^ {- \\int_{0}^{t} \\mu_{x + t}} \\\\ {}_{t}p_{x} &= e ^ {- \\int_{0}^{t} \\mu_{x + t}} \\end{aligned} \\] Expectation and Variance The Expectation of the Future Lifetime is known as the Life Expectancy of the individual. \\[ \\begin{aligned} \\mathring{e}_x &= E(T_x) \\\\ &= \\int_{0}^{t} t * {}_{t}p_{x} * \\mu_{x+t} \\\\ &= \\int_{0}^{t} t * -S'_x(t) \\\\ &= [-t * {}_{t}p_{x}]^{\\infty}_0 + \\int_{0}^{t} {}_{t}p_{x} \\\\ &= \\int_{0}^{t} {}_{t}p_{x} \\end{aligned} \\] Note that \\([-t * {}_{t}p_{x}]^{\\infty}_0\\) is always 0 as the probability of living forever is 0. Unfortunately, there is no simplification to be made for the Variance of the future lifetime: \\[ \\begin{aligned} Var(T_x) &= E(T^2_x) - [E(T_x)]^2 \\\\ &= \\int_{0}^{t} t^2 * {}_{t}p_{x} * \\mu_{x+t} - [\\mathring{e}_x]^2 \\end{aligned} \\] Discrete Lifetime K_x Curtate Life Tables Alpha starting age, terminal age omega l is the expected number of people who survive till age x Radix number of people alive at lalpha, radix number of people alive at l omega must be 0 (entire population assumed to die) dx is the number of people who die within a period x and x +1: lx - lx+t sum of all dx should bring back to original number, or since people dying, lx+t > lx tqx = lx - lx+t /lx = dx/lx tpx = lx+t/lx = 1 - dx/lx force of mortality","title":"Survival Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#survival-models","text":"Survival Models are probability distributions that measure the time to failure of an entity, or phrased another way, the future lifetime of an entity. In an Actuarial context, it measures the time to death of a person. Since survival models measure time , they are denoted by the continuous random variable \\(T_i\\) , where the subscript represents the age of the person being studied.","title":"Survival Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#newborn-lifetime","text":"The base survival model measures the future lifetime of a person aged 0 (newborn), denoted by \\(T_0\\) . The CDF thus represents the probability that the newborn dies before age \\(t\\) : \\[ F_0(t) = P(T_0 \\le t) \\] The complement of the CDF is the probability that the newborn survives past a certain age \\(t\\) . From a different perspective, it can also be seen as the newborn dying after age \\(t\\) . This is known as the Survival Function . \\[ S_0(t) = P(T_0 \\ge t) = 1 - P(T_0 \\le t) = 1 - F_0(t) \\]","title":"Newborn Lifetime"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#continuous-lifetime","text":"Similarly, we can generalize the above for a person aged \\(x\\) , denoted by \\(T_x\\) . However, the intepretation of the functions have a slightly different meaning. The CDF instead represents the probability that the person dies within a certain number of years \\(t\\) . Given how often this expression is used, it can be simplified to the notation \\(q\\) . \\[ F_x(t) = P(T_x \\le t) = {}_{t}q_{x} \\] It can also be written in terms of the newborn distribution , whereby the newborn dies by age \\(x+t\\) , given that it survives till age \\(x\\) : \\[ P(T_0 \\le x + t | T_0 > x) = \\frac{P(x < T_0 < x +t)}{P(T_0 > x)} = \\frac{P(T_0 < x +t)-P(T_0 < x)}{P(T_0 > x)} = \\frac{F_0(x+t) - F_0(x)}{S_0(x)} \\] Conversely, the survival function is the probability that the person survives another certain number of years \\(t\\) or dies after a certain number of years \\(t\\) . It can be simplified to the notation \\(p\\) . \\[ S_x(t) = P(T_x \\ge t) = {}_{t}p_{x} \\] It can also be written in terms of the newborn distribution, whereby the newborn survives till age \\(x+t\\) , given that it survives till age \\(x\\) . \\[ P(T_x \\ge t) = P(T_0 ge x + t | T_0 > x) = \\frac{P(T_0 > x + t)}{P(T_0 > x)} = \\frac{S_0(x+t)}{S_0(x)} \\] Note that since the surviving till age \\(x\\) is a subset of surviving till age \\(x+t\\) , the conditional probability simply uses \\(x+t\\) in the numerator. Recall that the two are complements of one another - the person will inevitably die. \\[ \\begin{aligned} F_x(t) + S_x(t) &= 1 \\\\ {}_{t}q_{x} + {}_{t}p_{x} &= 1 \\end{aligned} \\]","title":"Continuous Lifetime"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#deferred-death","text":"The death of the person can also be \" deferred \", whereby the person survives \\(s\\) years then dies within the following \\(t\\) years . Following this definition: \\[ {}_{s|t}q_{x} = {}_{s}p_{x} * {}_{t}q_{x+s} \\] From another perspective, it calculates the probability of the person dying within a specified range of time in the future: \\[ {}_{s|t}q_{x} = {}_{s+t}q_{x} - {}_{s}q_{x} \\]","title":"Deferred Death"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#force-of-mortality-tbc","text":"Loosely speaking, the rate of mortality (different from mortality rate) is the probability of death during a period per unit time . \\[ Rate~Of~Mortality = \\frac{P(T_x \\le h)}{h} = \\frac{{}_{h}q_{x}}{h} \\] If the period of time is gets smaller and smaller , then the result is the instantaneous change in probability, known as the Force of Mortality . \\[ \\mu_x = \\lim_{h \\to 0} \\frac{{}_{h}q_{x}}{h} \\] For very small periods of time , the force of mortality can be used to approximate the change in probability of death: \\[ {}_{h}q_{x} = {}_{0}q_{x} + h * \\mu_x = h * \\mu_x \\]","title":"Force of Mortality (TBC)"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#probability-density-function","text":"The PDF of \\(T_x\\) is the differential of the CDF: \\[ \\begin{aligned} f_x(t) &= \\frac{d}{dt} F_x(t) \\\\ &= \\frac{d}{dt} P(T_x \\le t) \\\\ &= \\lim_{h \\to 0} \\frac{P(T_x \\le t+h) - P(T_x \\le t)}{h} \\\\ &= \\lim_{h \\to 0} \\frac{P(x + t < T_0 < x + t +h)}{h * P(T_0 > t)} \\end{aligned} \\] The denominator of the limit can be adjusted to make it into a smoother conditional probability: \\[ \\begin{aligned} f_x(t) &= \\lim_{h \\to 0} \\frac{P(x + t < T_0 < x + t +h)}{h * P(T_0 > t)} \\\\ &= \\frac{P(T_0 > t)}{P(T_0 > x + t)} \\lim_{h \\to 0} \\frac{P(x + t < T_0 < x + t +h)}{h * P(T_0 > x + t)} \\\\ &= \\frac{P(T_0 > t)}{P(T_0 > x + t)} \\lim_{h \\to 0} \\frac{P(x + t < T_0 < x + t +h | T_0 > x + t)}{h} \\\\ &= {}_{t}p_{x} * \\mu_{x+t} \\end{aligned} \\]","title":"Probability Density Function"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#cdf-and-survival-function","text":"The CDF can also be expressed as a function of the force of mortality, making use of the PDF derived above: \\[ \\begin{aligned} F_x(t) &= \\int_{0}^{t} f_x(t) \\\\ F_x(t) &= \\int_{0}^{t} {}_{t}p_{x} * \\mu_{x+t} \\\\ {}_{t}q_{x} &= \\int_{0}^{t} {}_{t}p_{x} * \\mu_{x+t} \\end{aligned} \\] Similarly for the Survival Function , \\[ \\begin{aligned} f_x(t) &= {}_{t}p_{x} * \\mu_{x + t} \\\\ \\mu_{x + t} &= \\frac{f_x(t)}{S_x(t)} \\end{aligned} \\] Consider another expression for the PDF: \\[ \\begin{aligned} f_x(t) &= \\frac{d}{dt} F_x(t) \\\\ f_x(t) &= \\frac{d}{dt} (1 - S_x(t)) \\\\ f_x(t) &= -S'_x(t) \\end{aligned} \\] Combining both the above, \\[ \\begin{aligned} \\mu_{x + t} &= \\frac{-S'_x(t)}{S_x(t)} \\\\ \\mu_{x + t} &= - \\frac{d}{dt} (\\ln S_x(t)) \\\\ \\ln S_x(t) &= - \\int_{0}^{t} \\mu_{x + t} \\\\ S_x(t) &= e ^ {- \\int_{0}^{t} \\mu_{x + t}} \\\\ {}_{t}p_{x} &= e ^ {- \\int_{0}^{t} \\mu_{x + t}} \\end{aligned} \\]","title":"CDF and Survival Function"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#expectation-and-variance","text":"The Expectation of the Future Lifetime is known as the Life Expectancy of the individual. \\[ \\begin{aligned} \\mathring{e}_x &= E(T_x) \\\\ &= \\int_{0}^{t} t * {}_{t}p_{x} * \\mu_{x+t} \\\\ &= \\int_{0}^{t} t * -S'_x(t) \\\\ &= [-t * {}_{t}p_{x}]^{\\infty}_0 + \\int_{0}^{t} {}_{t}p_{x} \\\\ &= \\int_{0}^{t} {}_{t}p_{x} \\end{aligned} \\] Note that \\([-t * {}_{t}p_{x}]^{\\infty}_0\\) is always 0 as the probability of living forever is 0. Unfortunately, there is no simplification to be made for the Variance of the future lifetime: \\[ \\begin{aligned} Var(T_x) &= E(T^2_x) - [E(T_x)]^2 \\\\ &= \\int_{0}^{t} t^2 * {}_{t}p_{x} * \\mu_{x+t} - [\\mathring{e}_x]^2 \\end{aligned} \\]","title":"Expectation and Variance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#discrete-lifetime","text":"K_x Curtate","title":"Discrete Lifetime"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#life-tables","text":"Alpha starting age, terminal age omega l is the expected number of people who survive till age x Radix number of people alive at lalpha, radix number of people alive at l omega must be 0 (entire population assumed to die) dx is the number of people who die within a period x and x +1: lx - lx+t sum of all dx should bring back to original number, or since people dying, lx+t > lx tqx = lx - lx+t /lx = dx/lx tpx = lx+t/lx = 1 - dx/lx force of mortality","title":"Life Tables"},{"location":"3.%20Predictive%20Analytics/Exam%20Overview/","text":"Overview of Predictive Analytics Predictive Analytics is the usage of statistical models to analyze historical or current data to make predictions about the future or unknown events. Due to the growing relevance of Predictive Analytics , the SOA has added a significant amount of material on the topic into the credentialling process: Exam Statistics for Risk Modelling (SRM) Exam Predictive Analytics (PA) Exam Advanced Topics for Predictive Analytics (ATPA) Exam ATPA was added midway through 2022 to replace Exam IFM. Credit for IFM also counts towards credit for ATPA, thus this set of notes will NOT be covering ATPA. All of them share the same theme of working with statistical models: Constructing statistical models Intepreting their outputs Evaluating their performance Exam SRM covers the theory about the various types of models, tested in the typical MCQ format . Unlike the other exams where most of the questions are quantitative, most of SRM's questions are qualitative . Exam PA tests the same concepts in an applied manner , providing real data and a business problem to navigate through in a written format . Exam ATPA builds on both of these exams, covering more advanced concepts in a business context.","title":"Overview"},{"location":"3.%20Predictive%20Analytics/Exam%20Overview/#overview-of-predictive-analytics","text":"Predictive Analytics is the usage of statistical models to analyze historical or current data to make predictions about the future or unknown events. Due to the growing relevance of Predictive Analytics , the SOA has added a significant amount of material on the topic into the credentialling process: Exam Statistics for Risk Modelling (SRM) Exam Predictive Analytics (PA) Exam Advanced Topics for Predictive Analytics (ATPA) Exam ATPA was added midway through 2022 to replace Exam IFM. Credit for IFM also counts towards credit for ATPA, thus this set of notes will NOT be covering ATPA. All of them share the same theme of working with statistical models: Constructing statistical models Intepreting their outputs Evaluating their performance Exam SRM covers the theory about the various types of models, tested in the typical MCQ format . Unlike the other exams where most of the questions are quantitative, most of SRM's questions are qualitative . Exam PA tests the same concepts in an applied manner , providing real data and a business problem to navigate through in a written format . Exam ATPA builds on both of these exams, covering more advanced concepts in a business context.","title":"Overview of Predictive Analytics"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Review%20of%20Statistical%20Theory/","text":"Review of Statistical Theory This is meant to be a quick review of the basic statistical concepts of VEE: Mathematical Statistics and other undergrad statitics courses that will be relevant for this exam. Overview of Statistics Statistics is a discipline revolving around data. A Population refers to the theoretical set of all possible data of the event of interest. The goal of statistics is to determine certain attributes that summarizes or describes the population, known as Parameters . However, it is impossible to study the entire population at once, thus a subset of the population is studied instead, known as the Sample . Attributes that summarize or describe the sample are known as Statistics . Ideally, the sample is representative of the population, which means that findings from the sample can be applied to the population as a whole. This means that the sample statistics can be used estimate population parameters. We distinguish between the two (when they have the same notation) through the Hat accent (^) - Population Parameters are their written without the hat ( \\(x\\) ) while their corresponding sample statistics are written with the hat ( \\(\\hat{x}\\) ). Common Sample Statistics The Mean is the average of the population. Population Mean Sample Mean \\(\\mu = \\sum\\limits_{i=i}^n x_i * p(x_i)\\) \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=i}^n x_i\\) The Variance measures the spread of values about the mean. However, the units of Variance are unintuitive, thus we consider the Standard Deviation for more practical purposes, which is the square root of the variance . Population Variance Sample Variance \\(\\sigma^2 = \\sum\\limits_{i=i}^n (x_i - \\mu)^2 * p(x_i)\\) \\(s^2 = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x})^2\\) Covariance is a measure of the linear relationship between two variables: Positive Covariance - Variables move in the same direction Negative Covariance - Variables move in opposite directions Population Covariance Sample Covariance \\(\\sigma_{x, y} = \\mu_{xy} - \\mu_x \\mu_y\\) \\(\\hat{\\sigma}_{x, y} = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x}) * (y_i - \\bar{y})\\) However, there are two issues with Covariance - the units are unintuitive (similar to variance) and there is no benchmark as to what constitutes a strong/weak relationship. Thus, the Correlation is an adjusted measure of the relationship between -1 and 1 . Population Correlation Sample Correlation \\(\\rho = \\frac{\\sigma_{x,y}}{\\sigma_x * \\sigma_y}\\) \\(r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x} \\sum(y_i - \\bar{y})}\\) Properties of Estimators Biased Consistent Efficient Degrees of freedom Sampling Distribution Whenever a sample is drawn from a population and a statistic is calculated, it is known as a Point Estimate . If another sample were to be drawn, it would most likely result in a different point estimate as the underlying sample is likely to be different. If this process were to be repeated a large number times, the probability distribution of the resulting point estimates is known as the Sampling Distribution of the statistic. There is no rule surrounding the sampling distribution - it depends on the distribution of the population, statistic being measured, sampling method etc. The standard deviation of this sampling distribution is known as the Standard Error of the statistic: \\[ \\sigma_{\\theta} = \\sqrt {\\sigma^2_{\\theta}} \\] A special case is the Sample Mean . If the population is normally distributed, then it is normally distributed as well. Regardless of the population distribution, it is also approximately normally distributed through the Central Limit Theorem or Law of Large Numbers . \\[ \\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\] Following that, we can compute the standard error: \\[ \\sigma_{\\bar{x}} = \\sqrt \\frac{\\sigma^2_{\\bar{x}}}{n} = \\frac{\\sigma}{\\sqrt{n}} \\] However, the population variance is usually unknown. Thus, it can be approximated using the Sample Variance, which is an unbiased estimator for it. The result is known as an Estimate for the Standard Error : \\[ \\hat{\\sigma}_{\\bar{x}} = \\sqrt \\frac{s^2}{n} = \\frac{s}{\\sqrt{n}} \\] Note that only the sample variance is an unbiased estimator for the population variance. Although it may look like it, the sample SD is NOT an unbiased estimator for the population SD. Confidence Interval Given that there is only one true value for the population parameter and a whole distribution of estimators, it is unlikely that a point estimate will be equal to the population parameter. Thus, instead of a point estimate, a range of estimates is used, known as a Confidence Interval . The interval is made a chosen Confidence Level which represents the proportion of confidence intervals that will contain the true value . In other words, if a large number of confidence intervals constructed in the same manner were to be made, \\((1-\\alpha)%\\) of them would contain the true value. \\[ P(- \\text{Margin of Error} < \\theta < \\text{Margin of Error}) = 1 - \\alpha \\] The Margin of Error represents the range of values on either side of the point estimate that the true value could lie. For instance, for a 95% confidence interval, the confidence lies within the 0.025 and 0.975 percentile of the sampling distribution. The margin of error can be calculated by finding the corresponding values of the sampling distribution at these percentiles. Consider the 95% confidence interval for the Sample Mean , which is normally distributed. For convenience, it is usually normalized such that it will become a Standard Normal Distribution : \\[ \\begin{aligned} P(-1.96 < \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt n}} < 1.96) &= 0.95 \\\\ P(\\bar{x} - 1.96 \\frac{\\sigma}{\\sqrt n} < \\mu < \\bar{x} + 1.96 \\frac{\\sigma}{\\sqrt n}) &= 0.95 \\\\ \\end{aligned} \\] \\[ \\therefore \\text{Margin of Error} = \\bar{x} + Z_{\\frac{\\alpha}{2}} * \\frac{\\sigma}{\\sqrt n} \\] Hypothesis Testing Hypothesis Testing is a formal method of making inferences about the population parameters based on the sample statistic. It starts with a Hypothesis which is a conjecture about the population parameters: Null Hypothesis - What is currently believed to be true Alternative Hypothesis - What is to be proven A Test Statistic is then calculated, which would quantify the behaviour of population such that it would distinguish the null and alternative hypothesis. Assuming the Null Hypothesis is true , the sampling distribution of the test statistic is determined. From the sampling distribution, the p-value is calculated, which is the probability of observing the calculated sample statistic or more extreme . If the p-value is smaller than a pre-determined level of Statistical Significance ( \\(\\alpha\\) ), then the test is \"successful\" and the null hypothesis is rejected. If not, then the test \"fails\" and the null is not rejected. Note that it rejecting the null hypothesis does NOT mean that the alternative hypothesis is accepted - EG. Rejected then hypothesis of a large paw print being from a bear does not mean that it is from Bigfoot. Thus, the hypotheses are usually constructed such that the two hypothesis are complementary , such that rejecting the null allows acceptance of the alternative, leading to a definitive insight. In layman terms, a hypothesis test is a test of extremeness . Assuming the null hypothesis is true, how extreme (low probability) is the observed sample? If the p-value is sufficiently low, it means that the sample is rare - it could be due to pure chance or that it is actually not rare because the null is not true. We distinguish between the two mathematically through \\(\\alpha\\) . It is the probability of a False Positive - that the result obtained was by pure chance. It is typically set at 5%, which means that 5% of all statistics calculated are expected to be extreme under the null. Thus, if the p-value of the sample is smaller than \\(\\alpha\\) , then it is likely that the observation was not due to chance but instead because the null was false . Alternatively, instead of comparing p-value to \\(alpha\\) , the test-statistic and the corresponding value of \\(\\alpha\\) on the sampling distribution can be used. It is known as the Critical Value , which represents the boundary of Reject Null Do not Reject Null p-value smaller than \\(\\alpha\\) p-value smaller than \\(\\alpha\\) test-statistic larger than critical value test-statistic smaller than critical value Test Statistic Let the random variable \\(T\\) denote the test statistics. There are many different kinds of test statistics depending on the distribution and what is being investigated. The most simple test statistic involve the Sample Mean , which is normally distributed: \\[ T = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] If the population variance is known , then the test statistic has a Standard Normal Distribution and thus the test-statistic is known as a Z-Statistic . \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\sqrt n \\\\ &= Z * \\sqrt n \\\\ \\therefore T &\\sim N(0,1) \\end{aligned} \\] If the population variance is unknown, then it will be approximated by the Sample Variance . Through algebraic manipulation, the test statistic can still be expressed in the form of a Z variable, but with an additional term: \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}} \\\\ &= \\frac{\\bar{x} - \\mu}{s} * \\sqrt n \\\\ &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\frac{\\sigma}{s} * \\sqrt n \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{s^2}{\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{(n-1)s^2}{(n-1)\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt {\\frac{(n-1)s^2}{\\sigma^2} * \\frac{1}{n-1}} \\\\ \\end{aligned} \\] The additional term can be shown to have a Chi-Squared Distribution of \\(n-1\\) degrees of freedom, which by definition is the sum of \\(n-1\\) independent standard normal variables: \\[ \\begin{aligned} \\chi^2_n & \\sim Z^2 \\\\ & \\sim \\sum \\left(\\frac {\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ & \\sim \\sum \\left(\\frac {(x_i - \\bar{x}) + (\\bar{x} - \\mu)}{\\sigma}\\right)^2 \\\\ & \\sim \\sum \\left(\\frac{x_i - \\bar{x}}{\\sigma}\\right)^2 + \\sum \\left(\\frac{\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ & \\sim \\frac{1}{\\sigma^2} \\sum (x_i - \\bar{x})^2 + \\sum Z^2 + 0 \\\\ & \\sim \\frac{(n-1)}{\\sigma^2} \\frac{\\sum (x_i - \\bar{x})^2}{n-1} + \\chi^2_1 \\\\ & \\sim \\frac{(n-1)s^2}{\\sigma^2} + \\chi^2_1 \\\\ \\end{aligned} \\] \\[\\therefore \\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\] Thus, the test statistic is the ratio of a Standard Normal Variable to a Chi Squared Variable (divided by its degrees of freedom). By definition, this test statistic has a t-distribution with the same degrees of freedom and is known as the t-statistic : \\[ \\begin{align*} T &= \\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\\\ &= t_{n-1} * \\sqrt{n} \\\\ \\end{align*} \\] \\[\\therefore T \\sim t_{n-1}\\] Despite the slightly convoluted proof, the t-distribution is simply a standard normal distribution with heavier tails . This means that extreme values are slightly more likely, which is meant to account for the increase in variability due to the use of the sample variance rather than the population variance. Maximum Likelihood Estimation If the population distribution is known, there is an alternative method of estimating the parameters apart from calculating the corresponding sample statistics. We model a set of observations as a random sample from an unknown joint probability distribution which is expressed in terms of a set of parameters. The goal of maximum likelihood estimation is to determine the parameters for which the observed data have the highest joint probability. The goal of maximum likelihood estimation is to find the values of the model parameters that maximize the likelihood function over the parameter space","title":"Review of Statistical Theory"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Review%20of%20Statistical%20Theory/#review-of-statistical-theory","text":"This is meant to be a quick review of the basic statistical concepts of VEE: Mathematical Statistics and other undergrad statitics courses that will be relevant for this exam.","title":"Review of Statistical Theory"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Review%20of%20Statistical%20Theory/#overview-of-statistics","text":"Statistics is a discipline revolving around data. A Population refers to the theoretical set of all possible data of the event of interest. The goal of statistics is to determine certain attributes that summarizes or describes the population, known as Parameters . However, it is impossible to study the entire population at once, thus a subset of the population is studied instead, known as the Sample . Attributes that summarize or describe the sample are known as Statistics . Ideally, the sample is representative of the population, which means that findings from the sample can be applied to the population as a whole. This means that the sample statistics can be used estimate population parameters. We distinguish between the two (when they have the same notation) through the Hat accent (^) - Population Parameters are their written without the hat ( \\(x\\) ) while their corresponding sample statistics are written with the hat ( \\(\\hat{x}\\) ).","title":"Overview of Statistics"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Review%20of%20Statistical%20Theory/#common-sample-statistics","text":"The Mean is the average of the population. Population Mean Sample Mean \\(\\mu = \\sum\\limits_{i=i}^n x_i * p(x_i)\\) \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=i}^n x_i\\) The Variance measures the spread of values about the mean. However, the units of Variance are unintuitive, thus we consider the Standard Deviation for more practical purposes, which is the square root of the variance . Population Variance Sample Variance \\(\\sigma^2 = \\sum\\limits_{i=i}^n (x_i - \\mu)^2 * p(x_i)\\) \\(s^2 = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x})^2\\) Covariance is a measure of the linear relationship between two variables: Positive Covariance - Variables move in the same direction Negative Covariance - Variables move in opposite directions Population Covariance Sample Covariance \\(\\sigma_{x, y} = \\mu_{xy} - \\mu_x \\mu_y\\) \\(\\hat{\\sigma}_{x, y} = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x}) * (y_i - \\bar{y})\\) However, there are two issues with Covariance - the units are unintuitive (similar to variance) and there is no benchmark as to what constitutes a strong/weak relationship. Thus, the Correlation is an adjusted measure of the relationship between -1 and 1 . Population Correlation Sample Correlation \\(\\rho = \\frac{\\sigma_{x,y}}{\\sigma_x * \\sigma_y}\\) \\(r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x} \\sum(y_i - \\bar{y})}\\) Properties of Estimators Biased Consistent Efficient Degrees of freedom","title":"Common Sample Statistics"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Review%20of%20Statistical%20Theory/#sampling-distribution","text":"Whenever a sample is drawn from a population and a statistic is calculated, it is known as a Point Estimate . If another sample were to be drawn, it would most likely result in a different point estimate as the underlying sample is likely to be different. If this process were to be repeated a large number times, the probability distribution of the resulting point estimates is known as the Sampling Distribution of the statistic. There is no rule surrounding the sampling distribution - it depends on the distribution of the population, statistic being measured, sampling method etc. The standard deviation of this sampling distribution is known as the Standard Error of the statistic: \\[ \\sigma_{\\theta} = \\sqrt {\\sigma^2_{\\theta}} \\] A special case is the Sample Mean . If the population is normally distributed, then it is normally distributed as well. Regardless of the population distribution, it is also approximately normally distributed through the Central Limit Theorem or Law of Large Numbers . \\[ \\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\] Following that, we can compute the standard error: \\[ \\sigma_{\\bar{x}} = \\sqrt \\frac{\\sigma^2_{\\bar{x}}}{n} = \\frac{\\sigma}{\\sqrt{n}} \\] However, the population variance is usually unknown. Thus, it can be approximated using the Sample Variance, which is an unbiased estimator for it. The result is known as an Estimate for the Standard Error : \\[ \\hat{\\sigma}_{\\bar{x}} = \\sqrt \\frac{s^2}{n} = \\frac{s}{\\sqrt{n}} \\] Note that only the sample variance is an unbiased estimator for the population variance. Although it may look like it, the sample SD is NOT an unbiased estimator for the population SD.","title":"Sampling Distribution"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Review%20of%20Statistical%20Theory/#confidence-interval","text":"Given that there is only one true value for the population parameter and a whole distribution of estimators, it is unlikely that a point estimate will be equal to the population parameter. Thus, instead of a point estimate, a range of estimates is used, known as a Confidence Interval . The interval is made a chosen Confidence Level which represents the proportion of confidence intervals that will contain the true value . In other words, if a large number of confidence intervals constructed in the same manner were to be made, \\((1-\\alpha)%\\) of them would contain the true value. \\[ P(- \\text{Margin of Error} < \\theta < \\text{Margin of Error}) = 1 - \\alpha \\] The Margin of Error represents the range of values on either side of the point estimate that the true value could lie. For instance, for a 95% confidence interval, the confidence lies within the 0.025 and 0.975 percentile of the sampling distribution. The margin of error can be calculated by finding the corresponding values of the sampling distribution at these percentiles. Consider the 95% confidence interval for the Sample Mean , which is normally distributed. For convenience, it is usually normalized such that it will become a Standard Normal Distribution : \\[ \\begin{aligned} P(-1.96 < \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt n}} < 1.96) &= 0.95 \\\\ P(\\bar{x} - 1.96 \\frac{\\sigma}{\\sqrt n} < \\mu < \\bar{x} + 1.96 \\frac{\\sigma}{\\sqrt n}) &= 0.95 \\\\ \\end{aligned} \\] \\[ \\therefore \\text{Margin of Error} = \\bar{x} + Z_{\\frac{\\alpha}{2}} * \\frac{\\sigma}{\\sqrt n} \\]","title":"Confidence Interval"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Review%20of%20Statistical%20Theory/#hypothesis-testing","text":"Hypothesis Testing is a formal method of making inferences about the population parameters based on the sample statistic. It starts with a Hypothesis which is a conjecture about the population parameters: Null Hypothesis - What is currently believed to be true Alternative Hypothesis - What is to be proven A Test Statistic is then calculated, which would quantify the behaviour of population such that it would distinguish the null and alternative hypothesis. Assuming the Null Hypothesis is true , the sampling distribution of the test statistic is determined. From the sampling distribution, the p-value is calculated, which is the probability of observing the calculated sample statistic or more extreme . If the p-value is smaller than a pre-determined level of Statistical Significance ( \\(\\alpha\\) ), then the test is \"successful\" and the null hypothesis is rejected. If not, then the test \"fails\" and the null is not rejected. Note that it rejecting the null hypothesis does NOT mean that the alternative hypothesis is accepted - EG. Rejected then hypothesis of a large paw print being from a bear does not mean that it is from Bigfoot. Thus, the hypotheses are usually constructed such that the two hypothesis are complementary , such that rejecting the null allows acceptance of the alternative, leading to a definitive insight. In layman terms, a hypothesis test is a test of extremeness . Assuming the null hypothesis is true, how extreme (low probability) is the observed sample? If the p-value is sufficiently low, it means that the sample is rare - it could be due to pure chance or that it is actually not rare because the null is not true. We distinguish between the two mathematically through \\(\\alpha\\) . It is the probability of a False Positive - that the result obtained was by pure chance. It is typically set at 5%, which means that 5% of all statistics calculated are expected to be extreme under the null. Thus, if the p-value of the sample is smaller than \\(\\alpha\\) , then it is likely that the observation was not due to chance but instead because the null was false . Alternatively, instead of comparing p-value to \\(alpha\\) , the test-statistic and the corresponding value of \\(\\alpha\\) on the sampling distribution can be used. It is known as the Critical Value , which represents the boundary of Reject Null Do not Reject Null p-value smaller than \\(\\alpha\\) p-value smaller than \\(\\alpha\\) test-statistic larger than critical value test-statistic smaller than critical value","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Review%20of%20Statistical%20Theory/#test-statistic","text":"Let the random variable \\(T\\) denote the test statistics. There are many different kinds of test statistics depending on the distribution and what is being investigated. The most simple test statistic involve the Sample Mean , which is normally distributed: \\[ T = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] If the population variance is known , then the test statistic has a Standard Normal Distribution and thus the test-statistic is known as a Z-Statistic . \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\sqrt n \\\\ &= Z * \\sqrt n \\\\ \\therefore T &\\sim N(0,1) \\end{aligned} \\] If the population variance is unknown, then it will be approximated by the Sample Variance . Through algebraic manipulation, the test statistic can still be expressed in the form of a Z variable, but with an additional term: \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}} \\\\ &= \\frac{\\bar{x} - \\mu}{s} * \\sqrt n \\\\ &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\frac{\\sigma}{s} * \\sqrt n \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{s^2}{\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{(n-1)s^2}{(n-1)\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt {\\frac{(n-1)s^2}{\\sigma^2} * \\frac{1}{n-1}} \\\\ \\end{aligned} \\] The additional term can be shown to have a Chi-Squared Distribution of \\(n-1\\) degrees of freedom, which by definition is the sum of \\(n-1\\) independent standard normal variables: \\[ \\begin{aligned} \\chi^2_n & \\sim Z^2 \\\\ & \\sim \\sum \\left(\\frac {\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ & \\sim \\sum \\left(\\frac {(x_i - \\bar{x}) + (\\bar{x} - \\mu)}{\\sigma}\\right)^2 \\\\ & \\sim \\sum \\left(\\frac{x_i - \\bar{x}}{\\sigma}\\right)^2 + \\sum \\left(\\frac{\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ & \\sim \\frac{1}{\\sigma^2} \\sum (x_i - \\bar{x})^2 + \\sum Z^2 + 0 \\\\ & \\sim \\frac{(n-1)}{\\sigma^2} \\frac{\\sum (x_i - \\bar{x})^2}{n-1} + \\chi^2_1 \\\\ & \\sim \\frac{(n-1)s^2}{\\sigma^2} + \\chi^2_1 \\\\ \\end{aligned} \\] \\[\\therefore \\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\] Thus, the test statistic is the ratio of a Standard Normal Variable to a Chi Squared Variable (divided by its degrees of freedom). By definition, this test statistic has a t-distribution with the same degrees of freedom and is known as the t-statistic : \\[ \\begin{align*} T &= \\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\\\ &= t_{n-1} * \\sqrt{n} \\\\ \\end{align*} \\] \\[\\therefore T \\sim t_{n-1}\\] Despite the slightly convoluted proof, the t-distribution is simply a standard normal distribution with heavier tails . This means that extreme values are slightly more likely, which is meant to account for the increase in variability due to the use of the sample variance rather than the population variance.","title":"Test Statistic"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Review%20of%20Statistical%20Theory/#maximum-likelihood-estimation","text":"If the population distribution is known, there is an alternative method of estimating the parameters apart from calculating the corresponding sample statistics. We model a set of observations as a random sample from an unknown joint probability distribution which is expressed in terms of a set of parameters. The goal of maximum likelihood estimation is to determine the parameters for which the observed data have the highest joint probability. The goal of maximum likelihood estimation is to find the values of the model parameters that maximize the likelihood function over the parameter space","title":"Maximum Likelihood Estimation"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Regression%20Overview/","text":"Regression Overview Population Regression Model Regression is a statistical model that relates a Dependent Variable to one or more Independent Variables . The dependent variable is regressed on to the independent variable. They are fundamentally a function of the independent variables and several Regression Parameters , \\(\\beta\\) . The functional form of the regression is based on the relationship between the variables. The goal is to use a model that best captures the relationship between the variables. \\[ Y = f(X, \\beta) \\] Independent Variable(s) Dependent Variables Variable used to make predictions Variable being predicted Free to change the value Depends on the value of indepenent variable Deterministic Random Variable Denoted as \\(X\\) Denoted as \\(Y\\) To be precise, for every set of \\(X\\) , the \\(Y\\) has a Conditional Distribution dependent on the given \\(X\\) . For instance, the the \\(Y\\) could take any possible value (non-conditional distribution), but given these set of \\(X\\) , the possible values can be narrowed down to a certain range (conditional distribution). \\[ \\displaylines{ Y \\sim Distribution \\\\ Y|X \\sim Conditional~Distribution} \\] Thus, the output of the regression model is actually the Expected Value of the conditional distribution, \\(E(Y|X)\\) , for every possible \\(X\\) . \\[ E(Y|X) = f(X, \\beta) \\] The actual observations are unlikely to be exactly equal to its expectation, thus there is a difference between an observation and the corresponding regression output. It known as the Random Error Term which accounts for all other factors that affect the \\(Y\\) that are not captured in the regression. This means that the relationship between the \\(Y\\) and \\(X\\) is only approximate , as the true relationship is probably different due to the possibility of unaccounted variables. Note that the sign of the errors are significant - positive implies the actual value lies above the regression output while negative implies it lies below. \\[ \\varepsilon_i = y_i - f(x_i, \\beta) \\] This means that \\(Y\\) (not its expectation!) can be expressed as a sum of the regression model and the error terms: \\[ y_i = f(x_i,\\beta) + \\varepsilon_i \\] The Regression is known as the Systematic component as it is shared among all observations The Error is known as the Non-Systematic component as it is unique to each observation Sample Regression Model In practice, the population is unobservable hence it is impossible to construct the population regression model. Instead, a regression model is constructed from a sample instead, which aims to estimate the population model. \\[ \\hat{y} = f(X,\\hat{\\beta}) \\] Similarly, the output of this model can be compared to the actual observations. However, the resulting difference is known as the Residual of the model, which like all the other components, is an estimate for the Error term. \\[ \\hat{\\varepsilon_i} = y_i - \\hat{y_i} \\] There are several different methods to estimate the regression parameters, but they usually involve minimizing the residuals of the model, such that the resulting model best fits the given sample, which is why it is also known as the Fitted Regression Model . Hypothesis Testing Once a regression model has been fit, the next step is to determine if the relationship found in the sample is indicative of a relationship in the population. This can be determined through the following two-sided hypothesis test : Null Hypothesis: \\(\\beta = 0\\) Alternative Hypothesis: \\(\\beta \\ne 0\\) Under the null, the regression parameters are assumed to be 0, implying that there is no relationship between \\(Y\\) and \\(X\\) . The test should reject the null , proving that there IS a relationship between \\(Y\\) and \\(X\\) . Apart from testing the presence of a relationship, the strength of the relationship can be tested as well by specifying a value for the regression parameters. This can be determined through the following one-sided hypothesis test : Null Hypothesis: \\(\\beta = d\\) Alternative Hypothesis: \\(\\beta >< d\\) Goodness of Fit Prediction Once the best model has been determined, it can be used to make Predictions about unobserved values. These unobserved values come from the population, thus can be expressed as a function of the population model: \\[ y_* = f(x_*, \\beta) + \\varepsilon_* \\] The corresponding output from the sample regression model is an estimate for this unobserved value: \\[ \\hat{y_*} = f(x_*, \\hat{\\beta}) \\] Like before, the predicted value is unlikely to be exactly equal to the actual value. Thus, the difference between both values can be measured as the Prediction Error , which has two main components: Inherent Error in \\(Y\\) Error in estimating the Population Model \\[ y_* - \\hat{y_*} = \\varepsilon_* + [f(x_*, \\beta) - f(x_*, \\hat{\\beta})] \\] Based on the distribution of the prediction error, a Prediction Interval at a given confidence level can be calculated to accompany the regression estimate, which is essentially a confidence interval for the prediction error .","title":"Regression Overview"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Regression%20Overview/#regression-overview","text":"","title":"Regression Overview"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Regression%20Overview/#population-regression-model","text":"Regression is a statistical model that relates a Dependent Variable to one or more Independent Variables . The dependent variable is regressed on to the independent variable. They are fundamentally a function of the independent variables and several Regression Parameters , \\(\\beta\\) . The functional form of the regression is based on the relationship between the variables. The goal is to use a model that best captures the relationship between the variables. \\[ Y = f(X, \\beta) \\] Independent Variable(s) Dependent Variables Variable used to make predictions Variable being predicted Free to change the value Depends on the value of indepenent variable Deterministic Random Variable Denoted as \\(X\\) Denoted as \\(Y\\) To be precise, for every set of \\(X\\) , the \\(Y\\) has a Conditional Distribution dependent on the given \\(X\\) . For instance, the the \\(Y\\) could take any possible value (non-conditional distribution), but given these set of \\(X\\) , the possible values can be narrowed down to a certain range (conditional distribution). \\[ \\displaylines{ Y \\sim Distribution \\\\ Y|X \\sim Conditional~Distribution} \\] Thus, the output of the regression model is actually the Expected Value of the conditional distribution, \\(E(Y|X)\\) , for every possible \\(X\\) . \\[ E(Y|X) = f(X, \\beta) \\] The actual observations are unlikely to be exactly equal to its expectation, thus there is a difference between an observation and the corresponding regression output. It known as the Random Error Term which accounts for all other factors that affect the \\(Y\\) that are not captured in the regression. This means that the relationship between the \\(Y\\) and \\(X\\) is only approximate , as the true relationship is probably different due to the possibility of unaccounted variables. Note that the sign of the errors are significant - positive implies the actual value lies above the regression output while negative implies it lies below. \\[ \\varepsilon_i = y_i - f(x_i, \\beta) \\] This means that \\(Y\\) (not its expectation!) can be expressed as a sum of the regression model and the error terms: \\[ y_i = f(x_i,\\beta) + \\varepsilon_i \\] The Regression is known as the Systematic component as it is shared among all observations The Error is known as the Non-Systematic component as it is unique to each observation","title":"Population Regression Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Regression%20Overview/#sample-regression-model","text":"In practice, the population is unobservable hence it is impossible to construct the population regression model. Instead, a regression model is constructed from a sample instead, which aims to estimate the population model. \\[ \\hat{y} = f(X,\\hat{\\beta}) \\] Similarly, the output of this model can be compared to the actual observations. However, the resulting difference is known as the Residual of the model, which like all the other components, is an estimate for the Error term. \\[ \\hat{\\varepsilon_i} = y_i - \\hat{y_i} \\] There are several different methods to estimate the regression parameters, but they usually involve minimizing the residuals of the model, such that the resulting model best fits the given sample, which is why it is also known as the Fitted Regression Model .","title":"Sample Regression Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Regression%20Overview/#hypothesis-testing","text":"Once a regression model has been fit, the next step is to determine if the relationship found in the sample is indicative of a relationship in the population. This can be determined through the following two-sided hypothesis test : Null Hypothesis: \\(\\beta = 0\\) Alternative Hypothesis: \\(\\beta \\ne 0\\) Under the null, the regression parameters are assumed to be 0, implying that there is no relationship between \\(Y\\) and \\(X\\) . The test should reject the null , proving that there IS a relationship between \\(Y\\) and \\(X\\) . Apart from testing the presence of a relationship, the strength of the relationship can be tested as well by specifying a value for the regression parameters. This can be determined through the following one-sided hypothesis test : Null Hypothesis: \\(\\beta = d\\) Alternative Hypothesis: \\(\\beta >< d\\)","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Regression%20Overview/#goodness-of-fit","text":"","title":"Goodness of Fit"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Regression%20Overview/#prediction","text":"Once the best model has been determined, it can be used to make Predictions about unobserved values. These unobserved values come from the population, thus can be expressed as a function of the population model: \\[ y_* = f(x_*, \\beta) + \\varepsilon_* \\] The corresponding output from the sample regression model is an estimate for this unobserved value: \\[ \\hat{y_*} = f(x_*, \\hat{\\beta}) \\] Like before, the predicted value is unlikely to be exactly equal to the actual value. Thus, the difference between both values can be measured as the Prediction Error , which has two main components: Inherent Error in \\(Y\\) Error in estimating the Population Model \\[ y_* - \\hat{y_*} = \\varepsilon_* + [f(x_*, \\beta) - f(x_*, \\hat{\\beta})] \\] Based on the distribution of the prediction error, a Prediction Interval at a given confidence level can be calculated to accompany the regression estimate, which is essentially a confidence interval for the prediction error .","title":"Prediction"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Simple%20Linear%20Regression/","text":"Simple Linear Regression Simple Linear Regression (SLR) assumes a Linear Relationship between a Numeric \\(Y\\) and single numeric \\(X\\) . The model is considered simple because it only contains a single \\(X\\) variable. \\[ E(Y|X) = \\beta_0 + \\beta_1 X \\] \\(\\beta_0\\) \\(\\beta_1\\) Expected value of \\(Y\\) when \\(X = 0\\) Change in the expected value of \\(Y\\) given a one unit increase in \\(X\\) Intercept Parameter Slope Parameter Similarly, we can also express each observation of \\(Y\\) with its error term: \\[ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\] Ordinary Least Squares In practice, the regression parameters are estimated using the Ordinary Least Squares method, which minimizes the Sum of Squared Residuals of the fitted model. It is more commonly referred to as the Residual Sum Squared (RSS). \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 \\] The minimization is solved thorugh calculus by setting the partial derivatives of the RSS to 0: For the intercept parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_0} RSS &= 0 \\\\ -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum y_i - \\sum \\hat{\\beta}_0 - \\sum \\hat{\\beta}_1 x &= 0 \\\\ n\\bar{y} -n\\hat{\\beta}_0 - n\\hat{\\beta}_1 \\bar{x} &= 0 \\\\ \\end{aligned} \\] \\[\\therefore \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\] For the slope parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_1} RSS &= 0 \\\\ -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum (y_i x_i) - \\hat{\\beta}_0 \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} + n\\hat{\\beta}_1 \\bar{x}^2 - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} &= \\hat{\\beta}_1 \\sum (x^2_i) - n\\bar{x}^2 \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 = \\frac{\\sum (x_i y_i) - n\\bar{x}\\bar{y}}{\\sum (x_i^2) - n\\bar{x}^2} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} = r * \\frac{s_y}{s_x} \\] This results in the following fitted regression model: \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\\\ \\] Note that it should \\(\\hat{\\varepsilon}\\) in the above image, not \\(e_i\\) . OLS Properties By re-arranging the formula for \\(\\hat{\\beta}_0\\) , we obtain the following: \\[ \\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x} \\] This means that \\((\\bar{x}, \\bar{y})\\) always lies on the fitted regression model. Additionally, totice that the two estimates will also always satisfy the partial derivatives of the minimization problem, which leads to some interesting results: Intercept Parameter Slope Parameter \\(\\frac{\\partial}{\\partial \\beta_0} = -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\frac{\\partial}{\\partial \\beta_1} = -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\sum \\hat{\\varepsilon_i} = 0\\) \\(\\sum x_i \\hat{\\varepsilon_i} = 0\\) Residuals are negatively correlated Residuals and Independendent variables are uncorrelated OLS Assumption Assumption 1: Errors are independent of one another: If errors are correlated with one another, it is known as Serial Correlation or Autocorrelation . It implies that there are other unmodelled factors that can be used for prediction, which would imply the current model specification to be wrong. Thus, for the SLR model to be true, the errors must be independent of one another . \\[ Cov(\\varepsilon_i,\\varepsilon_j) = 0 \\] Assumption 2: Errors are independent of \\(X\\) : Note that the error term is NOT independent of \\(Y\\) , as it captures the unmodelled factors that could affect \\(Y\\) . If the errors are NOT independent of \\(X\\) as well, then the errors would influence \\(X\\) which would then influence \\(Y\\) . The resulting model would be the effect of both \\(X\\) and the unmodelled factors, when the model only specified \\(X\\) , leading to misleading results. In this case, the unmodelled factors are known as Confounding Variables , as it confounds the results of the regression. Thus, for the SLR model to be true, the errors must be independent of \\(X\\) . Phrased another way, \\(X\\) must be an Exogenous Variable . \\[ Cov(\\varepsilon_i,X) = 0 \\] Assumption 3: Errors have an expectation of 0: Building off the previous assumptions, it implies that the errors should be Random . Thus, they should have a conditional expectation of 0 . \\[ E(\\varepsilon_i | X) = 0 \\] Assumption 4: Errors have constant variance: Building off the previous assumptions, the errors should not exhibit any particular pattern. Thus, they should have a constant variance of \\(\\sigma^2\\) . \\[ var(\\varepsilon_i) = \\sigma^2 \\] Non constant then overdominated by outliers Assumption 5: Errors are normally distributed: Following the previous assumptions, the errors are assumed to be normally distributed: \\[ \\varepsilon \\sim N(0, \\sigma^2) \\] If the errors are normally distributed, then it follows that \\(y\\) and \\(\\beta\\) are normally distributed as well. While this assumption is not necessary to create the model, it greatly eases the computation for making statistical inferences. Goodness of Fit Ideally, the regression should fit the sample as closely, having as few as residuals as possible . All the residuals in the model can be summarized through the RSS. The lower the SSR, the better the fit of the model. Recall that the residuals naturally sum to 0 under OLS - the residuals are thus squared to remove the sign so that they can be summed together. \\[ RSS = \\sum (y_i - \\hat{y})^2 \\] However, the SSR on its own is hard to intepret as there is no indication of how low or high it actually is. Thus, the Total Sum of Squares (TSS) can be used as a benchmark for the RSS as it is at least equal to or higher than the RSS. The TSS represents the RSS for a Naive Regression - a model with containing only the intercept with no independent variables . The output of this regression is always the sample mean \\(\\bar{y}\\) , which is used for the computation of its residuals. It represents the worst possible model which thus has the highest possible RSS . The lower the RSS compared to the TSS, the better the fit of the model. \\[ TSS = \\sum (y_i - \\bar{y}) \\] Null Model Consider a regression with no independent variables: \\[ y = \\beta \\] We can estimate \\(\\hat{\\beta}\\) using OLS, which results in the following result: \\[ \\begin{aligned} -2 \\sum (y_i - \\hat{\\beta}) &= 0 \\\\ n \\bar{y} - n \\hat{\\beta} &= 0 \\\\ \\hat{\\beta} &= \\bar{y} \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{y} = \\bar{y} \\] Analysis of Variance Another intepretation of the TSS is that it is the Variation of the observed values about the sample mean. It can be further decomposed into a constituent parts for analysis. This process is referred to as the Analysis of Variance . \\[ \\begin{aligned} TSS &= \\sum (y_i - \\bar{y})^2 \\\\ TSS &= \\sum[(y_i - \\hat{y}) + (\\hat{y}-\\bar{y})]^2 \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 2 \\sum((y_i - \\hat{y})(\\hat{y}-\\bar{y})) \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 0 \\\\ TSS &= RSS + RegSS \\end{aligned} \\] Residual SS (RSS) Regression SS (RegSS) \\(\\sum(\\hat{y}-\\bar{y})^2\\) \\(\\sum(y_i - \\hat{y})^2\\) Variation of the observed values about the regression Variation of the regression output about the sample mean Variation explained by the regression Variation unexplained by the regression Note that it can also be expressed in terms of the Slope Parameter : \\[ \\begin{align} RegSS &= \\sum(\\hat{y}-\\bar{y})^2 \\\\ &= \\sum(\\hat{\\beta}_0 + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum(\\bar{y} - \\beta_1 \\bar{x} + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum[\\hat{\\beta}_1 (x_i - \\bar{x})]^2 \\\\ &= \\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2 \\\\ \\end{align} \\] The Coefficient of Determination \\(R^2\\) can also be used to demonstrate goodness of fit. It measures the proportion of variation explained by the regression model. \\[ R^2 = \\frac{RegSS}{TSS} = 1 - \\frac{RSS}{TSS} \\] Building off the above expression, it can also be expressed in terms of the Sample Correlation : \\[ \\begin{align} R^2 &= \\frac{\\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\\right)^2 \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^4} \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x}) \\sum (y_i - \\bar{y})}\\right)^2 \\\\ &= r^2 \\end{align} \\] Degrees of Freedom The TSS is based on the naive model with only the intercept parameter, thus, it is subject to the single contraint of all residuals summing to 0. The TSS thus has \\(n-1\\) degrees of freedom . The RSS is based on the SLR with both the intercept and slope parameter, thus it is subject to an additional constraint of the sumproduct of all residuals and independent variables being 0. The RSS thus has \\(n-2\\) degrees of freedom. The sum of the RSS and RegSS is equal to the TSS, thus the sum of their degrees of freedom must also be equal to that of the TSS. By working backwards, the RegSS thus has only \\(1\\) degree of freedom . Mean Squared The division of any Sum of Square (TSS, RSS, RegSS) by its Degrees of Freedom is known as the Mean Squared (MS), which is a measure of its average variance . The MS of the RSS is known as the Mean Squared Error as it is an estimate for the population variance of the error \\(\\sigma^2\\) : \\[ MSE = \\frac{RSS}{n-2} \\] The MS of the TSS is the Unbiased Estimator for Population Variance while the MS of the RegSS has no significant intepretation. F Statistic The F-statistic for the regression model is the ratio of the two Mean Squares: \\[ F = \\frac{MS_RegSS}{MS_RSS} = (n-2) * \\frac{RegSS}{RSS} \\] It can also be calculated in terms of \\(R^2\\) , by dividing the above formula by the TSS in both the numerator and denominator: \\[ F = (n-2) * \\frac{RegSS/TSS}{RSS/TSS} = (n-2) * \\frac{R^2}{1-R^2} \\] ANOVA Table All the above information is then summarized in a table for convenience, known as the ANOVA Table : Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(1\\) RegSS \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-2\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) - Statistical Inference Sampling Distributions Since the errors are assumed to be normally distributed, then \\(Y\\) is assumed to be normally distributed as well. Since \\(Y\\) is a linear combination of the regression parameters, then the parameters (& their estimates) are normally distributed as well. Both estimates can be expressed in another form that makes it more convenient to find their expectation & variances. \\[ \\begin{aligned} \\hat{\\beta}_1 &= \\frac{\\sum [(x_i - \\bar{x})(y_i - \\bar{y})]}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})y_i}{\\sum (x^2_i - \\bar{x})} - \\frac{\\bar{y} \\sum (x_i - \\bar{x})}{\\sum (x^2_i - \\bar{x})} \\\\ &= \\sum \\frac{(x_i - \\bar{x})}{(x^2_i - \\bar{x})}* y_i - 0 \\\\ &= \\sum w_i * y_i \\\\ \\\\ \\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\ &= \\frac{1}{n} \\sum y_i - \\bar{x} \\sum w_i * y_i \\\\ &= \\sum y_i (\\frac{1}{n} - \\bar{x}w_i) \\end{aligned} \\] \\(w_i\\) is a sort of \"weight\" parameter of the sum of squares. It has three interesting properties that makes it useful: \\[ \\begin{aligned} \\sum w_i &= \\frac{\\sum (x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{n\\bar{x}-n\\bar{x}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{0}{\\sum (x^2_i - \\bar{x})} \\\\ &= 0 \\\\ \\\\ \\sum w_i x_i &= \\frac{\\sum x_i(x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x^2_i - \\bar{x} \\sum x_i)}{\\sum x^2_i - 2\\bar{x}\\sum x_i + \\sum \\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - \\bar{x}(n \\bar{x})}{\\sum x^2_i - 2\\bar{x}(n\\bar{x}) + n\\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - 2n\\bar{x}^2 + n\\bar{x}^2} \\\\ &=\\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - n\\bar{x}^2} \\\\ &= 1 \\\\ \\\\ \\sum w_i^2 &= \\frac{\\sum (x_i - \\bar{x})^2}{\\sum (x_i - \\bar{x})^4} \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i) &= n(\\frac{1}{n}) - \\bar{x} \\sum w_i \\\\ &= 1 - 0 \\\\ &= 1 \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i &= \\frac{1}{n} \\sum x_i - \\bar{x} \\sum w_i x_i \\\\ &= \\frac{1}{n} (n\\bar{x}) - \\bar{x} (1) \\\\ &= \\bar{x} - \\bar{x} \\\\ &= 0 \\end{aligned} \\] Using this, the Expectation & Variance can be determined: \\[ \\begin{aligned} E(\\hat{\\beta}_1) &= \\sum w_i E(y_i) \\\\ &= \\sum w_i E(\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum w_i + \\beta_1 \\sum w_i x_i \\\\ &= \\beta_0 (0) + \\beta_1 (1) \\\\ &= \\beta_1\\\\ \\\\ Var(\\hat{\\beta}_1) &= Var(\\sum w_i y_i) \\\\ &= \\sum w_i^2 Var (y_i) \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2} \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 \\sim N(\\beta_1, \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2}) \\] \\[ \\begin{aligned} E(\\hat{\\beta}_0) &= \\sum (\\frac{1}{n} - \\bar{x}w_i) E(y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i) (\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum (\\frac{1}{n} - \\bar{x}w_i) + \\beta_1 \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i \\\\ &= \\beta_0 (1) + \\beta_1 (0) \\\\ &= \\beta_0 \\\\ \\\\ Var(\\hat{\\beta}_0) &= Var(\\sum (\\frac{1}{n} - \\bar{x}w_i)y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i)^2 Var (y_i) \\\\ &= \\sigma^2 \\sum (\\frac{1}{n^2} -\\frac{2\\bar{x}w_i}{n} + \\bar{x}^2 w_i^2) \\\\ &= \\sigma^2 (\\sum \\frac{1}{n^2} - \\frac{2\\bar{x}}{n} \\sum w_i + \\bar{x}^2 \\sum w_i^2) \\\\ &= \\sigma^2 [n(\\frac{1}{n^2}) - \\frac{2\\bar{x}}{n} (0) + \\bar{x}^2 (\\frac{1}{\\sum (x_i - \\bar{x})^2})] \\\\ &= \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2}) \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_0 \\sim N(\\beta_0, \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2})) \\] Hypothesis Testing As mentioned in the overview section, hypothesis testing can be used to test for the presence of a relationship or its strength. Since the regression parameters are normally distributed, a z-statistic can be used to conduct the tests. However, since the population variance is not known, a t-statistic is used instead: \\[ \\begin{aligned} t &= \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\end{aligned} \\] Since the population variance is estimated by the MSE which has \\(n-2\\) degrees of freedom, the corresponding chi-squared and hence t-distribution has \\(n-2\\) degrees of freedom as well. \\[ t(\\hat{\\beta_1}) \\sim t_{n-2} \\] Both the F-statistic and t-statistic are equivalent ways of hypothesis testing and will always lead to the same conclusions. It is advised to use whichever statistic is more convenient to calculate based on the given information. \\[ t^2 = \\left(\\right) \\] Confidence Intervals Since the distribution of t-statistic is known, it can be used to determine the confidence interval of the estimate: \\[ P\\left(\\text{Margin of Error} < \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} < \\text{Margin of Error}\\right) = 1 - \\alpha \\] \\[ \\text{Confidence Interval} = \\hat{\\beta}_1 + t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_1}} \\] Prediction Intervals Consider the Prediction Error of the SLR model: \\[ y_* - \\hat{y_*} = \\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\] Since both \\(y_*\\) and \\(\\hat{y_*}\\) are normally distributed, the prediction errors are normally distributed as well: \\[ \\begin{aligned} E(y_* - \\hat{y_*}) &= E[\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)]] \\\\ &= 0 + \\beta_0 + \\beta_1 E(x_*) - \\beta_0 - \\beta_1 E(x_*) \\\\ &= 0 \\\\ \\\\ Var(y_* - \\hat{y_*}) &= Var(\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= Var(\\varepsilon_*) + Var[(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= ... \\\\ &= \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}] \\end{aligned} \\] \\[ \\therefore y_* - \\hat{y_*} \\sim N(0, \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}]) \\] Similar to before, since the population variance is unknown, it can be approximated using the MSE. Thus, a t-statistic constructed from this sample will have a t-distribution , allowing the following prediction interval to be calculated: \\[ \\text{Prediction Interval} = \\hat{y}_* + t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\] Notice that the standard error of the prediction interval increases as \\(x_i\\) moves further away from the mean, indicating that the predictions become less accurate for those values.","title":"Simple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Simple%20Linear%20Regression/#simple-linear-regression","text":"Simple Linear Regression (SLR) assumes a Linear Relationship between a Numeric \\(Y\\) and single numeric \\(X\\) . The model is considered simple because it only contains a single \\(X\\) variable. \\[ E(Y|X) = \\beta_0 + \\beta_1 X \\] \\(\\beta_0\\) \\(\\beta_1\\) Expected value of \\(Y\\) when \\(X = 0\\) Change in the expected value of \\(Y\\) given a one unit increase in \\(X\\) Intercept Parameter Slope Parameter Similarly, we can also express each observation of \\(Y\\) with its error term: \\[ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\]","title":"Simple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Simple%20Linear%20Regression/#ordinary-least-squares","text":"In practice, the regression parameters are estimated using the Ordinary Least Squares method, which minimizes the Sum of Squared Residuals of the fitted model. It is more commonly referred to as the Residual Sum Squared (RSS). \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 \\] The minimization is solved thorugh calculus by setting the partial derivatives of the RSS to 0: For the intercept parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_0} RSS &= 0 \\\\ -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum y_i - \\sum \\hat{\\beta}_0 - \\sum \\hat{\\beta}_1 x &= 0 \\\\ n\\bar{y} -n\\hat{\\beta}_0 - n\\hat{\\beta}_1 \\bar{x} &= 0 \\\\ \\end{aligned} \\] \\[\\therefore \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\] For the slope parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_1} RSS &= 0 \\\\ -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum (y_i x_i) - \\hat{\\beta}_0 \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} + n\\hat{\\beta}_1 \\bar{x}^2 - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} &= \\hat{\\beta}_1 \\sum (x^2_i) - n\\bar{x}^2 \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 = \\frac{\\sum (x_i y_i) - n\\bar{x}\\bar{y}}{\\sum (x_i^2) - n\\bar{x}^2} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} = r * \\frac{s_y}{s_x} \\] This results in the following fitted regression model: \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\\\ \\] Note that it should \\(\\hat{\\varepsilon}\\) in the above image, not \\(e_i\\) .","title":"Ordinary Least Squares"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Simple%20Linear%20Regression/#ols-properties","text":"By re-arranging the formula for \\(\\hat{\\beta}_0\\) , we obtain the following: \\[ \\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x} \\] This means that \\((\\bar{x}, \\bar{y})\\) always lies on the fitted regression model. Additionally, totice that the two estimates will also always satisfy the partial derivatives of the minimization problem, which leads to some interesting results: Intercept Parameter Slope Parameter \\(\\frac{\\partial}{\\partial \\beta_0} = -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\frac{\\partial}{\\partial \\beta_1} = -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\sum \\hat{\\varepsilon_i} = 0\\) \\(\\sum x_i \\hat{\\varepsilon_i} = 0\\) Residuals are negatively correlated Residuals and Independendent variables are uncorrelated","title":"OLS Properties"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Simple%20Linear%20Regression/#ols-assumption","text":"Assumption 1: Errors are independent of one another: If errors are correlated with one another, it is known as Serial Correlation or Autocorrelation . It implies that there are other unmodelled factors that can be used for prediction, which would imply the current model specification to be wrong. Thus, for the SLR model to be true, the errors must be independent of one another . \\[ Cov(\\varepsilon_i,\\varepsilon_j) = 0 \\] Assumption 2: Errors are independent of \\(X\\) : Note that the error term is NOT independent of \\(Y\\) , as it captures the unmodelled factors that could affect \\(Y\\) . If the errors are NOT independent of \\(X\\) as well, then the errors would influence \\(X\\) which would then influence \\(Y\\) . The resulting model would be the effect of both \\(X\\) and the unmodelled factors, when the model only specified \\(X\\) , leading to misleading results. In this case, the unmodelled factors are known as Confounding Variables , as it confounds the results of the regression. Thus, for the SLR model to be true, the errors must be independent of \\(X\\) . Phrased another way, \\(X\\) must be an Exogenous Variable . \\[ Cov(\\varepsilon_i,X) = 0 \\] Assumption 3: Errors have an expectation of 0: Building off the previous assumptions, it implies that the errors should be Random . Thus, they should have a conditional expectation of 0 . \\[ E(\\varepsilon_i | X) = 0 \\] Assumption 4: Errors have constant variance: Building off the previous assumptions, the errors should not exhibit any particular pattern. Thus, they should have a constant variance of \\(\\sigma^2\\) . \\[ var(\\varepsilon_i) = \\sigma^2 \\] Non constant then overdominated by outliers Assumption 5: Errors are normally distributed: Following the previous assumptions, the errors are assumed to be normally distributed: \\[ \\varepsilon \\sim N(0, \\sigma^2) \\] If the errors are normally distributed, then it follows that \\(y\\) and \\(\\beta\\) are normally distributed as well. While this assumption is not necessary to create the model, it greatly eases the computation for making statistical inferences.","title":"OLS Assumption"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Simple%20Linear%20Regression/#goodness-of-fit","text":"Ideally, the regression should fit the sample as closely, having as few as residuals as possible . All the residuals in the model can be summarized through the RSS. The lower the SSR, the better the fit of the model. Recall that the residuals naturally sum to 0 under OLS - the residuals are thus squared to remove the sign so that they can be summed together. \\[ RSS = \\sum (y_i - \\hat{y})^2 \\] However, the SSR on its own is hard to intepret as there is no indication of how low or high it actually is. Thus, the Total Sum of Squares (TSS) can be used as a benchmark for the RSS as it is at least equal to or higher than the RSS. The TSS represents the RSS for a Naive Regression - a model with containing only the intercept with no independent variables . The output of this regression is always the sample mean \\(\\bar{y}\\) , which is used for the computation of its residuals. It represents the worst possible model which thus has the highest possible RSS . The lower the RSS compared to the TSS, the better the fit of the model. \\[ TSS = \\sum (y_i - \\bar{y}) \\]","title":"Goodness of Fit"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Simple%20Linear%20Regression/#null-model","text":"Consider a regression with no independent variables: \\[ y = \\beta \\] We can estimate \\(\\hat{\\beta}\\) using OLS, which results in the following result: \\[ \\begin{aligned} -2 \\sum (y_i - \\hat{\\beta}) &= 0 \\\\ n \\bar{y} - n \\hat{\\beta} &= 0 \\\\ \\hat{\\beta} &= \\bar{y} \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{y} = \\bar{y} \\]","title":"Null Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Simple%20Linear%20Regression/#analysis-of-variance","text":"Another intepretation of the TSS is that it is the Variation of the observed values about the sample mean. It can be further decomposed into a constituent parts for analysis. This process is referred to as the Analysis of Variance . \\[ \\begin{aligned} TSS &= \\sum (y_i - \\bar{y})^2 \\\\ TSS &= \\sum[(y_i - \\hat{y}) + (\\hat{y}-\\bar{y})]^2 \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 2 \\sum((y_i - \\hat{y})(\\hat{y}-\\bar{y})) \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 0 \\\\ TSS &= RSS + RegSS \\end{aligned} \\] Residual SS (RSS) Regression SS (RegSS) \\(\\sum(\\hat{y}-\\bar{y})^2\\) \\(\\sum(y_i - \\hat{y})^2\\) Variation of the observed values about the regression Variation of the regression output about the sample mean Variation explained by the regression Variation unexplained by the regression Note that it can also be expressed in terms of the Slope Parameter : \\[ \\begin{align} RegSS &= \\sum(\\hat{y}-\\bar{y})^2 \\\\ &= \\sum(\\hat{\\beta}_0 + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum(\\bar{y} - \\beta_1 \\bar{x} + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum[\\hat{\\beta}_1 (x_i - \\bar{x})]^2 \\\\ &= \\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2 \\\\ \\end{align} \\] The Coefficient of Determination \\(R^2\\) can also be used to demonstrate goodness of fit. It measures the proportion of variation explained by the regression model. \\[ R^2 = \\frac{RegSS}{TSS} = 1 - \\frac{RSS}{TSS} \\] Building off the above expression, it can also be expressed in terms of the Sample Correlation : \\[ \\begin{align} R^2 &= \\frac{\\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\\right)^2 \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^4} \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x}) \\sum (y_i - \\bar{y})}\\right)^2 \\\\ &= r^2 \\end{align} \\]","title":"Analysis of Variance"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Simple%20Linear%20Regression/#degrees-of-freedom","text":"The TSS is based on the naive model with only the intercept parameter, thus, it is subject to the single contraint of all residuals summing to 0. The TSS thus has \\(n-1\\) degrees of freedom . The RSS is based on the SLR with both the intercept and slope parameter, thus it is subject to an additional constraint of the sumproduct of all residuals and independent variables being 0. The RSS thus has \\(n-2\\) degrees of freedom. The sum of the RSS and RegSS is equal to the TSS, thus the sum of their degrees of freedom must also be equal to that of the TSS. By working backwards, the RegSS thus has only \\(1\\) degree of freedom .","title":"Degrees of Freedom"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Simple%20Linear%20Regression/#mean-squared","text":"The division of any Sum of Square (TSS, RSS, RegSS) by its Degrees of Freedom is known as the Mean Squared (MS), which is a measure of its average variance . The MS of the RSS is known as the Mean Squared Error as it is an estimate for the population variance of the error \\(\\sigma^2\\) : \\[ MSE = \\frac{RSS}{n-2} \\] The MS of the TSS is the Unbiased Estimator for Population Variance while the MS of the RegSS has no significant intepretation.","title":"Mean Squared"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Simple%20Linear%20Regression/#f-statistic","text":"The F-statistic for the regression model is the ratio of the two Mean Squares: \\[ F = \\frac{MS_RegSS}{MS_RSS} = (n-2) * \\frac{RegSS}{RSS} \\] It can also be calculated in terms of \\(R^2\\) , by dividing the above formula by the TSS in both the numerator and denominator: \\[ F = (n-2) * \\frac{RegSS/TSS}{RSS/TSS} = (n-2) * \\frac{R^2}{1-R^2} \\]","title":"F Statistic"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Simple%20Linear%20Regression/#anova-table","text":"All the above information is then summarized in a table for convenience, known as the ANOVA Table : Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(1\\) RegSS \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-2\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) -","title":"ANOVA Table"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Simple%20Linear%20Regression/#statistical-inference","text":"","title":"Statistical Inference"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Simple%20Linear%20Regression/#sampling-distributions","text":"Since the errors are assumed to be normally distributed, then \\(Y\\) is assumed to be normally distributed as well. Since \\(Y\\) is a linear combination of the regression parameters, then the parameters (& their estimates) are normally distributed as well. Both estimates can be expressed in another form that makes it more convenient to find their expectation & variances. \\[ \\begin{aligned} \\hat{\\beta}_1 &= \\frac{\\sum [(x_i - \\bar{x})(y_i - \\bar{y})]}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})y_i}{\\sum (x^2_i - \\bar{x})} - \\frac{\\bar{y} \\sum (x_i - \\bar{x})}{\\sum (x^2_i - \\bar{x})} \\\\ &= \\sum \\frac{(x_i - \\bar{x})}{(x^2_i - \\bar{x})}* y_i - 0 \\\\ &= \\sum w_i * y_i \\\\ \\\\ \\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\ &= \\frac{1}{n} \\sum y_i - \\bar{x} \\sum w_i * y_i \\\\ &= \\sum y_i (\\frac{1}{n} - \\bar{x}w_i) \\end{aligned} \\] \\(w_i\\) is a sort of \"weight\" parameter of the sum of squares. It has three interesting properties that makes it useful: \\[ \\begin{aligned} \\sum w_i &= \\frac{\\sum (x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{n\\bar{x}-n\\bar{x}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{0}{\\sum (x^2_i - \\bar{x})} \\\\ &= 0 \\\\ \\\\ \\sum w_i x_i &= \\frac{\\sum x_i(x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x^2_i - \\bar{x} \\sum x_i)}{\\sum x^2_i - 2\\bar{x}\\sum x_i + \\sum \\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - \\bar{x}(n \\bar{x})}{\\sum x^2_i - 2\\bar{x}(n\\bar{x}) + n\\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - 2n\\bar{x}^2 + n\\bar{x}^2} \\\\ &=\\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - n\\bar{x}^2} \\\\ &= 1 \\\\ \\\\ \\sum w_i^2 &= \\frac{\\sum (x_i - \\bar{x})^2}{\\sum (x_i - \\bar{x})^4} \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i) &= n(\\frac{1}{n}) - \\bar{x} \\sum w_i \\\\ &= 1 - 0 \\\\ &= 1 \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i &= \\frac{1}{n} \\sum x_i - \\bar{x} \\sum w_i x_i \\\\ &= \\frac{1}{n} (n\\bar{x}) - \\bar{x} (1) \\\\ &= \\bar{x} - \\bar{x} \\\\ &= 0 \\end{aligned} \\] Using this, the Expectation & Variance can be determined: \\[ \\begin{aligned} E(\\hat{\\beta}_1) &= \\sum w_i E(y_i) \\\\ &= \\sum w_i E(\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum w_i + \\beta_1 \\sum w_i x_i \\\\ &= \\beta_0 (0) + \\beta_1 (1) \\\\ &= \\beta_1\\\\ \\\\ Var(\\hat{\\beta}_1) &= Var(\\sum w_i y_i) \\\\ &= \\sum w_i^2 Var (y_i) \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2} \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 \\sim N(\\beta_1, \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2}) \\] \\[ \\begin{aligned} E(\\hat{\\beta}_0) &= \\sum (\\frac{1}{n} - \\bar{x}w_i) E(y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i) (\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum (\\frac{1}{n} - \\bar{x}w_i) + \\beta_1 \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i \\\\ &= \\beta_0 (1) + \\beta_1 (0) \\\\ &= \\beta_0 \\\\ \\\\ Var(\\hat{\\beta}_0) &= Var(\\sum (\\frac{1}{n} - \\bar{x}w_i)y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i)^2 Var (y_i) \\\\ &= \\sigma^2 \\sum (\\frac{1}{n^2} -\\frac{2\\bar{x}w_i}{n} + \\bar{x}^2 w_i^2) \\\\ &= \\sigma^2 (\\sum \\frac{1}{n^2} - \\frac{2\\bar{x}}{n} \\sum w_i + \\bar{x}^2 \\sum w_i^2) \\\\ &= \\sigma^2 [n(\\frac{1}{n^2}) - \\frac{2\\bar{x}}{n} (0) + \\bar{x}^2 (\\frac{1}{\\sum (x_i - \\bar{x})^2})] \\\\ &= \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2}) \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_0 \\sim N(\\beta_0, \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2})) \\]","title":"Sampling Distributions"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Simple%20Linear%20Regression/#hypothesis-testing","text":"As mentioned in the overview section, hypothesis testing can be used to test for the presence of a relationship or its strength. Since the regression parameters are normally distributed, a z-statistic can be used to conduct the tests. However, since the population variance is not known, a t-statistic is used instead: \\[ \\begin{aligned} t &= \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\end{aligned} \\] Since the population variance is estimated by the MSE which has \\(n-2\\) degrees of freedom, the corresponding chi-squared and hence t-distribution has \\(n-2\\) degrees of freedom as well. \\[ t(\\hat{\\beta_1}) \\sim t_{n-2} \\] Both the F-statistic and t-statistic are equivalent ways of hypothesis testing and will always lead to the same conclusions. It is advised to use whichever statistic is more convenient to calculate based on the given information. \\[ t^2 = \\left(\\right) \\]","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Simple%20Linear%20Regression/#confidence-intervals","text":"Since the distribution of t-statistic is known, it can be used to determine the confidence interval of the estimate: \\[ P\\left(\\text{Margin of Error} < \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} < \\text{Margin of Error}\\right) = 1 - \\alpha \\] \\[ \\text{Confidence Interval} = \\hat{\\beta}_1 + t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_1}} \\]","title":"Confidence Intervals"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Simple%20Linear%20Regression/#prediction-intervals","text":"Consider the Prediction Error of the SLR model: \\[ y_* - \\hat{y_*} = \\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\] Since both \\(y_*\\) and \\(\\hat{y_*}\\) are normally distributed, the prediction errors are normally distributed as well: \\[ \\begin{aligned} E(y_* - \\hat{y_*}) &= E[\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)]] \\\\ &= 0 + \\beta_0 + \\beta_1 E(x_*) - \\beta_0 - \\beta_1 E(x_*) \\\\ &= 0 \\\\ \\\\ Var(y_* - \\hat{y_*}) &= Var(\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= Var(\\varepsilon_*) + Var[(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= ... \\\\ &= \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}] \\end{aligned} \\] \\[ \\therefore y_* - \\hat{y_*} \\sim N(0, \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}]) \\] Similar to before, since the population variance is unknown, it can be approximated using the MSE. Thus, a t-statistic constructed from this sample will have a t-distribution , allowing the following prediction interval to be calculated: \\[ \\text{Prediction Interval} = \\hat{y}_* + t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\] Notice that the standard error of the prediction interval increases as \\(x_i\\) moves further away from the mean, indicating that the predictions become less accurate for those values.","title":"Prediction Intervals"}]}