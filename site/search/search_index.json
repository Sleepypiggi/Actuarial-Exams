{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Actuarial Exam Notes \u00b6 Hello","title":"**Actuarial Exam Notes**"},{"location":"#actuarial-exam-notes","text":"Hello","title":"Actuarial Exam Notes"},{"location":"Placeholder/","text":"Work in progress! \u00b6","title":"FSA-ALM"},{"location":"Placeholder/#work-in-progress","text":"","title":"Work in progress!"},{"location":"2.%20Actuarial%20Mathematics/Overview%20of%20Insurance/","text":"","title":"Overview of Insurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/","text":"Profit Testing \u00b6 Overview \u00b6 In most other industries, the cost of the product is known beforehand, thus it is possible to determine its profitability beforehand. Unfortunately, due to the contingent nature of life assurances and annuities, it is impossible to determine profitability beforehand. Recall that insurers make certain assumptions about the policyholder and thus price the product accordingly. If the policyholder follows these assumptions exactly , then the resulting profit is known as the Expected Profit . In reality, the policyholder will deviate from the assumptions to varying extents, resulting in the Actual Profit . From an accounting perspective, what the insurer recognises as \"profit\" is the excess of actual profit over expected profit, also known as the Gain of the policy: \\[ \\text{Gain} = \\text{Actual Profit} - \\text{Expected Profit} \\] Expected Profit \u00b6 Cashflow Projections \u00b6 We first consider the cashflows that the insurer expects every policy year : At the start of the policy year, they collect premiums and pay expenses They invest the excess amount to earn interest at the end of the policy year They expect some policyholders to die and hence payout claims and any claims expenses at the end of the policy year Alternative Perspective If the insurer pays more expenses than premiums, then they have to borrow money , which means that they pay interest at the end of the policy year instead. The combination of these cashflows are known as the expected net cashflows for that policy year, calculated at the end of the policy year : \\[ \\text{Expected NCF} = (P - e)(1+i) - q_x \\cdot (B+E) \\] Survival Benefits For Annuities where there is instead a survival benefit, the insurer expects some policyholders to remain alive and hence pay them the survival benefits and any survival expenses , thus the payout should be multiplied by the survival probability instead : \\[ \\text{Expected NCF Annuity } = (P - e)(1+i) - p_x \\cdot (B^s+E^s) \\] For Endowments in particular, the survival benefit only occurs in the final year. Thus, in the final year , both the death and survival benefits must be accounted for: \\[ \\text{Expected NCF Final Year EA} = (P - e)(1+i) - q_x \\cdot (B+E) - p_x \\cdot (B^s+E^s) \\] Since the insurer holds reserves that cannot be touched, it can be thought of as an account balance : The reserve held at the beginning of the policy year is the opening balance Since the reserve is invested, it earns interest at the end of the policy year They receive the NCF at the end of the policy year They set up a new reserve at the end of the policy year ( closing balance ) for the expected remaining policyholders Since the closing balance is known, any excess over the closing balance is the expected profit over that policy year: \\[ \\begin{aligned} \\text{Expected Profit} &= {}_{t-1}V \\cdot (1+i) + \\text{Expected NCF} - p_x \\cdot {}_{t}V \\\\ &= ({}_{t-1}V + P - e)(1+i) - q_x \\cdot (B+E) - p_x \\cdot {}_{t}V \\end{aligned} \\] Different Interest Rates The interest earned on the reserves may be different from the interest used to compute the premiums and/or reserves. Be sure to read the question properly. This is equivalent to saying that the expected profit in each policy year is the combination of the NCFs AND the change in reserves for that policy year: \\[ \\begin{aligned} \\text{Expected Profit} &= \\text{Expected NCF} + {}_{t-1}V \\cdot (1+i) - p_x \\cdot {}_{t}V \\\\ &= \\text{Expected NCF} + \\text{Expected change in reserve} \\end{aligned} \\] There are two special cases for the reserves: Beginning of policy - Starting reserve is 0 End of policy - Ending reserve is 0 Profit Vector & Signature \u00b6 If the expected profit is calculated every policy year and collected together, then the resulting vector is known as the Profit Vector . For a typical contract, the profits are usually negative in the first (few) years and are small in magnitude across all years. This can be used to sense check the profit vector calculations. \\[ \\begin{aligned} \\text{PR} &= \\begin{pmatrix} \\text{PR}_1 \\\\ \\text{PR}_2 \\\\ \\vdots \\\\ \\text{PR}_t \\end{pmatrix} \\end{aligned} \\] Calculation Questions Most questions will only require computing the profit vector for a short period of time given the intensity needed to calculate the reserves at various points in time. If required to compute the profit vector for a long duration, the question will usually specify to ignore the change in reserves or simply provide them , which greatly simplifies the required calculations. However, the expected profit for each policy year makes the implicit assumption that the policyholder survives till the BEGINNING of that policy year . Thus, every element of the profit vector is calculated based off varying assumptions. This is not very useful and thus can be corrected by adjusting the profit vector with the probability of survival till the beginning each policy year. The resulting vector is the unconditional profit , known as the Profit Signature . \\[ \\begin{aligned} \\Pi &= \\begin{pmatrix} {}_{0}p_x \\\\ {}_{1}p_x \\\\ \\vdots \\\\ {}_{t-1}p_x \\end{pmatrix} * \\begin{pmatrix} \\text{PR}_1 \\\\ \\text{PR}_2 \\\\ \\vdots \\\\ \\text{PR}_t \\end{pmatrix} \\end{aligned} \\] Recall that \\({}_{0}p_x = 1\\) . Profit Measures \u00b6 Using the profit signature, insurers can gauge the profitability of the products using a variety of metrics. Apart from calculation, it is important to know how to explain how a change in any of the parameters affect these metrics. Net Present Value \u00b6 Net Present Value (NPV) follows its finance definition and is the excess of PV Inflows over Outflows . In this context, it is the sum of the present values of all elements within the profit signature: \\[ \\begin{aligned} \\text{NPV} &= \\left(v^1 \\quad v^2 \\quad \\dots \\quad v^t \\right) \\cdot \\begin{pmatrix} \\Pi_1 \\\\ \\Pi_2 \\\\ \\vdots \\\\ \\Pi_t \\end{pmatrix} \\\\ &= v^1 \\cdot \\Pi_1 + v^2 \\cdot \\Pi_2 + \\dots + v^t \\cdot \\Pi_t \\end{aligned} \\] Net Expected Present Value Note that the NPV is calculated based off the profit signature, NOT the profit vector. Thus, it is more appropriately called the Net Expected Present Value , as survival probabilities are taken into account as well. The interest rate used to discount the cashflows is known as the Discount Rate , and is usually different from the earned interest rate or the one used in pricing/reserving. Internal Rate of Return \u00b6 The Internal Rate of Return (IRR) is the Discount Rate that will result in an NPV of 0 : \\[ \\text{NPV}_\\text{IRR} = 0 \\] The IRR cannot be calculated through an equation and must be solved using numerical methods. However, it can also be calculated using the cashflow function in most financial calculators. Discounted Payback Period \u00b6 The Discounted Payback Period (DPP) is the earliest time in the contract which the NPV becomes positive : \\[ \\text{DPP} = min [t: NPV(t) > 0] \\] It is best calculated by incrementally calculating the NPV at various policy years, stopping when the NPV becomes positive. Profit Margin \u00b6 The Profit Margin ( \\(\\pi\\) ) is the ratio of the NPV to the EPV of premiums: \\[ \\pi = \\frac{\\text{NPV}}{\\text{EPV Premiums}} \\] Note that the EPV of premiums are discounted using the same interest as the NPV . Pricing \u00b6 A more realistic way to price would be to consider the actual cashflows and thus pricing the policy such that the desired profitability is achieved . This is done by leaving the premium as a variable within the profit expressions and solving for it. Alternatively, the current premium and profitability of the policy may be provided. In this scenario, it may be easier to consider the change in premium and the corresponding change in profitability instead of calculating it directly. Reserving \u00b6 Insurers may want to set reserves such that they avoid negative profits throughout the lifetime of the policy. This is done by setting reserves such that the profits are floored at 0 . The reserves are calculated recursively starting from the last policy year : Recall that in the last policy year, the ending reserve is 0 Set the profit to 0 & solve for the starting reserve If the starting reserve is negative, floor it at 0 Use the calculated amount above as the new ending reserve for the previous year & repeat the process until all reserves are calculated Since this process sets negative reserves to 0, it is known as Zeroization Alternatively, the question may provide the current profit for each year, with the profit in some years being negative. The goal is then to calculate the reserves such that the negative profits are floored to 0. The key is to understand that if the profit for all future years are positive, then the starting reserve for that year will always be floored at 0 . Thus, using this, the reserves are recursively calculated: Start from the latest policy year with negative profits Stop once there are no more negative profits There is no need to calculate the reserve for earlier years once all negative profits have been set to 0 because following the initial logic, the reserve for these earlier years will be 0 given that all future profits are non-negative! After Zeroization, recalculate the profit for all years: For years with negative profits where zeroization occurred, the profit is 0 For years with positive profits but were NOT just before negative profit years, the profit is unchanged For years with positive profits but were JUST before negative profit years, the profit is changed The profits have changed because there is now a new starting reserve for the following year with negative profits due to the zeroization. This affects the amount of reserve that has to be set up at the end of the current year, resulting in a new profit. If this new profit turns out to be negative as well , then repeat the process once more. Gains by Source \u00b6 As alluded to earlier, actual profits are simply profits that are based on what actually happens rather than what was assumed. From this and the expected profit, the gain from the policy is determined. The difference in actual versus expected (AvE) assumptions typically comes in three aspects - Expenses, Interest & Mortality. Thus, the gain can also be decomposed into these three aspects to precisely pinpoint which assumptions need tweaking. \\[ \\begin{aligned} \\text{Total Gain} &= \\text{Actual Profit} - \\text{Expected Profit} \\\\ &= \\text{Expense Gain} + \\text{Interest Gain} + \\text{Mortality Gain} \\end{aligned} \\] For this section, Expected and Actual variables are differentiated using \" \\('\\) \". Expense Gain \u00b6 Since expense is a cash outflow , gain is recognised when the expected outflow is larger than the actual outflow. \\[ \\begin{aligned} \\text{Expense Gain} &= \\text{Assumed Expense Profit} - \\text{Actual Expense Profit} \\\\ &= e(1+i) - q_x \\cdot E - (e'(1+i) - q_x \\cdot E') \\\\ &= (e-e')(1+i) - q_x (E-E') \\end{aligned} \\] Note that since only the gain due to expenses are desired, only the expense assumptions go through the AvE process. The rest of the assumptions follow what was expected. Interest Gain \u00b6 On the flipside, since interest is a cash inflow , gain is recognised when the expected inflow is smaller than the actual outflow. \\[ \\begin{aligned} \\text{Interest Gain} &= \\text{Actual Interest Profit} - \\text{Expected Interest Profit} \\\\ &= ({}_{t-1}V + P - e') \\cdot i' - ({}_{t-1}V + P - e') \\cdot i \\\\ &= (i' - i)({}_{t-1}V + P - e') \\end{aligned} \\] Similarly, since only the gain due to interest is desired, only the interest assumptions go through the AvE process. However, since the expense gain was already calculated , the actual expense is directly used. Thus, the order of calculation is important as it determines whether the actual or expected values are used - if the gain has already been calculated, then in all subsequent calculations, only the actual assumptions should be used. TBC the reason for this Mortality Gain \u00b6 Similarly, mortality is a cash outflow , gain is recognised when the expected outflow is larger than the actual outflow. \\[ \\begin{aligned} \\text{Mortality Gain} &= \\text{Assumed Mortality Profit} - \\text{Actual Mortality Profit} \\\\ &= q_x \\cdot (B+E') + (1-q_x) \\cdot {}_{t}V - [q'_x \\cdot (B+E') + (1-q'_x) \\cdot {}_{t}V] \\\\ &= q_x \\cdot (B+E') + {}_{t}V - q_x \\cdot {}_{t}V - [q'_x \\cdot (B+E') + {}_{t}V - q'_x \\cdot {}_{t}V] \\\\ &= q_x \\cdot (B+E') + {}_{t}V - q_x \\cdot {}_{t}V - q'_x \\cdot (B+E') - {}_{t}V + q'_x \\cdot {}_{t}V \\\\ &= (q_x - q'_x)(B+E') - (q_x - q'_x){}_{t}V \\\\ &= (q_x - q'_x)(B+E'-{}_{t}V) \\end{aligned} \\] The second term is also known as the Net Amount at Risk , which represents the additional amount that the insurer has to pay in the event of a claim. In practice, the question may provide the actual number of policyholders (NOP) instead of the probabilities. Thus, we need to solve for the actual probabilities using: \\[ q'_x = \\frac{\\text{NOP}_\\text{Beginning} - \\text{NOP}_\\text{Ending}}{\\text{NOP}_\\text{Beginning}} \\] For annuities, the expression is slightly different (TBC) \\[ \\begin{aligned} \\text{Mortality Gain} &= \\text{Assumed Mortality Profit} - \\text{Actual Mortality Profit} \\\\ &= p_x \\cdot (B+E') + p_x \\cdot {}_{t}V - [p'_x \\cdot (B+E') + p'_x \\cdot {}_{t}V] \\\\ &= p_x \\cdot (B + E' + {}_{t}V) - p'_x (B + E' + {}_{t}V) \\\\ &= (p_x - p'_x)(B + E' + {}_{t}V) \\end{aligned} \\] Per Policy Gain \u00b6 All the above calculations are known as the Per Policy gains - if the actual number of policies are given, then each gain needs to be multiplied by the NOP to determine the total gain made by the insurer.","title":"Profit Testing"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#profit-testing","text":"","title":"Profit Testing"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#overview","text":"In most other industries, the cost of the product is known beforehand, thus it is possible to determine its profitability beforehand. Unfortunately, due to the contingent nature of life assurances and annuities, it is impossible to determine profitability beforehand. Recall that insurers make certain assumptions about the policyholder and thus price the product accordingly. If the policyholder follows these assumptions exactly , then the resulting profit is known as the Expected Profit . In reality, the policyholder will deviate from the assumptions to varying extents, resulting in the Actual Profit . From an accounting perspective, what the insurer recognises as \"profit\" is the excess of actual profit over expected profit, also known as the Gain of the policy: \\[ \\text{Gain} = \\text{Actual Profit} - \\text{Expected Profit} \\]","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#expected-profit","text":"","title":"Expected Profit"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#cashflow-projections","text":"We first consider the cashflows that the insurer expects every policy year : At the start of the policy year, they collect premiums and pay expenses They invest the excess amount to earn interest at the end of the policy year They expect some policyholders to die and hence payout claims and any claims expenses at the end of the policy year Alternative Perspective If the insurer pays more expenses than premiums, then they have to borrow money , which means that they pay interest at the end of the policy year instead. The combination of these cashflows are known as the expected net cashflows for that policy year, calculated at the end of the policy year : \\[ \\text{Expected NCF} = (P - e)(1+i) - q_x \\cdot (B+E) \\] Survival Benefits For Annuities where there is instead a survival benefit, the insurer expects some policyholders to remain alive and hence pay them the survival benefits and any survival expenses , thus the payout should be multiplied by the survival probability instead : \\[ \\text{Expected NCF Annuity } = (P - e)(1+i) - p_x \\cdot (B^s+E^s) \\] For Endowments in particular, the survival benefit only occurs in the final year. Thus, in the final year , both the death and survival benefits must be accounted for: \\[ \\text{Expected NCF Final Year EA} = (P - e)(1+i) - q_x \\cdot (B+E) - p_x \\cdot (B^s+E^s) \\] Since the insurer holds reserves that cannot be touched, it can be thought of as an account balance : The reserve held at the beginning of the policy year is the opening balance Since the reserve is invested, it earns interest at the end of the policy year They receive the NCF at the end of the policy year They set up a new reserve at the end of the policy year ( closing balance ) for the expected remaining policyholders Since the closing balance is known, any excess over the closing balance is the expected profit over that policy year: \\[ \\begin{aligned} \\text{Expected Profit} &= {}_{t-1}V \\cdot (1+i) + \\text{Expected NCF} - p_x \\cdot {}_{t}V \\\\ &= ({}_{t-1}V + P - e)(1+i) - q_x \\cdot (B+E) - p_x \\cdot {}_{t}V \\end{aligned} \\] Different Interest Rates The interest earned on the reserves may be different from the interest used to compute the premiums and/or reserves. Be sure to read the question properly. This is equivalent to saying that the expected profit in each policy year is the combination of the NCFs AND the change in reserves for that policy year: \\[ \\begin{aligned} \\text{Expected Profit} &= \\text{Expected NCF} + {}_{t-1}V \\cdot (1+i) - p_x \\cdot {}_{t}V \\\\ &= \\text{Expected NCF} + \\text{Expected change in reserve} \\end{aligned} \\] There are two special cases for the reserves: Beginning of policy - Starting reserve is 0 End of policy - Ending reserve is 0","title":"Cashflow Projections"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#profit-vector-signature","text":"If the expected profit is calculated every policy year and collected together, then the resulting vector is known as the Profit Vector . For a typical contract, the profits are usually negative in the first (few) years and are small in magnitude across all years. This can be used to sense check the profit vector calculations. \\[ \\begin{aligned} \\text{PR} &= \\begin{pmatrix} \\text{PR}_1 \\\\ \\text{PR}_2 \\\\ \\vdots \\\\ \\text{PR}_t \\end{pmatrix} \\end{aligned} \\] Calculation Questions Most questions will only require computing the profit vector for a short period of time given the intensity needed to calculate the reserves at various points in time. If required to compute the profit vector for a long duration, the question will usually specify to ignore the change in reserves or simply provide them , which greatly simplifies the required calculations. However, the expected profit for each policy year makes the implicit assumption that the policyholder survives till the BEGINNING of that policy year . Thus, every element of the profit vector is calculated based off varying assumptions. This is not very useful and thus can be corrected by adjusting the profit vector with the probability of survival till the beginning each policy year. The resulting vector is the unconditional profit , known as the Profit Signature . \\[ \\begin{aligned} \\Pi &= \\begin{pmatrix} {}_{0}p_x \\\\ {}_{1}p_x \\\\ \\vdots \\\\ {}_{t-1}p_x \\end{pmatrix} * \\begin{pmatrix} \\text{PR}_1 \\\\ \\text{PR}_2 \\\\ \\vdots \\\\ \\text{PR}_t \\end{pmatrix} \\end{aligned} \\] Recall that \\({}_{0}p_x = 1\\) .","title":"Profit Vector &amp; Signature"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#profit-measures","text":"Using the profit signature, insurers can gauge the profitability of the products using a variety of metrics. Apart from calculation, it is important to know how to explain how a change in any of the parameters affect these metrics.","title":"Profit Measures"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#net-present-value","text":"Net Present Value (NPV) follows its finance definition and is the excess of PV Inflows over Outflows . In this context, it is the sum of the present values of all elements within the profit signature: \\[ \\begin{aligned} \\text{NPV} &= \\left(v^1 \\quad v^2 \\quad \\dots \\quad v^t \\right) \\cdot \\begin{pmatrix} \\Pi_1 \\\\ \\Pi_2 \\\\ \\vdots \\\\ \\Pi_t \\end{pmatrix} \\\\ &= v^1 \\cdot \\Pi_1 + v^2 \\cdot \\Pi_2 + \\dots + v^t \\cdot \\Pi_t \\end{aligned} \\] Net Expected Present Value Note that the NPV is calculated based off the profit signature, NOT the profit vector. Thus, it is more appropriately called the Net Expected Present Value , as survival probabilities are taken into account as well. The interest rate used to discount the cashflows is known as the Discount Rate , and is usually different from the earned interest rate or the one used in pricing/reserving.","title":"Net Present Value"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#internal-rate-of-return","text":"The Internal Rate of Return (IRR) is the Discount Rate that will result in an NPV of 0 : \\[ \\text{NPV}_\\text{IRR} = 0 \\] The IRR cannot be calculated through an equation and must be solved using numerical methods. However, it can also be calculated using the cashflow function in most financial calculators.","title":"Internal Rate of Return"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#discounted-payback-period","text":"The Discounted Payback Period (DPP) is the earliest time in the contract which the NPV becomes positive : \\[ \\text{DPP} = min [t: NPV(t) > 0] \\] It is best calculated by incrementally calculating the NPV at various policy years, stopping when the NPV becomes positive.","title":"Discounted Payback Period"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#profit-margin","text":"The Profit Margin ( \\(\\pi\\) ) is the ratio of the NPV to the EPV of premiums: \\[ \\pi = \\frac{\\text{NPV}}{\\text{EPV Premiums}} \\] Note that the EPV of premiums are discounted using the same interest as the NPV .","title":"Profit Margin"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#pricing","text":"A more realistic way to price would be to consider the actual cashflows and thus pricing the policy such that the desired profitability is achieved . This is done by leaving the premium as a variable within the profit expressions and solving for it. Alternatively, the current premium and profitability of the policy may be provided. In this scenario, it may be easier to consider the change in premium and the corresponding change in profitability instead of calculating it directly.","title":"Pricing"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#reserving","text":"Insurers may want to set reserves such that they avoid negative profits throughout the lifetime of the policy. This is done by setting reserves such that the profits are floored at 0 . The reserves are calculated recursively starting from the last policy year : Recall that in the last policy year, the ending reserve is 0 Set the profit to 0 & solve for the starting reserve If the starting reserve is negative, floor it at 0 Use the calculated amount above as the new ending reserve for the previous year & repeat the process until all reserves are calculated Since this process sets negative reserves to 0, it is known as Zeroization Alternatively, the question may provide the current profit for each year, with the profit in some years being negative. The goal is then to calculate the reserves such that the negative profits are floored to 0. The key is to understand that if the profit for all future years are positive, then the starting reserve for that year will always be floored at 0 . Thus, using this, the reserves are recursively calculated: Start from the latest policy year with negative profits Stop once there are no more negative profits There is no need to calculate the reserve for earlier years once all negative profits have been set to 0 because following the initial logic, the reserve for these earlier years will be 0 given that all future profits are non-negative! After Zeroization, recalculate the profit for all years: For years with negative profits where zeroization occurred, the profit is 0 For years with positive profits but were NOT just before negative profit years, the profit is unchanged For years with positive profits but were JUST before negative profit years, the profit is changed The profits have changed because there is now a new starting reserve for the following year with negative profits due to the zeroization. This affects the amount of reserve that has to be set up at the end of the current year, resulting in a new profit. If this new profit turns out to be negative as well , then repeat the process once more.","title":"Reserving"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#gains-by-source","text":"As alluded to earlier, actual profits are simply profits that are based on what actually happens rather than what was assumed. From this and the expected profit, the gain from the policy is determined. The difference in actual versus expected (AvE) assumptions typically comes in three aspects - Expenses, Interest & Mortality. Thus, the gain can also be decomposed into these three aspects to precisely pinpoint which assumptions need tweaking. \\[ \\begin{aligned} \\text{Total Gain} &= \\text{Actual Profit} - \\text{Expected Profit} \\\\ &= \\text{Expense Gain} + \\text{Interest Gain} + \\text{Mortality Gain} \\end{aligned} \\] For this section, Expected and Actual variables are differentiated using \" \\('\\) \".","title":"Gains by Source"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#expense-gain","text":"Since expense is a cash outflow , gain is recognised when the expected outflow is larger than the actual outflow. \\[ \\begin{aligned} \\text{Expense Gain} &= \\text{Assumed Expense Profit} - \\text{Actual Expense Profit} \\\\ &= e(1+i) - q_x \\cdot E - (e'(1+i) - q_x \\cdot E') \\\\ &= (e-e')(1+i) - q_x (E-E') \\end{aligned} \\] Note that since only the gain due to expenses are desired, only the expense assumptions go through the AvE process. The rest of the assumptions follow what was expected.","title":"Expense Gain"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#interest-gain","text":"On the flipside, since interest is a cash inflow , gain is recognised when the expected inflow is smaller than the actual outflow. \\[ \\begin{aligned} \\text{Interest Gain} &= \\text{Actual Interest Profit} - \\text{Expected Interest Profit} \\\\ &= ({}_{t-1}V + P - e') \\cdot i' - ({}_{t-1}V + P - e') \\cdot i \\\\ &= (i' - i)({}_{t-1}V + P - e') \\end{aligned} \\] Similarly, since only the gain due to interest is desired, only the interest assumptions go through the AvE process. However, since the expense gain was already calculated , the actual expense is directly used. Thus, the order of calculation is important as it determines whether the actual or expected values are used - if the gain has already been calculated, then in all subsequent calculations, only the actual assumptions should be used. TBC the reason for this","title":"Interest Gain"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#mortality-gain","text":"Similarly, mortality is a cash outflow , gain is recognised when the expected outflow is larger than the actual outflow. \\[ \\begin{aligned} \\text{Mortality Gain} &= \\text{Assumed Mortality Profit} - \\text{Actual Mortality Profit} \\\\ &= q_x \\cdot (B+E') + (1-q_x) \\cdot {}_{t}V - [q'_x \\cdot (B+E') + (1-q'_x) \\cdot {}_{t}V] \\\\ &= q_x \\cdot (B+E') + {}_{t}V - q_x \\cdot {}_{t}V - [q'_x \\cdot (B+E') + {}_{t}V - q'_x \\cdot {}_{t}V] \\\\ &= q_x \\cdot (B+E') + {}_{t}V - q_x \\cdot {}_{t}V - q'_x \\cdot (B+E') - {}_{t}V + q'_x \\cdot {}_{t}V \\\\ &= (q_x - q'_x)(B+E') - (q_x - q'_x){}_{t}V \\\\ &= (q_x - q'_x)(B+E'-{}_{t}V) \\end{aligned} \\] The second term is also known as the Net Amount at Risk , which represents the additional amount that the insurer has to pay in the event of a claim. In practice, the question may provide the actual number of policyholders (NOP) instead of the probabilities. Thus, we need to solve for the actual probabilities using: \\[ q'_x = \\frac{\\text{NOP}_\\text{Beginning} - \\text{NOP}_\\text{Ending}}{\\text{NOP}_\\text{Beginning}} \\] For annuities, the expression is slightly different (TBC) \\[ \\begin{aligned} \\text{Mortality Gain} &= \\text{Assumed Mortality Profit} - \\text{Actual Mortality Profit} \\\\ &= p_x \\cdot (B+E') + p_x \\cdot {}_{t}V - [p'_x \\cdot (B+E') + p'_x \\cdot {}_{t}V] \\\\ &= p_x \\cdot (B + E' + {}_{t}V) - p'_x (B + E' + {}_{t}V) \\\\ &= (p_x - p'_x)(B + E' + {}_{t}V) \\end{aligned} \\]","title":"Mortality Gain"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#per-policy-gain","text":"All the above calculations are known as the Per Policy gains - if the actual number of policies are given, then each gain needs to be multiplied by the NOP to determine the total gain made by the insurer.","title":"Per Policy Gain"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/","text":"Survival Models \u00b6 Overview \u00b6 Survival Models are probability distributions that measure the time to failure of an entity, or phrased another way, the future lifetime of an entity. In an Actuarial context, it measures the time to death of a person. Since survival models measure time , they are denoted by the continuous random variable \\(T_x\\) , where the subscript \\(x\\) represents the age of the person. Newborn Lifetime \u00b6 The base survival model measures the future lifetime of a newborn ( aged 0 ), denoted by the random variable \\(T_0\\) . The CDF thus represents the probability that the newborn dies before age \\(t\\) : \\[ F_0(t) = P(T_0 \\le t) \\] The Survival Function is the complement of the CDF and is the probability that the newborn survives till a certain age \\(t\\) . From a different perspective, it can also be seen as the probability of the newborn dying AFTER age \\(t\\) : \\[ \\begin{aligned} S_0(t) &= P(T_0 \\ge t) \\\\ &= 1 - P(T_0 \\le t) \\\\ &= 1 - F_0(t) \\end{aligned} \\] Death is inevitable, which means that the probability of living forever must be 0: \\[ \\begin{aligned} \\lim_{t \\to \\infty} S_0 (t) = 0 \\iff \\lim_{t \\to \\infty} F_0 (t) = 1 \\end{aligned} \\] Continuous Lifetime \u00b6 The above can be generalized for a person aged \\(x\\) , denoted by \\(T_x\\) . However, the intepretation of the two functions changes slightly. The CDF instead represents the probability that the person dies within \\(t\\) years from age \\(x\\) : \\[ \\begin{aligned} F_x(t) &= P(T_x \\le t) \\end{aligned} \\] It can also be written in terms of the newborn lifetime . The key is to understand that the newborn is assumed to survive till age \\(x\\) and then die within the next \\(t\\) years: \\[ \\begin{aligned} P(T_x \\le t) &= P(T_0 \\le x + t | T_0 > x) \\\\ &= \\frac{P(x < T_0 < x +t)}{P(T_0 > x)} \\\\ &= \\frac{P(T_0 < x +t)-P(T_0 < x)}{P(T_0 > x)} \\\\ &= \\frac{F_0(x+t) - F_0(x)}{S_0(x)} \\end{aligned} \\] Conversely, the survival function is the probability that the person survives another \\(t\\) years or dies AFTER \\(t\\) years : \\[ S_x(t) = P(T_x \\ge t) \\] It can also be written in terms of the newborn lifetime following the same logic as above. The newborn is assumed to survive till age \\(x\\) and then survive another \\(t\\) years: \\[ \\begin{aligned} P(T_x \\ge t) &= P(T_0 \\ge x + t | T_0 > x) \\\\ &= \\frac{P(T_0 > x + t)}{P(T_0 > x)} \\\\ &= \\frac{S_0(x+t)}{S_0(x)} \\end{aligned} \\] Note Since the surviving till age \\(x\\) is a subset of surviving till age \\(x+t\\) , the numerator can be simplified to just the probability of surviving till age \\(x+t\\) . The survival function can thus be re-written as the following: \\[ \\begin{aligned} S_x(t) &= \\frac{S_0(x+t)}{S_0(x)} \\\\ S_0(x+t) &= S_0(x)S_x(t) \\\\ \\end{aligned} \\] The probability of the survival of a newborn till age \\(x+t\\) is the product of: The probability of survival of a newborn till age \\(x\\) Given that the newborn survived till age \\(x\\) , that they will survive another \\(t\\) years This can be generalized for people of any age to obtain: \\[ S_x(t+u) = S_x(t) S_{x+t}(u) \\] Remember to update the age! It is a common mistake to forget to use the \"new\" age of the person in the second expression. The opposite of the above is when the person surives another \\(t\\) years and then dies within the following \\(u\\) years . This is known as the probability of Deferred Death : \\[ \\begin{aligned} P(t < T_x < t+u) &= P(T_x < t+u) - P(T_x < t) \\\\ &= F_x(t+u) - F_x(t) \\\\ &= 1 - S_x(t+u) - (1 - S_x(t)) \\\\ &= 1 - S_x(t)S_{x+t}(u) - 1 + S_x(t) \\\\ &= S_x(t) - S_x(t)S_{x+t}(u) \\\\ &= S_x(t)(1-S_{x+t}(u)) \\\\ &= S_x(t)F_{x+t}(u) \\end{aligned} \\] Probability Tree Perspective \u00b6 Since the probabilites of death and survival are conditional on the age of the person, they can be better expressed in the form of a probability tree: Thus, it can be shown through recursion that the probability of surviving \\(t\\) years is equal to the sum of the probabilities of deferred deaths for every year after: \\[ \\begin{aligned} S_x(1) &= S_x(1)F_{x+1}(1) + S_x(1)S_{x+1}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(1) [S_{x+1}(1)F_{x+2}(1) + S_{x+1}(1)S_{x+2}(1)] \\\\ &= S_x(1)F_{x+1}(1) + S_x(1) S_{x+1}(1)F_{x+2}(1) + S_x(1) S_{x+1}(1)S_{x+2}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(2)F_{x+2}(1) + S_x(2)S_{x+2}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(2)F_{x+2}(1) + \\dots \\\\ \\\\ \\therefore S_x(t) &= \\sum S_x(t)F{x+t}(1) \\end{aligned} \\] Recall that the probability of surviving till a certain age is simply the probability that the person will die sometime after that age. The above expression solidifies this, where the probability of survival is equivalent to the probability of dying in ever possible age after . Force of Mortality \u00b6 The Force of Mortality is the probability of dying instantly ; in an infinitely small unit of time (normalized by unit time): \\[ \\begin{aligned} \\mu_{0+x} &= \\lim_{h \\to 0} \\frac{P(T_x < h)}{h} \\\\ &= \\lim_{h \\to 0} \\frac{1 - S_x(h)}{h} \\\\ &= \\lim_{h \\to 0} \\frac{1 - (\\frac{S_0(x+h)}{S_0(x)})}{h} \\\\ &= \\lim_{h \\to 0} \\frac{S_0(x) - S_0(x+h)}{S_0(x)h} \\\\ &= \\frac{1}{S_0(x)} \\lim_{h \\to 0} \\frac{S_0(x) - S_0(x+h)}{h} \\\\ &= - \\frac{1}{S_0(x)} \\lim_{h \\to 0} \\frac{S_0(x+h) - S_0(x)}{h} \\\\ &= - \\frac{1}{S_0(x)} S'_0(x) \\\\ &= - \\frac{S'_0(x)}{S_0(x)} \\end{aligned} \\] Since \\(S_0(x)\\) is the complement of \\(F_0(x)\\) , the derivative of the survival function can be written in terms of the PDF of the future lifetime: \\[ \\begin{aligned} f_0(x) &= \\frac{d}{dx} F_0(x) \\\\ &= \\frac{d}{dx} (1-S_0(x)) \\\\ &= -S'_0(x) \\end{aligned} \\] The force of mortality can be rewritten as the following: \\[ \\mu_{0+x} = \\frac{f_0(x)}{S_0(x)} \\] Thus, the force of mortality can also be intepreted as the CONDITIONAL distribution of future lifetime for a person aged \\(x\\) , as it is conditional on the person surviving till age \\(x\\) . Following that, \\(f_0(x)\\) is the UNCONDITIONAL distribution of the future lifetime as it does not assume the person lives to any age. Alternative Death Function \u00b6 More generally, the force of mortality can be rewritten as: \\[ \\begin{aligned} \\mu_{0+x} &= \\frac{f_0(x)}{S_0(x)} \\\\ \\mu_{x+t} &= \\frac{f_x(t)}{S_x(t)} \\\\ f_x(t) &= S_x(t)\\mu_{x+t} \\\\ F_x(t) &= \\int_0^t S_x(t)\\mu_{x+t} \\end{aligned} \\] The probability of death in a given interval is the sum of the various probabilities of surviving till each age and then dying instantly afterwards, which can be computed via integration. Following this definition, for extremely small time intervals , the force of mortality can be used to approximate the probability of death in that interval: \\[ P(T_x < h) \\approx h * \\mu_{x} \\] Alternative Survival Function \u00b6 More generally, the force of mortality can be rewritten as: \\[ \\begin{aligned} \\mu_{0+x} &= - \\frac{S'_0(x)}{S_0(x)} \\\\ \\mu_{x + t} &= \\frac{-S'_x(t)}{S_x(t)} \\\\ \\mu_{x + t} &= - \\frac{d}{dt} (\\ln S_x(t)) \\\\ \\ln S_x(t) &= - \\int_{0}^{t} \\mu_{x + t} \\\\ S_x(t) &= e ^ {- \\int_{0}^{t} \\mu_{x + t}} \\\\ \\end{aligned} \\] Note that most expressions give \\(\\mu_x\\) , but it is \\(\\mu_{x+t}\\) that is usually of interest - simply adjust the parameters accordingly. Thus, if \\(\\mu_x\\) is known for all ages, then the survival probabilites for any age can be calculated from it. In other words, the distribution of \\(T_x\\) can be determined from the force of mortality . Actuarial Notation \u00b6 Given how often these expressions are used, they are often abbreviated using the International Actuarial Notation : \\[ \\begin{aligned} {}_{t}p_{x} &= S_x(t) \\\\ {}_{t}q_{x} &= F_x(t) \\\\ {}_{t}q_{x} + {}_{t}p_{x} &= 1 \\end{aligned} \\] Convention for Notation If \\(t=1\\) , it is usually omitted - EG. \\({}_{1}q_{x} = q_{x}\\) . Deferred Deaths are expressed using the pipe symbol isntead: \\[ \\begin{aligned} {}_{t|u}q_{x} &= S_x(t)F_{x+t}(u) \\\\ &= {}_{t}p_{x} * {}_{u}q_{x+t} \\end{aligned} \\] Continuous Expectation \u00b6 The Expectation of the distribution of future lifetime is the Life Expectancy of the person. It is commonly calculated for newborns as a measure of the general health of the population. In an actuarial context, it is known as the Complete Expectation of Life : \\[ \\begin{aligned} E(T_x) &= \\int_{0}^{\\infty} t f_x(t) \\\\ e_{x} &= \\int_{0}^{\\infty} t S_x(t)\\mu_{x+t} \\end{aligned} \\] Through integration by parts, \\[ \\begin{aligned} \\mathring{e}_{x} &= \\left(\\Bigl[-tS_x(t)\\Bigr]_0^\\infty + \\int_0^\\infty S_x(t)\\right) \\\\ &= \\int_0^\\infty {}_{t}p_x \\end{aligned} \\] Inevitable Death Recall that death is inevitable - thus, the probability of living forever ( \\([-t * S_x(t)]^{\\infty}_0\\) ) must be 0. If the future lifetime variable is artificially limited to \\(n\\) years , then the expectation of the future lifetime is known as the Term Expectation of Life : \\[ \\begin{aligned} E \\left(min(T_x, n) \\right) &= \\int^{n}_0 {}_{t}p_x \\\\ \\mathring{e}_{x:\\enclose{actuarial}{n}} &= \\int^{n}_0 {}_{t}p_x \\end{aligned} \\] This allows the complete expectation can be decomposed into two components: Term Expectation at the current age representing the \"early\" years Complete Expectation at a future age represenging the \"later\" years \\[ \\mathring{e}_x = \\mathring{e}_{x:\\enclose{actuarial}{n}} + \\mathring{e}_{x+n} \\] However, the above makes an implicit assumption that the person will survive the first n years. Thus, the second term needs to account for the probability of surviving those n years: \\[ \\mathring{e}_x = \\mathring{e}_{x:n} + {}_{n}p_x \\cdot \\mathring{e}_{x+n} \\] Continuous Variance \u00b6 The second moment can be calculated in a similar fashion through integration by parts: \\[ \\begin{aligned} E(T^2_x) &= \\int_{0}^{\\infty} t^2 f_x(t) \\\\ &= -\\left(\\Bigl[t^2 S_x(t)\\Bigr]_0^\\infty - \\int_{0}^{\\infty} 2tS_x(t) \\right) \\\\ &= 2 \\int_{0}^{\\infty} t \\cdot {}_{t}p_x \\end{aligned} \\] Thus, the Variance of the future lifetime about the Expectation is the following: \\[ \\begin{aligned} Var(T_x) &= E(T^2_x) - [E(T_x)]^2 \\\\ &= 2 \\int_{0}^{\\infty} t \\cdot {}_{t}p_x - \\left(\\int_0^\\infty {}_{t}p_x \\right)^2 \\end{aligned} \\] Discrete Lifetime \u00b6 If only the integer components of the future lifetime are considered, then it becomes a discrete distribution known as the Curtate Future Lifetime of the person. It is denoted by the random variable \\(K_x\\) , which is a truncated version of \\(T_x\\) . Formally, it is known as a Floor Function : \\[ \\begin{aligned} K_x &= \\lfloor T_x \\rfloor \\end{aligned} \\] It measures the number of full years that the person is expected to live for. If \\(K_x = k\\) , then it means the person will another \\(k\\) full years but not live past \\(k+1\\) years full years. In other words, they will die between \\(k\\) (inclusive) and \\(k+1\\) (Exclusive) years . The distribution starts from 0 , where \\(K_x = 0\\) represents a newborn that will die within their birth year. Similarly, the probability that a person will die within \\(k\\) years is given by \\(K_x = k-1\\) , denoting that they will live \\(k-1\\) full years and die before reaching the kth year mark. \\[ \\begin{aligned} P(K_x = k) &= P(x+k \\le T_0 < x+k+1) \\\\ &= P(k \\le T_x < k+1) \\\\ &= P(T_x < k+1) - P(T_x \\le k) \\\\ &= F_x(k+1) - F_x(k) \\\\ &= S_x(k) - S_x(k+1) \\\\ &= S_x(k) - S_x(k) * S_{x+k}(1) \\\\ &= S_x(k) (1 - S_{x+k}(1)) \\\\ &= S_x(k) F_{x+k}(1) \\\\ &= {} {}_{k}p_{x} {}_{}q_{x + k} \\\\ &= {}_{k|}q_{x} \\end{aligned} \\] Discrete Expectation \u00b6 Similarly, the Expectation of Curtate Lifetime is the expectation of \\(K_x\\) : \\[ \\begin{aligned} e_x &= E(K_x) \\\\ &= \\sum_{k=0}^{\\infty} k ({}_{k}p_{x} - {}_{k+1}p_{x}) \\\\ &= 1 ({}_{1}p_{x} - {}_{2}p_{x}) + 2 ({}_{2}p_{x} - {}_{3}p_{x}) + 3 ({}_{3}p_{x} - {}_{4}p_{x}) + ... \\\\ &= {}_{1}p_{x} - {}_{2}p_{x} + 2{}_{2}p_{x} - 2{}_{3}p_{x} + 3 {}_{3}p_{x} - 3 {}_{4}p_{x} \\\\ &= {}_{1}p_{x} + 2{}_{2}p_{x} + 3 {}_{3}p_{x} + ... \\\\ &= \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\end{aligned} \\] The expectation can also recursively calculated: If the person dies within the year with probability \\(q_x\\) , then the future lifetime is 0 If the person survives the year with probability \\(p_x\\) , then the future lifetime is 1 (for surviving the current year) AND the expected lifetime past that \\[ \\begin{aligned} e_x &= \\begin{cases} 0,& q_x \\\\ 1+e_{x+1},& p_x \\end{cases} \\\\ \\\\ \\therefore e_x &= p_x (1+e_{x+1}) \\end{aligned} \\] Discrete Variance \u00b6 The second moment can be calculated in a similar fashion: \\[ \\begin{aligned} E(K^2_x) &= \\sum_{k=0}^{\\infty} k^2 ({}_{k}p_{x} - {}_{k+1}p_{x}) \\\\ &= 1 ({}_{1}p_{x} - {}_{2}p_{x}) + 4 ({}_{2}p_{x} - {}_{3}p_{x}) + 9 ({}_{3}p_{x} - {}_{4}p_{x}) + ... \\\\ &= {}_{1}p_{x} - {}_{2}p_{x} + 4{}_{2}p_{x} - 4{}_{3}p_{x} + 9 {}_{3}p_{x} - 9 {}_{4}p_{x} \\\\ &= {}_{1}p_{x} + 3{}_{2}p_{x} + 5 {}_{3}p_{x} + ... \\\\ &= \\sum_{k=1}^{\\infty} (2k-1) {}_{k}p_{x} \\\\ &= \\sum_{k=1}^{\\infty} 2k{}_{k}p_{x} - {}_{k}p_{x} \\\\ &= \\sum_{k=1}^{\\infty} 2k{}_{k}p_{x} - \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\\\ &= 2 \\sum_{k=1}^{\\infty} k{}_{k}p_{x} - e_x \\end{aligned} \\] Thus, the variance can be calculated as: \\[ \\begin{aligned} Var(K_x) &= E(K_x^2) - [E(K_x)]^2 \\\\ &= 2 \\sum_{k=1}^{\\infty} k{}_{k}p_{x} - e_x - (e_x)^2 \\end{aligned} \\] Trapezoidal Rule \u00b6 Note that the two expectations are similar to one another: Continuous Expectation - Area under survival function Discrete Expectation - Right Riemann Sum of the area under the survival function Recall from Calculus that the area under a curve can be estimated through the Trapezium Rule , which states that the area is approximately equal to the sum of the area of discrete trapeziums formed under the curve. \\[ \\begin{aligned} \\int^b_a f(x) & \\approx \\sum \\text{Area of Trapezium} \\\\ & \\approx \\sum \\frac{1}{2} h \\left[f(a+kh) + f(a+(k+1)h) \\right] \\\\ & \\approx \\frac{h}{2}[f(a) + f(a+h)] + \\frac{h}{2}[f(a+h) + f(a+2h)] + \\dots + \\frac{h}{2}[f(b-h) + f(b)] \\\\ & \\approx \\frac{h}{2} f(a) + h[f(a+h) + f(a+2h) + \\dots + f(b-h)] + \\frac{h}{2} f(b) \\\\ & \\approx h[f(a+h) + f(a+2h) + \\dots + f(b-h)] + \\frac{h}{2} [f(a)+f(b)] \\\\ & \\approx h[f(a) + f(a+h) + \\dots + f(b)] - \\frac{h}{2} [f(a)+f(b)] \\\\ & \\approx h[f(a) + f(a+h) + \\dots + f(b-h)] - \\frac{h}{2} [f(a)-f(b)] \\\\ & \\approx h[f(a+h) + f(a+2h) + \\dots + f(b)] + \\frac{h}{2} [f(a)-f(b)] \\end{aligned} \\] The proof relies on the fact that other than \\(f(a)\\) and \\(f(b)\\) , all other terms are repeated twice . The last four lines are just different variations that showcase this property: Both \\(f(a)\\) and \\(f(b)\\) excluded from main expression Both \\(f(a)\\) and \\(f(b)\\) included in main expression but subtracted Only \\(f(a)\\) included in main expression; Main expression is a Left Riemman Sum Only \\(f(b)\\) included in main expression; Main expression is a Right Riemman Sum Since the discrete expectation is a right riemann sum, the last expression of the trapezoidal rule should be used. This allows the continuous expectation to be expressed using the discrete expectation, assuming \\(h=1\\) : \\[ \\begin{aligned} \\mathring{e}_x &= \\int^{\\infty}_{0} S_x(t) \\\\ & \\approx h[f(a+h) + f(a+2h) + \\dots + f(b)] + \\frac{h}{2} [f(a)-f(b)] \\\\ & \\approx (1) [S_x(1) + S_x(2) + \\dots + S_x(\\infty)] + \\frac{1}{2} [S_x(0)-S_x(\\infty)] \\\\ & \\approx [S_x(1) + S_x(2) + \\dots + S_x(\\infty)] + \\frac{1}{2} [1-0] \\\\ & \\approx e_x + \\frac{1}{2} \\end{aligned} \\] An alternative way to view the above is that the area of the trapezoid is the sum of the riemann rectangles and a triangle . Euler Maclaurin Formula \u00b6 Note that the Trapezoidal Rule is not perfect - there is an inherent error in trying to approximate a curve using a line. \\[ \\begin{aligned} \\int^b_a f(x) & \\approx \\sum (\\text{Area of Trapezium} + \\text{Error})\\\\ \\end{aligned} \\] The error can be positive or negative, depending on the shape of the curve in that interval: The error term is calculated by taking the difference between the integral and the trapezium, and is generalized using the taylor series. The final result is known as the Euler Maclaurin Formula : \\[ \\sum \\text{Error} = \\frac{h^2}{12} [f'(a)-f'(b)] \\] Given that a taylor series was used, the error can be expressed as the sum of many different terms, but higher powers are ignored . Thus, the trapezoidal approximation for the expectation can be made more precise: \\[ \\begin{aligned} \\mathring{e}_x & \\approx e_x + \\frac{1}{2} + \\frac{1^2}{12} [S'(0)-S'(\\infty)] \\\\ & \\approx e_x + \\frac{1}{2} + \\frac{1}{12} [S'(0)] \\end{aligned} \\] The above will rarely be used in this manner - it is more than sufficient to know the relationship between the two expectations of life. However, it sets as a strong foundation to understand the Woolhouse Approximation in the life annuities section. Parametric Survival Models \u00b6 Given the importance of \\(\\mu_x\\) , several mathematical functions have been made to describe the force of mortality, known as a Parametric Survival Model . De Moivre's Model \u00b6 Gompertz Model \u00b6 The most commonly used model is Gompertz Model , which suggests that mortality increases exponentially with age . The is highly intuitive, as older people have a higher probability of death due to health issues related to age. It is interesting to note that the ratio of death probabilities year over year after the age of 30 is approximately constant - which means that the death probability increases geometrically . Let \\(\\lambda\\) be the geometric mean of the death probabilities: \\[ \\begin{aligned} q_{71} &= \\lambda \\cdot q_{70} \\\\ \\\\ q_{72} &= \\lambda \\cdot q_{71} \\\\ &= \\lambda^2 \\cdot q_{70} \\\\ \\\\ \\therefore q_{x+t} &= \\lambda^t q_{x} \\end{aligned} \\] Let \\(\\lambda = e^b\\) . Thus, the model can be expressed continuously as the following: \\[ \\mu_{x+t} = Bc^{x+t} \\] Makeham Gompertz Model \u00b6 Makeham added a constant \\(A\\) into the Gompertz model, resulting in the Makeham Gompertz Model : \\[ \\mu_{x+t} = A + Bc^{x+t} \\] \\(A\\) represents the age independent mortality - dying from reasons unrelated to age, such as from accidents or natural catastrophes. \\[ \\begin{aligned} S_x(t) &= - \\int^{t}_{0} A + Bc^{x+t} \\\\ &= \\int^{t}_{0} -A - Bc^{x+t} \\\\ &= \\left[-At - \\frac{B}{\\ln c} c^{x+t} \\right]^{t}_{0} \\\\ &= \\left[-At - \\frac{B}{\\ln c} c^{x+t} \\right] - \\left[-\\frac{B}{\\ln c} c^{x} \\right] \\\\ &= -At - \\frac{B}{\\ln c} c^{x+t} + \\frac{B}{\\ln c} c^{x} \\\\ &= -At - \\frac{B}{\\ln c} c^x [c^t - 1] \\end{aligned} \\]","title":"Survival Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#survival-models","text":"","title":"Survival Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#overview","text":"Survival Models are probability distributions that measure the time to failure of an entity, or phrased another way, the future lifetime of an entity. In an Actuarial context, it measures the time to death of a person. Since survival models measure time , they are denoted by the continuous random variable \\(T_x\\) , where the subscript \\(x\\) represents the age of the person.","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#newborn-lifetime","text":"The base survival model measures the future lifetime of a newborn ( aged 0 ), denoted by the random variable \\(T_0\\) . The CDF thus represents the probability that the newborn dies before age \\(t\\) : \\[ F_0(t) = P(T_0 \\le t) \\] The Survival Function is the complement of the CDF and is the probability that the newborn survives till a certain age \\(t\\) . From a different perspective, it can also be seen as the probability of the newborn dying AFTER age \\(t\\) : \\[ \\begin{aligned} S_0(t) &= P(T_0 \\ge t) \\\\ &= 1 - P(T_0 \\le t) \\\\ &= 1 - F_0(t) \\end{aligned} \\] Death is inevitable, which means that the probability of living forever must be 0: \\[ \\begin{aligned} \\lim_{t \\to \\infty} S_0 (t) = 0 \\iff \\lim_{t \\to \\infty} F_0 (t) = 1 \\end{aligned} \\]","title":"Newborn Lifetime"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#continuous-lifetime","text":"The above can be generalized for a person aged \\(x\\) , denoted by \\(T_x\\) . However, the intepretation of the two functions changes slightly. The CDF instead represents the probability that the person dies within \\(t\\) years from age \\(x\\) : \\[ \\begin{aligned} F_x(t) &= P(T_x \\le t) \\end{aligned} \\] It can also be written in terms of the newborn lifetime . The key is to understand that the newborn is assumed to survive till age \\(x\\) and then die within the next \\(t\\) years: \\[ \\begin{aligned} P(T_x \\le t) &= P(T_0 \\le x + t | T_0 > x) \\\\ &= \\frac{P(x < T_0 < x +t)}{P(T_0 > x)} \\\\ &= \\frac{P(T_0 < x +t)-P(T_0 < x)}{P(T_0 > x)} \\\\ &= \\frac{F_0(x+t) - F_0(x)}{S_0(x)} \\end{aligned} \\] Conversely, the survival function is the probability that the person survives another \\(t\\) years or dies AFTER \\(t\\) years : \\[ S_x(t) = P(T_x \\ge t) \\] It can also be written in terms of the newborn lifetime following the same logic as above. The newborn is assumed to survive till age \\(x\\) and then survive another \\(t\\) years: \\[ \\begin{aligned} P(T_x \\ge t) &= P(T_0 \\ge x + t | T_0 > x) \\\\ &= \\frac{P(T_0 > x + t)}{P(T_0 > x)} \\\\ &= \\frac{S_0(x+t)}{S_0(x)} \\end{aligned} \\] Note Since the surviving till age \\(x\\) is a subset of surviving till age \\(x+t\\) , the numerator can be simplified to just the probability of surviving till age \\(x+t\\) . The survival function can thus be re-written as the following: \\[ \\begin{aligned} S_x(t) &= \\frac{S_0(x+t)}{S_0(x)} \\\\ S_0(x+t) &= S_0(x)S_x(t) \\\\ \\end{aligned} \\] The probability of the survival of a newborn till age \\(x+t\\) is the product of: The probability of survival of a newborn till age \\(x\\) Given that the newborn survived till age \\(x\\) , that they will survive another \\(t\\) years This can be generalized for people of any age to obtain: \\[ S_x(t+u) = S_x(t) S_{x+t}(u) \\] Remember to update the age! It is a common mistake to forget to use the \"new\" age of the person in the second expression. The opposite of the above is when the person surives another \\(t\\) years and then dies within the following \\(u\\) years . This is known as the probability of Deferred Death : \\[ \\begin{aligned} P(t < T_x < t+u) &= P(T_x < t+u) - P(T_x < t) \\\\ &= F_x(t+u) - F_x(t) \\\\ &= 1 - S_x(t+u) - (1 - S_x(t)) \\\\ &= 1 - S_x(t)S_{x+t}(u) - 1 + S_x(t) \\\\ &= S_x(t) - S_x(t)S_{x+t}(u) \\\\ &= S_x(t)(1-S_{x+t}(u)) \\\\ &= S_x(t)F_{x+t}(u) \\end{aligned} \\]","title":"Continuous Lifetime"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#probability-tree-perspective","text":"Since the probabilites of death and survival are conditional on the age of the person, they can be better expressed in the form of a probability tree: Thus, it can be shown through recursion that the probability of surviving \\(t\\) years is equal to the sum of the probabilities of deferred deaths for every year after: \\[ \\begin{aligned} S_x(1) &= S_x(1)F_{x+1}(1) + S_x(1)S_{x+1}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(1) [S_{x+1}(1)F_{x+2}(1) + S_{x+1}(1)S_{x+2}(1)] \\\\ &= S_x(1)F_{x+1}(1) + S_x(1) S_{x+1}(1)F_{x+2}(1) + S_x(1) S_{x+1}(1)S_{x+2}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(2)F_{x+2}(1) + S_x(2)S_{x+2}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(2)F_{x+2}(1) + \\dots \\\\ \\\\ \\therefore S_x(t) &= \\sum S_x(t)F{x+t}(1) \\end{aligned} \\] Recall that the probability of surviving till a certain age is simply the probability that the person will die sometime after that age. The above expression solidifies this, where the probability of survival is equivalent to the probability of dying in ever possible age after .","title":"Probability Tree Perspective"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#force-of-mortality","text":"The Force of Mortality is the probability of dying instantly ; in an infinitely small unit of time (normalized by unit time): \\[ \\begin{aligned} \\mu_{0+x} &= \\lim_{h \\to 0} \\frac{P(T_x < h)}{h} \\\\ &= \\lim_{h \\to 0} \\frac{1 - S_x(h)}{h} \\\\ &= \\lim_{h \\to 0} \\frac{1 - (\\frac{S_0(x+h)}{S_0(x)})}{h} \\\\ &= \\lim_{h \\to 0} \\frac{S_0(x) - S_0(x+h)}{S_0(x)h} \\\\ &= \\frac{1}{S_0(x)} \\lim_{h \\to 0} \\frac{S_0(x) - S_0(x+h)}{h} \\\\ &= - \\frac{1}{S_0(x)} \\lim_{h \\to 0} \\frac{S_0(x+h) - S_0(x)}{h} \\\\ &= - \\frac{1}{S_0(x)} S'_0(x) \\\\ &= - \\frac{S'_0(x)}{S_0(x)} \\end{aligned} \\] Since \\(S_0(x)\\) is the complement of \\(F_0(x)\\) , the derivative of the survival function can be written in terms of the PDF of the future lifetime: \\[ \\begin{aligned} f_0(x) &= \\frac{d}{dx} F_0(x) \\\\ &= \\frac{d}{dx} (1-S_0(x)) \\\\ &= -S'_0(x) \\end{aligned} \\] The force of mortality can be rewritten as the following: \\[ \\mu_{0+x} = \\frac{f_0(x)}{S_0(x)} \\] Thus, the force of mortality can also be intepreted as the CONDITIONAL distribution of future lifetime for a person aged \\(x\\) , as it is conditional on the person surviving till age \\(x\\) . Following that, \\(f_0(x)\\) is the UNCONDITIONAL distribution of the future lifetime as it does not assume the person lives to any age.","title":"Force of Mortality"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#alternative-death-function","text":"More generally, the force of mortality can be rewritten as: \\[ \\begin{aligned} \\mu_{0+x} &= \\frac{f_0(x)}{S_0(x)} \\\\ \\mu_{x+t} &= \\frac{f_x(t)}{S_x(t)} \\\\ f_x(t) &= S_x(t)\\mu_{x+t} \\\\ F_x(t) &= \\int_0^t S_x(t)\\mu_{x+t} \\end{aligned} \\] The probability of death in a given interval is the sum of the various probabilities of surviving till each age and then dying instantly afterwards, which can be computed via integration. Following this definition, for extremely small time intervals , the force of mortality can be used to approximate the probability of death in that interval: \\[ P(T_x < h) \\approx h * \\mu_{x} \\]","title":"Alternative Death Function"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#alternative-survival-function","text":"More generally, the force of mortality can be rewritten as: \\[ \\begin{aligned} \\mu_{0+x} &= - \\frac{S'_0(x)}{S_0(x)} \\\\ \\mu_{x + t} &= \\frac{-S'_x(t)}{S_x(t)} \\\\ \\mu_{x + t} &= - \\frac{d}{dt} (\\ln S_x(t)) \\\\ \\ln S_x(t) &= - \\int_{0}^{t} \\mu_{x + t} \\\\ S_x(t) &= e ^ {- \\int_{0}^{t} \\mu_{x + t}} \\\\ \\end{aligned} \\] Note that most expressions give \\(\\mu_x\\) , but it is \\(\\mu_{x+t}\\) that is usually of interest - simply adjust the parameters accordingly. Thus, if \\(\\mu_x\\) is known for all ages, then the survival probabilites for any age can be calculated from it. In other words, the distribution of \\(T_x\\) can be determined from the force of mortality .","title":"Alternative Survival Function"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#actuarial-notation","text":"Given how often these expressions are used, they are often abbreviated using the International Actuarial Notation : \\[ \\begin{aligned} {}_{t}p_{x} &= S_x(t) \\\\ {}_{t}q_{x} &= F_x(t) \\\\ {}_{t}q_{x} + {}_{t}p_{x} &= 1 \\end{aligned} \\] Convention for Notation If \\(t=1\\) , it is usually omitted - EG. \\({}_{1}q_{x} = q_{x}\\) . Deferred Deaths are expressed using the pipe symbol isntead: \\[ \\begin{aligned} {}_{t|u}q_{x} &= S_x(t)F_{x+t}(u) \\\\ &= {}_{t}p_{x} * {}_{u}q_{x+t} \\end{aligned} \\]","title":"Actuarial Notation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#continuous-expectation","text":"The Expectation of the distribution of future lifetime is the Life Expectancy of the person. It is commonly calculated for newborns as a measure of the general health of the population. In an actuarial context, it is known as the Complete Expectation of Life : \\[ \\begin{aligned} E(T_x) &= \\int_{0}^{\\infty} t f_x(t) \\\\ e_{x} &= \\int_{0}^{\\infty} t S_x(t)\\mu_{x+t} \\end{aligned} \\] Through integration by parts, \\[ \\begin{aligned} \\mathring{e}_{x} &= \\left(\\Bigl[-tS_x(t)\\Bigr]_0^\\infty + \\int_0^\\infty S_x(t)\\right) \\\\ &= \\int_0^\\infty {}_{t}p_x \\end{aligned} \\] Inevitable Death Recall that death is inevitable - thus, the probability of living forever ( \\([-t * S_x(t)]^{\\infty}_0\\) ) must be 0. If the future lifetime variable is artificially limited to \\(n\\) years , then the expectation of the future lifetime is known as the Term Expectation of Life : \\[ \\begin{aligned} E \\left(min(T_x, n) \\right) &= \\int^{n}_0 {}_{t}p_x \\\\ \\mathring{e}_{x:\\enclose{actuarial}{n}} &= \\int^{n}_0 {}_{t}p_x \\end{aligned} \\] This allows the complete expectation can be decomposed into two components: Term Expectation at the current age representing the \"early\" years Complete Expectation at a future age represenging the \"later\" years \\[ \\mathring{e}_x = \\mathring{e}_{x:\\enclose{actuarial}{n}} + \\mathring{e}_{x+n} \\] However, the above makes an implicit assumption that the person will survive the first n years. Thus, the second term needs to account for the probability of surviving those n years: \\[ \\mathring{e}_x = \\mathring{e}_{x:n} + {}_{n}p_x \\cdot \\mathring{e}_{x+n} \\]","title":"Continuous Expectation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#continuous-variance","text":"The second moment can be calculated in a similar fashion through integration by parts: \\[ \\begin{aligned} E(T^2_x) &= \\int_{0}^{\\infty} t^2 f_x(t) \\\\ &= -\\left(\\Bigl[t^2 S_x(t)\\Bigr]_0^\\infty - \\int_{0}^{\\infty} 2tS_x(t) \\right) \\\\ &= 2 \\int_{0}^{\\infty} t \\cdot {}_{t}p_x \\end{aligned} \\] Thus, the Variance of the future lifetime about the Expectation is the following: \\[ \\begin{aligned} Var(T_x) &= E(T^2_x) - [E(T_x)]^2 \\\\ &= 2 \\int_{0}^{\\infty} t \\cdot {}_{t}p_x - \\left(\\int_0^\\infty {}_{t}p_x \\right)^2 \\end{aligned} \\]","title":"Continuous Variance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#discrete-lifetime","text":"If only the integer components of the future lifetime are considered, then it becomes a discrete distribution known as the Curtate Future Lifetime of the person. It is denoted by the random variable \\(K_x\\) , which is a truncated version of \\(T_x\\) . Formally, it is known as a Floor Function : \\[ \\begin{aligned} K_x &= \\lfloor T_x \\rfloor \\end{aligned} \\] It measures the number of full years that the person is expected to live for. If \\(K_x = k\\) , then it means the person will another \\(k\\) full years but not live past \\(k+1\\) years full years. In other words, they will die between \\(k\\) (inclusive) and \\(k+1\\) (Exclusive) years . The distribution starts from 0 , where \\(K_x = 0\\) represents a newborn that will die within their birth year. Similarly, the probability that a person will die within \\(k\\) years is given by \\(K_x = k-1\\) , denoting that they will live \\(k-1\\) full years and die before reaching the kth year mark. \\[ \\begin{aligned} P(K_x = k) &= P(x+k \\le T_0 < x+k+1) \\\\ &= P(k \\le T_x < k+1) \\\\ &= P(T_x < k+1) - P(T_x \\le k) \\\\ &= F_x(k+1) - F_x(k) \\\\ &= S_x(k) - S_x(k+1) \\\\ &= S_x(k) - S_x(k) * S_{x+k}(1) \\\\ &= S_x(k) (1 - S_{x+k}(1)) \\\\ &= S_x(k) F_{x+k}(1) \\\\ &= {} {}_{k}p_{x} {}_{}q_{x + k} \\\\ &= {}_{k|}q_{x} \\end{aligned} \\]","title":"Discrete Lifetime"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#discrete-expectation","text":"Similarly, the Expectation of Curtate Lifetime is the expectation of \\(K_x\\) : \\[ \\begin{aligned} e_x &= E(K_x) \\\\ &= \\sum_{k=0}^{\\infty} k ({}_{k}p_{x} - {}_{k+1}p_{x}) \\\\ &= 1 ({}_{1}p_{x} - {}_{2}p_{x}) + 2 ({}_{2}p_{x} - {}_{3}p_{x}) + 3 ({}_{3}p_{x} - {}_{4}p_{x}) + ... \\\\ &= {}_{1}p_{x} - {}_{2}p_{x} + 2{}_{2}p_{x} - 2{}_{3}p_{x} + 3 {}_{3}p_{x} - 3 {}_{4}p_{x} \\\\ &= {}_{1}p_{x} + 2{}_{2}p_{x} + 3 {}_{3}p_{x} + ... \\\\ &= \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\end{aligned} \\] The expectation can also recursively calculated: If the person dies within the year with probability \\(q_x\\) , then the future lifetime is 0 If the person survives the year with probability \\(p_x\\) , then the future lifetime is 1 (for surviving the current year) AND the expected lifetime past that \\[ \\begin{aligned} e_x &= \\begin{cases} 0,& q_x \\\\ 1+e_{x+1},& p_x \\end{cases} \\\\ \\\\ \\therefore e_x &= p_x (1+e_{x+1}) \\end{aligned} \\]","title":"Discrete Expectation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#discrete-variance","text":"The second moment can be calculated in a similar fashion: \\[ \\begin{aligned} E(K^2_x) &= \\sum_{k=0}^{\\infty} k^2 ({}_{k}p_{x} - {}_{k+1}p_{x}) \\\\ &= 1 ({}_{1}p_{x} - {}_{2}p_{x}) + 4 ({}_{2}p_{x} - {}_{3}p_{x}) + 9 ({}_{3}p_{x} - {}_{4}p_{x}) + ... \\\\ &= {}_{1}p_{x} - {}_{2}p_{x} + 4{}_{2}p_{x} - 4{}_{3}p_{x} + 9 {}_{3}p_{x} - 9 {}_{4}p_{x} \\\\ &= {}_{1}p_{x} + 3{}_{2}p_{x} + 5 {}_{3}p_{x} + ... \\\\ &= \\sum_{k=1}^{\\infty} (2k-1) {}_{k}p_{x} \\\\ &= \\sum_{k=1}^{\\infty} 2k{}_{k}p_{x} - {}_{k}p_{x} \\\\ &= \\sum_{k=1}^{\\infty} 2k{}_{k}p_{x} - \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\\\ &= 2 \\sum_{k=1}^{\\infty} k{}_{k}p_{x} - e_x \\end{aligned} \\] Thus, the variance can be calculated as: \\[ \\begin{aligned} Var(K_x) &= E(K_x^2) - [E(K_x)]^2 \\\\ &= 2 \\sum_{k=1}^{\\infty} k{}_{k}p_{x} - e_x - (e_x)^2 \\end{aligned} \\]","title":"Discrete Variance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#trapezoidal-rule","text":"Note that the two expectations are similar to one another: Continuous Expectation - Area under survival function Discrete Expectation - Right Riemann Sum of the area under the survival function Recall from Calculus that the area under a curve can be estimated through the Trapezium Rule , which states that the area is approximately equal to the sum of the area of discrete trapeziums formed under the curve. \\[ \\begin{aligned} \\int^b_a f(x) & \\approx \\sum \\text{Area of Trapezium} \\\\ & \\approx \\sum \\frac{1}{2} h \\left[f(a+kh) + f(a+(k+1)h) \\right] \\\\ & \\approx \\frac{h}{2}[f(a) + f(a+h)] + \\frac{h}{2}[f(a+h) + f(a+2h)] + \\dots + \\frac{h}{2}[f(b-h) + f(b)] \\\\ & \\approx \\frac{h}{2} f(a) + h[f(a+h) + f(a+2h) + \\dots + f(b-h)] + \\frac{h}{2} f(b) \\\\ & \\approx h[f(a+h) + f(a+2h) + \\dots + f(b-h)] + \\frac{h}{2} [f(a)+f(b)] \\\\ & \\approx h[f(a) + f(a+h) + \\dots + f(b)] - \\frac{h}{2} [f(a)+f(b)] \\\\ & \\approx h[f(a) + f(a+h) + \\dots + f(b-h)] - \\frac{h}{2} [f(a)-f(b)] \\\\ & \\approx h[f(a+h) + f(a+2h) + \\dots + f(b)] + \\frac{h}{2} [f(a)-f(b)] \\end{aligned} \\] The proof relies on the fact that other than \\(f(a)\\) and \\(f(b)\\) , all other terms are repeated twice . The last four lines are just different variations that showcase this property: Both \\(f(a)\\) and \\(f(b)\\) excluded from main expression Both \\(f(a)\\) and \\(f(b)\\) included in main expression but subtracted Only \\(f(a)\\) included in main expression; Main expression is a Left Riemman Sum Only \\(f(b)\\) included in main expression; Main expression is a Right Riemman Sum Since the discrete expectation is a right riemann sum, the last expression of the trapezoidal rule should be used. This allows the continuous expectation to be expressed using the discrete expectation, assuming \\(h=1\\) : \\[ \\begin{aligned} \\mathring{e}_x &= \\int^{\\infty}_{0} S_x(t) \\\\ & \\approx h[f(a+h) + f(a+2h) + \\dots + f(b)] + \\frac{h}{2} [f(a)-f(b)] \\\\ & \\approx (1) [S_x(1) + S_x(2) + \\dots + S_x(\\infty)] + \\frac{1}{2} [S_x(0)-S_x(\\infty)] \\\\ & \\approx [S_x(1) + S_x(2) + \\dots + S_x(\\infty)] + \\frac{1}{2} [1-0] \\\\ & \\approx e_x + \\frac{1}{2} \\end{aligned} \\] An alternative way to view the above is that the area of the trapezoid is the sum of the riemann rectangles and a triangle .","title":"Trapezoidal Rule"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#euler-maclaurin-formula","text":"Note that the Trapezoidal Rule is not perfect - there is an inherent error in trying to approximate a curve using a line. \\[ \\begin{aligned} \\int^b_a f(x) & \\approx \\sum (\\text{Area of Trapezium} + \\text{Error})\\\\ \\end{aligned} \\] The error can be positive or negative, depending on the shape of the curve in that interval: The error term is calculated by taking the difference between the integral and the trapezium, and is generalized using the taylor series. The final result is known as the Euler Maclaurin Formula : \\[ \\sum \\text{Error} = \\frac{h^2}{12} [f'(a)-f'(b)] \\] Given that a taylor series was used, the error can be expressed as the sum of many different terms, but higher powers are ignored . Thus, the trapezoidal approximation for the expectation can be made more precise: \\[ \\begin{aligned} \\mathring{e}_x & \\approx e_x + \\frac{1}{2} + \\frac{1^2}{12} [S'(0)-S'(\\infty)] \\\\ & \\approx e_x + \\frac{1}{2} + \\frac{1}{12} [S'(0)] \\end{aligned} \\] The above will rarely be used in this manner - it is more than sufficient to know the relationship between the two expectations of life. However, it sets as a strong foundation to understand the Woolhouse Approximation in the life annuities section.","title":"Euler Maclaurin Formula"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#parametric-survival-models","text":"Given the importance of \\(\\mu_x\\) , several mathematical functions have been made to describe the force of mortality, known as a Parametric Survival Model .","title":"Parametric Survival Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#de-moivres-model","text":"","title":"De Moivre's Model"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#gompertz-model","text":"The most commonly used model is Gompertz Model , which suggests that mortality increases exponentially with age . The is highly intuitive, as older people have a higher probability of death due to health issues related to age. It is interesting to note that the ratio of death probabilities year over year after the age of 30 is approximately constant - which means that the death probability increases geometrically . Let \\(\\lambda\\) be the geometric mean of the death probabilities: \\[ \\begin{aligned} q_{71} &= \\lambda \\cdot q_{70} \\\\ \\\\ q_{72} &= \\lambda \\cdot q_{71} \\\\ &= \\lambda^2 \\cdot q_{70} \\\\ \\\\ \\therefore q_{x+t} &= \\lambda^t q_{x} \\end{aligned} \\] Let \\(\\lambda = e^b\\) . Thus, the model can be expressed continuously as the following: \\[ \\mu_{x+t} = Bc^{x+t} \\]","title":"Gompertz Model"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#makeham-gompertz-model","text":"Makeham added a constant \\(A\\) into the Gompertz model, resulting in the Makeham Gompertz Model : \\[ \\mu_{x+t} = A + Bc^{x+t} \\] \\(A\\) represents the age independent mortality - dying from reasons unrelated to age, such as from accidents or natural catastrophes. \\[ \\begin{aligned} S_x(t) &= - \\int^{t}_{0} A + Bc^{x+t} \\\\ &= \\int^{t}_{0} -A - Bc^{x+t} \\\\ &= \\left[-At - \\frac{B}{\\ln c} c^{x+t} \\right]^{t}_{0} \\\\ &= \\left[-At - \\frac{B}{\\ln c} c^{x+t} \\right] - \\left[-\\frac{B}{\\ln c} c^{x} \\right] \\\\ &= -At - \\frac{B}{\\ln c} c^{x+t} + \\frac{B}{\\ln c} c^{x} \\\\ &= -At - \\frac{B}{\\ln c} c^x [c^t - 1] \\end{aligned} \\]","title":"Makeham Gompertz Model"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/","text":"Life Tables \u00b6 Overview \u00b6 Life Tables existed long before the survival models in the previous section. The mortality functions were presented in tabular form, where the probabilities and expectations were then calculated from. Both the survival model and life table are equivalent ways of obtaining the same results. Basic Life Table \u00b6 The life table is constructed based on the mortality of a group of people known as the Cohort . The initial age of the group people is known as the Starting Age , denoted by \\(\\alpha\\) . The initial number of people in the cohort is known as the Radix , which is usually a large round number (EG. 1000) Conversely, the maximum age is known as the Terminal Age , denoted by \\(\\omega\\) . Everybody in the cohort is expected to gradually die by the terminal age . The number of people alive from the cohort at age \\(x\\) is denoted as \\(\\ell_x\\) : \\[ \\begin{aligned} \\ell_0 &= 1000 \\\\ \\ell_{\\omega} &= 0 \\\\ \\\\ \\therefore \\ell_{x+t} &< \\ell_{x} \\end{aligned} \\] From this basic life table, several important values can be calculated: The expected number of deaths over a period, \\({}_{n}d_{x}\\) The probability of survival past a period, \\({}_{n}p_{x}\\) The probability of death within a period, \\({}_{n}q_{x}\\) The deferred probability of death within a period, \\({}_{s|t}q_{x}\\) \\[ \\begin{aligned} {}_{n}d_{x} &= \\ell_{x} - \\ell_{x+n} \\\\ {}_{n}p_{x} &= \\frac{\\ell_{x+t}}{\\ell_{x}} \\\\ {}_{n}q_{x} &= \\frac{{}_{n}d_{x}}{\\ell_{x}} \\\\ {}_{s|t}q_{x} &= \\frac{{}_{t}d_{x+s}}{l_{x}} \\\\ \\end{aligned} \\] Thus, it can be seen that the probabilities are simply the proportion of people who died/survived over the a given period. Fractional Age Assumptions \u00b6 One limitation of the life table is that it is computed at discrete ages while many problems require probabilities for non-discrete ages. Thus, several assumptions about the life table that allows non-discrete values to be interpolated from the discrete ones, known as Fractional Age Assumptions . Uniform Distribution of Deaths \u00b6 The Uniform Distribution of Deaths (UDD) is an assumption that allows for Linear Interpolation between discrete ages. It assumes that there is a uniform distribution of deaths between ages such that the probability of survival decreases linearly over the year. The UDD assumption is less appropriate for older individuals as the probability of death begins to increase exponentially, even from month to month. Let the fractional age be \\(s\\) . Thus, the probability using fractional ages is simply the weighted average of the discrete points: \\[ \\begin{aligned} {}_{s}p_{x} &= (1-s) {}_{0}p_{x} + s {}_{1}p_{x} \\\\ &= (1-s)\\frac{\\ell_{x}}{\\ell_{x}} + s \\frac{\\ell_{x+1}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x}-s\\ell_{x} + s\\ell_{x+1}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x} - s(\\ell_{x}-\\ell_{x+1})}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x} - s d_{x}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x}}{\\ell_{x}} - s\\frac{d_{x}}{\\ell_{x}} \\\\ &= 1 - s q_{x} \\\\ \\\\ \\therefore {}_{s}q_{x} &= s \\cdot q_x \\end{aligned} \\] Note that \\({}_{0}p_{x}=1\\) as it is the probability of surviving in that moment; which should be guaranteed. Based on this, the PDF and Force of Mortality can also be determined: \\[ \\begin{aligned} F_x(s) &= s \\cdot q_x \\\\ \\\\ f_x(s) &= \\frac{d}{ds} (s \\cdot q_x) \\\\ &= q_x \\\\ \\\\ \\mu_{x+t} &= \\frac{f_x(s)}{S_x(s)} \\\\ &= \\frac{q_x}{1-sq_x} \\end{aligned} \\] Constant Force of Mortality \u00b6 The Constant Force of Mortality is an assumption that allows for Exponential Interpolation between discrete ages. It assumes that there is a constant force of mortality between ages, such that the survival probability decreases exponentially over the year. \\[ \\begin{aligned} {}_{1}p_{x} &= e^{-\\int^{x+1}_{x} \\mu_c} \\\\ &= e^{-[t\\mu_c]^{x+1}_{x}} \\\\ &= e^{-[(x+1)\\mu_c - (x)\\mu_c ]} \\\\ &= e^{-[\\mu_c (x+1-x)]} \\\\ &= e^{-\\mu_c} \\\\ \\mu_c &= - \\ln p_{x} \\\\ \\\\ \\therefore {}_{s}p_{x} &= e^{-\\int^{x+s}_{x} \\mu_c} \\\\ &= e^{-s\\mu_c} \\\\ &= e^{s\\ln p_{x}} \\\\ &= e^{\\ln (p_{x})^{s}} \\\\ &= (p_{x})^{s} \\\\ \\\\ \\therefore {}_{s}q_{x} &= 1 - {}_{s}p_{x} \\\\ &= 1 - (p_{x})^{s} \\\\ &= 1 - [(1-q_{x})]^{s} \\end{aligned} \\] Select & Ultimate Mortality \u00b6 Mortality rates for the general population versus individuals who buy life insurance tend to be different. Generally speaking, people who purchase insurance tend to be richer and thus have better mortality rates than the general population . Within the individuals who purchase life insurance, those who have recently purchased a policy tend to have better mortality . This is because these individuals would have gone through Medical Underwriting and thus is expected to be in better health. The extent of the better mortality decreases over time and after a few years, they should experience the same mortality as the rest of the individuals who purchased life insurance. The duration of time is dependent on the rigorousness of the underwriting process . Formally speaking, the individuals who purchased life insurance were selected by the underwriting process and thus the better mortality experienced is known as Select Mortality . Similarly, the time whereby the select mortality is better is known as the Select Period . The select mortality ultimately converges with the non-select group, known as the Ultimate Mortality . In actuarial notation, subscript \\([x]\\) is used to distinguish select mortality from ultimate mortality, where the select period is denoted as \\(d\\) : \\[ \\begin{aligned} \\begin{cases} q_{[x]+t} < q_{x+t},& t < d \\\\ q_{[x]+t} = q_{x+t},& t \\ge d \\end{cases} \\end{aligned} \\] Note that the definition of \\(x\\) is dependent on the questions: \\(x\\) is the select age - \\([x], [x]+1, [x]+2, ...\\) \\(x\\) is the ultimate age - \\([x], [x-1], [x-2], ...\\)","title":"Life Tables"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#life-tables","text":"","title":"Life Tables"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#overview","text":"Life Tables existed long before the survival models in the previous section. The mortality functions were presented in tabular form, where the probabilities and expectations were then calculated from. Both the survival model and life table are equivalent ways of obtaining the same results.","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#basic-life-table","text":"The life table is constructed based on the mortality of a group of people known as the Cohort . The initial age of the group people is known as the Starting Age , denoted by \\(\\alpha\\) . The initial number of people in the cohort is known as the Radix , which is usually a large round number (EG. 1000) Conversely, the maximum age is known as the Terminal Age , denoted by \\(\\omega\\) . Everybody in the cohort is expected to gradually die by the terminal age . The number of people alive from the cohort at age \\(x\\) is denoted as \\(\\ell_x\\) : \\[ \\begin{aligned} \\ell_0 &= 1000 \\\\ \\ell_{\\omega} &= 0 \\\\ \\\\ \\therefore \\ell_{x+t} &< \\ell_{x} \\end{aligned} \\] From this basic life table, several important values can be calculated: The expected number of deaths over a period, \\({}_{n}d_{x}\\) The probability of survival past a period, \\({}_{n}p_{x}\\) The probability of death within a period, \\({}_{n}q_{x}\\) The deferred probability of death within a period, \\({}_{s|t}q_{x}\\) \\[ \\begin{aligned} {}_{n}d_{x} &= \\ell_{x} - \\ell_{x+n} \\\\ {}_{n}p_{x} &= \\frac{\\ell_{x+t}}{\\ell_{x}} \\\\ {}_{n}q_{x} &= \\frac{{}_{n}d_{x}}{\\ell_{x}} \\\\ {}_{s|t}q_{x} &= \\frac{{}_{t}d_{x+s}}{l_{x}} \\\\ \\end{aligned} \\] Thus, it can be seen that the probabilities are simply the proportion of people who died/survived over the a given period.","title":"Basic Life Table"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#fractional-age-assumptions","text":"One limitation of the life table is that it is computed at discrete ages while many problems require probabilities for non-discrete ages. Thus, several assumptions about the life table that allows non-discrete values to be interpolated from the discrete ones, known as Fractional Age Assumptions .","title":"Fractional Age Assumptions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#uniform-distribution-of-deaths","text":"The Uniform Distribution of Deaths (UDD) is an assumption that allows for Linear Interpolation between discrete ages. It assumes that there is a uniform distribution of deaths between ages such that the probability of survival decreases linearly over the year. The UDD assumption is less appropriate for older individuals as the probability of death begins to increase exponentially, even from month to month. Let the fractional age be \\(s\\) . Thus, the probability using fractional ages is simply the weighted average of the discrete points: \\[ \\begin{aligned} {}_{s}p_{x} &= (1-s) {}_{0}p_{x} + s {}_{1}p_{x} \\\\ &= (1-s)\\frac{\\ell_{x}}{\\ell_{x}} + s \\frac{\\ell_{x+1}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x}-s\\ell_{x} + s\\ell_{x+1}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x} - s(\\ell_{x}-\\ell_{x+1})}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x} - s d_{x}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x}}{\\ell_{x}} - s\\frac{d_{x}}{\\ell_{x}} \\\\ &= 1 - s q_{x} \\\\ \\\\ \\therefore {}_{s}q_{x} &= s \\cdot q_x \\end{aligned} \\] Note that \\({}_{0}p_{x}=1\\) as it is the probability of surviving in that moment; which should be guaranteed. Based on this, the PDF and Force of Mortality can also be determined: \\[ \\begin{aligned} F_x(s) &= s \\cdot q_x \\\\ \\\\ f_x(s) &= \\frac{d}{ds} (s \\cdot q_x) \\\\ &= q_x \\\\ \\\\ \\mu_{x+t} &= \\frac{f_x(s)}{S_x(s)} \\\\ &= \\frac{q_x}{1-sq_x} \\end{aligned} \\]","title":"Uniform Distribution of Deaths"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#constant-force-of-mortality","text":"The Constant Force of Mortality is an assumption that allows for Exponential Interpolation between discrete ages. It assumes that there is a constant force of mortality between ages, such that the survival probability decreases exponentially over the year. \\[ \\begin{aligned} {}_{1}p_{x} &= e^{-\\int^{x+1}_{x} \\mu_c} \\\\ &= e^{-[t\\mu_c]^{x+1}_{x}} \\\\ &= e^{-[(x+1)\\mu_c - (x)\\mu_c ]} \\\\ &= e^{-[\\mu_c (x+1-x)]} \\\\ &= e^{-\\mu_c} \\\\ \\mu_c &= - \\ln p_{x} \\\\ \\\\ \\therefore {}_{s}p_{x} &= e^{-\\int^{x+s}_{x} \\mu_c} \\\\ &= e^{-s\\mu_c} \\\\ &= e^{s\\ln p_{x}} \\\\ &= e^{\\ln (p_{x})^{s}} \\\\ &= (p_{x})^{s} \\\\ \\\\ \\therefore {}_{s}q_{x} &= 1 - {}_{s}p_{x} \\\\ &= 1 - (p_{x})^{s} \\\\ &= 1 - [(1-q_{x})]^{s} \\end{aligned} \\]","title":"Constant Force of Mortality"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#select-ultimate-mortality","text":"Mortality rates for the general population versus individuals who buy life insurance tend to be different. Generally speaking, people who purchase insurance tend to be richer and thus have better mortality rates than the general population . Within the individuals who purchase life insurance, those who have recently purchased a policy tend to have better mortality . This is because these individuals would have gone through Medical Underwriting and thus is expected to be in better health. The extent of the better mortality decreases over time and after a few years, they should experience the same mortality as the rest of the individuals who purchased life insurance. The duration of time is dependent on the rigorousness of the underwriting process . Formally speaking, the individuals who purchased life insurance were selected by the underwriting process and thus the better mortality experienced is known as Select Mortality . Similarly, the time whereby the select mortality is better is known as the Select Period . The select mortality ultimately converges with the non-select group, known as the Ultimate Mortality . In actuarial notation, subscript \\([x]\\) is used to distinguish select mortality from ultimate mortality, where the select period is denoted as \\(d\\) : \\[ \\begin{aligned} \\begin{cases} q_{[x]+t} < q_{x+t},& t < d \\\\ q_{[x]+t} = q_{x+t},& t \\ge d \\end{cases} \\end{aligned} \\] Note that the definition of \\(x\\) is dependent on the questions: \\(x\\) is the select age - \\([x], [x]+1, [x]+2, ...\\) \\(x\\) is the ultimate age - \\([x], [x-1], [x-2], ...\\)","title":"Select &amp; Ultimate Mortality"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/","text":"Life Assurances \u00b6 Overview \u00b6 Life Assurances are contracts that promise to pay out a benefit if the insured event occurs in the future . The value of a life assurance must reflect these two aspects: Uncertainty of cashflows - Expected Value , based on Survival Models Time value of money - Present Value , based on Interest Theory Thus, the value of a life assurance is the Expected Present Value (EPV) of the promised cashflows. Payable Discretely \u00b6 The simplest form of life assurance pays the benefits at the end of the year that the insured event occurs. While not common in practice, it provides a simple framework to understand the core concepts. For an assurance payable discretely, \\(K_x\\) is used as the survival model . Thus, the present value of the benefit payable in each period is \\(v^{K_x + 1}\\) . The EPV is the Triple Product Summation of the various components: Amount of benefit, \\(B\\) Discounting of the benefit, \\(v^{K_x + 1}\\) Probability of paying the benefit, \\({}_{K_x}p_{x} {}_{}q_{x + K_x}\\) \\[ \\begin{aligned} EPV &= \\sum B \\cdot v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\\\ &= \\sum B \\cdot v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] For simplicity, all of the proofs in this section will use \\(B=1\\) . This also has the benefit of allowing the EPVs to be easily scaled for any level of \\(B\\) . Actuarial Notation \u00b6 Similar to the survival model, given how often these values are calculated, they are abbreviated using the International Actuarial Notation as well. \\(A\\) represents the first moment (expectation) of the present value of a contract where a benefit of 1 is payable discretely when the status fails . The subscript \\(x\\) is the age of the policyholder, known as the Life Status . It fails when the policyholder dies . An additional subscript \\(:\\enclose{actuarial}{n}\\) is the duration that the assurance remains valid, known as the Duration Status . It fails if the policyholder survives past the policyterm. A \\(1\\) above the status indicates that the assurance pays only pays if that particular status fails first . If nothing is indicated, then the assurance pays whichever status fails first. If a particular status is omitted, then the assurance is not dependent on that status. For instance, omitting the duration status means that the contract is only dependent on the age of the policyholder. An assurance that begins \\(n\\) years later is known as a Deferred Assurance , which is denoted by \\({}_{n|}A\\) , following the same notation as deferred probabilities. Thus, putting everything together, the EPV of each assurance can be denoted as follows: Whole Life Assurance - Payable whenever policyholder dies; \\(A_x\\) Term Assurance - Payable only if policyholder dies during assurance period; \\(A^{1}_{x:\\enclose{actuarial}{n}}\\) Pure Endowment - Payable only if policyholder survives past assurance period; \\(A^{\\>\\>\\> 1}_{x:\\enclose{actuarial}{n}}\\) Endowment Assurance - Payable whichever of the above two occur first; \\(A_{x:\\enclose{actuarial}{n}}\\) Deferred Whole Life Assurance - Payable only if policyholder dies after \\(n\\) years; \\({}_{n|}A_x\\) Pure Endowments can also be expressed as \\({}_{n}E_x\\) for easier typesetting. Whole Life Assurance \u00b6 Whole Life Assurances cover the insured indefinitely and thus will pay out whenever the insured dies. Let WL be the random variable denoting the PV of the death benefit: \\[ \\begin{aligned} \\text{WL} &= v^{K_x + 1} \\\\ K_x &= 0, 1, \\dots, \\infty \\end{aligned} \\] Technically speaking, the upper limit of \\(K_x\\) should \\(\\omega-x\\) as it represents the maximum attainable age in the discrete survival model. However, since the \\({}_{k}p_{x} = 0\\) for all \\(k \\ge \\omega-x\\) , it does not matter what the upper limit is as long as it is larger than \\(\\omega-x\\) . Thus, \\(\\infty\\) is used for conciseness instead. The EPV is the Expectation/First Moment of the WL random variable: \\[ \\begin{aligned} E(\\text{WL}) &= \\sum^\\infty_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ A_{x} &= \\sum^\\infty_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Another commonly used metric is the Variance of the contract. In order to get it, the Second Moment must first be determined: \\[ \\begin{aligned} E({\\text{WL}}^2) &= \\sum^\\infty_{K_x = 0} \\left(v^{K_x + 1}\\right)^2 \\cdot {}_{K_x|}q_{x} \\\\ &= \\sum^\\infty_{K_x = 0} \\left((v^2)^{K_x + 1}\\right) \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Notice that the expression can be simplified to a form that looks almost identical to the first moment - with the only difference being that is uses \\(v^2\\) instead of \\(v\\) . Thus, the second moment is simply the first moment evaluated at a higher interest rate \\(i^*=(1+i)^2-1\\) , such that \\(v^* = v^2\\) . Generally, the k-th moment is denoted as \\({}^{k}A_x\\) , where the \\(k\\) is the multiplier on the interest rate used to evaluate the moment, \\(i^*=(1+i)^k-1\\) . It can also be written as \\(A_x |_{i = (1+i)^2-1}\\) using non-actuarial notation, which will come in handy for more complicated expressions. The Second Moment and hence Variance can be shown as: \\[ \\begin{aligned} E({\\text{WL}}^2) &= \\sum^\\infty_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ {}^{2} A_{x} &= \\sum^\\infty_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ \\\\ \\therefore Var(\\text{WL}) &= {}^{2} A_{x} - (A_{x})^2 \\end{aligned} \\] Note that the actual benefit must also be squared - this is likely to result in a relatively large value for the second moment and variance. Term Assurance \u00b6 Term Assurance covers the insured for a specified period \\(n\\) and only pays out if the insured dies within that period. NOTHING is paid out if the insured survives beyond that. A WL Assurance can be thought of as a special Term Assurance with infinite coverage. Let TA be the random variable denoting the PV of the death benefit: \\[ \\begin{aligned} \\text{TA} &= \\begin{cases} v^{K_x + 1} ,& K_x = 0, 1, \\dots, n-1 \\\\ 0 ,& K_x \\ge n \\end{cases} \\\\ &= {}_{\\{K_x \\le n-1\\}} v^{K_x+1} \\end{aligned} \\] \\(\\{K_x \\le n-1\\}\\) is known as an Indicator Function , which is a binary variable that takes 1 if the condition is true and 0 if the condition if false . It provides a concise way to express a piecewise function in a single expression. The EPV is the expectation of the TA random variable: \\[ \\begin{aligned} E(\\text{TA}) &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + 0 \\cdot {}_{n}p_{x} \\\\ A^1_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Similar as before, the second moment and thus Variance can be calculated as the following: \\[ \\begin{aligned} E({\\text{TA}}^2) &= \\sum^{n-1}_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} + 0^2 \\cdot {}_{n}p_{x} \\\\ {}^{2} A^1_{x:\\enclose{actuarial}{n}} &= \\sum^\\infty_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ \\\\ \\therefore Var(\\text{TA}) &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} - (A^1_{x:\\enclose{actuarial}{n}})^2 \\end{aligned} \\] Pure Endowment \u00b6 Pure Endowments are a special kind of contract that instead only pays out if the insured survives past a specified period \\(n\\) . NOTHING is paid out if the insured dies before that. Let PE be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{PE} &= \\begin{cases} 0 ,& K_x = 0, 1, 2 \\dots, n \\\\ v^n ,& K_x \\ge n \\end{cases} \\end{aligned} \\] The EPV is the expectation of the PE random variable. However, note that the probability of surviving past the period is given by a single probability \\({}_{n}p_{x}\\) : \\[ \\begin{aligned} E(\\text{PE}) &= 0 \\cdot {}_{n}q_{x} + v^n {}_{n}p_{x} \\\\ {}_{n}E_x &= v^n {}_{n}p_{x} \\end{aligned} \\] Similar as before, the second moment and thus Variance can be calculated: \\[ \\begin{aligned} E({\\text{PE}}^2) &= 0^2 \\cdot {}_{n}q_{x} + (v^2)^n {}_{n}p_{x} \\\\ {}^{2}_{n}E_x &= (v^*)^n {}_{n}p_{x} \\\\ \\\\ \\therefore Var(\\text{PE}) &= {}^{2}_{n}E_x - ({}_{n}E_x)^2 \\end{aligned} \\] Endowment Assurance \u00b6 Endowment Assurances are a combination of term assurances and pure endowments: Term Assurance - Pays out if the insured dies within the period Pure Endowment - Pays out if the insured survives past the period Thus, endowment assurances WILL pay out no matter what Let EA be the random variable denoting the PV of the benefits: \\[ \\begin{aligned} \\text{EA} &= \\begin{cases} v^{K_x + 1} ,& K_x = 0, 1, \\dots, n-1 \\\\ v^n ,& K_x \\ge n \\end{cases} \\\\ &= v^{min(K_x + 1, n)} \\end{aligned} \\] The EPV is the expectation of the EA random variable: \\[ \\begin{aligned} E(\\text{EA}) &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n {}_{n}p_{x} \\\\ A_{x:\\enclose{actuarial}{n}} &= A^1_{x:\\enclose{actuarial}{n}} + {}_{n}E_{x} \\end{aligned} \\] Note that the in the final year of the contract, the assurance will pay \\(v^n\\) regardless of the outcome - TA pays if they die while PE pays if they survive. Thus, a simplification can be made to the EPV: \\[ \\begin{aligned} A_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n {}_{n}p_{x} \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} + v^n {}_{n-1}p_{x} {}_{}q_{x + n - 1} + v^n {}_{n}p_{x} \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n ({}_{n-1}p_{x} {}_{}q_{x + n - 1} + {}_{n}p_{x}) \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n {}_{n-1}p_{x} \\end{aligned} \\] Similar as before, the second moment and thus Variance can be calculated: \\[ \\begin{aligned} E(\\text{EA}^2) &= \\sum^{n-1}_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} + (v^2)^n {}_{n}p_{x} \\\\ {}^{2} A_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} + (v^*)^n {}_{n}p_{x} \\\\ &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\\\ \\\\ \\therefore Var(\\text{EA}) &= {}^{2} A_{x:\\enclose{actuarial}{n}} - \\left(A_{x:\\enclose{actuarial}{n}}\\right)^2 \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left(A^1_{x:\\enclose{actuarial}{n}} - ({}_{n}E_x) \\right)^2 \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left[(A^1_{x:\\enclose{actuarial}{n}})^2 + 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) + ({}_{n}E_x)^2 \\right] \\end{aligned} \\] The same outcome can be reached using a slightly different approach: \\[ \\begin{aligned} Var(EA) &= Var(\\text{TA} + \\text{PE}) \\\\ &= Var(\\text{TA}) + Var(\\text{PE}) + 2 Cov(\\text{TA}, \\text{PE}) \\\\ &= Var(\\text{TA}) + Var(\\text{PE}) + 2 [E(\\text{TA} \\cdot \\text{PE}) - E(\\text{TA}) \\cdot E(\\text{PE})] \\end{aligned} \\] Consider the distribution of TA and PE: \\[ \\begin{aligned} TA \\cdot PE &= \\begin{cases} v^{K_x + 1} \\cdot 0, K_x \\lt n \\\\ 0 \\cdot v^n, K_x \\ge n \\end{cases} \\\\ &= \\begin{cases} 0, K_x \\lt n \\\\ 0, K_x \\ge n \\end{cases} \\\\ \\\\ \\therefore E(\\text{TA} \\cdot \\text{PE}) &= 0 \\cdot {}_{n}q_x + 0 \\cdot {}_{n}p_x \\\\ &= 0 \\end{aligned} \\] This results in the same variance as before: \\[ \\begin{aligned} Var(EA) &= Var(\\text{TA}) + Var(\\text{PE}) + 2 [E(\\text{TA} \\cdot \\text{PE}) - E(\\text{TA}) \\cdot E(\\text{PE})] \\\\ &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} - (A^1_{x:\\enclose{actuarial}{n}})^2 + {}^{2}_{n}E_x - ({}_{n}E_x)^2 - 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left[(A^1_{x:\\enclose{actuarial}{n}})^2 + 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) + ({}_{n}E_x)^2 \\right] \\\\ \\end{aligned} \\] While this process might seem redundant, it provides an easy to understand example of how the variance of a combination of assurances is derived by considering the covariance. Deferred Assurances \u00b6 Deferred Assurances are variations of any of the above assurances, where the assurance starts \\(n\\) years later rather than immediately. While any assurance can be deferred, the most common is the Deferred Whole Life . Deferred WL \u00b6 Let DWL be the random variable denoting the PV of the benefits of a deferred whole life assurance: \\[ \\begin{aligned} \\text{DWL} &= \\begin{cases} 0,& K_x = 0, 1, 2, \\dots, n-1 \\\\ v^{K_x + 1} ,& K_x \\ge n \\end{cases} \\\\ &= {}_{\\{K_x \\ge n\\}} v^{K_x + 1} \\end{aligned} \\] The EPV is the expectation of the DWL random variable: \\[ \\begin{aligned} E(\\text{DWL}) &= 0 \\cdot {}_{n}p_{x} + \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ {}_{n|} A_{x} &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} - \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ &= A_x - A^1_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] Alternatively, since a DWL is simply a WL assurance issued \\(n\\) years later, the EPV of the DWL is equivalent to the EPV of a WL issued at age \\(x+n\\) after adjusting for interest and survival : \\[ \\begin{aligned} {}_{n|} A_{x} &= \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ &= \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x}p_{x} \\cdot q_{x+K_x} \\\\ &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1 + n} \\cdot {}_{K_x+n}p_{x} \\cdot q_{x+K_x+n} \\\\ &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} v^{n} \\cdot {}_{n}p_{x} {}_{K_x}p_{x+n} \\cdot q_{x+K_x+n} \\\\ &= v^n {}_{n} p_{x} \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x+n} \\\\ &= {}_{n}E_{x} \\cdot A_{x+n} \\\\ \\end{aligned} \\] PEs can be use as a discount factor for functions that takes mortality into consideration . If provided by the life table, this reduces the need for computation. When going \"back\" in time, it must reflect that the policyholder will eventually survive till the current age, which is why the probability of surviving the period must be multiplied. This allows a TA to be expressed as the difference of two WL assurances issued at different times : \\[ \\begin{aligned} {}_{n|} A_{x} &= {}_{n}E_{x} * A_{x+n} \\\\ A_x - A^1_{x:\\enclose{actuarial}{n}} &= {}_{n}E_{x} * A_{x+n} \\\\ A^1_{x:\\enclose{actuarial}{n}} &= A_{x} - {}_{n}E_{x} * A_{x+n} \\\\ \\end{aligned} \\] This result is extremely important as this is the main method of calculating the EPV of a TA as the values for the WL can be easily found in the SULT. If the interest is not 0.05 or if mortality does NOT follow the SULT, then the EPV of a TA must be calculated manually. However, there is usually a catch that allows the EPV to be easily calculated manually: Different Interest : Term of the contract is short (EG. 3 Years) Different Mortality : Mortality can be simplified (EG. Becomes a constant) It is rare to have problems that have both different interest and mortality. In such cases, it is likely that an adjusted mortality table is provided. The second moment of a TA is still an expectation, thus the above method applies to it as well: \\[ \\begin{aligned} {}^{2} A^1_{x:\\enclose{actuarial}{n}} &= {}^{2} A_{x} - {}^{2}_{n} E_{x} \\cdot {}^{2} A_{x+n} \\end{aligned} \\] The key intuition is understanding that the discounting factor should be squared as well, which is why \\({}^{2}_{n} E_{x}\\) is used instead. Finally, the variance of a DWL can be easily calculated by applying previous results: \\[ \\begin{aligned} Var (DWL) &= {}^{2}_{n|}A_x - ({}_{n|}A_x)^2 \\\\ &= \\left({}^{2}A_x - {}^{2}A^1_{x:\\enclose{actuarial}{n}}\\right) - \\left(A_x - A^1_{x:\\enclose{actuarial}{n}}\\right)^2 \\\\ &= \\left({}^{2}A_x - {}^{2}A^1_{x:\\enclose{actuarial}{n}}\\right) + \\left[(A_x)^2 - 2(A_x)(A^1_{x:\\enclose{actuarial}{n}}) + (A^1_{x:\\enclose{actuarial}{n}})^2 \\right] \\end{aligned} \\] Deferred TA \u00b6 Another less commonly used deferred assurance is the Deferred Term Assurance . As both a deferred and term assurance, both results apply to it: \\[ \\begin{aligned} {}_{k|}A^1_{x:\\enclose{actuarial}{n}} &= {}_{k}E_{x} \\cdot A^1_{x+k:\\enclose{actuarial}{n}} \\\\ &= {}_{k}E_{x} \\cdot (A_{x+k} - {}_{n}E_{x} \\cdot A_{x+k+n}) \\end{aligned} \\] Although rarely used, this result can be tricky due to the different PEs used to discount - one is for the deferred assurance while the other is for the term. Alternatively, it can be used as a building block to decompose a regular assurance. An \\(n\\) year term assurance can be thought of as the sum of \\(n\\) deferred TAs , each with a one year term: \\[ \\begin{aligned} A^1_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{k=0} {}_{k|}A^1_{x:\\enclose{actuarial}{1}} \\\\ A_{x} &= \\sum^{\\infty}_{k=0} {}_{k|}A_{x} \\end{aligned} \\] Since WLs are just term assurances with infinite coverage, it can be extended to WLs as well if needed. This result will come in handy in later sections. Recursions \u00b6 The EPV of each contract can also be expressed through backwards recursion , where it is calculated as a function of itself. Consider the WL random variable: If the policyholder dies in the year, then a benefit of 1 is paid at the end of the year. If the policyholder survives past the year, then the policyholder will die in some future year. The PV of the benefit at the end of the year is given is the EPV of the same contract but at that future time ; \\(A_{x+1}\\) . \\[ \\begin{aligned} WL &= \\begin{cases} v \\cdot 1 ,& {}_{}q_{x} \\\\ v \\cdot A_{x+1} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore A_x = v{}_{}q_{x} + v{}_{}p_{x}A_{x+1} \\] The same exercise can be shown for the TA random variable. The main difference is understanding how the age & duration changes: \\(x+1\\) reflects the new age of the policyholder (same as WL) \\(n-1\\) reflects that one year of coverage has passed (not applicable for WL) \\[ \\begin{aligned} TA &= \\begin{cases} v \\cdot 1 ,& {}_{}q_{x} \\\\ v \\cdot A^1_{x+1:\\enclose{actuarial}{n-1}} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore A^1_{x:\\enclose{actuarial}{n}} = v{}_{}q_{x} + v{}_{}p_{x}A^1_{x+1:\\enclose{actuarial}{n-1}} \\] Note that since it requires a term policy with a reduced term , this recursion is not particularly useful as it is difficult to obtain it. The PE variable is similar, with the main difference being that the policyholder will receive nothing if the policyholder dies . Thus, only the second component of the recursion remains: \\[ \\begin{aligned} PE &= \\begin{cases} 0 ,& {}_{}q_{x} \\\\ v \\cdot {}_{n-1}E_{x+1} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore {}_{n}E_{x} = v{}_{}p_{x} \\cdot {}_{n-1}E_{x+1} \\] EA is omitted from this section as it is simply the combination of a TA and PE. These identities are most useful in a spreadsheet setting where the calculations can be easily repeated to fill up an entire life table. However, even then it is necessary to have a starting point for the recusions to occur. The most common starting point is the terminal age as the EPVs can be intuitively determined since the policyholder will inevitably die at the end of the year: \\[ \\begin{aligned} A_{\\omega-1} &= v \\\\ A^{\\> \\> 1}_{\\omega-1:\\enclose{actuarial}{n}} &= v \\\\ {}_{n}E_{\\omega-1} &= 0 \\\\ A^{1}_{\\omega-1:\\enclose{actuarial}{n}} &= v \\end{aligned} \\] Intuitions \u00b6 Although the exam questions are mostly computational, it is good to have an understanding of how the different EPVs compare against one another to serve as a sort of \"sense check\" against the SULT provided to determine if the answer is in the right direction. Same Assurance \u00b6 Recall that the probability of death is an increasing function with age. The death benefit is more likely to be paid out to an older policyholder - in other words, they receive the death benefit \"sooner\" than a younger policyholder. Thus, an older policyholder has larger expected cashflows that are discounted less (due to receiving it sooner), which results in a higher EPV than a younger policyholder, all else equal: \\[ \\begin{aligned} A_{x+k} & \\gt A_{x} \\\\ A^{\\> \\> 1} _{x+k:\\enclose{actuarial}{n}} & \\gt A^{\\> \\> 1}_{x+k:\\enclose{actuarial}{n}} \\end{aligned} \\] Conversely, the probability of survival is a decreasing function with age. The survival benefit is less likely to be paid out to an older policyholder - smaller expected cashflows. Regardless of the age of the policyholder, the survival benefit is paid at the same time ( same discounting ). Thus, since an older policyholder has smaller expected cashflows , it has a lower EPV than a younger policyholder: \\[ {}_{n}E_{x+k} \\le {}_{n}E_{x} \\] Endowment Assurances have both a death and survival component , thus the comparison is a combination of the two: A younger policyholder is more likely to survive and receive the survival benefit at the end of the term (discounted more) An older policyholder is more likely to die and receive the death benefit during the term (discounted less) Assuming that the difference in expected cashflows are negligible , then an older policyholder would have an higher EPV due to the lower discounting : \\[ A_{x+k:\\enclose{actuarial}{n}} \\gt A_{x:\\enclose{actuarial}{n}} \\] Naturally, all else equal, assurances with a lower interest rate are discounted less and thus have a higher EPV . If a seperate mortality table with EPVs are given, it is likely that the question is not using an interest rate of 5%. Different Assurances \u00b6 At a young age where the probability of death is low, all else equal (where applicable), the EPVs of each assurance rank as follows: TA will have the smallest EPV . Although their benefits are paid out sooner, the expected benefits are small as the probability of death is small. WL has the next largest EPV . They have the same benefits as term in the short run, but have large expected benefits in the future . However, these large benefits are heavily discounted , still resuling in a small EPV. PE has the next largest EPV . Given the high probability of survival, the expected benefits are large . EA has the largest EPV . Since it is a combination of TA and PE, it is naturally the highest. \\[ \\begin{aligned} A^{1}_{30:\\enclose{actuarial}{n}} < A_{30} < {}_{n}E_{30} < A_{30:\\enclose{actuarial}{n}} \\\\ \\end{aligned} \\] At an old age where the probability of death is high, all else equal (where applicable), the EPVs of each assurance rank as follows: PE will have the smallest EPV . Given the low probability of survival, the expected benefits are are small . TA will have the next largest EPV . Given the high probability of death, the expected benefits are high . EA will have the next largest EPV . Since it is combination of TA and PE, it is naturally higher than both of them. WL has the largest EPV . Given the inevitable death of the policyholder, the expected benefits are the highest . \\[ \\begin{aligned} {}_{n}E_{100} < A^{1}_{100:\\enclose{actuarial}{n}} < A_{100:\\enclose{actuarial}{n}} < A_{100} \\\\ \\end{aligned} \\] As the policyholder approaches the terminal age, the EPVs tend to one another: \\[ \\begin{aligned} x &\\to \\omega \\\\ E(\\text{PE}) &\\to 0 \\\\ E(\\text{EA}) &\\to E(\\text{TA}) \\\\ E(\\text{TA}) &\\to E(\\text{WL}) \\end{aligned} \\] TA tends to WL whenever the end of the term exceeds the terminal age - thus the cashflows and hence EPV for both assurances become identical. Probabilities and Percentiles \u00b6 Apart from just calculating the Expectation and Variance, the probabilities and hence percentiles of the contract benefits can be calculated as well. The former refers to calculating the probability that the random variable is at most some value \\(u\\) . In other words, that the PV ( NOT the EPV! ) is at most \\(u\\) . \\[ \\begin{aligned} \\text{WL} &\\le u \\\\ v^{K_x+1} &\\le u \\\\ (K_x+1) &\\ln v \\le \\ln u \\\\ K_x+1 &\\ge \\frac{\\ln u}{\\ln v} \\\\ K_x & \\ge \\frac{\\ln u}{\\ln v} - 1 \\\\ \\end{aligned} \\] Note that it is a common mistake to forget to flip the inequality sign as \\(\\ln v \\lt 0\\) . To avoid this error, it is advised to plot a graph to remember that \\(K_x\\) should be larger than the calculated value: The RHS of the expression is unlikely to be an integer. However, \\(K_x\\) can only take integer values. Thus, the values can rounded up to the nearest whole number (EG. 5): \\[ \\begin{aligned} P(K_x \\ge \\frac{\\ln u}{\\ln v} - 1) &= P(K_x \\ge 5) \\\\ &= P(T_x \\ge 5) \\\\ &= {}_{5}p_x \\end{aligned} \\] The latter refers to calculating the percentile of the random variable , which is the smallest value of the RV that results in the specified probability. This process is the opposite of the previous one - it involves solving for \\(T_x\\) , converting it to \\(K_x\\) and then subsituting it back into the RV, which results in the associated percentile. Payable Continuously \u00b6 In practice, life assurances pay benefits as soon as the insured event occurs , thus is akin to paying out continuously throughout the year. Notice that paying out continuously is actually a special case of a contract that pays out \\(m\\) times a year , where \\(m\\) tends to infinity. \\(m=4\\) ; Quarterly Payments \\(m=12\\) ; Monthly Payments \\(m=\\infty\\) ; Continuous Payments Thus, despite the header stating \"payable continously\", this section will instead cover paying out \\(m\\) times a year , which can be used to derive the continuous case. Note Questions with Assurances that pay out \\(m\\) times a year are rare - if anything, the continuous case will be directly tested instead. However, the \\(m\\) times a year case is still covered because the ideas can be extended to Annuities , where such questions are common. The survival model used must now reflect the probability of death at fractional ages . Intuitively, it can be thought of as the probability living till a discrete age and then dying within a sub-period within that year: \\[ \\begin{aligned} K_x + \\frac{j+1}{m}, \\>\\> j = 0, 1, 2, ..., m-1 \\end{aligned} \\] Thus, the corresponding random variable representing the PV of the benefits: \\[ \\text{PV} = v^{K_x + \\frac{j+1}{m}} \\] The EPV of an assurance payable \\(m\\) times a year can be denoted with the \\((m)\\) superscript with the same notations as before: \\[ A^{(m)}_{x} = \\sum^{\\infty}_{K_x=0} v^{K_x} {}_{K_x}p_x \\cdot \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}} {}_{\\frac{j}{m}}p_{x+K_x} \\cdot {}_{\\frac{1}{m}} q_{x+K_x+\\frac{j}{m}} \\\\ \\] Annual Benefit Note that the benefit used is still the ANNUAL BENEFIT . To obtain the actual benefit payable per period , then it should be divided by the number of periods; \\(\\frac{B}{m}\\) . Unfortunately, most life tables do not naturally provide probabilities for death at fractional ages, thus the EPVs must be approximated from the discrete case , which will be covered later. Continuous Case \u00b6 As \\(m \\to \\infty\\) , then the survival model becomes \\(T_x\\) and hence random variable is: \\[ \\begin{aligned} \\text{PV} &= v^{T_x} \\\\ &= e^{-\\delta \\cdot T_x} \\end{aligned} \\] The EPV of an assurance payable continously is denoted with the \\(\\bar{}\\) accent with the same notations as before: \\[ \\begin{aligned} \\bar{A}_x &= \\int^{\\infty}_{0} v^{T_x} \\cdot f_x(t) \\\\ &= \\int^{\\infty}_{0} e^{-\\delta \\cdot T_x} \\cdot {}_{t}p_{x} \\mu_{x+t} \\end{aligned} \\] Recall that the second term represents the probability of living to a certain age \\(x+t\\) and then dying in an infinitely small time after that (definition of force of interest). Thus, although the EPV looks very different, it is still fundamentally a Triple Product Summation of the same components: Amount of Benefit, 1 Discounting of the Benefit, \\(e^{-\\delta \\cdot T_x}\\) Probability of paying the Benefit, \\({}_{t}p_{x} \\mu_{x+t}\\) Unlike the payable \\(m\\) times case, the EPVs can be calculated directly if the survival distribution is provided. However, since they are special cases of the payable \\(m\\) times case, it can also be calculated through approximation from the discrete case. The same logic can be applied to all other assurances, EXCEPT for PEs . This is because they pay out at a fixed time , thus there is NO such thing as a payable \\(m\\) times PE or a continuously payable PE. Uniform Distribution of Deaths \u00b6 Recall that the UDD assumption can be used to approximate survival probabilities of fractional ages from discrete probabilities. Using this approach, the Continuous EPVs can be approximated from the Discrete ones. Assuming UDD between integer ages, \\[ {}_{\\frac{j}{m}}p_{x+K_x} \\cdot {}_{\\frac{1}{m}} q_{x+K_x+\\frac{j}{m}} \\approx \\frac{1}{m} \\cdot q_{x+K_x} \\] Thus, the EPV of a continuous contract can be expressed as a function of the discrete case: \\[ \\begin{aligned} A^{(m)}_{x} &= \\sum^{\\infty}_{K_x=0} v^{K_x} {}_{K_x}p_x \\cdot \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}} {}_{\\frac{j}{m}}p_{x+K_x} \\cdot {}_{\\frac{1}{m}} q_{x+K_x+\\frac{j}{m}} \\\\ & \\approx \\sum^{\\infty}_{K_x=0} v^{K_x} {}_{K_x}p_x \\cdot \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}} \\frac{1}{m} \\cdot q_{x+K_x} \\\\ & \\approx \\sum^{\\infty}_{K_x=0} v^{K_x+1} {}_{K_x}p_x q_{x+K_x} \\cdot \\frac{1}{m} \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}-1} \\\\ & \\approx A_x \\cdot \\frac{v^{-1}}{m} \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}} \\\\ & \\approx A_x \\cdot \\frac{1+i}{m} \\cdot \\frac{v^{\\frac{1}{m}}(1-v)}{1-v^{\\frac{1}{m}}} \\\\ & \\approx A_x \\cdot \\frac{1+i}{m} \\cdot \\frac{v^{\\frac{1}{m}}(1-v)}{v^{\\frac{1}{m}}[(1+i)^{m}-1]} \\\\ & \\approx A_x \\cdot \\frac{1+i}{m} \\cdot \\frac{1-v}{(1+i)^{m}-1} \\\\ & \\approx A_x \\cdot \\frac{1+i-1}{m[(1+i)^{m}-1]} \\\\ & \\approx \\frac{i}{i^{(m)}} \\cdot A_x \\end{aligned} \\] As \\(m \\to \\infty\\) , \\[ \\begin{aligned} i^{(m)} &\\to \\delta, \\\\ A^{(m)}_{x} &\\to \\bar{A}_x, \\\\ \\therefore \\bar{A}_x &= \\frac{i}{\\delta} A_x \\end{aligned} \\] Claims Acceleration Approach \u00b6 Given that claims occur every \\(\\frac{1}{m}\\) period, then on average , the claims within a year occur at \\(\\frac{m+1}{2m}\\) : \\[ \\begin{aligned} \\text{Average Death} &= \\frac{\\frac{1}{m} + \\frac{m}{m}}{2} \\\\ &= \\frac{\\frac{1+m}{m}}{2} \\\\ &= \\frac{m+1}{2m} \\end{aligned} \\] Thus, the EPV of a continuous contract can be expressed as a function of the discrete case: \\[ \\begin{aligned} A^{(m)}_{x} & \\approx \\sum^{\\infty}_{K_x = 0} v^{K_x + \\frac{m+1}{2m}} {}_{K_x|} q_{x} \\\\ & \\approx v^{\\frac{m+1}{2m}-1} \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} {}_{K_x|} q_{x} \\\\ & \\approx v^{\\frac{m+1-2m}{2m}} A_x \\\\ & \\approx v^{\\frac{-m+1}{2m}} A_x \\\\ & \\approx v^{-\\frac{m-1}{2m}} A_x \\\\ & \\approx (1+i)^{\\frac{m-1}{2m}} A_x \\end{aligned} \\] As \\(m \\to \\infty\\) , \\[ \\begin{aligned} m &\\to \\infty, \\\\ \\frac{m-1}{2m} &\\to \\frac{1}{2}, \\\\ A^{(m)}_{x} &\\to \\bar{A}_x, \\\\ \\therefore \\bar{A}_x &= (1+i)^{\\frac{1}{2}} A_x \\end{aligned} \\] This is known as the Claims Acceleration Approach as the claims are paid out earlier on in the year as compared to the end. Generally speaking, this approach is preferred as it only has one parameter compared to UDD which has two. Common Pitfalls \u00b6 Recall that there are no continuous PEs - thus, be very careful when approximating an EA as ONLY the TA portion needs to be approximated ! This means that we cannot directly apply an approximation to the EA values provided in the SULT - they must be broken down into the TA and EA components first. If calculating manually, it is always advised to calculate the TA and EA components seperately before combining into an EA to prevent falling into the trap of directly approximating the EA. \\[ \\begin{aligned} \\bar{A}_{x:\\enclose{actuarial}{n}} & \\ne \\frac{i}{\\delta} A_{x:\\enclose{actuarial}{n}} \\\\ \\bar{A}_{x:\\enclose{actuarial}{n}} & = \\frac{i}{\\delta} A^{1}_{x:\\enclose{actuarial}{n}} + {}_{n}E_x \\end{aligned} \\] Similarly, if approximating the second moment for variance, note that the NEW interest rate must be used in the approximation term as well: \\[ \\begin{aligned} {}^{2} \\bar{A}_x &= \\frac{i^*}{\\delta^*} \\cdot {}^{2} A_x \\end{aligned} \\] Note Although the above two points were illustrated with UDD, they apply to the claims acceleration approach as well. Lastly, the two approaches should produce similar results which differ by a few decimal places. However, these differences are enlarged when dealing with large benefits as these decimal places will be brought forward, resulting in seemingly large differences - do not be alarmed!","title":"Life Assurances"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#life-assurances","text":"","title":"Life Assurances"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#overview","text":"Life Assurances are contracts that promise to pay out a benefit if the insured event occurs in the future . The value of a life assurance must reflect these two aspects: Uncertainty of cashflows - Expected Value , based on Survival Models Time value of money - Present Value , based on Interest Theory Thus, the value of a life assurance is the Expected Present Value (EPV) of the promised cashflows.","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#payable-discretely","text":"The simplest form of life assurance pays the benefits at the end of the year that the insured event occurs. While not common in practice, it provides a simple framework to understand the core concepts. For an assurance payable discretely, \\(K_x\\) is used as the survival model . Thus, the present value of the benefit payable in each period is \\(v^{K_x + 1}\\) . The EPV is the Triple Product Summation of the various components: Amount of benefit, \\(B\\) Discounting of the benefit, \\(v^{K_x + 1}\\) Probability of paying the benefit, \\({}_{K_x}p_{x} {}_{}q_{x + K_x}\\) \\[ \\begin{aligned} EPV &= \\sum B \\cdot v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\\\ &= \\sum B \\cdot v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] For simplicity, all of the proofs in this section will use \\(B=1\\) . This also has the benefit of allowing the EPVs to be easily scaled for any level of \\(B\\) .","title":"Payable Discretely"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#actuarial-notation","text":"Similar to the survival model, given how often these values are calculated, they are abbreviated using the International Actuarial Notation as well. \\(A\\) represents the first moment (expectation) of the present value of a contract where a benefit of 1 is payable discretely when the status fails . The subscript \\(x\\) is the age of the policyholder, known as the Life Status . It fails when the policyholder dies . An additional subscript \\(:\\enclose{actuarial}{n}\\) is the duration that the assurance remains valid, known as the Duration Status . It fails if the policyholder survives past the policyterm. A \\(1\\) above the status indicates that the assurance pays only pays if that particular status fails first . If nothing is indicated, then the assurance pays whichever status fails first. If a particular status is omitted, then the assurance is not dependent on that status. For instance, omitting the duration status means that the contract is only dependent on the age of the policyholder. An assurance that begins \\(n\\) years later is known as a Deferred Assurance , which is denoted by \\({}_{n|}A\\) , following the same notation as deferred probabilities. Thus, putting everything together, the EPV of each assurance can be denoted as follows: Whole Life Assurance - Payable whenever policyholder dies; \\(A_x\\) Term Assurance - Payable only if policyholder dies during assurance period; \\(A^{1}_{x:\\enclose{actuarial}{n}}\\) Pure Endowment - Payable only if policyholder survives past assurance period; \\(A^{\\>\\>\\> 1}_{x:\\enclose{actuarial}{n}}\\) Endowment Assurance - Payable whichever of the above two occur first; \\(A_{x:\\enclose{actuarial}{n}}\\) Deferred Whole Life Assurance - Payable only if policyholder dies after \\(n\\) years; \\({}_{n|}A_x\\) Pure Endowments can also be expressed as \\({}_{n}E_x\\) for easier typesetting.","title":"Actuarial Notation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#whole-life-assurance","text":"Whole Life Assurances cover the insured indefinitely and thus will pay out whenever the insured dies. Let WL be the random variable denoting the PV of the death benefit: \\[ \\begin{aligned} \\text{WL} &= v^{K_x + 1} \\\\ K_x &= 0, 1, \\dots, \\infty \\end{aligned} \\] Technically speaking, the upper limit of \\(K_x\\) should \\(\\omega-x\\) as it represents the maximum attainable age in the discrete survival model. However, since the \\({}_{k}p_{x} = 0\\) for all \\(k \\ge \\omega-x\\) , it does not matter what the upper limit is as long as it is larger than \\(\\omega-x\\) . Thus, \\(\\infty\\) is used for conciseness instead. The EPV is the Expectation/First Moment of the WL random variable: \\[ \\begin{aligned} E(\\text{WL}) &= \\sum^\\infty_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ A_{x} &= \\sum^\\infty_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Another commonly used metric is the Variance of the contract. In order to get it, the Second Moment must first be determined: \\[ \\begin{aligned} E({\\text{WL}}^2) &= \\sum^\\infty_{K_x = 0} \\left(v^{K_x + 1}\\right)^2 \\cdot {}_{K_x|}q_{x} \\\\ &= \\sum^\\infty_{K_x = 0} \\left((v^2)^{K_x + 1}\\right) \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Notice that the expression can be simplified to a form that looks almost identical to the first moment - with the only difference being that is uses \\(v^2\\) instead of \\(v\\) . Thus, the second moment is simply the first moment evaluated at a higher interest rate \\(i^*=(1+i)^2-1\\) , such that \\(v^* = v^2\\) . Generally, the k-th moment is denoted as \\({}^{k}A_x\\) , where the \\(k\\) is the multiplier on the interest rate used to evaluate the moment, \\(i^*=(1+i)^k-1\\) . It can also be written as \\(A_x |_{i = (1+i)^2-1}\\) using non-actuarial notation, which will come in handy for more complicated expressions. The Second Moment and hence Variance can be shown as: \\[ \\begin{aligned} E({\\text{WL}}^2) &= \\sum^\\infty_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ {}^{2} A_{x} &= \\sum^\\infty_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ \\\\ \\therefore Var(\\text{WL}) &= {}^{2} A_{x} - (A_{x})^2 \\end{aligned} \\] Note that the actual benefit must also be squared - this is likely to result in a relatively large value for the second moment and variance.","title":"Whole Life Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#term-assurance","text":"Term Assurance covers the insured for a specified period \\(n\\) and only pays out if the insured dies within that period. NOTHING is paid out if the insured survives beyond that. A WL Assurance can be thought of as a special Term Assurance with infinite coverage. Let TA be the random variable denoting the PV of the death benefit: \\[ \\begin{aligned} \\text{TA} &= \\begin{cases} v^{K_x + 1} ,& K_x = 0, 1, \\dots, n-1 \\\\ 0 ,& K_x \\ge n \\end{cases} \\\\ &= {}_{\\{K_x \\le n-1\\}} v^{K_x+1} \\end{aligned} \\] \\(\\{K_x \\le n-1\\}\\) is known as an Indicator Function , which is a binary variable that takes 1 if the condition is true and 0 if the condition if false . It provides a concise way to express a piecewise function in a single expression. The EPV is the expectation of the TA random variable: \\[ \\begin{aligned} E(\\text{TA}) &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + 0 \\cdot {}_{n}p_{x} \\\\ A^1_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Similar as before, the second moment and thus Variance can be calculated as the following: \\[ \\begin{aligned} E({\\text{TA}}^2) &= \\sum^{n-1}_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} + 0^2 \\cdot {}_{n}p_{x} \\\\ {}^{2} A^1_{x:\\enclose{actuarial}{n}} &= \\sum^\\infty_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ \\\\ \\therefore Var(\\text{TA}) &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} - (A^1_{x:\\enclose{actuarial}{n}})^2 \\end{aligned} \\]","title":"Term Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#pure-endowment","text":"Pure Endowments are a special kind of contract that instead only pays out if the insured survives past a specified period \\(n\\) . NOTHING is paid out if the insured dies before that. Let PE be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{PE} &= \\begin{cases} 0 ,& K_x = 0, 1, 2 \\dots, n \\\\ v^n ,& K_x \\ge n \\end{cases} \\end{aligned} \\] The EPV is the expectation of the PE random variable. However, note that the probability of surviving past the period is given by a single probability \\({}_{n}p_{x}\\) : \\[ \\begin{aligned} E(\\text{PE}) &= 0 \\cdot {}_{n}q_{x} + v^n {}_{n}p_{x} \\\\ {}_{n}E_x &= v^n {}_{n}p_{x} \\end{aligned} \\] Similar as before, the second moment and thus Variance can be calculated: \\[ \\begin{aligned} E({\\text{PE}}^2) &= 0^2 \\cdot {}_{n}q_{x} + (v^2)^n {}_{n}p_{x} \\\\ {}^{2}_{n}E_x &= (v^*)^n {}_{n}p_{x} \\\\ \\\\ \\therefore Var(\\text{PE}) &= {}^{2}_{n}E_x - ({}_{n}E_x)^2 \\end{aligned} \\]","title":"Pure Endowment"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#endowment-assurance","text":"Endowment Assurances are a combination of term assurances and pure endowments: Term Assurance - Pays out if the insured dies within the period Pure Endowment - Pays out if the insured survives past the period Thus, endowment assurances WILL pay out no matter what Let EA be the random variable denoting the PV of the benefits: \\[ \\begin{aligned} \\text{EA} &= \\begin{cases} v^{K_x + 1} ,& K_x = 0, 1, \\dots, n-1 \\\\ v^n ,& K_x \\ge n \\end{cases} \\\\ &= v^{min(K_x + 1, n)} \\end{aligned} \\] The EPV is the expectation of the EA random variable: \\[ \\begin{aligned} E(\\text{EA}) &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n {}_{n}p_{x} \\\\ A_{x:\\enclose{actuarial}{n}} &= A^1_{x:\\enclose{actuarial}{n}} + {}_{n}E_{x} \\end{aligned} \\] Note that the in the final year of the contract, the assurance will pay \\(v^n\\) regardless of the outcome - TA pays if they die while PE pays if they survive. Thus, a simplification can be made to the EPV: \\[ \\begin{aligned} A_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n {}_{n}p_{x} \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} + v^n {}_{n-1}p_{x} {}_{}q_{x + n - 1} + v^n {}_{n}p_{x} \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n ({}_{n-1}p_{x} {}_{}q_{x + n - 1} + {}_{n}p_{x}) \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n {}_{n-1}p_{x} \\end{aligned} \\] Similar as before, the second moment and thus Variance can be calculated: \\[ \\begin{aligned} E(\\text{EA}^2) &= \\sum^{n-1}_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} + (v^2)^n {}_{n}p_{x} \\\\ {}^{2} A_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} + (v^*)^n {}_{n}p_{x} \\\\ &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\\\ \\\\ \\therefore Var(\\text{EA}) &= {}^{2} A_{x:\\enclose{actuarial}{n}} - \\left(A_{x:\\enclose{actuarial}{n}}\\right)^2 \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left(A^1_{x:\\enclose{actuarial}{n}} - ({}_{n}E_x) \\right)^2 \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left[(A^1_{x:\\enclose{actuarial}{n}})^2 + 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) + ({}_{n}E_x)^2 \\right] \\end{aligned} \\] The same outcome can be reached using a slightly different approach: \\[ \\begin{aligned} Var(EA) &= Var(\\text{TA} + \\text{PE}) \\\\ &= Var(\\text{TA}) + Var(\\text{PE}) + 2 Cov(\\text{TA}, \\text{PE}) \\\\ &= Var(\\text{TA}) + Var(\\text{PE}) + 2 [E(\\text{TA} \\cdot \\text{PE}) - E(\\text{TA}) \\cdot E(\\text{PE})] \\end{aligned} \\] Consider the distribution of TA and PE: \\[ \\begin{aligned} TA \\cdot PE &= \\begin{cases} v^{K_x + 1} \\cdot 0, K_x \\lt n \\\\ 0 \\cdot v^n, K_x \\ge n \\end{cases} \\\\ &= \\begin{cases} 0, K_x \\lt n \\\\ 0, K_x \\ge n \\end{cases} \\\\ \\\\ \\therefore E(\\text{TA} \\cdot \\text{PE}) &= 0 \\cdot {}_{n}q_x + 0 \\cdot {}_{n}p_x \\\\ &= 0 \\end{aligned} \\] This results in the same variance as before: \\[ \\begin{aligned} Var(EA) &= Var(\\text{TA}) + Var(\\text{PE}) + 2 [E(\\text{TA} \\cdot \\text{PE}) - E(\\text{TA}) \\cdot E(\\text{PE})] \\\\ &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} - (A^1_{x:\\enclose{actuarial}{n}})^2 + {}^{2}_{n}E_x - ({}_{n}E_x)^2 - 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left[(A^1_{x:\\enclose{actuarial}{n}})^2 + 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) + ({}_{n}E_x)^2 \\right] \\\\ \\end{aligned} \\] While this process might seem redundant, it provides an easy to understand example of how the variance of a combination of assurances is derived by considering the covariance.","title":"Endowment Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#deferred-assurances","text":"Deferred Assurances are variations of any of the above assurances, where the assurance starts \\(n\\) years later rather than immediately. While any assurance can be deferred, the most common is the Deferred Whole Life .","title":"Deferred Assurances"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#deferred-wl","text":"Let DWL be the random variable denoting the PV of the benefits of a deferred whole life assurance: \\[ \\begin{aligned} \\text{DWL} &= \\begin{cases} 0,& K_x = 0, 1, 2, \\dots, n-1 \\\\ v^{K_x + 1} ,& K_x \\ge n \\end{cases} \\\\ &= {}_{\\{K_x \\ge n\\}} v^{K_x + 1} \\end{aligned} \\] The EPV is the expectation of the DWL random variable: \\[ \\begin{aligned} E(\\text{DWL}) &= 0 \\cdot {}_{n}p_{x} + \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ {}_{n|} A_{x} &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} - \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ &= A_x - A^1_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] Alternatively, since a DWL is simply a WL assurance issued \\(n\\) years later, the EPV of the DWL is equivalent to the EPV of a WL issued at age \\(x+n\\) after adjusting for interest and survival : \\[ \\begin{aligned} {}_{n|} A_{x} &= \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ &= \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x}p_{x} \\cdot q_{x+K_x} \\\\ &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1 + n} \\cdot {}_{K_x+n}p_{x} \\cdot q_{x+K_x+n} \\\\ &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} v^{n} \\cdot {}_{n}p_{x} {}_{K_x}p_{x+n} \\cdot q_{x+K_x+n} \\\\ &= v^n {}_{n} p_{x} \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x+n} \\\\ &= {}_{n}E_{x} \\cdot A_{x+n} \\\\ \\end{aligned} \\] PEs can be use as a discount factor for functions that takes mortality into consideration . If provided by the life table, this reduces the need for computation. When going \"back\" in time, it must reflect that the policyholder will eventually survive till the current age, which is why the probability of surviving the period must be multiplied. This allows a TA to be expressed as the difference of two WL assurances issued at different times : \\[ \\begin{aligned} {}_{n|} A_{x} &= {}_{n}E_{x} * A_{x+n} \\\\ A_x - A^1_{x:\\enclose{actuarial}{n}} &= {}_{n}E_{x} * A_{x+n} \\\\ A^1_{x:\\enclose{actuarial}{n}} &= A_{x} - {}_{n}E_{x} * A_{x+n} \\\\ \\end{aligned} \\] This result is extremely important as this is the main method of calculating the EPV of a TA as the values for the WL can be easily found in the SULT. If the interest is not 0.05 or if mortality does NOT follow the SULT, then the EPV of a TA must be calculated manually. However, there is usually a catch that allows the EPV to be easily calculated manually: Different Interest : Term of the contract is short (EG. 3 Years) Different Mortality : Mortality can be simplified (EG. Becomes a constant) It is rare to have problems that have both different interest and mortality. In such cases, it is likely that an adjusted mortality table is provided. The second moment of a TA is still an expectation, thus the above method applies to it as well: \\[ \\begin{aligned} {}^{2} A^1_{x:\\enclose{actuarial}{n}} &= {}^{2} A_{x} - {}^{2}_{n} E_{x} \\cdot {}^{2} A_{x+n} \\end{aligned} \\] The key intuition is understanding that the discounting factor should be squared as well, which is why \\({}^{2}_{n} E_{x}\\) is used instead. Finally, the variance of a DWL can be easily calculated by applying previous results: \\[ \\begin{aligned} Var (DWL) &= {}^{2}_{n|}A_x - ({}_{n|}A_x)^2 \\\\ &= \\left({}^{2}A_x - {}^{2}A^1_{x:\\enclose{actuarial}{n}}\\right) - \\left(A_x - A^1_{x:\\enclose{actuarial}{n}}\\right)^2 \\\\ &= \\left({}^{2}A_x - {}^{2}A^1_{x:\\enclose{actuarial}{n}}\\right) + \\left[(A_x)^2 - 2(A_x)(A^1_{x:\\enclose{actuarial}{n}}) + (A^1_{x:\\enclose{actuarial}{n}})^2 \\right] \\end{aligned} \\]","title":"Deferred WL"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#deferred-ta","text":"Another less commonly used deferred assurance is the Deferred Term Assurance . As both a deferred and term assurance, both results apply to it: \\[ \\begin{aligned} {}_{k|}A^1_{x:\\enclose{actuarial}{n}} &= {}_{k}E_{x} \\cdot A^1_{x+k:\\enclose{actuarial}{n}} \\\\ &= {}_{k}E_{x} \\cdot (A_{x+k} - {}_{n}E_{x} \\cdot A_{x+k+n}) \\end{aligned} \\] Although rarely used, this result can be tricky due to the different PEs used to discount - one is for the deferred assurance while the other is for the term. Alternatively, it can be used as a building block to decompose a regular assurance. An \\(n\\) year term assurance can be thought of as the sum of \\(n\\) deferred TAs , each with a one year term: \\[ \\begin{aligned} A^1_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{k=0} {}_{k|}A^1_{x:\\enclose{actuarial}{1}} \\\\ A_{x} &= \\sum^{\\infty}_{k=0} {}_{k|}A_{x} \\end{aligned} \\] Since WLs are just term assurances with infinite coverage, it can be extended to WLs as well if needed. This result will come in handy in later sections.","title":"Deferred TA"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#recursions","text":"The EPV of each contract can also be expressed through backwards recursion , where it is calculated as a function of itself. Consider the WL random variable: If the policyholder dies in the year, then a benefit of 1 is paid at the end of the year. If the policyholder survives past the year, then the policyholder will die in some future year. The PV of the benefit at the end of the year is given is the EPV of the same contract but at that future time ; \\(A_{x+1}\\) . \\[ \\begin{aligned} WL &= \\begin{cases} v \\cdot 1 ,& {}_{}q_{x} \\\\ v \\cdot A_{x+1} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore A_x = v{}_{}q_{x} + v{}_{}p_{x}A_{x+1} \\] The same exercise can be shown for the TA random variable. The main difference is understanding how the age & duration changes: \\(x+1\\) reflects the new age of the policyholder (same as WL) \\(n-1\\) reflects that one year of coverage has passed (not applicable for WL) \\[ \\begin{aligned} TA &= \\begin{cases} v \\cdot 1 ,& {}_{}q_{x} \\\\ v \\cdot A^1_{x+1:\\enclose{actuarial}{n-1}} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore A^1_{x:\\enclose{actuarial}{n}} = v{}_{}q_{x} + v{}_{}p_{x}A^1_{x+1:\\enclose{actuarial}{n-1}} \\] Note that since it requires a term policy with a reduced term , this recursion is not particularly useful as it is difficult to obtain it. The PE variable is similar, with the main difference being that the policyholder will receive nothing if the policyholder dies . Thus, only the second component of the recursion remains: \\[ \\begin{aligned} PE &= \\begin{cases} 0 ,& {}_{}q_{x} \\\\ v \\cdot {}_{n-1}E_{x+1} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore {}_{n}E_{x} = v{}_{}p_{x} \\cdot {}_{n-1}E_{x+1} \\] EA is omitted from this section as it is simply the combination of a TA and PE. These identities are most useful in a spreadsheet setting where the calculations can be easily repeated to fill up an entire life table. However, even then it is necessary to have a starting point for the recusions to occur. The most common starting point is the terminal age as the EPVs can be intuitively determined since the policyholder will inevitably die at the end of the year: \\[ \\begin{aligned} A_{\\omega-1} &= v \\\\ A^{\\> \\> 1}_{\\omega-1:\\enclose{actuarial}{n}} &= v \\\\ {}_{n}E_{\\omega-1} &= 0 \\\\ A^{1}_{\\omega-1:\\enclose{actuarial}{n}} &= v \\end{aligned} \\]","title":"Recursions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#intuitions","text":"Although the exam questions are mostly computational, it is good to have an understanding of how the different EPVs compare against one another to serve as a sort of \"sense check\" against the SULT provided to determine if the answer is in the right direction.","title":"Intuitions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#same-assurance","text":"Recall that the probability of death is an increasing function with age. The death benefit is more likely to be paid out to an older policyholder - in other words, they receive the death benefit \"sooner\" than a younger policyholder. Thus, an older policyholder has larger expected cashflows that are discounted less (due to receiving it sooner), which results in a higher EPV than a younger policyholder, all else equal: \\[ \\begin{aligned} A_{x+k} & \\gt A_{x} \\\\ A^{\\> \\> 1} _{x+k:\\enclose{actuarial}{n}} & \\gt A^{\\> \\> 1}_{x+k:\\enclose{actuarial}{n}} \\end{aligned} \\] Conversely, the probability of survival is a decreasing function with age. The survival benefit is less likely to be paid out to an older policyholder - smaller expected cashflows. Regardless of the age of the policyholder, the survival benefit is paid at the same time ( same discounting ). Thus, since an older policyholder has smaller expected cashflows , it has a lower EPV than a younger policyholder: \\[ {}_{n}E_{x+k} \\le {}_{n}E_{x} \\] Endowment Assurances have both a death and survival component , thus the comparison is a combination of the two: A younger policyholder is more likely to survive and receive the survival benefit at the end of the term (discounted more) An older policyholder is more likely to die and receive the death benefit during the term (discounted less) Assuming that the difference in expected cashflows are negligible , then an older policyholder would have an higher EPV due to the lower discounting : \\[ A_{x+k:\\enclose{actuarial}{n}} \\gt A_{x:\\enclose{actuarial}{n}} \\] Naturally, all else equal, assurances with a lower interest rate are discounted less and thus have a higher EPV . If a seperate mortality table with EPVs are given, it is likely that the question is not using an interest rate of 5%.","title":"Same Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#different-assurances","text":"At a young age where the probability of death is low, all else equal (where applicable), the EPVs of each assurance rank as follows: TA will have the smallest EPV . Although their benefits are paid out sooner, the expected benefits are small as the probability of death is small. WL has the next largest EPV . They have the same benefits as term in the short run, but have large expected benefits in the future . However, these large benefits are heavily discounted , still resuling in a small EPV. PE has the next largest EPV . Given the high probability of survival, the expected benefits are large . EA has the largest EPV . Since it is a combination of TA and PE, it is naturally the highest. \\[ \\begin{aligned} A^{1}_{30:\\enclose{actuarial}{n}} < A_{30} < {}_{n}E_{30} < A_{30:\\enclose{actuarial}{n}} \\\\ \\end{aligned} \\] At an old age where the probability of death is high, all else equal (where applicable), the EPVs of each assurance rank as follows: PE will have the smallest EPV . Given the low probability of survival, the expected benefits are are small . TA will have the next largest EPV . Given the high probability of death, the expected benefits are high . EA will have the next largest EPV . Since it is combination of TA and PE, it is naturally higher than both of them. WL has the largest EPV . Given the inevitable death of the policyholder, the expected benefits are the highest . \\[ \\begin{aligned} {}_{n}E_{100} < A^{1}_{100:\\enclose{actuarial}{n}} < A_{100:\\enclose{actuarial}{n}} < A_{100} \\\\ \\end{aligned} \\] As the policyholder approaches the terminal age, the EPVs tend to one another: \\[ \\begin{aligned} x &\\to \\omega \\\\ E(\\text{PE}) &\\to 0 \\\\ E(\\text{EA}) &\\to E(\\text{TA}) \\\\ E(\\text{TA}) &\\to E(\\text{WL}) \\end{aligned} \\] TA tends to WL whenever the end of the term exceeds the terminal age - thus the cashflows and hence EPV for both assurances become identical.","title":"Different Assurances"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#probabilities-and-percentiles","text":"Apart from just calculating the Expectation and Variance, the probabilities and hence percentiles of the contract benefits can be calculated as well. The former refers to calculating the probability that the random variable is at most some value \\(u\\) . In other words, that the PV ( NOT the EPV! ) is at most \\(u\\) . \\[ \\begin{aligned} \\text{WL} &\\le u \\\\ v^{K_x+1} &\\le u \\\\ (K_x+1) &\\ln v \\le \\ln u \\\\ K_x+1 &\\ge \\frac{\\ln u}{\\ln v} \\\\ K_x & \\ge \\frac{\\ln u}{\\ln v} - 1 \\\\ \\end{aligned} \\] Note that it is a common mistake to forget to flip the inequality sign as \\(\\ln v \\lt 0\\) . To avoid this error, it is advised to plot a graph to remember that \\(K_x\\) should be larger than the calculated value: The RHS of the expression is unlikely to be an integer. However, \\(K_x\\) can only take integer values. Thus, the values can rounded up to the nearest whole number (EG. 5): \\[ \\begin{aligned} P(K_x \\ge \\frac{\\ln u}{\\ln v} - 1) &= P(K_x \\ge 5) \\\\ &= P(T_x \\ge 5) \\\\ &= {}_{5}p_x \\end{aligned} \\] The latter refers to calculating the percentile of the random variable , which is the smallest value of the RV that results in the specified probability. This process is the opposite of the previous one - it involves solving for \\(T_x\\) , converting it to \\(K_x\\) and then subsituting it back into the RV, which results in the associated percentile.","title":"Probabilities and Percentiles"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#payable-continuously","text":"In practice, life assurances pay benefits as soon as the insured event occurs , thus is akin to paying out continuously throughout the year. Notice that paying out continuously is actually a special case of a contract that pays out \\(m\\) times a year , where \\(m\\) tends to infinity. \\(m=4\\) ; Quarterly Payments \\(m=12\\) ; Monthly Payments \\(m=\\infty\\) ; Continuous Payments Thus, despite the header stating \"payable continously\", this section will instead cover paying out \\(m\\) times a year , which can be used to derive the continuous case. Note Questions with Assurances that pay out \\(m\\) times a year are rare - if anything, the continuous case will be directly tested instead. However, the \\(m\\) times a year case is still covered because the ideas can be extended to Annuities , where such questions are common. The survival model used must now reflect the probability of death at fractional ages . Intuitively, it can be thought of as the probability living till a discrete age and then dying within a sub-period within that year: \\[ \\begin{aligned} K_x + \\frac{j+1}{m}, \\>\\> j = 0, 1, 2, ..., m-1 \\end{aligned} \\] Thus, the corresponding random variable representing the PV of the benefits: \\[ \\text{PV} = v^{K_x + \\frac{j+1}{m}} \\] The EPV of an assurance payable \\(m\\) times a year can be denoted with the \\((m)\\) superscript with the same notations as before: \\[ A^{(m)}_{x} = \\sum^{\\infty}_{K_x=0} v^{K_x} {}_{K_x}p_x \\cdot \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}} {}_{\\frac{j}{m}}p_{x+K_x} \\cdot {}_{\\frac{1}{m}} q_{x+K_x+\\frac{j}{m}} \\\\ \\] Annual Benefit Note that the benefit used is still the ANNUAL BENEFIT . To obtain the actual benefit payable per period , then it should be divided by the number of periods; \\(\\frac{B}{m}\\) . Unfortunately, most life tables do not naturally provide probabilities for death at fractional ages, thus the EPVs must be approximated from the discrete case , which will be covered later.","title":"Payable Continuously"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#continuous-case","text":"As \\(m \\to \\infty\\) , then the survival model becomes \\(T_x\\) and hence random variable is: \\[ \\begin{aligned} \\text{PV} &= v^{T_x} \\\\ &= e^{-\\delta \\cdot T_x} \\end{aligned} \\] The EPV of an assurance payable continously is denoted with the \\(\\bar{}\\) accent with the same notations as before: \\[ \\begin{aligned} \\bar{A}_x &= \\int^{\\infty}_{0} v^{T_x} \\cdot f_x(t) \\\\ &= \\int^{\\infty}_{0} e^{-\\delta \\cdot T_x} \\cdot {}_{t}p_{x} \\mu_{x+t} \\end{aligned} \\] Recall that the second term represents the probability of living to a certain age \\(x+t\\) and then dying in an infinitely small time after that (definition of force of interest). Thus, although the EPV looks very different, it is still fundamentally a Triple Product Summation of the same components: Amount of Benefit, 1 Discounting of the Benefit, \\(e^{-\\delta \\cdot T_x}\\) Probability of paying the Benefit, \\({}_{t}p_{x} \\mu_{x+t}\\) Unlike the payable \\(m\\) times case, the EPVs can be calculated directly if the survival distribution is provided. However, since they are special cases of the payable \\(m\\) times case, it can also be calculated through approximation from the discrete case. The same logic can be applied to all other assurances, EXCEPT for PEs . This is because they pay out at a fixed time , thus there is NO such thing as a payable \\(m\\) times PE or a continuously payable PE.","title":"Continuous Case"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#uniform-distribution-of-deaths","text":"Recall that the UDD assumption can be used to approximate survival probabilities of fractional ages from discrete probabilities. Using this approach, the Continuous EPVs can be approximated from the Discrete ones. Assuming UDD between integer ages, \\[ {}_{\\frac{j}{m}}p_{x+K_x} \\cdot {}_{\\frac{1}{m}} q_{x+K_x+\\frac{j}{m}} \\approx \\frac{1}{m} \\cdot q_{x+K_x} \\] Thus, the EPV of a continuous contract can be expressed as a function of the discrete case: \\[ \\begin{aligned} A^{(m)}_{x} &= \\sum^{\\infty}_{K_x=0} v^{K_x} {}_{K_x}p_x \\cdot \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}} {}_{\\frac{j}{m}}p_{x+K_x} \\cdot {}_{\\frac{1}{m}} q_{x+K_x+\\frac{j}{m}} \\\\ & \\approx \\sum^{\\infty}_{K_x=0} v^{K_x} {}_{K_x}p_x \\cdot \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}} \\frac{1}{m} \\cdot q_{x+K_x} \\\\ & \\approx \\sum^{\\infty}_{K_x=0} v^{K_x+1} {}_{K_x}p_x q_{x+K_x} \\cdot \\frac{1}{m} \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}-1} \\\\ & \\approx A_x \\cdot \\frac{v^{-1}}{m} \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}} \\\\ & \\approx A_x \\cdot \\frac{1+i}{m} \\cdot \\frac{v^{\\frac{1}{m}}(1-v)}{1-v^{\\frac{1}{m}}} \\\\ & \\approx A_x \\cdot \\frac{1+i}{m} \\cdot \\frac{v^{\\frac{1}{m}}(1-v)}{v^{\\frac{1}{m}}[(1+i)^{m}-1]} \\\\ & \\approx A_x \\cdot \\frac{1+i}{m} \\cdot \\frac{1-v}{(1+i)^{m}-1} \\\\ & \\approx A_x \\cdot \\frac{1+i-1}{m[(1+i)^{m}-1]} \\\\ & \\approx \\frac{i}{i^{(m)}} \\cdot A_x \\end{aligned} \\] As \\(m \\to \\infty\\) , \\[ \\begin{aligned} i^{(m)} &\\to \\delta, \\\\ A^{(m)}_{x} &\\to \\bar{A}_x, \\\\ \\therefore \\bar{A}_x &= \\frac{i}{\\delta} A_x \\end{aligned} \\]","title":"Uniform Distribution of Deaths"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#claims-acceleration-approach","text":"Given that claims occur every \\(\\frac{1}{m}\\) period, then on average , the claims within a year occur at \\(\\frac{m+1}{2m}\\) : \\[ \\begin{aligned} \\text{Average Death} &= \\frac{\\frac{1}{m} + \\frac{m}{m}}{2} \\\\ &= \\frac{\\frac{1+m}{m}}{2} \\\\ &= \\frac{m+1}{2m} \\end{aligned} \\] Thus, the EPV of a continuous contract can be expressed as a function of the discrete case: \\[ \\begin{aligned} A^{(m)}_{x} & \\approx \\sum^{\\infty}_{K_x = 0} v^{K_x + \\frac{m+1}{2m}} {}_{K_x|} q_{x} \\\\ & \\approx v^{\\frac{m+1}{2m}-1} \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} {}_{K_x|} q_{x} \\\\ & \\approx v^{\\frac{m+1-2m}{2m}} A_x \\\\ & \\approx v^{\\frac{-m+1}{2m}} A_x \\\\ & \\approx v^{-\\frac{m-1}{2m}} A_x \\\\ & \\approx (1+i)^{\\frac{m-1}{2m}} A_x \\end{aligned} \\] As \\(m \\to \\infty\\) , \\[ \\begin{aligned} m &\\to \\infty, \\\\ \\frac{m-1}{2m} &\\to \\frac{1}{2}, \\\\ A^{(m)}_{x} &\\to \\bar{A}_x, \\\\ \\therefore \\bar{A}_x &= (1+i)^{\\frac{1}{2}} A_x \\end{aligned} \\] This is known as the Claims Acceleration Approach as the claims are paid out earlier on in the year as compared to the end. Generally speaking, this approach is preferred as it only has one parameter compared to UDD which has two.","title":"Claims Acceleration Approach"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#common-pitfalls","text":"Recall that there are no continuous PEs - thus, be very careful when approximating an EA as ONLY the TA portion needs to be approximated ! This means that we cannot directly apply an approximation to the EA values provided in the SULT - they must be broken down into the TA and EA components first. If calculating manually, it is always advised to calculate the TA and EA components seperately before combining into an EA to prevent falling into the trap of directly approximating the EA. \\[ \\begin{aligned} \\bar{A}_{x:\\enclose{actuarial}{n}} & \\ne \\frac{i}{\\delta} A_{x:\\enclose{actuarial}{n}} \\\\ \\bar{A}_{x:\\enclose{actuarial}{n}} & = \\frac{i}{\\delta} A^{1}_{x:\\enclose{actuarial}{n}} + {}_{n}E_x \\end{aligned} \\] Similarly, if approximating the second moment for variance, note that the NEW interest rate must be used in the approximation term as well: \\[ \\begin{aligned} {}^{2} \\bar{A}_x &= \\frac{i^*}{\\delta^*} \\cdot {}^{2} A_x \\end{aligned} \\] Note Although the above two points were illustrated with UDD, they apply to the claims acceleration approach as well. Lastly, the two approaches should produce similar results which differ by a few decimal places. However, these differences are enlarged when dealing with large benefits as these decimal places will be brought forward, resulting in seemingly large differences - do not be alarmed!","title":"Common Pitfalls"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/","text":"Life Annuities \u00b6 Overview \u00b6 Life Annuity contracts promise to pay out a stream of benefits in the future for as long the policyholder remains alive. Similar to life insurance, the benefits can be paid discretely or continuously and can be valued through their EPV. Note that these are different from the annuities covered in Exam FM. The payments for those annuities are made regardless of the life of the policyholder, known as Annuity Certain . They serve as the foundation to understanding Life Annuities. Review: Annuities Certain \u00b6 There are two types of payment structures: Period Start Period End Paid in Advance Paid in Arrears Annuity Due Annuity Immediate \\(\\ddot{a}_{\\enclose{actuarial}{n}}\\) \\(a_{\\enclose{actuarial}{n}}\\) The overall PV of the annuity is the sum of the PV of the stream of payments: \\[ \\begin{aligned} a_{\\enclose{actuarial}{n}} &= v + v^2 + v^3 + \\dots + v^n \\\\ &= \\frac{v-v^{n+1}}{1-v} \\\\ &= \\sum^n_{k=1} v^k \\\\ &= \\frac{v(1-v^n)}{1-v} \\\\ &= \\frac{1-v^n}{i} \\\\ \\\\ \\ddot{a}_{\\enclose{actuarial}{n}} &= 1 + v + v^2 + \\dots + v^n \\\\ &= \\sum^{n-1}_{k=0} v^k \\\\ &= \\frac{1- v^{n}}{1-v} \\\\ &= \\frac{1- v^{n}}{d} \\end{aligned} \\] Notice that the payments simply differ by one period and hence one discounting factor: \\[ \\begin{aligned} \\ddot{a}_{\\enclose{actuarial}{n}} &= 1 + v + v^2 + v^3 + \\dots + v^n \\\\ &= (1+i) (v + v^2 + v^3 + \\dots + v^n) \\\\ &= (1+i) \\sum^{n}_{k=1} v^k \\\\ &= (1+i) a_{\\enclose{actuarial}{n}} \\end{aligned} \\] From another perspective, only the payments at the end points \\(t=0\\) and \\(t=n\\) are different: \\[ \\begin{aligned} \\ddot{a}_{\\enclose{actuarial}{n}} &= \\sum^{n-1}_0 v^k \\\\ &= v^0 + \\sum^{n-1}_{k=1} v^k \\\\ &= 1 + a_{\\enclose{actuarial}{n-1}} \\\\ &= 1 + a_{\\enclose{actuarial}{n}} - v^n \\end{aligned} \\] Payable Discretely \u00b6 If the contract pays out benefits discretely, then \\(k\\) is used as the survival model. Since a life annuity pays out a stream of benefits, the PV of the contract at every time period can be slightly confusing. It is important to remember the following: The PV is calculated with respect to time 0 The PV is the sum of the PV of all the payments the individual is expected to live \\[ \\begin{aligned} PV &= \\begin{cases} a_{\\enclose{actuarial}{K_x}}, & \\text{Life Annuity Immediate} \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x + 1}}, & \\text{Life Annuity Due} \\end{cases} \\end{aligned} \\] Thus, the EPV of the contract is the sum product of the PV of the benefit in each period and the probability of death in that period: \\[ \\begin{aligned} EPV &= \\begin{cases} \\text{Life Annuity Immediate} &=\\ \\sum a_{\\enclose{actuarial}{k}} \\cdot {}_{k|}q_{x} \\\\ \\text{Life Annuity Due} &= \\sum \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} \\end{cases} \\end{aligned} \\] For the remainder of the section, to be concise, only the formulas for Annuity Due will be given. The corresponding formulas for Annuity Immediates can be easily calculated from it. Actuarial Notation \u00b6 Most of the notation for Life Insurance applies to Life Annuities as well. The key difference is that \\({}^{k}a\\) is used to represent the k-th moment of the present value of a contract where a benefit of 1 is paid discretely for as long as the status does NOT fail . \\(A\\) stands for Assurance while \\(a\\) stands for Annuity. Whole Life Annuity \u00b6 Whole Life Annuities covers the insured indefinitely and thus will pay out for as long as the insured survives . Let WL be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{WL}_{\\text{Due}} &=\\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\end{aligned} \\] Thus, the EPV is the Expectation of the WLA random variable: \\[ \\begin{aligned} E(\\text{WLA}_{\\text{Due}}) &= \\sum^{\\infty}_{k = 0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} \\\\ \\ddot{a}_{x} &= \\sum^{\\infty}_{k = 0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} \\end{aligned} \\] The EPV can be furthered simplified, allowing a life annuity to be viewed as the sum of a series of pure endowments : \\[ \\begin{aligned} \\ddot{a}_{x} &= \\sum^{\\infty}_{k = 0} \\ddot{a}_{\\enclose{actuarial}{k}} \\cdot {}_{k|}q_{x} \\\\ &=\\sum^{\\infty}_{k = 0} \\left(\\sum^{n-1}_{j=0} v^j \\right) \\cdot {}_{k|}q_{x} \\\\ &= v^0 \\cdot {}_{0|}q_{x} + (v^0 + v^1) \\cdot {}_{1|}q_{x} + (v^0 + v^1 + v^2) \\cdot {}_{2|}q_{x} + \\dots \\\\ &= v^0 ({}_{0|}q_{x} + {}_{1|}q_{x} + \\dots) + v^1 ({}_{1|}q_{x} + {}_{2|}q_{x} + \\dots) + v^2 ({}_{2|}q_{x} + {}_{3|}q_{x} + \\dots) + \\dots \\\\ &= \\sum^{\\infty}_{j=0} v^j \\cdot \\left(\\sum^{\\infty}_{k = j} {}_{k|}q_{x} \\right) \\\\ &= \\sum^{\\infty}_{j=0} v^j \\cdot {}_{j}p_{x} \\end{aligned} \\] Note that the above formula can be used to easily prove the relationships between different kinds of annuities, but is NOT a proper definition of an annuity - it hence cannot be adjusted to compute the variance. The second moment and variance for all life annuities will be covered in a later section. Temporary Annuity \u00b6 Temporary Life Annuities covers the insured for a specified period \\(n\\) and thus pays out for as long as the insured survives during that period only . Let TA be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{TA}_\\text{Due} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{K_x+1}},& k = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{n}},& k \\ge n \\\\ \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{min(K_x + 1, n)}} \\end{aligned} \\] Thus, the EPV is the expectation of the TA random variable, which can be simplified using the same approach as before: \\[ \\begin{aligned} E(\\text{TA}_\\text{Due}) &= \\sum^{n-1}_{k = 0} \\ddot{a}_{\\enclose{actuarial}{k}} \\cdot {}_{k|}q_x + \\ddot{a}_{\\enclose{actuarial}{n}} \\cdot {}_{n}p_x \\\\ \\ddot{a}_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{j=0} v^j \\cdot {}_{j}p_{x} \\end{aligned} \\] Note the change in limits for the simplified approach - an annuity is the combination of pure endowments for as long as the annuity lasts . This gives rise to a very simple sense check - the EPV of an \\(n\\) year temporary annuity should NOT be larger than \\(n\\) as there are \\(n\\) payments of 1 which are discounted. Deferred Annuities \u00b6 Deferred Annuities are variations of the above contracts where the coverage is deferred by \\(n\\) years . While any contract can be deferred, the most useful is the Deferred Whole Life Annuity . Let DWL be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{DWL}_{\\text{Due}} &= \\begin{cases} 0, & k = 0, 1, 2, \\dots, n-1 \\\\ v^n \\cdot \\ddot{a}_{\\enclose{actuarial}{K_x+1-n}}, & k \\ge n \\end{cases} \\end{aligned} \\] Intuitively, the RV can also be expressed as the difference between two certain annuities: \\[ \\begin{aligned} \\text{DWL}_{\\text{Due}} &= v^n \\cdot \\ddot{a}_{\\enclose{actuarial}{k+1-n}} \\\\ &= v^n \\cdot \\frac{1-v^{k+1-n}}{d} \\\\ &= \\frac{v^{n}-v^{k+1}}{d} \\\\ &= \\frac{(1-v^{k+1}) - (1-v^{n})}{d} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{k+1}} -\\ddot{a}_{\\enclose{actuarial}{n}} \\\\ \\\\ \\therefore \\text{DWL}_{\\text{Due}} &= \\begin{cases} 0, & k = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{k+1}} -\\ddot{a}_{\\enclose{actuarial}{n}} , & k \\ge n \\end{cases} \\end{aligned} \\] Thus, the EPV is the expectation of the DWL random variable: \\[ \\begin{aligned} E(\\text{DWL}_{\\text{Due}}) &= \\sum^{\\infty}_{k=n} (\\ddot{a}_{\\enclose{actuarial}{k+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{k|}q_{x} \\\\ {}_{n|}\\ddot{a}_{x} &= \\sum^{\\infty}_{k=0} (\\ddot{a}_{\\enclose{actuarial}{k+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{k|}q_{x} - \\sum^{n-1}_{k=0} (\\ddot{a}_{\\enclose{actuarial}{k+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{k|}q_{x} \\\\ &= \\sum^{\\infty}_{k=0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} - \\sum^{\\infty}_{k=0} \\ddot{a}_{\\enclose{actuarial}{n}} \\cdot {}_{k|}q_{x} - \\sum^{n-1}_{k=0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} + \\sum^{n-1}_{k=0} \\ddot{a}_{\\enclose{actuarial}{n}} \\cdot {}_{k|}q_{x} \\\\ &= \\sum^{\\infty}_{k=0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} - \\sum^{n-1}_{k=0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} \\\\ &= \\ddot{a}_x - \\ddot{a}_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] Similar to before, the EPV can be shown to be a sum of pure endowments: \\[ {}_{n|}\\ddot{a}_{x} = \\sum^{\\infty}_{j=n} v^j \\cdot {}_{j}p_{x} \\] Since a DWL is effectively a WL that is issued \\(n\\) years later, the EPV of a DWL is the EPV of the WL, adjusted for both interest and survival: \\[ \\begin{aligned} {}_{n|}a_{x} &= \\sum^{\\infty}_{j=n} v^j \\cdot {}_{j}p_{x} \\\\ &= \\sum^{\\infty}_{j=0} v^{j+n} \\cdot {}_{j+n}p_{x} \\\\ &= \\sum^{\\infty}_{j=0} v^{j} v^{n} \\cdot {}_{n}p_{x} {}_{j}p_{x+n} \\\\ &= v^{n} \\cdot {}_{n}p_{x} \\sum^{\\infty}_{j=0} v^{j} \\cdot {}_{j}p_{x+n} \\\\ &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\end{aligned} \\] This allows a TA to be expressed as the difference of two WL annuities issued at different times : \\[ \\begin{aligned} {}_{n|}\\ddot{a}_{x} &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\\\ \\ddot{a}_x - \\ddot{a}_{x:\\enclose{actuarial}{n}} &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\\\ \\ddot{a}_{x:\\enclose{actuarial}{n}} &= \\ddot{a}_x - {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\end{aligned} \\] Guaranteed Annuities \u00b6 Guaranteed Life Annuities are whole life annuities with the benefits in the first \\(n\\) years being guaranteed - they will be paid out even if the insured dies during this period. Let GA be the random variable denoting the PV of the survival benefit. Since the payments are guaranteed, they can be represented using a certain annuity : \\[ \\begin{aligned} \\text{GA}_{\\text{Due}} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& k = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}},& k \\ge n \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{max(K_x+1,n)}} \\end{aligned} \\] The random variable can be manipulated to be easier to work with: \\[ \\begin{aligned} \\text{GA}_{\\text{Due}} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}},& K_x \\ge n \\end{cases} \\\\ &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}} + \\ddot{a}_{\\enclose{actuarial}{n}},& K_x \\ge n \\end{cases} \\\\ &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}} \\\\ \\ddot{a}_{\\enclose{actuarial}{n}} \\end{cases} + \\begin{cases} 0,& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}},& K_x \\ge n \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{n}} + \\begin{cases} 0,& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}},& K_x \\ge n \\end{cases} \\end{aligned} \\] Thus, the EPV can be more easily calculated: \\[ \\begin{aligned} E(\\text{GA}_{\\text{Due}}) &= \\ddot{a}_{\\enclose{actuarial}{n}} + 0 \\cdot {}_{n}q_x + \\sum^{\\infty}_{k = n} (\\ddot{a}_{\\enclose{actuarial}{k+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{k|}q_{x}\\\\ \\ddot{a}_{\\overline{x:\\enclose{actuarial}{n}}} &= \\ddot{a}_{\\enclose{actuarial}{n}} + {}_{n} \\ddot{a}_{x} \\end{aligned} \\] Note that a Guaranteed Annuity is denoted via an additional bar above the status. Assurances and Annuities \u00b6 Consider the random variable for the PV Whole Life Assurance and Annuity Due. Notice that both RVs are related through the certain annuity formula: \\[ \\begin{aligned} \\text{WL} &= v^{K_x + 1} \\\\ \\text{WLA}_{Due} &= \\ddot{a}_{\\enclose{actuarial}{K_x+1}} = \\frac{1-v^{K_x + 1}}{d} \\end{aligned} \\] This is why Annuity Due, rather Annuity Immediates, are provided in the SULT. Thus, given the EPV of a life annuity, the EPV of a life assurance can be determined, vice-versa: \\[ \\begin{aligned} \\text{WLA} &= \\frac{1-\\text{WL}}{d} \\\\ E(\\text{WLA}_{Due}) &= \\frac{1-E(\\text{WL})}{d} \\\\ \\ddot{a}_x &= \\frac{1-A_x}{d} \\\\ d \\ddot{a}_x &= 1 - A_x \\\\ A_x &= 1 - d \\ddot{a}_x \\end{aligned} \\] The same relationship can be shown for a temporary annuity and an Endowment Assurance, NOT a term assurance : \\[ \\begin{aligned} A_{x:\\enclose{actuarial}{n}} &= 1 - d \\ddot{a}_{x:\\enclose{actuarial}{n}} \\\\ \\end{aligned} \\] It is a very common mistake to confuse the two as both are denoted by TA. However, it is the endowment assurance that shares the same random variable as the temporary annuity. It also makes sense when comparing the actuarial notation - both share the same subscript. Variance \u00b6 However, the second moment is slightly different as it must reflect the new interest rate used. The same logic can be applied for the Variance as well: \\[ \\begin{aligned} \\text{WLA}_{Due} &= \\frac{1-\\text{WL}}{d} \\\\ Var(\\text{WLA}_{Due}) &= Var \\left( \\frac{1-\\text{WL}}{d} \\right) \\\\ Var(\\text{WLA}_{Due}) &= \\frac{1}{d^2} var(\\text{WL}) \\\\ Var(\\text{WLA}_{Due}) &= \\frac{1}{d^2} \\left({}_{}^2 A_x - (A_x)^2 \\right) \\end{aligned} \\] Var(\\text{TA}_{Due}) &= \\frac{1}{d^2} \\left({}_{}^2 A_{x:\\enclose{actuarial}{n}} - (A_{x:\\enclose{actuarial}{n}})^2 \\right) WLDue and WL Immediate same variance but DWLDue and DWL immediate different variance because it is dependent on survival GA = Certain + Deferred Var (GA) = Var (Deferred) Problem is Var(Deferred) Immediate and Due \u00b6 Recall that for Certain Annuities, the only difference between Due and Immediate ones are the payments at the start and end times. This relationship can be extended for Life Annuities as well. For WL annuities, since the end point is infinity , the difference in the end points can be ignored. Thus, the only difference is the first payment of 1, which can be easily accounted for: \\[ a_x = \\ddot{a}_x - 1 \\] Note This section uses \\(a_x\\) as the focus, as it is assumed that if needed, the due versions will be calculated and then converted to immediate , rather than calculating immediate annuities directly. Unfortunately, the difference at the end cannot be ignored for TA annuities: \\[ a_{x:\\enclose{actuarial}{n}} = \\ddot{a}_{x:\\enclose{actuarial}{n}} - 1 + {}_{n}E_x \\] There is no need to memorize this expression as it can be easily derived using the WL conversion: \\[ \\begin{aligned} a_{x:\\enclose{actuarial}{n}} &= a_{x} - {}_{n}E_x a_{x+n} \\\\ &= \\ddot{a}_{x} - 1 - {}_{n}E_x (\\ddot{a}_{x+n} - 1) \\\\ &= \\ddot{a}_{x} - 1 - {}_{n}E_x \\cdot \\ddot{a}_{x+n} + {}_{n}E_x \\\\ &= \\ddot{a}_{x} - {}_{n}E_x \\cdot \\ddot{a}_{x+n} - 1 + {}_{n}E_x \\\\ &= \\ddot{a}_{x:\\enclose{actuarial}{n}} - 1 + {}_{n}E_x \\end{aligned} \\] For variance, the random variables (which are certain annuities) are used instead: \\[ \\begin{aligned} Var(\\text{WL}_{Immediate}) &= Var(a_{\\enclose{actuarial}{k}}) \\\\ &= Var(\\ddot{a}_{\\enclose{actuarial}{k+1}}-1) \\\\ &= Var(\\ddot{a}_{\\enclose{actuarial}{k+1}}) \\\\ &= Var(\\text{WL}_{Due}) \\\\ \\\\ Var(\\text{TA}_{Immediate}) &= Var(a_{\\enclose{actuarial}{min(k,n)}}) \\\\ &= Var(\\ddot{a}_{\\enclose{actuarial}{min(k+1,n+1)}}-1) \\\\ &= Var(\\ddot{a}_{\\enclose{actuarial}{min(k+1,n+1)}}) \\\\ &= Var \\left( \\frac{1-v^{min(k+1,n+1)}}{d} \\right) \\\\ &= \\frac{1}{d^2} Var(v^{min(k+1,n+1)}) \\\\ &= \\frac{1}{d^2} \\left({}_{2}A_{x:\\enclose{actuarial}{n+1}} - (A_{x:\\enclose{actuarial}{n+1}})^2 \\right) \\end{aligned} \\] Thus, the variance of a WL annuity is the same for both Immediate and Due but NOT for a TA annuity. Recursions \u00b6 Following the same logic as assurances, Annuities can also be recursively expressed as a function of itself: If the policyholder dies, they would have only received the benefit of 1 at the start of the year If the policyholder survives, they would receive the additional benefits of the future periods The benefit of 1 is received REGARDLESS of death or survival as it was paid at the start of the year \\[ \\begin{aligned} \\text{WL}_\\text{Due} &= \\begin{cases} 1,& q_x \\\\ 1 + v\\ddot{a}_{x+1} ,& p_x \\end{cases} \\\\ &= 1 + \\begin{cases} 0,& q_x \\\\ v\\ddot{a}_{x+1} ,& p_x \\end{cases} \\\\ \\\\ \\therefore \\ddot{a}_{x} &= 1 + vp_x\\ddot{a}_{x+1} \\end{aligned} \\] The same can be shown for TAs, but remember that the remaining duration of the policy must decrease as well: \\[ \\ddot{a}_{x:\\enclose{actuarial}{n}} = 1 + vp_x \\ddot{a}_{x+1:\\enclose{actuarial}{n-1}} \\] Intuitions \u00b6 Similar to assurances, several intuitions can be made about the EPV of various annuities to serve as a sort of \"sense check\" against the SULT provided to determine if the answer is in the right direction. Same Annuity \u00b6 Since annuities are all contingent on the survival of the policyholder, only one case needs to be considered. The probability of survival is a decreasing function with age. The benefits are less likely to be paid out to an older policyholder, resulting in smaller expected cashflows . Since the cashflows are discounted the same amount, an older policyholder will have a lower EPV than a younger one: \\[ \\ddot{a}_{x+n} \\lt \\ddot{a}_{x} \\] As shown previously, Annuity Dues are always smaller than Immediates as the cashflows occur earlier and are hence discounted less . \\[ a_x < \\ddot{a}_x \\] Naturally, all else equal, annuities with a lower interest rate are discounted less and thus have a higher EPV . Different Annuities \u00b6 Since annuities are all contingent on the survival of the policyholder, the age of the policyholder for comparison does not matter. TA has the smallest EPV as it can only pay for a maximum of \\(n\\) years, while WLs and GA can pay indefinitely. WL is always smaller than GA as its payments in the first \\(n\\) years are not guaranteed; the payments after that are identical. \\[ \\ddot{a}_{x:\\enclose{actuarial}{n}} < \\ddot{a}_x < \\ddot{a}_{\\bar{x:\\enclose{actuarial}{n}}} \\] Immediate VS Due \u00b6 Consider two temporary life annuity with a term of \\(n\\) years: Annuity Immediate issued at age \\(x\\) Annuity Due issued at age \\(x+1\\) Both have the same cashflows : However, both of them are valued at different times: Annuity Immediate valued at age \\(x\\) Annuity Due valued at age \\(x+1\\) Thus, the PV of the cashflows are NOT the same : Thus, although they have the same cashflows, the annuity due has a larger EPV : \\[ \\begin{aligned} a_{x:\\enclose{actuarial}{n}} &= \\sum^n_{j=1} v^j {}_{j}p_{x} \\\\ \\\\ \\ddot{a}_{x+1:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{j=0} v^j {}_{j}p_{x+1} \\\\ &= \\frac{1}{vp_{x}} \\sum^{n-1}_{j=0} v^{j+1} {}_{j}p_{x+1} p_{x} \\\\ &= \\frac{1}{vp_{x}} \\sum^{n-1}_{j=0} v^{j+1} {}_{j+1}p_{x} \\\\ &= \\frac{1}{vp_{x}} \\sum^{n}_{j=1} v^{j} {}_{j}p_{x} \\\\ &= \\underbrace{\\frac{1}{vp_{x}}}_{>1} \\cdot a_{x:\\enclose{actuarial}{n}} \\\\ \\\\ \\therefore \\ddot{a}_{x+1:\\enclose{actuarial}{n}} &> a_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] This approach might seem long winded, as it seems that it is sufficient to simply compare the cashflows of \\(1 > v^n\\) . However, that ignores the probabilities , which is properly accounted for in the above approach. Probabilities and Percentiles \u00b6 Similar to assurances, apart from just calculating the Expectation and Variance, the probabilities and hence percentiles of the contract benefits can be calculated as well. \\[ \\begin{aligned} \\text{WL}_\\text{Due} &\\le u \\\\ \\frac{1-v^{k+1}}{d} &\\le u \\\\ 1-v^{k+1} &\\le ud \\\\ v^{k+1} &\\ge 1-ud \\\\ (k+1) \\ln v &\\ge \\ln(1-ud) \\\\ k+1 &\\le \\frac{\\ln(1-ud)}{\\ln v} \\\\ k &\\le \\frac{\\ln(1-ud)}{\\ln v} - 1 \\\\ \\end{aligned} \\] The common mistake of not flipping the inequality sign is relevant in annuities as well. To avoid this error, it is advised to plot a graph to remember that \\(k\\) should be smaller than the calculated value: The RHS of the expression is unlikely to be an integer. However, \\(k\\) can only take integer values. Thus, the values can rounded DOWN to the nearest whole number (EG. 4): \\[ \\begin{aligned} P(k \\le \\frac{\\ln(1-ud)}{\\ln v} - 1) &= P(k \\le 4) \\\\ &= P(T_x \\le 4) \\\\ &= {}_{4}p_x \\end{aligned} \\] The latter refers to calculating the percentile of the random variable , which is the smallest value of the RV that results in the specified probability. This process is the opposite of the previous one - it involves solving for \\(T_x\\) , converting it to \\(k\\) and then subsituting it back into the RV, which results in the associated percentile. Payable Continuously \u00b6 Similar to assurances, annuities can also be payable \\(m\\) times a year or payable continuously. Most of the key ideas carry over from the assurances, thus this section will mainly focus on the approximations from discrete annuities. However, something unique to annuities is that the conversion between due and immediates is slightly different because of the differences in cashflows: \\[ a^{(m)}_x = \\ddot{a}^{(m)}_x - \\frac{1}{m} \\] Uniform Distribution of Deaths \u00b6 Using the relationship between Assurances and Annuities, a continuous WL annuity can also be expressed in the form of a continuous assurance: \\[ \\begin{aligned} \\ddot{a}^{(m)}_x &= \\frac{1 - A^{(m)}_x}{d^{(m)}} \\end{aligned} \\] Assuming UDD, the continuous assurance and hence the overarching continuous annuity can be simplified: \\[ \\begin{aligned} \\ddot{a}^{(m)}_x &= \\frac{1 - A^{(m)}_x}{d^{(m)}} \\\\ &= \\frac{1 - \\frac{i}{i^{(m)}}A_x}{d^{(m)}} \\\\ &= \\frac{\\frac{i^{(m)}-iA_x}{i^{(m)}}}{d^{(m)}} \\\\ &= \\frac{i^{(m)}-iA_x}{i^{(m)}d^{(m)}} \\\\ &= \\frac{i^{(m)}-i(1-d\\ddot{a}_x)}{i^{(m)}d^{(m)}} \\\\ &= \\frac{i^{(m)}-i+id\\ddot{a}_x}{i^{(m)}d^{(m)}} \\\\ &= \\frac{id}{i^{(m)}d^{(m)}} \\ddot{a}_x - \\frac{i-i^{(m)}}{i^{(m)}d^{(m)}} \\\\ &= \\alpha(m)\\ddot{a}_x - \\beta(m) \\\\ \\\\ \\therefore \\alpha(m) &= \\frac{id}{i^{(m)}d^{(m)}} \\\\ \\\\ \\therefore \\beta(m) &= \\frac{i-i^{(m)}}{i^{(m)}d^{(m)}} \\end{aligned} \\] Parameter Calculation Recall that nominal and effective rates are linked using: \\[ \\begin{aligned} i^{(m)} &= m[(1+i)^\\frac{1}{m}-1] \\\\ d^{(m)} &= m[1-(1+d)^\\frac{1}{m}] \\end{aligned} \\] However, for \\(i=0.05\\) , \\(\\alpha\\) and \\(\\beta\\) are provided inside the SULT for common values of \\(m\\) , thus there is no need to waste precious time calculating them. An adjustment can be made for temporary annuities: \\[ \\begin{aligned} \\ddot{a}^{(m)}_{x:\\enclose{actuarial}{n}} &= \\ddot{a}^{(m)}_{x} - {}_{n}E_x \\ddot{a}^{(m)}_{x+n} \\\\ &= \\alpha(m)\\ddot{a}_x - \\beta(m) - {}_{n}E_x [\\alpha(m)\\ddot{a}_{x:\\enclose{actuarial}{n}} - \\beta(m)] \\\\ &= \\alpha(m)\\ddot{a}_x - \\beta(m) - {}_{n}E_x \\alpha(m)\\ddot{a}_{x:\\enclose{actuarial}{n}} - {}_{n}E_x \\beta(m) \\\\ &= \\alpha(m) [\\ddot{a}_x - {}_{n}E_x \\ddot{a}_{x:\\enclose{actuarial}{n}}] - \\beta(m) [1-{}_{n}E_x] \\\\ &= \\alpha(m) \\ddot{a}_{x:\\enclose{actuarial}{n}} - \\beta(m) [1-{}_{n}E_x] \\end{aligned} \\] Woolhouse Approximation \u00b6 Since a continuous annuity can be expressed as a sum of discrete pure endowments , the trapezoidal rule and euler-macluarin correction can be used to approximate its EPV. The key difference from before is understanding that the EPV of an annuity due is a LEFT riemann sum as there is a payment at time 0. \\[ \\begin{aligned} \\bar{a}_x &= \\int^{\\infty}_0 v^t {}_{t}p_x \\\\ & \\approx h[f(a) + f(a+h) + \\dots + f(b-h)] - \\frac{h}{2} [f(a)-f(b)] + \\frac{h^2}{12} [f'(a)-f'(b)] \\end{aligned} \\] The first derivatives can be found below: \\[ \\begin{aligned} f'(n) &= \\frac{d}{dt} (e^{-\\delta n} {}_{n}p_x) \\\\ &= {}_{n}p_x \\delta e^{-\\delta n} - e^{-\\delta n} {}_{n}p_x \\mu_{x+n} \\\\ &= e^{-\\delta n} {}_{n}p_x (\\delta + \\mu_{x+n}) \\\\ &= {}_{n}E_x (\\delta + \\mu_{x+n}) \\\\ \\\\ \\therefore f'(0) &= -(\\delta + \\mu_{x}) \\\\ \\therefore f'(\\infty) &= 0 \\end{aligned} \\] Assuming \\(h=1\\) , \\[ \\begin{aligned} \\bar{a}_x & \\approx (1) [{}_{0}E_x + {}_{1}E_x + \\dots + {}_{\\infty-1}E_x] - \\frac{1}{2} [{}_{0}E_x-{}_{\\infty-1}E_x] + \\frac{1}{12} [-(\\delta + \\mu_{x}) - 0] \\\\ & \\approx [1 + {}_{n}E_1 + \\dots] - \\frac{1}{2} [1-0] - \\frac{1}{12} (\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12} (\\delta + \\mu_{x}) \\end{aligned} \\] Assuming \\(h=\\frac{1}{m}\\) instead, \\[ \\begin{aligned} \\bar{a}_x & \\approx \\left(\\frac{1}{m} \\right) [{}_{0}E_x + {}_{\\frac{1}{m}}E_x + \\dots + {}_{\\infty-\\frac{1}{m}}E_x] - \\frac{1}{2m} [{}_{0}E_x-{}_{\\infty-\\frac{1}{m}}E_x] + \\frac{1}{12m^2} [-(\\delta + \\mu_{x}) - 0] \\\\ & \\approx \\left(\\frac{1}{m} \\right) [1 + {}_{\\frac{1}{m}}E_x + \\dots] - \\frac{1}{2m} [1-0] - \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}^{(m)}_x - \\frac{1}{2m} - \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\end{aligned} \\] Combining both together, \\[ \\begin{aligned} \\bar{a}_x &= \\bar{a}_x \\\\ \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12} (\\delta + \\mu_{x}) & \\approx \\ddot{a}^{(m)}_x - \\frac{1}{2m} - \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\\\ \\ddot{a}^{(m)}_x & \\approx \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12} (\\delta + \\mu_{x}) + \\frac{1}{2m} + \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}_x - (\\frac{1}{2}-\\frac{1}{2m}) - (\\frac{1}{12} - \\frac{1}{12m^2})(\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}_x - \\frac{m-1}{2m} - \\frac{m^2-1}{12m^2}(\\delta + \\mu_{x}) \\end{aligned} \\] This process is known as the Woolhouse Approximation . If the error term is omitted, then it is known as the 2-term Woolhouse Approximation else it is known as the 3-term Woolhouse Approximation . The 3-term woolhouse approximation is extremely accurate due to its mathematical roots. It so much more accurate than UDD such that when calculating a continuous assurance , it is advisable to calculate the corresponding annuity using the 3 term approximation first and then convert it, for the best results. Unfortunately, the drawback is that it requires knowledge about the force of mortality, which is not commonly provided in life tables. Thus, although less accurate, the 2 term approach can be easily used in any situation. Term Approximations \u00b6 Unlike assurances, the approximations for annuities have multiple components. Thus, the approximations for term annuities are slightly different from the ones used for WL. However, there is NO need to remember a seperate expression for it, it can be calculated by converting the WL to a TA: \\[ \\begin{aligned} a^{(m)}_{x:\\enclose{actuarial}{n}} &= a^{(m)}_{x} - {}_{n}E_{x} a^{(m)}_{x+n} \\\\ &= \\alpha(m) \\ddot{a}_{x} - \\beta(m) - {}_{n}E_{x} (\\alpha(m) \\ddot{a}_{x+n} - \\beta(m)) \\\\ &= \\alpha(m) \\ddot{a}_{x} - \\beta(m) - {}_{n}E_{x} \\alpha(m) \\ddot{a}_{x+n} + {}_{n}E_{x} \\beta(m) \\\\ &= \\alpha(m) (\\ddot{a}_{x} - {}_{n}E_{x} \\ddot{a}_{x+n}) - \\beta(m) (1-{}_{n}E_{x}) \\\\ &= \\alpha(m) (\\ddot{a}_{x+n}) - \\beta(m) (1-{}_{n}E_{x}) \\end{aligned} \\] A similar approach can be taken for the woolhouse approximation, resulting in the following: \\[ a^{(m)}_{x:\\enclose{actuarial}{n}} = \\ddot{a}_{x+n} - \\frac{m-1}{2m} (1-{}_{n}E_{x}) \\] For simplicity, the three term approach is not shown.","title":"Life Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#life-annuities","text":"","title":"Life Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#overview","text":"Life Annuity contracts promise to pay out a stream of benefits in the future for as long the policyholder remains alive. Similar to life insurance, the benefits can be paid discretely or continuously and can be valued through their EPV. Note that these are different from the annuities covered in Exam FM. The payments for those annuities are made regardless of the life of the policyholder, known as Annuity Certain . They serve as the foundation to understanding Life Annuities.","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#review-annuities-certain","text":"There are two types of payment structures: Period Start Period End Paid in Advance Paid in Arrears Annuity Due Annuity Immediate \\(\\ddot{a}_{\\enclose{actuarial}{n}}\\) \\(a_{\\enclose{actuarial}{n}}\\) The overall PV of the annuity is the sum of the PV of the stream of payments: \\[ \\begin{aligned} a_{\\enclose{actuarial}{n}} &= v + v^2 + v^3 + \\dots + v^n \\\\ &= \\frac{v-v^{n+1}}{1-v} \\\\ &= \\sum^n_{k=1} v^k \\\\ &= \\frac{v(1-v^n)}{1-v} \\\\ &= \\frac{1-v^n}{i} \\\\ \\\\ \\ddot{a}_{\\enclose{actuarial}{n}} &= 1 + v + v^2 + \\dots + v^n \\\\ &= \\sum^{n-1}_{k=0} v^k \\\\ &= \\frac{1- v^{n}}{1-v} \\\\ &= \\frac{1- v^{n}}{d} \\end{aligned} \\] Notice that the payments simply differ by one period and hence one discounting factor: \\[ \\begin{aligned} \\ddot{a}_{\\enclose{actuarial}{n}} &= 1 + v + v^2 + v^3 + \\dots + v^n \\\\ &= (1+i) (v + v^2 + v^3 + \\dots + v^n) \\\\ &= (1+i) \\sum^{n}_{k=1} v^k \\\\ &= (1+i) a_{\\enclose{actuarial}{n}} \\end{aligned} \\] From another perspective, only the payments at the end points \\(t=0\\) and \\(t=n\\) are different: \\[ \\begin{aligned} \\ddot{a}_{\\enclose{actuarial}{n}} &= \\sum^{n-1}_0 v^k \\\\ &= v^0 + \\sum^{n-1}_{k=1} v^k \\\\ &= 1 + a_{\\enclose{actuarial}{n-1}} \\\\ &= 1 + a_{\\enclose{actuarial}{n}} - v^n \\end{aligned} \\]","title":"Review: Annuities Certain"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#payable-discretely","text":"If the contract pays out benefits discretely, then \\(k\\) is used as the survival model. Since a life annuity pays out a stream of benefits, the PV of the contract at every time period can be slightly confusing. It is important to remember the following: The PV is calculated with respect to time 0 The PV is the sum of the PV of all the payments the individual is expected to live \\[ \\begin{aligned} PV &= \\begin{cases} a_{\\enclose{actuarial}{K_x}}, & \\text{Life Annuity Immediate} \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x + 1}}, & \\text{Life Annuity Due} \\end{cases} \\end{aligned} \\] Thus, the EPV of the contract is the sum product of the PV of the benefit in each period and the probability of death in that period: \\[ \\begin{aligned} EPV &= \\begin{cases} \\text{Life Annuity Immediate} &=\\ \\sum a_{\\enclose{actuarial}{k}} \\cdot {}_{k|}q_{x} \\\\ \\text{Life Annuity Due} &= \\sum \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} \\end{cases} \\end{aligned} \\] For the remainder of the section, to be concise, only the formulas for Annuity Due will be given. The corresponding formulas for Annuity Immediates can be easily calculated from it.","title":"Payable Discretely"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#actuarial-notation","text":"Most of the notation for Life Insurance applies to Life Annuities as well. The key difference is that \\({}^{k}a\\) is used to represent the k-th moment of the present value of a contract where a benefit of 1 is paid discretely for as long as the status does NOT fail . \\(A\\) stands for Assurance while \\(a\\) stands for Annuity.","title":"Actuarial Notation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#whole-life-annuity","text":"Whole Life Annuities covers the insured indefinitely and thus will pay out for as long as the insured survives . Let WL be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{WL}_{\\text{Due}} &=\\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\end{aligned} \\] Thus, the EPV is the Expectation of the WLA random variable: \\[ \\begin{aligned} E(\\text{WLA}_{\\text{Due}}) &= \\sum^{\\infty}_{k = 0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} \\\\ \\ddot{a}_{x} &= \\sum^{\\infty}_{k = 0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} \\end{aligned} \\] The EPV can be furthered simplified, allowing a life annuity to be viewed as the sum of a series of pure endowments : \\[ \\begin{aligned} \\ddot{a}_{x} &= \\sum^{\\infty}_{k = 0} \\ddot{a}_{\\enclose{actuarial}{k}} \\cdot {}_{k|}q_{x} \\\\ &=\\sum^{\\infty}_{k = 0} \\left(\\sum^{n-1}_{j=0} v^j \\right) \\cdot {}_{k|}q_{x} \\\\ &= v^0 \\cdot {}_{0|}q_{x} + (v^0 + v^1) \\cdot {}_{1|}q_{x} + (v^0 + v^1 + v^2) \\cdot {}_{2|}q_{x} + \\dots \\\\ &= v^0 ({}_{0|}q_{x} + {}_{1|}q_{x} + \\dots) + v^1 ({}_{1|}q_{x} + {}_{2|}q_{x} + \\dots) + v^2 ({}_{2|}q_{x} + {}_{3|}q_{x} + \\dots) + \\dots \\\\ &= \\sum^{\\infty}_{j=0} v^j \\cdot \\left(\\sum^{\\infty}_{k = j} {}_{k|}q_{x} \\right) \\\\ &= \\sum^{\\infty}_{j=0} v^j \\cdot {}_{j}p_{x} \\end{aligned} \\] Note that the above formula can be used to easily prove the relationships between different kinds of annuities, but is NOT a proper definition of an annuity - it hence cannot be adjusted to compute the variance. The second moment and variance for all life annuities will be covered in a later section.","title":"Whole Life Annuity"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#temporary-annuity","text":"Temporary Life Annuities covers the insured for a specified period \\(n\\) and thus pays out for as long as the insured survives during that period only . Let TA be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{TA}_\\text{Due} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{K_x+1}},& k = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{n}},& k \\ge n \\\\ \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{min(K_x + 1, n)}} \\end{aligned} \\] Thus, the EPV is the expectation of the TA random variable, which can be simplified using the same approach as before: \\[ \\begin{aligned} E(\\text{TA}_\\text{Due}) &= \\sum^{n-1}_{k = 0} \\ddot{a}_{\\enclose{actuarial}{k}} \\cdot {}_{k|}q_x + \\ddot{a}_{\\enclose{actuarial}{n}} \\cdot {}_{n}p_x \\\\ \\ddot{a}_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{j=0} v^j \\cdot {}_{j}p_{x} \\end{aligned} \\] Note the change in limits for the simplified approach - an annuity is the combination of pure endowments for as long as the annuity lasts . This gives rise to a very simple sense check - the EPV of an \\(n\\) year temporary annuity should NOT be larger than \\(n\\) as there are \\(n\\) payments of 1 which are discounted.","title":"Temporary Annuity"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#deferred-annuities","text":"Deferred Annuities are variations of the above contracts where the coverage is deferred by \\(n\\) years . While any contract can be deferred, the most useful is the Deferred Whole Life Annuity . Let DWL be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{DWL}_{\\text{Due}} &= \\begin{cases} 0, & k = 0, 1, 2, \\dots, n-1 \\\\ v^n \\cdot \\ddot{a}_{\\enclose{actuarial}{K_x+1-n}}, & k \\ge n \\end{cases} \\end{aligned} \\] Intuitively, the RV can also be expressed as the difference between two certain annuities: \\[ \\begin{aligned} \\text{DWL}_{\\text{Due}} &= v^n \\cdot \\ddot{a}_{\\enclose{actuarial}{k+1-n}} \\\\ &= v^n \\cdot \\frac{1-v^{k+1-n}}{d} \\\\ &= \\frac{v^{n}-v^{k+1}}{d} \\\\ &= \\frac{(1-v^{k+1}) - (1-v^{n})}{d} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{k+1}} -\\ddot{a}_{\\enclose{actuarial}{n}} \\\\ \\\\ \\therefore \\text{DWL}_{\\text{Due}} &= \\begin{cases} 0, & k = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{k+1}} -\\ddot{a}_{\\enclose{actuarial}{n}} , & k \\ge n \\end{cases} \\end{aligned} \\] Thus, the EPV is the expectation of the DWL random variable: \\[ \\begin{aligned} E(\\text{DWL}_{\\text{Due}}) &= \\sum^{\\infty}_{k=n} (\\ddot{a}_{\\enclose{actuarial}{k+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{k|}q_{x} \\\\ {}_{n|}\\ddot{a}_{x} &= \\sum^{\\infty}_{k=0} (\\ddot{a}_{\\enclose{actuarial}{k+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{k|}q_{x} - \\sum^{n-1}_{k=0} (\\ddot{a}_{\\enclose{actuarial}{k+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{k|}q_{x} \\\\ &= \\sum^{\\infty}_{k=0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} - \\sum^{\\infty}_{k=0} \\ddot{a}_{\\enclose{actuarial}{n}} \\cdot {}_{k|}q_{x} - \\sum^{n-1}_{k=0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} + \\sum^{n-1}_{k=0} \\ddot{a}_{\\enclose{actuarial}{n}} \\cdot {}_{k|}q_{x} \\\\ &= \\sum^{\\infty}_{k=0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} - \\sum^{n-1}_{k=0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} \\\\ &= \\ddot{a}_x - \\ddot{a}_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] Similar to before, the EPV can be shown to be a sum of pure endowments: \\[ {}_{n|}\\ddot{a}_{x} = \\sum^{\\infty}_{j=n} v^j \\cdot {}_{j}p_{x} \\] Since a DWL is effectively a WL that is issued \\(n\\) years later, the EPV of a DWL is the EPV of the WL, adjusted for both interest and survival: \\[ \\begin{aligned} {}_{n|}a_{x} &= \\sum^{\\infty}_{j=n} v^j \\cdot {}_{j}p_{x} \\\\ &= \\sum^{\\infty}_{j=0} v^{j+n} \\cdot {}_{j+n}p_{x} \\\\ &= \\sum^{\\infty}_{j=0} v^{j} v^{n} \\cdot {}_{n}p_{x} {}_{j}p_{x+n} \\\\ &= v^{n} \\cdot {}_{n}p_{x} \\sum^{\\infty}_{j=0} v^{j} \\cdot {}_{j}p_{x+n} \\\\ &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\end{aligned} \\] This allows a TA to be expressed as the difference of two WL annuities issued at different times : \\[ \\begin{aligned} {}_{n|}\\ddot{a}_{x} &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\\\ \\ddot{a}_x - \\ddot{a}_{x:\\enclose{actuarial}{n}} &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\\\ \\ddot{a}_{x:\\enclose{actuarial}{n}} &= \\ddot{a}_x - {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\end{aligned} \\]","title":"Deferred Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#guaranteed-annuities","text":"Guaranteed Life Annuities are whole life annuities with the benefits in the first \\(n\\) years being guaranteed - they will be paid out even if the insured dies during this period. Let GA be the random variable denoting the PV of the survival benefit. Since the payments are guaranteed, they can be represented using a certain annuity : \\[ \\begin{aligned} \\text{GA}_{\\text{Due}} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& k = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}},& k \\ge n \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{max(K_x+1,n)}} \\end{aligned} \\] The random variable can be manipulated to be easier to work with: \\[ \\begin{aligned} \\text{GA}_{\\text{Due}} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}},& K_x \\ge n \\end{cases} \\\\ &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}} + \\ddot{a}_{\\enclose{actuarial}{n}},& K_x \\ge n \\end{cases} \\\\ &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}} \\\\ \\ddot{a}_{\\enclose{actuarial}{n}} \\end{cases} + \\begin{cases} 0,& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}},& K_x \\ge n \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{n}} + \\begin{cases} 0,& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}},& K_x \\ge n \\end{cases} \\end{aligned} \\] Thus, the EPV can be more easily calculated: \\[ \\begin{aligned} E(\\text{GA}_{\\text{Due}}) &= \\ddot{a}_{\\enclose{actuarial}{n}} + 0 \\cdot {}_{n}q_x + \\sum^{\\infty}_{k = n} (\\ddot{a}_{\\enclose{actuarial}{k+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{k|}q_{x}\\\\ \\ddot{a}_{\\overline{x:\\enclose{actuarial}{n}}} &= \\ddot{a}_{\\enclose{actuarial}{n}} + {}_{n} \\ddot{a}_{x} \\end{aligned} \\] Note that a Guaranteed Annuity is denoted via an additional bar above the status.","title":"Guaranteed Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#assurances-and-annuities","text":"Consider the random variable for the PV Whole Life Assurance and Annuity Due. Notice that both RVs are related through the certain annuity formula: \\[ \\begin{aligned} \\text{WL} &= v^{K_x + 1} \\\\ \\text{WLA}_{Due} &= \\ddot{a}_{\\enclose{actuarial}{K_x+1}} = \\frac{1-v^{K_x + 1}}{d} \\end{aligned} \\] This is why Annuity Due, rather Annuity Immediates, are provided in the SULT. Thus, given the EPV of a life annuity, the EPV of a life assurance can be determined, vice-versa: \\[ \\begin{aligned} \\text{WLA} &= \\frac{1-\\text{WL}}{d} \\\\ E(\\text{WLA}_{Due}) &= \\frac{1-E(\\text{WL})}{d} \\\\ \\ddot{a}_x &= \\frac{1-A_x}{d} \\\\ d \\ddot{a}_x &= 1 - A_x \\\\ A_x &= 1 - d \\ddot{a}_x \\end{aligned} \\] The same relationship can be shown for a temporary annuity and an Endowment Assurance, NOT a term assurance : \\[ \\begin{aligned} A_{x:\\enclose{actuarial}{n}} &= 1 - d \\ddot{a}_{x:\\enclose{actuarial}{n}} \\\\ \\end{aligned} \\] It is a very common mistake to confuse the two as both are denoted by TA. However, it is the endowment assurance that shares the same random variable as the temporary annuity. It also makes sense when comparing the actuarial notation - both share the same subscript.","title":"Assurances and Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#variance","text":"However, the second moment is slightly different as it must reflect the new interest rate used. The same logic can be applied for the Variance as well: \\[ \\begin{aligned} \\text{WLA}_{Due} &= \\frac{1-\\text{WL}}{d} \\\\ Var(\\text{WLA}_{Due}) &= Var \\left( \\frac{1-\\text{WL}}{d} \\right) \\\\ Var(\\text{WLA}_{Due}) &= \\frac{1}{d^2} var(\\text{WL}) \\\\ Var(\\text{WLA}_{Due}) &= \\frac{1}{d^2} \\left({}_{}^2 A_x - (A_x)^2 \\right) \\end{aligned} \\] Var(\\text{TA}_{Due}) &= \\frac{1}{d^2} \\left({}_{}^2 A_{x:\\enclose{actuarial}{n}} - (A_{x:\\enclose{actuarial}{n}})^2 \\right) WLDue and WL Immediate same variance but DWLDue and DWL immediate different variance because it is dependent on survival GA = Certain + Deferred Var (GA) = Var (Deferred) Problem is Var(Deferred)","title":"Variance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#immediate-and-due","text":"Recall that for Certain Annuities, the only difference between Due and Immediate ones are the payments at the start and end times. This relationship can be extended for Life Annuities as well. For WL annuities, since the end point is infinity , the difference in the end points can be ignored. Thus, the only difference is the first payment of 1, which can be easily accounted for: \\[ a_x = \\ddot{a}_x - 1 \\] Note This section uses \\(a_x\\) as the focus, as it is assumed that if needed, the due versions will be calculated and then converted to immediate , rather than calculating immediate annuities directly. Unfortunately, the difference at the end cannot be ignored for TA annuities: \\[ a_{x:\\enclose{actuarial}{n}} = \\ddot{a}_{x:\\enclose{actuarial}{n}} - 1 + {}_{n}E_x \\] There is no need to memorize this expression as it can be easily derived using the WL conversion: \\[ \\begin{aligned} a_{x:\\enclose{actuarial}{n}} &= a_{x} - {}_{n}E_x a_{x+n} \\\\ &= \\ddot{a}_{x} - 1 - {}_{n}E_x (\\ddot{a}_{x+n} - 1) \\\\ &= \\ddot{a}_{x} - 1 - {}_{n}E_x \\cdot \\ddot{a}_{x+n} + {}_{n}E_x \\\\ &= \\ddot{a}_{x} - {}_{n}E_x \\cdot \\ddot{a}_{x+n} - 1 + {}_{n}E_x \\\\ &= \\ddot{a}_{x:\\enclose{actuarial}{n}} - 1 + {}_{n}E_x \\end{aligned} \\] For variance, the random variables (which are certain annuities) are used instead: \\[ \\begin{aligned} Var(\\text{WL}_{Immediate}) &= Var(a_{\\enclose{actuarial}{k}}) \\\\ &= Var(\\ddot{a}_{\\enclose{actuarial}{k+1}}-1) \\\\ &= Var(\\ddot{a}_{\\enclose{actuarial}{k+1}}) \\\\ &= Var(\\text{WL}_{Due}) \\\\ \\\\ Var(\\text{TA}_{Immediate}) &= Var(a_{\\enclose{actuarial}{min(k,n)}}) \\\\ &= Var(\\ddot{a}_{\\enclose{actuarial}{min(k+1,n+1)}}-1) \\\\ &= Var(\\ddot{a}_{\\enclose{actuarial}{min(k+1,n+1)}}) \\\\ &= Var \\left( \\frac{1-v^{min(k+1,n+1)}}{d} \\right) \\\\ &= \\frac{1}{d^2} Var(v^{min(k+1,n+1)}) \\\\ &= \\frac{1}{d^2} \\left({}_{2}A_{x:\\enclose{actuarial}{n+1}} - (A_{x:\\enclose{actuarial}{n+1}})^2 \\right) \\end{aligned} \\] Thus, the variance of a WL annuity is the same for both Immediate and Due but NOT for a TA annuity.","title":"Immediate and Due"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#recursions","text":"Following the same logic as assurances, Annuities can also be recursively expressed as a function of itself: If the policyholder dies, they would have only received the benefit of 1 at the start of the year If the policyholder survives, they would receive the additional benefits of the future periods The benefit of 1 is received REGARDLESS of death or survival as it was paid at the start of the year \\[ \\begin{aligned} \\text{WL}_\\text{Due} &= \\begin{cases} 1,& q_x \\\\ 1 + v\\ddot{a}_{x+1} ,& p_x \\end{cases} \\\\ &= 1 + \\begin{cases} 0,& q_x \\\\ v\\ddot{a}_{x+1} ,& p_x \\end{cases} \\\\ \\\\ \\therefore \\ddot{a}_{x} &= 1 + vp_x\\ddot{a}_{x+1} \\end{aligned} \\] The same can be shown for TAs, but remember that the remaining duration of the policy must decrease as well: \\[ \\ddot{a}_{x:\\enclose{actuarial}{n}} = 1 + vp_x \\ddot{a}_{x+1:\\enclose{actuarial}{n-1}} \\]","title":"Recursions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#intuitions","text":"Similar to assurances, several intuitions can be made about the EPV of various annuities to serve as a sort of \"sense check\" against the SULT provided to determine if the answer is in the right direction.","title":"Intuitions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#same-annuity","text":"Since annuities are all contingent on the survival of the policyholder, only one case needs to be considered. The probability of survival is a decreasing function with age. The benefits are less likely to be paid out to an older policyholder, resulting in smaller expected cashflows . Since the cashflows are discounted the same amount, an older policyholder will have a lower EPV than a younger one: \\[ \\ddot{a}_{x+n} \\lt \\ddot{a}_{x} \\] As shown previously, Annuity Dues are always smaller than Immediates as the cashflows occur earlier and are hence discounted less . \\[ a_x < \\ddot{a}_x \\] Naturally, all else equal, annuities with a lower interest rate are discounted less and thus have a higher EPV .","title":"Same Annuity"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#different-annuities","text":"Since annuities are all contingent on the survival of the policyholder, the age of the policyholder for comparison does not matter. TA has the smallest EPV as it can only pay for a maximum of \\(n\\) years, while WLs and GA can pay indefinitely. WL is always smaller than GA as its payments in the first \\(n\\) years are not guaranteed; the payments after that are identical. \\[ \\ddot{a}_{x:\\enclose{actuarial}{n}} < \\ddot{a}_x < \\ddot{a}_{\\bar{x:\\enclose{actuarial}{n}}} \\]","title":"Different Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#immediate-vs-due","text":"Consider two temporary life annuity with a term of \\(n\\) years: Annuity Immediate issued at age \\(x\\) Annuity Due issued at age \\(x+1\\) Both have the same cashflows : However, both of them are valued at different times: Annuity Immediate valued at age \\(x\\) Annuity Due valued at age \\(x+1\\) Thus, the PV of the cashflows are NOT the same : Thus, although they have the same cashflows, the annuity due has a larger EPV : \\[ \\begin{aligned} a_{x:\\enclose{actuarial}{n}} &= \\sum^n_{j=1} v^j {}_{j}p_{x} \\\\ \\\\ \\ddot{a}_{x+1:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{j=0} v^j {}_{j}p_{x+1} \\\\ &= \\frac{1}{vp_{x}} \\sum^{n-1}_{j=0} v^{j+1} {}_{j}p_{x+1} p_{x} \\\\ &= \\frac{1}{vp_{x}} \\sum^{n-1}_{j=0} v^{j+1} {}_{j+1}p_{x} \\\\ &= \\frac{1}{vp_{x}} \\sum^{n}_{j=1} v^{j} {}_{j}p_{x} \\\\ &= \\underbrace{\\frac{1}{vp_{x}}}_{>1} \\cdot a_{x:\\enclose{actuarial}{n}} \\\\ \\\\ \\therefore \\ddot{a}_{x+1:\\enclose{actuarial}{n}} &> a_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] This approach might seem long winded, as it seems that it is sufficient to simply compare the cashflows of \\(1 > v^n\\) . However, that ignores the probabilities , which is properly accounted for in the above approach.","title":"Immediate VS Due"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#probabilities-and-percentiles","text":"Similar to assurances, apart from just calculating the Expectation and Variance, the probabilities and hence percentiles of the contract benefits can be calculated as well. \\[ \\begin{aligned} \\text{WL}_\\text{Due} &\\le u \\\\ \\frac{1-v^{k+1}}{d} &\\le u \\\\ 1-v^{k+1} &\\le ud \\\\ v^{k+1} &\\ge 1-ud \\\\ (k+1) \\ln v &\\ge \\ln(1-ud) \\\\ k+1 &\\le \\frac{\\ln(1-ud)}{\\ln v} \\\\ k &\\le \\frac{\\ln(1-ud)}{\\ln v} - 1 \\\\ \\end{aligned} \\] The common mistake of not flipping the inequality sign is relevant in annuities as well. To avoid this error, it is advised to plot a graph to remember that \\(k\\) should be smaller than the calculated value: The RHS of the expression is unlikely to be an integer. However, \\(k\\) can only take integer values. Thus, the values can rounded DOWN to the nearest whole number (EG. 4): \\[ \\begin{aligned} P(k \\le \\frac{\\ln(1-ud)}{\\ln v} - 1) &= P(k \\le 4) \\\\ &= P(T_x \\le 4) \\\\ &= {}_{4}p_x \\end{aligned} \\] The latter refers to calculating the percentile of the random variable , which is the smallest value of the RV that results in the specified probability. This process is the opposite of the previous one - it involves solving for \\(T_x\\) , converting it to \\(k\\) and then subsituting it back into the RV, which results in the associated percentile.","title":"Probabilities and Percentiles"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#payable-continuously","text":"Similar to assurances, annuities can also be payable \\(m\\) times a year or payable continuously. Most of the key ideas carry over from the assurances, thus this section will mainly focus on the approximations from discrete annuities. However, something unique to annuities is that the conversion between due and immediates is slightly different because of the differences in cashflows: \\[ a^{(m)}_x = \\ddot{a}^{(m)}_x - \\frac{1}{m} \\]","title":"Payable Continuously"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#uniform-distribution-of-deaths","text":"Using the relationship between Assurances and Annuities, a continuous WL annuity can also be expressed in the form of a continuous assurance: \\[ \\begin{aligned} \\ddot{a}^{(m)}_x &= \\frac{1 - A^{(m)}_x}{d^{(m)}} \\end{aligned} \\] Assuming UDD, the continuous assurance and hence the overarching continuous annuity can be simplified: \\[ \\begin{aligned} \\ddot{a}^{(m)}_x &= \\frac{1 - A^{(m)}_x}{d^{(m)}} \\\\ &= \\frac{1 - \\frac{i}{i^{(m)}}A_x}{d^{(m)}} \\\\ &= \\frac{\\frac{i^{(m)}-iA_x}{i^{(m)}}}{d^{(m)}} \\\\ &= \\frac{i^{(m)}-iA_x}{i^{(m)}d^{(m)}} \\\\ &= \\frac{i^{(m)}-i(1-d\\ddot{a}_x)}{i^{(m)}d^{(m)}} \\\\ &= \\frac{i^{(m)}-i+id\\ddot{a}_x}{i^{(m)}d^{(m)}} \\\\ &= \\frac{id}{i^{(m)}d^{(m)}} \\ddot{a}_x - \\frac{i-i^{(m)}}{i^{(m)}d^{(m)}} \\\\ &= \\alpha(m)\\ddot{a}_x - \\beta(m) \\\\ \\\\ \\therefore \\alpha(m) &= \\frac{id}{i^{(m)}d^{(m)}} \\\\ \\\\ \\therefore \\beta(m) &= \\frac{i-i^{(m)}}{i^{(m)}d^{(m)}} \\end{aligned} \\] Parameter Calculation Recall that nominal and effective rates are linked using: \\[ \\begin{aligned} i^{(m)} &= m[(1+i)^\\frac{1}{m}-1] \\\\ d^{(m)} &= m[1-(1+d)^\\frac{1}{m}] \\end{aligned} \\] However, for \\(i=0.05\\) , \\(\\alpha\\) and \\(\\beta\\) are provided inside the SULT for common values of \\(m\\) , thus there is no need to waste precious time calculating them. An adjustment can be made for temporary annuities: \\[ \\begin{aligned} \\ddot{a}^{(m)}_{x:\\enclose{actuarial}{n}} &= \\ddot{a}^{(m)}_{x} - {}_{n}E_x \\ddot{a}^{(m)}_{x+n} \\\\ &= \\alpha(m)\\ddot{a}_x - \\beta(m) - {}_{n}E_x [\\alpha(m)\\ddot{a}_{x:\\enclose{actuarial}{n}} - \\beta(m)] \\\\ &= \\alpha(m)\\ddot{a}_x - \\beta(m) - {}_{n}E_x \\alpha(m)\\ddot{a}_{x:\\enclose{actuarial}{n}} - {}_{n}E_x \\beta(m) \\\\ &= \\alpha(m) [\\ddot{a}_x - {}_{n}E_x \\ddot{a}_{x:\\enclose{actuarial}{n}}] - \\beta(m) [1-{}_{n}E_x] \\\\ &= \\alpha(m) \\ddot{a}_{x:\\enclose{actuarial}{n}} - \\beta(m) [1-{}_{n}E_x] \\end{aligned} \\]","title":"Uniform Distribution of Deaths"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#woolhouse-approximation","text":"Since a continuous annuity can be expressed as a sum of discrete pure endowments , the trapezoidal rule and euler-macluarin correction can be used to approximate its EPV. The key difference from before is understanding that the EPV of an annuity due is a LEFT riemann sum as there is a payment at time 0. \\[ \\begin{aligned} \\bar{a}_x &= \\int^{\\infty}_0 v^t {}_{t}p_x \\\\ & \\approx h[f(a) + f(a+h) + \\dots + f(b-h)] - \\frac{h}{2} [f(a)-f(b)] + \\frac{h^2}{12} [f'(a)-f'(b)] \\end{aligned} \\] The first derivatives can be found below: \\[ \\begin{aligned} f'(n) &= \\frac{d}{dt} (e^{-\\delta n} {}_{n}p_x) \\\\ &= {}_{n}p_x \\delta e^{-\\delta n} - e^{-\\delta n} {}_{n}p_x \\mu_{x+n} \\\\ &= e^{-\\delta n} {}_{n}p_x (\\delta + \\mu_{x+n}) \\\\ &= {}_{n}E_x (\\delta + \\mu_{x+n}) \\\\ \\\\ \\therefore f'(0) &= -(\\delta + \\mu_{x}) \\\\ \\therefore f'(\\infty) &= 0 \\end{aligned} \\] Assuming \\(h=1\\) , \\[ \\begin{aligned} \\bar{a}_x & \\approx (1) [{}_{0}E_x + {}_{1}E_x + \\dots + {}_{\\infty-1}E_x] - \\frac{1}{2} [{}_{0}E_x-{}_{\\infty-1}E_x] + \\frac{1}{12} [-(\\delta + \\mu_{x}) - 0] \\\\ & \\approx [1 + {}_{n}E_1 + \\dots] - \\frac{1}{2} [1-0] - \\frac{1}{12} (\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12} (\\delta + \\mu_{x}) \\end{aligned} \\] Assuming \\(h=\\frac{1}{m}\\) instead, \\[ \\begin{aligned} \\bar{a}_x & \\approx \\left(\\frac{1}{m} \\right) [{}_{0}E_x + {}_{\\frac{1}{m}}E_x + \\dots + {}_{\\infty-\\frac{1}{m}}E_x] - \\frac{1}{2m} [{}_{0}E_x-{}_{\\infty-\\frac{1}{m}}E_x] + \\frac{1}{12m^2} [-(\\delta + \\mu_{x}) - 0] \\\\ & \\approx \\left(\\frac{1}{m} \\right) [1 + {}_{\\frac{1}{m}}E_x + \\dots] - \\frac{1}{2m} [1-0] - \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}^{(m)}_x - \\frac{1}{2m} - \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\end{aligned} \\] Combining both together, \\[ \\begin{aligned} \\bar{a}_x &= \\bar{a}_x \\\\ \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12} (\\delta + \\mu_{x}) & \\approx \\ddot{a}^{(m)}_x - \\frac{1}{2m} - \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\\\ \\ddot{a}^{(m)}_x & \\approx \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12} (\\delta + \\mu_{x}) + \\frac{1}{2m} + \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}_x - (\\frac{1}{2}-\\frac{1}{2m}) - (\\frac{1}{12} - \\frac{1}{12m^2})(\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}_x - \\frac{m-1}{2m} - \\frac{m^2-1}{12m^2}(\\delta + \\mu_{x}) \\end{aligned} \\] This process is known as the Woolhouse Approximation . If the error term is omitted, then it is known as the 2-term Woolhouse Approximation else it is known as the 3-term Woolhouse Approximation . The 3-term woolhouse approximation is extremely accurate due to its mathematical roots. It so much more accurate than UDD such that when calculating a continuous assurance , it is advisable to calculate the corresponding annuity using the 3 term approximation first and then convert it, for the best results. Unfortunately, the drawback is that it requires knowledge about the force of mortality, which is not commonly provided in life tables. Thus, although less accurate, the 2 term approach can be easily used in any situation.","title":"Woolhouse Approximation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#term-approximations","text":"Unlike assurances, the approximations for annuities have multiple components. Thus, the approximations for term annuities are slightly different from the ones used for WL. However, there is NO need to remember a seperate expression for it, it can be calculated by converting the WL to a TA: \\[ \\begin{aligned} a^{(m)}_{x:\\enclose{actuarial}{n}} &= a^{(m)}_{x} - {}_{n}E_{x} a^{(m)}_{x+n} \\\\ &= \\alpha(m) \\ddot{a}_{x} - \\beta(m) - {}_{n}E_{x} (\\alpha(m) \\ddot{a}_{x+n} - \\beta(m)) \\\\ &= \\alpha(m) \\ddot{a}_{x} - \\beta(m) - {}_{n}E_{x} \\alpha(m) \\ddot{a}_{x+n} + {}_{n}E_{x} \\beta(m) \\\\ &= \\alpha(m) (\\ddot{a}_{x} - {}_{n}E_{x} \\ddot{a}_{x+n}) - \\beta(m) (1-{}_{n}E_{x}) \\\\ &= \\alpha(m) (\\ddot{a}_{x+n}) - \\beta(m) (1-{}_{n}E_{x}) \\end{aligned} \\] A similar approach can be taken for the woolhouse approximation, resulting in the following: \\[ a^{(m)}_{x:\\enclose{actuarial}{n}} = \\ddot{a}_{x+n} - \\frac{m-1}{2m} (1-{}_{n}E_{x}) \\] For simplicity, the three term approach is not shown.","title":"Term Approximations"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/","text":"Variable Benefits \u00b6 Overview \u00b6 Previously, the benefit of the assurances were assumed to be fixed at 1 for simplicity. In practice, it is common to have assurances where the benefits change over time . Geometric Contracts \u00b6 If the benefits change by a common factor each period, then it is known as a Geometric contract. The benefit of the policy starts at 1 in the first year and changes by \\((1+b)\\) each period: If \\(b>0\\) , then the benefits are increasing If \\(b<0\\) , then the benefits are decreasing \\[ \\text{Geometric Benefit} = (1+b)^{K_x} \\] Note that the power is \\(K_x\\) and NOT \\(K_x+1\\) to reflect that the benefit starts at 1 when \\(K_x=0\\) . Geometric Assurance \u00b6 Let \\(\\text{Geom WL}\\) be the random variable denoting the PV of a Geometric Assurance. \\[ \\text{Geom WL} = (1+b)^{K_x} v^{K_x+1} \\] Thus, the EPV of a WL Geometric Assurance can be calculated as: \\[ \\begin{aligned} \\text{EPV Geom WL} &= \\sum^{\\infty}_{k=0} (1+b)^{k} v^{k+1} {}_{k|}q_x \\\\ &= (1+b)^{-1} \\sum^{\\infty}_{k=0} (1+b)^{k+1} v^{k+1} {}_{k|}q_x \\\\ &= \\frac{1}{1+b} \\sum^{\\infty}_{k=0} \\left(\\frac{1+b}{1+i}\\right)^{k+1} {}_{k|}q_x \\\\ &= \\frac{1}{1+b} \\sum^{\\infty}_{k=0} (v')^{k+1} {}_{k|}q_x \\\\ &= \\frac{1}{1+b} A_x |_{i=\\frac{1+i}{1+b}-1} \\end{aligned} \\] Note that this means that the term \\(A_x |_{i=\\frac{1+i}{1+b}-1}\\) by itself is a Geometric Assurance that starts with payments of \\((1+b)\\) , which is why \\(\\frac{1}{1+b}\\) is required to scale it down by one factor such that it starts at one. In practice, most questions will provide a value of \\(b\\) such that \\((i=\\frac{1+i}{1+b}-1)\\) simplifies to a nice percentage . However, this usually means that the value of \\(b\\) provided is some complicated number, so do not be taken aback! The expression follows similar intuition to the second moment - it is simply a regular EPV expression evaluated at a different interest rate . Unfortunately, there is no specified actuarial notation for this niche case. One common approach is to denote it using \\(A_x^i\\) . Geometric Annuity \u00b6 Let \\(\\text{EPV Geom WL}_\\text{Due}\\) be the random variable denoting the PV of a Geometric Annuity Due. Thus, the EPV of a Geometric Annuity Due can be calculated as: \\[ \\begin{aligned} \\text{EPV Geom WL}_\\text{Due} &= \\sum^{\\infty}_{k=0} (1+c)^k v^k {}_{k}p_x \\\\ &= \\sum^{\\infty}_{k=0} \\left(\\frac{1+c}{1+i}\\right)^k {}_{k}p_x \\\\ &= \\ddot{a}_x|_{i = \\frac{1+i}{1+c}-1} \\end{aligned} \\] Continuous Geometric \u00b6 The EPV of continuously payable geometric contracts can be calculated using the usual UDD, Claims Acceleration or Woolhouse Approximations. However, despite having a \"new\" interest rate, the original interest rate is used in the continuous approximation parameters. Warning This is different from the second moment where the new interest rate must be reflected in the approximation as well. Take note of the different treatments. Arithmetic Contracts \u00b6 If the benefits change by a fixed constant each period, then it is instead known as an Arithmetic contract. For simplicity, the change each period is assumed to be 1: Arithmetically Increasing - Increases by 1 each period Arithmetically Decreasing - Decreased by 1 each period Since a WL contract lasts forever, the benefits of a decreasing WL contract would inevitably become negative , which do not make sense. Thus, WLs can only be arithmetically increasing contracts . \\[ \\text{Arithmetically Increasing Benefit} = K_x + 1 \\] Arithmetic Assurance \u00b6 Let \\(\\text{Arith WL}\\) be the random variable denoting the PV of an arithmetically increasing WL Assurance: \\[ \\text{Arith WL} = (K_x + 1) v^{K_x+1} \\] Thus, the EPV of a WL Arithmetic Assurance can be shown to be: \\[ \\begin{aligned} \\text{EPV Arith WL} &= \\sum^{\\infty}_{k=0} (k+1) v^{k+1} {}_{k|}q_x \\\\ (IA)_x &= \\sum^{\\infty}_{k=0} (k+1) v^{k+1} {}_{k|}q_x \\end{aligned} \\] \\((IA)_x\\) is the actuarial notation for the EPV of a contract with arithmetically increasing benefits starting at 1 and increasing by 1 each period. It can also be shown to be a sum of deferred WLs , each deferred by one period from the previous. This creates a step-like benefit which creates the increasing effect of the assurance: \\[ \\begin{aligned} (IA)_x &= {}_{0|}A_x + {}_{1|}A_x + {}_{2|}A_x + ... \\\\ &= \\sum^{\\infty}_{K_x = 0} {}_{K_x|}A_x \\end{aligned} \\] In practice, the magnitude of the change is unlikely to be one. Thus, the EPV must be scaled to match the actual change. This can be problematic if the starting benefit is NOT the same as the change . Consider a product with benefit \\(B\\) that increases by \\(k\\) each year. Due to the definition of the IWL variable, the starting benefit and change each period are equal - we cannot simply multiply \\(B\\) or \\(k\\) to the IWL EPV. Thus, we must split this product into a fixed component with \\(B-k\\) benefits and a variable component that starts and increases by \\(k\\) each period. The sum of these two components results in a benefit of \\(B, B+k, B+2k, \\dots\\) Arithmetic Annuities \u00b6 Let \\(\\text{Arith WL}_\\text{Due}\\) be the random variable denoting the PV of an arithmetically increasing annuity. The PV can be denoted by an arithmetically increasing annuity certain : \\[ \\begin{aligned} \\text{Arith WL}_\\text{Due} &= (I\\ddot{a})_{\\enclose{actuarial}{n}} \\\\ &= \\sum^{n-1}_{k=0} v^k \\cdot \\ddot{a}_{\\enclose{actuarial}{n-k}} \\\\ &= \\sum^{n-1}_{k=0} v^k \\cdot \\frac{1-v^{n-k}}{d} \\\\ &= \\sum^{n-1}_{k=0} \\frac{v^k - v^n}{d} \\\\ &= \\frac{1}{d} \\left(\\sum^{n-1}_{k=0} v^k - \\sum^{n-1}_{k=0} v^n \\right) \\\\ &= \\frac{1}{d} (\\ddot{a}_{\\enclose{actuarial}{n}} - nv^n) \\\\ &= \\frac{\\ddot{a}_{\\enclose{actuarial}{n}} - nv^n}{d} \\end{aligned} \\] Similarly, the EPV of an arithmetically increasing WL annuity Due can be calculated as: \\[ \\begin{aligned} \\text{EPV Arith WL}_\\text{Due} &= \\sum^{\\infty}_{k=0} (I\\ddot{a})_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_x \\\\ (I\\ddot{a})_x &= \\sum^{\\infty}_{k=0} (k+1) v^k {}_{k}p_x \\\\ \\end{aligned} \\] Alternatively, it can be calculated from an assurance instead: \\[ \\begin{aligned} \\text{Arith WL}_\\text{Due} &= \\frac{\\ddot{a}_{\\enclose{actuarial}{k+1}} - (k+1)v^{k+1}}{d} \\\\ E(\\text{Arith WL}_\\text{Due}) &= \\frac{E(\\ddot{a}_{\\enclose{actuarial}{k+1}}) - E((k+1)v^{k+1})}{d} \\\\ (I\\ddot{a})_x &= \\frac{\\ddot{a}_x - (IA)_x}{d} \\end{aligned} \\] Notice that the main difference between an increasing annuity due and immediate is that the immediate case now lags the due by 1 every period . This difference can be accounted for using a level annuity due: \\[ \\begin{aligned} (I\\ddot{a})_x &= \\sum^{\\infty}_{k=0} (k+1) v^k {}_{k}p_{x} \\\\ &= v^{0} {}_{0}p_{x} + 2v^{1} {}_{1}p_{x} + 3v^{2} {}_{2}p_{x} + \\dots \\\\ &= (v^{1} {}_{1}p_{x} + 2v^{2} {}_{2}p_{x} + \\dots) + (v^{0} {}_{0}p_{x} + v^{1} {}_{1}p_{x} + v^{2} {}_{2}p_{x} + \\dots) \\\\ &= (Ia)_x + \\ddot{a}_x \\\\ (Ia)_x &= (I\\ddot{a})_x - \\ddot{a}_x \\end{aligned} \\] Arithmetic Term/Temporary \u00b6 For contract with level benefits , term/temporary contracts can be expressed as a difference of two WLs issued at different times because the benefits for the time after the specified term would cancel out . For contracts with variable benefits, this is slightly more complicated as the benefits after the specified term do NOT cancel out : The \"earlier\" WL would have increased significantly from its starting value The \"later\" WL would only be at its starting value Thus, an additional expression needs to be subtracted in order to remove the remaining benefit past the specified term. This can be done using a level benefit contract with the a benefit equal to the remaining amount . Thus, the EPV of an Increasing Term Assurance can be calculated as the following: \\[ \\begin{aligned} \\text{EPV Arith TA} &= \\sum^{n-1}_{K_x=0} (k+1) v^{K_x+1} {}_{k|}q_x \\\\ (IA)^1_{x:\\enclose{actuarial}{n}} &= (IA)_x - {}_{n}E_x [(IA)_{x+n} + nA_{x+n}] \\end{aligned} \\] Thus, the EPV of an Increasing Term Annuity can be calculated as the following: \\[ \\begin{aligned} \\text{EPV Arith TA}_\\text{Due} &= \\sum^{n-1}_{k=0} (I\\ddot{a})_{\\enclose{actuarial}{n}} \\cdot {}_{k|}q_x \\\\ (I\\ddot{a})_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{k=0} (k+1) v^k {}_{k}p_x \\\\ &= (I\\ddot{a})_x - {}_{n}E_x[(I\\ddot{a})_{x+n}+n\\ddot{a}_{x+n}] \\end{aligned} \\] Due to Immediate Conversion As mentioned in the Life Annuity section, it is not useful to memorize the conversion from Due to Immediate for TAs. Thus, simply express the ITA in the form of IWLs and then perform the conversion there. Decreasing Benefits \u00b6 As mentioned previously, another key feature of term/temporary contracts is that they can have decreasing benefits . This is because the benefit of the policy can be set such that it would exactly decrease to 0 by the end of the policy term. This is done by setting a starting benefit of \\(n\\) and decreasing by \\(1\\) each period : \\[ \\text{Arithmetically Decreasing Benefit} = n-k \\] Thus, the EPV of a Decreasing Term Assurance can be calculated as the following: \\[ \\begin{aligned} \\text{EPV Arith Decreasing TA} &= \\sum^{\\infty}_{k=0} (n-k) v^{k+1} {}_{k|}q_x \\\\ (D_{\\enclose{actuarial}{n}}A)_{x:\\enclose{actuarial}{n}} &= \\sum^{\\infty}_{k=0} (n-k+1-1) v^{k+1} {}_{k|}q_x \\\\ &= \\sum^{\\infty}_{k=0} (n+1) v^{k+1} {}_{k|}q_x - \\sum^{\\infty}_{k=0} (k+1) v^{k+1} {}_{k|}q_x \\\\ &= (n+1)A^1_{x:\\enclose{actuarial}{n}} - (IA)^1_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] \\((DA)\\) is the actuarial notation for an Assurance with Arithmetically decreasing benefits starting at \\(n\\) and decreasing by 1 each period. Similarly, the EPV of a decreasing temporary annuity can be calculated as the following: \\[ (D_{\\enclose{actuarial}{n}}\\ddot{a})_{x:\\enclose{actuarial}{n}} = (n+1)\\ddot{a}_{x:\\enclose{actuarial}{n}} - (I\\ddot{a})_{x:\\enclose{actuarial}{n}} \\] Continuous Arithmetic \u00b6 Similar to Geometric Contracts, the EPV of a continuously payable Arithmetic Contracts can be calculated using the usual approximations. However, take note that for TAs, the added term should also be continuous : \\[ \\begin{aligned} (I\\bar{A})_{x:\\enclose{actuarial}{n}} &= (I\\bar{A})_x - {}_{n}E_x[(I\\bar{A})_{x+n}+ n\\bar{A}_{x+n}] \\\\ (I\\bar{a})_{x:\\enclose{actuarial}{n}} &= (I\\bar{a})_x - {}_{n}E_x[(I\\bar{a})_{x+n}+ n\\bar{a}_{x+n}] \\end{aligned} \\] Increasing Woolhouse Approximation For IWL Annuities, the woolhouse approximation is slightly different: \\[ (I\\bar{a})_{x} = (I\\ddot{a})_{x} - \\frac{1}{2} \\ddot{a}_x \\] Continuous arithmetic contracts could also refer to a contract that not only payable continuously, but changes continuously as well. The benefits change at a constant rate throughout the period , such that the total change is still equal to 1. For Assurances , it can be understood from a graphical approach: \\[ \\begin{aligned} (\\bar{I}\\bar{A})_{x} &= (I\\bar{A})_{x} - \\frac{1}{2} \\bar{A}_x \\end{aligned} \\] For Annuities, it can be derived using the woolhouse approximation : \\[ \\begin{aligned} (\\bar{I}\\bar{a})_x &= (Ia)_x + \\frac{1}{12} \\\\ &= (I\\ddot{a})_x - \\ddot{a}_x + \\frac{1}{12} \\end{aligned} \\] Note that these approximations are ONLY applicable for WL policies . For continously increasing TAs, convert them to WLs first before applying the approximations. Variable Recursions \u00b6 Since the benefit payable at the end of the first year is still 1 , the recursion for variable contracts are similar to the level ones. Variable Assurances \u00b6 Consider the two scenarios: If the policyholder dies in the current year with probability \\(q_x\\) , then a benefit of 1 is paid at the end of the year If the policyholder lives past the current year with probability \\(p_x\\) , then the they will receive the EPV of a policy with benefits \\(2, 3, \\dots\\) or \\((1+b)^1, (1+b)^2, \\dots\\) at some future time The second component must be the EPV of an assurance that STARTS with an increased benefit of \\((1+b)\\) or \\(2\\) . The issue is that the random variable is currently defined as a contract with benefits starting at 1 , which is why the recursive formula is different from the level case . For Geometric Assurances, \\(A_{x+1}|_{i=\\frac{1+i}{1+b}-1}\\) is already an Geometric Assurance that starts at \\((1+b)\\) , thus no adjustment is needed for the RHS: \\[ \\text{EPV Geom WL}_{x} = vq_x + vp_x A_{x+1}|_{i=\\frac{1+i}{1+b}-1} \\] For Arithmetic Assurances, an EPV representing an additional benefit of 1 each period must be added to the second component so that the RHS expression starts from 2 and increases by an additional 1 each period. \\[ (IA)_{x} = vq_x + vp_x[(IA)_{x+1} + A_{x+1}] \\] Variable Annuities \u00b6 Recall that a benefit of 1 is already received at the beginning of the year regardless of whether the policyholder lives or dies. If the policyholder dies with probability \\(q_x\\) , then they receive nothing extra If the policyholder lives past the current year with probability \\(p_x\\) , then they will receive benefits that are larger than 1 at some future time For Geometric Annuities, TBC. \\[ \\begin{aligned} \\text{EPV Geom WL}_{\\text{Due},x} = 1 + vp_x \\cdot (1+c) \\left(\\ddot{a}_{x+1}|_{i = \\frac{1+i}{1+c}-1} \\right) \\end{aligned} \\] For Arithmetic Annuities, the same adjustment as before must be made: \\[ (I\\ddot{a})_x = 1 + vp_x[(I\\ddot{a})_{x+1} + \\ddot{a}_{x+1}] \\] In Force Recursions \u00b6 Previously, we have only considered the EPV for a newly issued policy at age \\(x\\) . However, now we want to consider the EPV of a policy that has already been in force for some period of time \\(t\\) . For policies with level benefits , the EPV of an already in force policy is exactly the same as the EPV of a newly issued policy at the current age with the same expiration date . This means that the same recursion can be used for both newly issued policies and inforce policies. However, this is NOT the case for policies with variable benefits as the inforce policy would have a benefit of \\(t\\) or \\((1+b)^{t-1}\\) while a newly issued policy would have benefit of 1. This means that a seperate recursion must be defined for inforce variable policies. For simplicity, we will consider an in force arithmetically increasing WL Assurance : If the policyholder dies with probability \\(q_{x+t-1}\\) , they will receive a benefit of \\(t\\) at the end of the year If the poliycholder survives with probability \\(p_{x+t-1}\\) , they will receive the EPV of a contract with benefits \\(t+1, t+2, \\dots\\) at some future time \\[ RHS = vq_{x+t-1} \\cdot t + vp_{x+t-1} [(IA)_{x+t} + tA_{x+t}] \\] Since (IA) is used to denote a policy with benefits starting at 1, an additional term must be added to the LHS to show that the in force policy has benefits of t: \\[ LHS = (IA)_{x+t-1} + (t-1) A_{x+t-1} \\] The same exercise can be performed for Annuities as well, resulting in the following: \\[ \\begin{aligned} (IA)_{x+t-1} + (t-1) A_{x+t-1} &= vq_{x+t-1} \\cdot t + vp_{x+t-1} [(IA)_{x+t} + tA_{x+t}] \\\\ (I\\ddot{a})_{x+t-1} + (t-1) \\ddot{a}_{x+t-1} &= t + vp_{x+t-1} [(I\\ddot{a})_{x+t} + t\\ddot{a}_{x+t}] \\end{aligned} \\] Term/Temporary Policies The same can be shown for Variable Term/Temporary policies, but remember that the remaining duration of the policy must decrease as well. Variance and Second Moment \u00b6 Consider the second moment of both types of variable assurances: \\[ \\begin{aligned} E\\left[(\\text{Arith WL})^2\\right] &= \\sum^{\\infty}_{K_x=0} (k+1)^2 (v^2)^{K_x+1} {}_{k|}q_x \\\\ \\\\ E\\left[(\\text{Geom WL})^2\\right] &= \\sum^{\\infty}_{K_x=0} (1+b)^{2K_x} (v^2)^{K_x+1} {}_{k|}q_x \\end{aligned} \\] Notice that the usual approximation of \\(v=v^2\\) is insufficient to solve for the second moment due to the squared benefit. Thus, there are no \\({}^{2}A\\) defined for variable assurances.","title":"Variable Benefits"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#variable-benefits","text":"","title":"Variable Benefits"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#overview","text":"Previously, the benefit of the assurances were assumed to be fixed at 1 for simplicity. In practice, it is common to have assurances where the benefits change over time .","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#geometric-contracts","text":"If the benefits change by a common factor each period, then it is known as a Geometric contract. The benefit of the policy starts at 1 in the first year and changes by \\((1+b)\\) each period: If \\(b>0\\) , then the benefits are increasing If \\(b<0\\) , then the benefits are decreasing \\[ \\text{Geometric Benefit} = (1+b)^{K_x} \\] Note that the power is \\(K_x\\) and NOT \\(K_x+1\\) to reflect that the benefit starts at 1 when \\(K_x=0\\) .","title":"Geometric Contracts"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#geometric-assurance","text":"Let \\(\\text{Geom WL}\\) be the random variable denoting the PV of a Geometric Assurance. \\[ \\text{Geom WL} = (1+b)^{K_x} v^{K_x+1} \\] Thus, the EPV of a WL Geometric Assurance can be calculated as: \\[ \\begin{aligned} \\text{EPV Geom WL} &= \\sum^{\\infty}_{k=0} (1+b)^{k} v^{k+1} {}_{k|}q_x \\\\ &= (1+b)^{-1} \\sum^{\\infty}_{k=0} (1+b)^{k+1} v^{k+1} {}_{k|}q_x \\\\ &= \\frac{1}{1+b} \\sum^{\\infty}_{k=0} \\left(\\frac{1+b}{1+i}\\right)^{k+1} {}_{k|}q_x \\\\ &= \\frac{1}{1+b} \\sum^{\\infty}_{k=0} (v')^{k+1} {}_{k|}q_x \\\\ &= \\frac{1}{1+b} A_x |_{i=\\frac{1+i}{1+b}-1} \\end{aligned} \\] Note that this means that the term \\(A_x |_{i=\\frac{1+i}{1+b}-1}\\) by itself is a Geometric Assurance that starts with payments of \\((1+b)\\) , which is why \\(\\frac{1}{1+b}\\) is required to scale it down by one factor such that it starts at one. In practice, most questions will provide a value of \\(b\\) such that \\((i=\\frac{1+i}{1+b}-1)\\) simplifies to a nice percentage . However, this usually means that the value of \\(b\\) provided is some complicated number, so do not be taken aback! The expression follows similar intuition to the second moment - it is simply a regular EPV expression evaluated at a different interest rate . Unfortunately, there is no specified actuarial notation for this niche case. One common approach is to denote it using \\(A_x^i\\) .","title":"Geometric Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#geometric-annuity","text":"Let \\(\\text{EPV Geom WL}_\\text{Due}\\) be the random variable denoting the PV of a Geometric Annuity Due. Thus, the EPV of a Geometric Annuity Due can be calculated as: \\[ \\begin{aligned} \\text{EPV Geom WL}_\\text{Due} &= \\sum^{\\infty}_{k=0} (1+c)^k v^k {}_{k}p_x \\\\ &= \\sum^{\\infty}_{k=0} \\left(\\frac{1+c}{1+i}\\right)^k {}_{k}p_x \\\\ &= \\ddot{a}_x|_{i = \\frac{1+i}{1+c}-1} \\end{aligned} \\]","title":"Geometric Annuity"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#continuous-geometric","text":"The EPV of continuously payable geometric contracts can be calculated using the usual UDD, Claims Acceleration or Woolhouse Approximations. However, despite having a \"new\" interest rate, the original interest rate is used in the continuous approximation parameters. Warning This is different from the second moment where the new interest rate must be reflected in the approximation as well. Take note of the different treatments.","title":"Continuous Geometric"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#arithmetic-contracts","text":"If the benefits change by a fixed constant each period, then it is instead known as an Arithmetic contract. For simplicity, the change each period is assumed to be 1: Arithmetically Increasing - Increases by 1 each period Arithmetically Decreasing - Decreased by 1 each period Since a WL contract lasts forever, the benefits of a decreasing WL contract would inevitably become negative , which do not make sense. Thus, WLs can only be arithmetically increasing contracts . \\[ \\text{Arithmetically Increasing Benefit} = K_x + 1 \\]","title":"Arithmetic Contracts"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#arithmetic-assurance","text":"Let \\(\\text{Arith WL}\\) be the random variable denoting the PV of an arithmetically increasing WL Assurance: \\[ \\text{Arith WL} = (K_x + 1) v^{K_x+1} \\] Thus, the EPV of a WL Arithmetic Assurance can be shown to be: \\[ \\begin{aligned} \\text{EPV Arith WL} &= \\sum^{\\infty}_{k=0} (k+1) v^{k+1} {}_{k|}q_x \\\\ (IA)_x &= \\sum^{\\infty}_{k=0} (k+1) v^{k+1} {}_{k|}q_x \\end{aligned} \\] \\((IA)_x\\) is the actuarial notation for the EPV of a contract with arithmetically increasing benefits starting at 1 and increasing by 1 each period. It can also be shown to be a sum of deferred WLs , each deferred by one period from the previous. This creates a step-like benefit which creates the increasing effect of the assurance: \\[ \\begin{aligned} (IA)_x &= {}_{0|}A_x + {}_{1|}A_x + {}_{2|}A_x + ... \\\\ &= \\sum^{\\infty}_{K_x = 0} {}_{K_x|}A_x \\end{aligned} \\] In practice, the magnitude of the change is unlikely to be one. Thus, the EPV must be scaled to match the actual change. This can be problematic if the starting benefit is NOT the same as the change . Consider a product with benefit \\(B\\) that increases by \\(k\\) each year. Due to the definition of the IWL variable, the starting benefit and change each period are equal - we cannot simply multiply \\(B\\) or \\(k\\) to the IWL EPV. Thus, we must split this product into a fixed component with \\(B-k\\) benefits and a variable component that starts and increases by \\(k\\) each period. The sum of these two components results in a benefit of \\(B, B+k, B+2k, \\dots\\)","title":"Arithmetic Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#arithmetic-annuities","text":"Let \\(\\text{Arith WL}_\\text{Due}\\) be the random variable denoting the PV of an arithmetically increasing annuity. The PV can be denoted by an arithmetically increasing annuity certain : \\[ \\begin{aligned} \\text{Arith WL}_\\text{Due} &= (I\\ddot{a})_{\\enclose{actuarial}{n}} \\\\ &= \\sum^{n-1}_{k=0} v^k \\cdot \\ddot{a}_{\\enclose{actuarial}{n-k}} \\\\ &= \\sum^{n-1}_{k=0} v^k \\cdot \\frac{1-v^{n-k}}{d} \\\\ &= \\sum^{n-1}_{k=0} \\frac{v^k - v^n}{d} \\\\ &= \\frac{1}{d} \\left(\\sum^{n-1}_{k=0} v^k - \\sum^{n-1}_{k=0} v^n \\right) \\\\ &= \\frac{1}{d} (\\ddot{a}_{\\enclose{actuarial}{n}} - nv^n) \\\\ &= \\frac{\\ddot{a}_{\\enclose{actuarial}{n}} - nv^n}{d} \\end{aligned} \\] Similarly, the EPV of an arithmetically increasing WL annuity Due can be calculated as: \\[ \\begin{aligned} \\text{EPV Arith WL}_\\text{Due} &= \\sum^{\\infty}_{k=0} (I\\ddot{a})_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_x \\\\ (I\\ddot{a})_x &= \\sum^{\\infty}_{k=0} (k+1) v^k {}_{k}p_x \\\\ \\end{aligned} \\] Alternatively, it can be calculated from an assurance instead: \\[ \\begin{aligned} \\text{Arith WL}_\\text{Due} &= \\frac{\\ddot{a}_{\\enclose{actuarial}{k+1}} - (k+1)v^{k+1}}{d} \\\\ E(\\text{Arith WL}_\\text{Due}) &= \\frac{E(\\ddot{a}_{\\enclose{actuarial}{k+1}}) - E((k+1)v^{k+1})}{d} \\\\ (I\\ddot{a})_x &= \\frac{\\ddot{a}_x - (IA)_x}{d} \\end{aligned} \\] Notice that the main difference between an increasing annuity due and immediate is that the immediate case now lags the due by 1 every period . This difference can be accounted for using a level annuity due: \\[ \\begin{aligned} (I\\ddot{a})_x &= \\sum^{\\infty}_{k=0} (k+1) v^k {}_{k}p_{x} \\\\ &= v^{0} {}_{0}p_{x} + 2v^{1} {}_{1}p_{x} + 3v^{2} {}_{2}p_{x} + \\dots \\\\ &= (v^{1} {}_{1}p_{x} + 2v^{2} {}_{2}p_{x} + \\dots) + (v^{0} {}_{0}p_{x} + v^{1} {}_{1}p_{x} + v^{2} {}_{2}p_{x} + \\dots) \\\\ &= (Ia)_x + \\ddot{a}_x \\\\ (Ia)_x &= (I\\ddot{a})_x - \\ddot{a}_x \\end{aligned} \\]","title":"Arithmetic Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#arithmetic-termtemporary","text":"For contract with level benefits , term/temporary contracts can be expressed as a difference of two WLs issued at different times because the benefits for the time after the specified term would cancel out . For contracts with variable benefits, this is slightly more complicated as the benefits after the specified term do NOT cancel out : The \"earlier\" WL would have increased significantly from its starting value The \"later\" WL would only be at its starting value Thus, an additional expression needs to be subtracted in order to remove the remaining benefit past the specified term. This can be done using a level benefit contract with the a benefit equal to the remaining amount . Thus, the EPV of an Increasing Term Assurance can be calculated as the following: \\[ \\begin{aligned} \\text{EPV Arith TA} &= \\sum^{n-1}_{K_x=0} (k+1) v^{K_x+1} {}_{k|}q_x \\\\ (IA)^1_{x:\\enclose{actuarial}{n}} &= (IA)_x - {}_{n}E_x [(IA)_{x+n} + nA_{x+n}] \\end{aligned} \\] Thus, the EPV of an Increasing Term Annuity can be calculated as the following: \\[ \\begin{aligned} \\text{EPV Arith TA}_\\text{Due} &= \\sum^{n-1}_{k=0} (I\\ddot{a})_{\\enclose{actuarial}{n}} \\cdot {}_{k|}q_x \\\\ (I\\ddot{a})_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{k=0} (k+1) v^k {}_{k}p_x \\\\ &= (I\\ddot{a})_x - {}_{n}E_x[(I\\ddot{a})_{x+n}+n\\ddot{a}_{x+n}] \\end{aligned} \\] Due to Immediate Conversion As mentioned in the Life Annuity section, it is not useful to memorize the conversion from Due to Immediate for TAs. Thus, simply express the ITA in the form of IWLs and then perform the conversion there.","title":"Arithmetic Term/Temporary"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#decreasing-benefits","text":"As mentioned previously, another key feature of term/temporary contracts is that they can have decreasing benefits . This is because the benefit of the policy can be set such that it would exactly decrease to 0 by the end of the policy term. This is done by setting a starting benefit of \\(n\\) and decreasing by \\(1\\) each period : \\[ \\text{Arithmetically Decreasing Benefit} = n-k \\] Thus, the EPV of a Decreasing Term Assurance can be calculated as the following: \\[ \\begin{aligned} \\text{EPV Arith Decreasing TA} &= \\sum^{\\infty}_{k=0} (n-k) v^{k+1} {}_{k|}q_x \\\\ (D_{\\enclose{actuarial}{n}}A)_{x:\\enclose{actuarial}{n}} &= \\sum^{\\infty}_{k=0} (n-k+1-1) v^{k+1} {}_{k|}q_x \\\\ &= \\sum^{\\infty}_{k=0} (n+1) v^{k+1} {}_{k|}q_x - \\sum^{\\infty}_{k=0} (k+1) v^{k+1} {}_{k|}q_x \\\\ &= (n+1)A^1_{x:\\enclose{actuarial}{n}} - (IA)^1_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] \\((DA)\\) is the actuarial notation for an Assurance with Arithmetically decreasing benefits starting at \\(n\\) and decreasing by 1 each period. Similarly, the EPV of a decreasing temporary annuity can be calculated as the following: \\[ (D_{\\enclose{actuarial}{n}}\\ddot{a})_{x:\\enclose{actuarial}{n}} = (n+1)\\ddot{a}_{x:\\enclose{actuarial}{n}} - (I\\ddot{a})_{x:\\enclose{actuarial}{n}} \\]","title":"Decreasing Benefits"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#continuous-arithmetic","text":"Similar to Geometric Contracts, the EPV of a continuously payable Arithmetic Contracts can be calculated using the usual approximations. However, take note that for TAs, the added term should also be continuous : \\[ \\begin{aligned} (I\\bar{A})_{x:\\enclose{actuarial}{n}} &= (I\\bar{A})_x - {}_{n}E_x[(I\\bar{A})_{x+n}+ n\\bar{A}_{x+n}] \\\\ (I\\bar{a})_{x:\\enclose{actuarial}{n}} &= (I\\bar{a})_x - {}_{n}E_x[(I\\bar{a})_{x+n}+ n\\bar{a}_{x+n}] \\end{aligned} \\] Increasing Woolhouse Approximation For IWL Annuities, the woolhouse approximation is slightly different: \\[ (I\\bar{a})_{x} = (I\\ddot{a})_{x} - \\frac{1}{2} \\ddot{a}_x \\] Continuous arithmetic contracts could also refer to a contract that not only payable continuously, but changes continuously as well. The benefits change at a constant rate throughout the period , such that the total change is still equal to 1. For Assurances , it can be understood from a graphical approach: \\[ \\begin{aligned} (\\bar{I}\\bar{A})_{x} &= (I\\bar{A})_{x} - \\frac{1}{2} \\bar{A}_x \\end{aligned} \\] For Annuities, it can be derived using the woolhouse approximation : \\[ \\begin{aligned} (\\bar{I}\\bar{a})_x &= (Ia)_x + \\frac{1}{12} \\\\ &= (I\\ddot{a})_x - \\ddot{a}_x + \\frac{1}{12} \\end{aligned} \\] Note that these approximations are ONLY applicable for WL policies . For continously increasing TAs, convert them to WLs first before applying the approximations.","title":"Continuous Arithmetic"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#variable-recursions","text":"Since the benefit payable at the end of the first year is still 1 , the recursion for variable contracts are similar to the level ones.","title":"Variable Recursions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#variable-assurances","text":"Consider the two scenarios: If the policyholder dies in the current year with probability \\(q_x\\) , then a benefit of 1 is paid at the end of the year If the policyholder lives past the current year with probability \\(p_x\\) , then the they will receive the EPV of a policy with benefits \\(2, 3, \\dots\\) or \\((1+b)^1, (1+b)^2, \\dots\\) at some future time The second component must be the EPV of an assurance that STARTS with an increased benefit of \\((1+b)\\) or \\(2\\) . The issue is that the random variable is currently defined as a contract with benefits starting at 1 , which is why the recursive formula is different from the level case . For Geometric Assurances, \\(A_{x+1}|_{i=\\frac{1+i}{1+b}-1}\\) is already an Geometric Assurance that starts at \\((1+b)\\) , thus no adjustment is needed for the RHS: \\[ \\text{EPV Geom WL}_{x} = vq_x + vp_x A_{x+1}|_{i=\\frac{1+i}{1+b}-1} \\] For Arithmetic Assurances, an EPV representing an additional benefit of 1 each period must be added to the second component so that the RHS expression starts from 2 and increases by an additional 1 each period. \\[ (IA)_{x} = vq_x + vp_x[(IA)_{x+1} + A_{x+1}] \\]","title":"Variable Assurances"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#variable-annuities","text":"Recall that a benefit of 1 is already received at the beginning of the year regardless of whether the policyholder lives or dies. If the policyholder dies with probability \\(q_x\\) , then they receive nothing extra If the policyholder lives past the current year with probability \\(p_x\\) , then they will receive benefits that are larger than 1 at some future time For Geometric Annuities, TBC. \\[ \\begin{aligned} \\text{EPV Geom WL}_{\\text{Due},x} = 1 + vp_x \\cdot (1+c) \\left(\\ddot{a}_{x+1}|_{i = \\frac{1+i}{1+c}-1} \\right) \\end{aligned} \\] For Arithmetic Annuities, the same adjustment as before must be made: \\[ (I\\ddot{a})_x = 1 + vp_x[(I\\ddot{a})_{x+1} + \\ddot{a}_{x+1}] \\]","title":"Variable Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#in-force-recursions","text":"Previously, we have only considered the EPV for a newly issued policy at age \\(x\\) . However, now we want to consider the EPV of a policy that has already been in force for some period of time \\(t\\) . For policies with level benefits , the EPV of an already in force policy is exactly the same as the EPV of a newly issued policy at the current age with the same expiration date . This means that the same recursion can be used for both newly issued policies and inforce policies. However, this is NOT the case for policies with variable benefits as the inforce policy would have a benefit of \\(t\\) or \\((1+b)^{t-1}\\) while a newly issued policy would have benefit of 1. This means that a seperate recursion must be defined for inforce variable policies. For simplicity, we will consider an in force arithmetically increasing WL Assurance : If the policyholder dies with probability \\(q_{x+t-1}\\) , they will receive a benefit of \\(t\\) at the end of the year If the poliycholder survives with probability \\(p_{x+t-1}\\) , they will receive the EPV of a contract with benefits \\(t+1, t+2, \\dots\\) at some future time \\[ RHS = vq_{x+t-1} \\cdot t + vp_{x+t-1} [(IA)_{x+t} + tA_{x+t}] \\] Since (IA) is used to denote a policy with benefits starting at 1, an additional term must be added to the LHS to show that the in force policy has benefits of t: \\[ LHS = (IA)_{x+t-1} + (t-1) A_{x+t-1} \\] The same exercise can be performed for Annuities as well, resulting in the following: \\[ \\begin{aligned} (IA)_{x+t-1} + (t-1) A_{x+t-1} &= vq_{x+t-1} \\cdot t + vp_{x+t-1} [(IA)_{x+t} + tA_{x+t}] \\\\ (I\\ddot{a})_{x+t-1} + (t-1) \\ddot{a}_{x+t-1} &= t + vp_{x+t-1} [(I\\ddot{a})_{x+t} + t\\ddot{a}_{x+t}] \\end{aligned} \\] Term/Temporary Policies The same can be shown for Variable Term/Temporary policies, but remember that the remaining duration of the policy must decrease as well.","title":"In Force Recursions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#variance-and-second-moment","text":"Consider the second moment of both types of variable assurances: \\[ \\begin{aligned} E\\left[(\\text{Arith WL})^2\\right] &= \\sum^{\\infty}_{K_x=0} (k+1)^2 (v^2)^{K_x+1} {}_{k|}q_x \\\\ \\\\ E\\left[(\\text{Geom WL})^2\\right] &= \\sum^{\\infty}_{K_x=0} (1+b)^{2K_x} (v^2)^{K_x+1} {}_{k|}q_x \\end{aligned} \\] Notice that the usual approximation of \\(v=v^2\\) is insufficient to solve for the second moment due to the squared benefit. Thus, there are no \\({}^{2}A\\) defined for variable assurances.","title":"Variance and Second Moment"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Premiums/","text":"Premiums \u00b6 Overview \u00b6 Unlike other products, the cost of a Life Assurance or Annuity are not known when it is issued, as it is impossible to precisely predict when an individual will die or how long they will live for. Premiums are the amount that the insurance company charges for a Life Assurance or Annuity. They aim to charge a sufficiently high premium such that they expect to at least break even on the sale of the policy on an EPV basis . Premiums are always paid in advance , but the frequency of payment depends on the type of premium: Single Premium Limited Premium Regular Premium Single Lump Sum Recurring for a fixed period Recurring as long as the contract is valid Use dollar amount Modelled using TA Annuities Modelled using WL Annuities If premiums are paid in arrears instead, the policyholder may refuse to pay premiums on the flawed grounds that since they did not die in the period, they did not utilize the coverage and hence should not pay. There are two types of premiums that can be calculated: Net Premiums, \\(P\\) - Excluding expenses; based on benefits only Gross Premiums, \\(G\\) - Including expenses; based on BOTH benefits, expenses and other cashflows The key thing to notice is that the components of net premiums are fixed while gross premiums differ from insurer to insurer. Thus, this section will only show formulas for net premiums . Equivalence Principle Approach \u00b6 To determine the breakeven point of the insurer, the expected amount of loss must first be defined. Thus, let \\(L_x\\) be the random variable denoting the loss of the policy: If \\(L_x\\) is positive, then the policy is losing money If \\(L_x\\) is negative, then the policy is making money The intepretation is reversed since \\(L\\) represents an outflow rather than an inflow! \\[ \\begin{aligned} L_x &= \\text{PV Outflow} - \\text{PV Inflows} \\end{aligned} \\] The goal is to solve for the premiums such that there is no expected loss . This is known as the Equivalence Principle approach as it sets the EPVs to be equal: \\[ \\begin{aligned} E(L_x) &= 0 \\\\ \\text{EPV Outflow} &= \\text{EPV Inflow} \\end{aligned} \\] The loss variable for Net and Gross can be differentiated using superscripts: Net Premium : \\(L^{B}_{x}\\) , where \\(B\\) represents the **B**enefits only Gross Premium : \\(L^{B+E}_{x}\\) , where \\(B+E\\) represents **B**enefits and **E**xpenses For Net Premiums only, \\[ \\begin{aligned} E \\left(L^{B}_{x} \\right) &= 0 \\\\ \\text{EPV Benefits} &= \\text{EPV Premiums} \\\\ B \\cdot A_x &= P \\cdot \\ddot{a}_x \\\\ P &= B \\cdot \\frac{A_x}{\\ddot{a}_x} \\end{aligned} \\] Note that several implicit assumptions were made: Premiums are payable annually Benefits are payable discretely If premiums or benefits were payable continously, then the formula would use the corresponding continuous assurance or annuity. However, note that even when calculating this way \\(P\\) will still represent the Annual Net Premium , thus we must still divide to obtain the appropriate monthly or quarterly premium . Portfolio Percentile Approach \u00b6 If there are a large number of policies within the insurer's portfolio, then the insurer can consider the breakeven on a portfolio of policies rather than a single policy. Let \\(Y\\) be the random variable denoting the aggregate loss on a portfolio of \\(n\\) iid policies: \\[ \\begin{aligned} Y & \\sim iid \\\\ E(Y) &= n \\cdot E(L) \\\\ Var(Y) &= n \\cdot Var(L) \\\\ \\end{aligned} \\] If the insurer has a large portfolio of policies ( \\(n\\) is sufficiently large), then through Central Limit Theorem , \\[ Y \\sim N(E(Y), Var(Y)) \\] The premium is then set such the probability of profit is set a pre-determined level close to 1: \\[ \\begin{aligned} P(Y < 0) &= \\alpha \\\\ P\\left(\\frac{Y - E(Y)}{\\sqrt{Var (Y)}} \\le \\frac{0 - E(Y)}{\\sqrt{Var (Y)}}\\right) &= \\alpha \\\\ P\\left(Z \\le \\frac{- E(Y)}{\\sqrt{Var (Y)}}\\right) &= \\alpha \\\\ P\\left(Z \\le \\frac{- n \\cdot E(L)}{n \\cdot \\sqrt{Var (L)}}\\right) &= \\alpha \\\\ P\\left(Z \\le \\frac{- E(L)}{\\sqrt{Var (L)}}\\right) &= \\alpha \\end{aligned} \\] Thus, only the expectation and variance of an individual policy is required. For net premiums only, \\[ \\begin{aligned} L_x &= B \\cdot \\text{WL} - P \\cdot \\text{WL}_{Due} \\\\ &= B \\cdot \\text{WL} - P \\cdot \\frac{1- \\text{WL}}{d} \\\\ &= B \\cdot \\text{WL} - P \\left(\\frac{1}{d}-\\frac{\\text{WL}}{d} \\right) \\\\ &= B \\cdot \\text{WL} - P \\left[\\left(\\frac{1}{d}\\right) + P\\left(\\frac{\\text{WL}}{d}\\right) \\right] \\\\ &= \\text{WL} \\left(B + \\frac{P}{d} \\right) - \\frac{P}{d} \\\\ \\\\ \\therefore E(L_x) &= E \\left[\\text{WL} \\left(B + \\frac{P}{d} \\right) - \\frac{P}{d} \\right] \\\\ &= A_x \\left(B + \\frac{P}{d} \\right) - \\frac{P}{d} \\\\ \\\\ \\therefore Var(L_x) &= Var\\left[\\text{WL} \\left(B + \\frac{P}{d} \\right) - \\frac{P}{d} \\right] \\\\ &= Var \\left[\\text{WL} \\left(B + \\frac{P}{d} \\right) \\right] \\\\ &= \\left(B + \\frac{P}{d} \\right)^2 \\cdot Var(\\text{WL}) \\\\ &= \\left(B + \\frac{P}{d} \\right)^2 \\cdot \\left[{}^{2}A_x - (A_x)^2 \\right] \\end{aligned} \\] The premiums determined this way will always be larger than the premiums determined through the equivalence principle: \\[ \\begin{aligned} P_{\\text{Portfolio Percentile}} &\\ge P_{\\text{Equivalence Principle}} \\\\ \\\\ \\text{As n} &\\to \\infty \\\\ P_{\\text{Portfolio Percentile}} &\\to P_{\\text{Equivalence Principle}} \\end{aligned} \\] Gross Premiums \u00b6 What makes gross premiums so much trickier than net premiums is that there are many different types of expenses , where every question can ask for a unique combination of them. Expenses can be differentiated in terms of when they are charged: Acquisition Expenses Renewal Expenses Termination Expense At policy inception During policy lifetime At policy termination Single payment at time 0 Recurring payments from time 1 onwards Single payment at unknown time Denoted by \\(e_0\\) Denoted by \\(e_t\\) Denoted by \\(E\\) Directly use amount Corresponding Annuity Functions Corresponding Assurance Function EG. Commissions expenses EG. Administrative Expenses EG. Claims expenses Renewal Expenses can also inflate over time, where they increase by a factpr of \\((1+r)\\) each year. In this case, they are modelled using a geometrically increasing annuity instead. IMPORTANT : Acquisition expenses are usually higher than renewal ones as agent commissions are mostly front-loaded in the first year to give agents cash faster, incentivizing them to sell more. They can also be differentiated in terms of how much they charge: Overhead Expenses Direct Expenses Shared among all policies Borne by a specific policy Fixed amount Percentage of policy premium or benefit Spread out among all policies Borne only by that policy EG. Office Rental EG. Sales Commission or Underwriting Expenses Note that gross premiums can also include other cashflows, such as a pre-determined profit margin . Since these are rare and highly dependent on the situation, they will not be covered in this section. Expense Calculations \u00b6 Consider a policy that only has acquisition and renewal expenses , denoted by A and R respectively: The PV of the expenses can be easily calculated by using an annuity immediate: \\[ \\text{EPV Expenses} = A + R \\cdot a_x \\] While this is not challenging per se, it is slightly inconvenient as there is a need to use an annuity immediate while premiums use annuity dues - this would make it harder to simplify or more computationally intensive. This can be worked around by splitting the acquisition expense , such that one part of it forms the first payment for the renewal expenses, allowing an annuity due to be used instead: \\[ \\begin{aligned} \\text{EPV Expenses} &= A + R \\cdot a_x \\\\ &= A - R + R + R \\cdot a_x \\\\ &= A - R + R \\cdot \\ddot{a}_x \\\\ &= A + R \\cdot (\\ddot{a}_x - 1) \\end{aligned} \\] However, note that the simplification does NOT hold true if expense inflation is taken into account - the level annuity immediate CANNOT simply be replaced with the geometric annuity. This is because the expenses only start to inflate from the third payment onwards: Expense Premiums \u00b6 The difference between gross and net premiums represent the amount of premiums needed to cover just the expenses, known as the Expense Premiums , \\(E\\) : \\[ \\begin{aligned} G &= P + E \\\\ E &= G - P \\end{aligned} \\] It is also known as the Expense Loading as it represents the amount that is added ( loaded ) on to the net premiums to cover expenses.","title":"Premiums"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Premiums/#premiums","text":"","title":"Premiums"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Premiums/#overview","text":"Unlike other products, the cost of a Life Assurance or Annuity are not known when it is issued, as it is impossible to precisely predict when an individual will die or how long they will live for. Premiums are the amount that the insurance company charges for a Life Assurance or Annuity. They aim to charge a sufficiently high premium such that they expect to at least break even on the sale of the policy on an EPV basis . Premiums are always paid in advance , but the frequency of payment depends on the type of premium: Single Premium Limited Premium Regular Premium Single Lump Sum Recurring for a fixed period Recurring as long as the contract is valid Use dollar amount Modelled using TA Annuities Modelled using WL Annuities If premiums are paid in arrears instead, the policyholder may refuse to pay premiums on the flawed grounds that since they did not die in the period, they did not utilize the coverage and hence should not pay. There are two types of premiums that can be calculated: Net Premiums, \\(P\\) - Excluding expenses; based on benefits only Gross Premiums, \\(G\\) - Including expenses; based on BOTH benefits, expenses and other cashflows The key thing to notice is that the components of net premiums are fixed while gross premiums differ from insurer to insurer. Thus, this section will only show formulas for net premiums .","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Premiums/#equivalence-principle-approach","text":"To determine the breakeven point of the insurer, the expected amount of loss must first be defined. Thus, let \\(L_x\\) be the random variable denoting the loss of the policy: If \\(L_x\\) is positive, then the policy is losing money If \\(L_x\\) is negative, then the policy is making money The intepretation is reversed since \\(L\\) represents an outflow rather than an inflow! \\[ \\begin{aligned} L_x &= \\text{PV Outflow} - \\text{PV Inflows} \\end{aligned} \\] The goal is to solve for the premiums such that there is no expected loss . This is known as the Equivalence Principle approach as it sets the EPVs to be equal: \\[ \\begin{aligned} E(L_x) &= 0 \\\\ \\text{EPV Outflow} &= \\text{EPV Inflow} \\end{aligned} \\] The loss variable for Net and Gross can be differentiated using superscripts: Net Premium : \\(L^{B}_{x}\\) , where \\(B\\) represents the **B**enefits only Gross Premium : \\(L^{B+E}_{x}\\) , where \\(B+E\\) represents **B**enefits and **E**xpenses For Net Premiums only, \\[ \\begin{aligned} E \\left(L^{B}_{x} \\right) &= 0 \\\\ \\text{EPV Benefits} &= \\text{EPV Premiums} \\\\ B \\cdot A_x &= P \\cdot \\ddot{a}_x \\\\ P &= B \\cdot \\frac{A_x}{\\ddot{a}_x} \\end{aligned} \\] Note that several implicit assumptions were made: Premiums are payable annually Benefits are payable discretely If premiums or benefits were payable continously, then the formula would use the corresponding continuous assurance or annuity. However, note that even when calculating this way \\(P\\) will still represent the Annual Net Premium , thus we must still divide to obtain the appropriate monthly or quarterly premium .","title":"Equivalence Principle Approach"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Premiums/#portfolio-percentile-approach","text":"If there are a large number of policies within the insurer's portfolio, then the insurer can consider the breakeven on a portfolio of policies rather than a single policy. Let \\(Y\\) be the random variable denoting the aggregate loss on a portfolio of \\(n\\) iid policies: \\[ \\begin{aligned} Y & \\sim iid \\\\ E(Y) &= n \\cdot E(L) \\\\ Var(Y) &= n \\cdot Var(L) \\\\ \\end{aligned} \\] If the insurer has a large portfolio of policies ( \\(n\\) is sufficiently large), then through Central Limit Theorem , \\[ Y \\sim N(E(Y), Var(Y)) \\] The premium is then set such the probability of profit is set a pre-determined level close to 1: \\[ \\begin{aligned} P(Y < 0) &= \\alpha \\\\ P\\left(\\frac{Y - E(Y)}{\\sqrt{Var (Y)}} \\le \\frac{0 - E(Y)}{\\sqrt{Var (Y)}}\\right) &= \\alpha \\\\ P\\left(Z \\le \\frac{- E(Y)}{\\sqrt{Var (Y)}}\\right) &= \\alpha \\\\ P\\left(Z \\le \\frac{- n \\cdot E(L)}{n \\cdot \\sqrt{Var (L)}}\\right) &= \\alpha \\\\ P\\left(Z \\le \\frac{- E(L)}{\\sqrt{Var (L)}}\\right) &= \\alpha \\end{aligned} \\] Thus, only the expectation and variance of an individual policy is required. For net premiums only, \\[ \\begin{aligned} L_x &= B \\cdot \\text{WL} - P \\cdot \\text{WL}_{Due} \\\\ &= B \\cdot \\text{WL} - P \\cdot \\frac{1- \\text{WL}}{d} \\\\ &= B \\cdot \\text{WL} - P \\left(\\frac{1}{d}-\\frac{\\text{WL}}{d} \\right) \\\\ &= B \\cdot \\text{WL} - P \\left[\\left(\\frac{1}{d}\\right) + P\\left(\\frac{\\text{WL}}{d}\\right) \\right] \\\\ &= \\text{WL} \\left(B + \\frac{P}{d} \\right) - \\frac{P}{d} \\\\ \\\\ \\therefore E(L_x) &= E \\left[\\text{WL} \\left(B + \\frac{P}{d} \\right) - \\frac{P}{d} \\right] \\\\ &= A_x \\left(B + \\frac{P}{d} \\right) - \\frac{P}{d} \\\\ \\\\ \\therefore Var(L_x) &= Var\\left[\\text{WL} \\left(B + \\frac{P}{d} \\right) - \\frac{P}{d} \\right] \\\\ &= Var \\left[\\text{WL} \\left(B + \\frac{P}{d} \\right) \\right] \\\\ &= \\left(B + \\frac{P}{d} \\right)^2 \\cdot Var(\\text{WL}) \\\\ &= \\left(B + \\frac{P}{d} \\right)^2 \\cdot \\left[{}^{2}A_x - (A_x)^2 \\right] \\end{aligned} \\] The premiums determined this way will always be larger than the premiums determined through the equivalence principle: \\[ \\begin{aligned} P_{\\text{Portfolio Percentile}} &\\ge P_{\\text{Equivalence Principle}} \\\\ \\\\ \\text{As n} &\\to \\infty \\\\ P_{\\text{Portfolio Percentile}} &\\to P_{\\text{Equivalence Principle}} \\end{aligned} \\]","title":"Portfolio Percentile Approach"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Premiums/#gross-premiums","text":"What makes gross premiums so much trickier than net premiums is that there are many different types of expenses , where every question can ask for a unique combination of them. Expenses can be differentiated in terms of when they are charged: Acquisition Expenses Renewal Expenses Termination Expense At policy inception During policy lifetime At policy termination Single payment at time 0 Recurring payments from time 1 onwards Single payment at unknown time Denoted by \\(e_0\\) Denoted by \\(e_t\\) Denoted by \\(E\\) Directly use amount Corresponding Annuity Functions Corresponding Assurance Function EG. Commissions expenses EG. Administrative Expenses EG. Claims expenses Renewal Expenses can also inflate over time, where they increase by a factpr of \\((1+r)\\) each year. In this case, they are modelled using a geometrically increasing annuity instead. IMPORTANT : Acquisition expenses are usually higher than renewal ones as agent commissions are mostly front-loaded in the first year to give agents cash faster, incentivizing them to sell more. They can also be differentiated in terms of how much they charge: Overhead Expenses Direct Expenses Shared among all policies Borne by a specific policy Fixed amount Percentage of policy premium or benefit Spread out among all policies Borne only by that policy EG. Office Rental EG. Sales Commission or Underwriting Expenses Note that gross premiums can also include other cashflows, such as a pre-determined profit margin . Since these are rare and highly dependent on the situation, they will not be covered in this section.","title":"Gross Premiums"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Premiums/#expense-calculations","text":"Consider a policy that only has acquisition and renewal expenses , denoted by A and R respectively: The PV of the expenses can be easily calculated by using an annuity immediate: \\[ \\text{EPV Expenses} = A + R \\cdot a_x \\] While this is not challenging per se, it is slightly inconvenient as there is a need to use an annuity immediate while premiums use annuity dues - this would make it harder to simplify or more computationally intensive. This can be worked around by splitting the acquisition expense , such that one part of it forms the first payment for the renewal expenses, allowing an annuity due to be used instead: \\[ \\begin{aligned} \\text{EPV Expenses} &= A + R \\cdot a_x \\\\ &= A - R + R + R \\cdot a_x \\\\ &= A - R + R \\cdot \\ddot{a}_x \\\\ &= A + R \\cdot (\\ddot{a}_x - 1) \\end{aligned} \\] However, note that the simplification does NOT hold true if expense inflation is taken into account - the level annuity immediate CANNOT simply be replaced with the geometric annuity. This is because the expenses only start to inflate from the third payment onwards:","title":"Expense Calculations"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Premiums/#expense-premiums","text":"The difference between gross and net premiums represent the amount of premiums needed to cover just the expenses, known as the Expense Premiums , \\(E\\) : \\[ \\begin{aligned} G &= P + E \\\\ E &= G - P \\end{aligned} \\] It is also known as the Expense Loading as it represents the amount that is added ( loaded ) on to the net premiums to cover expenses.","title":"Expense Premiums"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/","text":"Reserves \u00b6 Overview \u00b6 Most life insurance contracts charge level premiums - charging the same amount throughout the lifetime of the policy. When the policyholder is young, the probability of death is small, thus the premiums collected are larger than the expected outflow in the year, leading to a surplus . When the policyholder is older, the probability of death is high, thus the premiums collected are smaller than the expected outflow in the year, leading to a deficit . Given the inevitable deficit, the surplus in the earlier years cannot be recognized as a profit and are instead pooled together and safely invested into an account, from which the deficit in later years will draw from. It is a common misconception to think that each policy has its own account. Following this logic, the insurer would not have enough funds to pay an insured who dies shortly after death. In order to determine the adequacy of the account, insurers will calculate the size of the expected loss of ALL policies and compare it against the (hopefully larger) account value. The expected loss is known as the Reserve of the policy because it represents an amount inside the pooled account that is reserved for paying the benefits of the policy. Prospective Approach \u00b6 There are two approaches to calculate the reserves: Prospective Approach - Based on Present Values of future cashflows Retrospective Approach - Based on Future Values of past cashflows (Not covered) Similar to premiums, there are also two types of reserves, depending on what cashflows are being considered: Net Premium Reserves - Net Premiums and Benefits Gross Premium Reserves - Gross Premiums, Benefits and Expenses While this looks similar to premium calculation, note that the Gross Reserves do NOT include any other cashflows (EG. Profit Margin). This is intuitive, as reserves should only be to meet essential cashflows . Reserves are calculated at the beginning of the policy year : AFTER any year end cashflows from the previous period (Claims & Claim Expenses) BEFORE any beginning of year cashflows from the current period (Premiums & Renewal Expenses) This means that the reserves are the additional amount needed to cover claims from the current year onwards, including the current year. Beginning VS End Recall that the beginning of the policy year is also the end of the previous policy year - avoid confusion by drawing out the timelines. Questions can use either phrasing, so be sure to read very carefully. Let \\({}_{t-1}V\\) represent the reserve at the beginning of policy year \\(t\\) : \\[ \\begin{aligned} {}_{t-1}V &= E(L_{x+(t-1)}) \\\\ &= \\text{PV Benefits}_{x+(t-1)} - \\text{PV Premiums}_{x+(t-1)} \\end{aligned} \\] Note that reserves only consider FUTURE cashflows. For single premium or deferred products, there is no need to consider the premiums after the payment period is over. Similar to premiums, we will only define formulas for the Net Premium Reserves . They can be simplified into an expression involving only the following, which allows it to be easily calculated in situations where limited information is provided: Annuities only Assurances only Annuities and Premiums Assurance and Premiums Case 1: Annuities Only \u00b6 \\[ \\begin{aligned} {}_{t-1}V &= A_{x+(t-1)} - P \\ddot{a}_{x+(t-1)} \\\\ &= A_{x+(t-1)} - \\frac{A_x}{\\ddot{a}_x} \\cdot \\ddot{a}_{x+(t-1)} \\\\ &= (1-\\delta\\ddot{a}_{x+(t-1)}) - \\frac{1-\\delta\\ddot{a}_{x}}{\\ddot{a}_{x}} \\cdot \\ddot{a}_{x+(t-1)} \\\\ &= 1-\\delta\\ddot{a}_{x+(t-1)} - \\left (\\frac{1}{\\ddot{a}_x}-\\delta \\right) \\cdot \\ddot{a}_{x+(t-1)} \\\\ &= 1-\\delta\\ddot{a}_{x+(t-1)} - \\frac{\\ddot{a}_{x+(t-1)}}{\\ddot{a}_x} + \\delta\\ddot{a}_{x+(t-1)} \\\\ &= 1 - \\frac{\\ddot{a}_{x+(t-1)}}{\\ddot{a}_x} \\end{aligned} \\] Case 2: Assurances Only \u00b6 \\[ \\begin{aligned} {}_{t-1}V &= A_{x+(t-1)} - P \\ddot{a}_{x+(t-1)} \\\\ &= A_{x+(t-1)} - \\frac{A_x}{\\ddot{a}_x} \\cdot \\ddot{a}_{x+(t-1)} \\\\ &= A_{x+(t-1)} - \\frac{A_x}{\\frac{1-A_x}{d}} \\cdot \\frac{1-A_{x+(t-1)}}{d} \\\\ &= A_{x+(t-1)} - \\frac{A_x}{1-A_x} \\cdot (1-A_{x+(t-1)}) \\\\ &= \\frac{A_{x+(t-1)} \\cdot (1-A_{x}) - A_x(1-A_{x+(t-1)})}{1-A_x} \\\\ &= \\frac{A_{x+(t-1)} - A_{x}A_{x+(t-1)} - A_x + A_x A_{x+(t-1)}}{1-A_x} \\\\ &= \\frac{A_{x+(t-1)} - A_{x}}{1 - A_{x}} \\end{aligned} \\] Case 3: Annuities & Premiums \u00b6 \\[ \\begin{aligned} {}_{t}V &= A_{x+(t-1)} - P_x \\cdot \\ddot{a}_{x+(t-1)} \\\\ &= \\ddot{a}_{x+(t-1)} \\cdot \\left(\\frac{A_{x+(t-1)}}{\\ddot{a}_{x+(t-1)}} - P_{x} \\right) \\\\ &= \\ddot{a}_{x+(t-1)} \\cdot \\left(P_{x+(t-1)} - P_{x} \\right) \\end{aligned} \\] Case 4: Assurances & Premiums \u00b6 \\[ \\begin{aligned} {}_{t}V &= A_{x+(t-1)} - P_x \\cdot \\ddot{a}_{x+(t-1)} \\\\ &= A_{x+(t-1)} \\left(1 - P_x \\cdot \\frac{\\ddot{a}_{x+(t-1)}}{A_{x+(t-1)}} \\right) \\\\ &= A_{x+(t-1)} \\left(1 - P_x \\cdot \\frac{1}{\\frac{A_{x+(t-1)}}{\\ddot{a}_{x+(t-1)}}} \\right) \\\\ &= A_{x+(t-1)} \\left(1 - \\frac{P_x}{P_{x+(t-1)}} \\right) \\end{aligned} \\] Reserves & Premiums \u00b6 The calculation of reserves and premiums both involve the expectation of the random variable \\(L\\) : Premiums Reserves \\(E(L)\\) given as 0 \\(E(L)\\) is the target \\(P\\) is the target \\(P\\) is given Calculated at age \\(x\\) Calculated at age \\(x+t\\) If net premium reserves are calculated at the start of the policy ( \\(t=0\\) ), then following the equivalence principle, the reserves will be 0 . This is intuitive, as the premiums are set such that they expect to exactly cover the losses of the policy. However, this is not the case for gross premiums. This is because gross premiums include provisions for other cashflows like profit margins which are not considered during the reserve calculation. Thus, gross premiums collect more than enough to cover the expected losses, resulting in a negative reserve at time 0 . Technically speaking, not all gross premiums include profit margins. If they do not, then the gross premium reserve will also be 0. \\[ \\begin{aligned} {}_{0}V^{Net} &= 0 \\\\ {}_{0}V^{Gross} &\\le 0 \\end{aligned} \\] Note that a newly issued policy for another individual aged \\(x+t\\) uses the exact same \\(L\\) as the reserve, \\(L_{x+t}\\) . This is why some of the expressions previously involve \\(P_{x+t}\\) , which is the premium of this newly issued policy. Red Herrings \u00b6 Premium and Reserve calculations are similar, so it is easy to get them mixed up with each other. The confusing part is that some questions requires us to compute the premiums ourselves before calculating the reserves. Given how similar the calculations are, this may mistakenly cause us to use the same cashflows for both, when the reserves should be different. Even if the gross premiums are provided, some information exclusively used to compute gross premiums may still be provided as a red herring to intentionally confuse us: Acquisition Expenses - Reserves are usually calculated after inception; no need for it Profit Margin - Reserves DO NOT consider profit margin Premium Frequency - Reserves use annual premiums; monthly premiums may be required or provided Note that the reverse is also possible, where gross premiums are provided but the question actually requires us to calculate net premiums, which can cause us to mistakenly use the gross premium instead. Recursions \u00b6 Following the same logic as assurances and annuities, net premium reserves can also be recursively expressed as a function of itself: If the policyholder dies, the insurer would have to pay a benefit of 1 at the end of the year If the policyholder survives, the insurer would have to keep a reserve for future payments Premiums are received REGARDLESS of death or survival as it was paid at the start of the year \\[ \\begin{aligned} {}_{t}V^{\\text{Net}} &= - P + \\begin{cases} v, & q_{x+t} \\\\ v \\cdot {}_{t+1}V, & p_{x+t} \\end{cases} \\\\ \\\\ \\therefore {}_{t}V^{\\text{Net}} &= - P + q_{x+t} \\cdot v + p_{x+t} \\cdot v \\cdot {}_{t+1}V \\end{aligned} \\] Gross premium reserves are similar as any expenses are always paid at the beginning of the year with premiums: \\[ {}_{t}V^{\\text{Gross}} = (E-G) + q_{x+t} \\cdot v + p_{x+t} \\cdot v \\cdot {}_{t+1}V \\] Modified Net Premium Reserves \u00b6 The method and basis for calculating the reserve is based on the prevailing regulation at that time and region. In the US, reserves are calculated using net premiums due to its simplicity at the cost of some accuracy loss. However, net premium reserves tend to be higher than gross premium reserves . This is due to the negative expense reserves, as the excess expense premiums help to offset the future benefits as well. Since net premiums reserves do not recognize expenses, they do not have this benefit! \\[ \\begin{aligned} {}_{t}v^{\\text{Net}} &= {}_{t}v^{\\text{Gross}} + \\underbrace{{}_{t}v^{\\text{Expense}}}_{\\text{-ve}} \\\\ {}_{t}v^{\\text{Net}} &\\gt {}_{t}v^{\\text{Gross}} \\end{aligned} \\] This means that insurers are holding too much reserves under this valuation basis. This is problematic because reserves can only be invested in low risk assets, which typically have lower returns relative to what the capital would have usually be invested in. This incurs an opportunity cost in investment income . To account for this, US regulators allow for insurers to use a modified net premium reserve instead that implicitly accounts for expenses while retaining the simplicity of a net premium approach. Expense Reserves \u00b6 Following expense premiums, the difference between the gross and net premium reserves represent the additional amount needed to cover the expenses, known as the Expense Reserves . \\[ \\begin{aligned} {}_{t}V^{\\text{Expense}} &= {}_{t}V^{\\text{Gross}} - {}_{t}V^{\\text{Net}} \\\\ &= \\text{EPV(Benefits)} + \\text{EPV(Expenses)} - \\text{EPV(Gross Premium)} - \\text{EPV(Benfits)} + \\text{EPV(Net Premiums)} \\\\ &= \\text{EPV(Expenses)} - (\\text{EPV(Gross Premium)} - \\text{EPV(Net Premiums)}) \\\\ &= \\text{EPV(Expenses)} - \\text{EPV(Expense Premium)} \\end{aligned} \\] The expense reserve is 0 at time 0 . However, the expense reserve is typically negative for all other years. This is because the expense premiums are typically smaller than the acquisition expenses , resulting in a positive loss in the first year, known as the New Business Strain . On the other hand, expense premiums are typically higher than the renewal expenses , resulting in a negative loss in all subsequent years, recovering the initial loss over time. This is known as the Deferred Acquisition Cost as the recovery of the acquisition expenses are deferred to all future years. At time 1, all future expense premiums will be larger than the corresponding renewal expenses, resulting in a negative expense reserve from that time onwards: \\[ \\begin{aligned} \\text{Expense Premium} &\\gt \\text{Renewal Expense} \\\\ \\text{EPV(Expense Premium)} &\\gt \\text{EPV(Renewal Expense)} \\\\ {}_{t}V^{\\text{Expense}} < 0 \\end{aligned} \\] Full Preliminary Term \u00b6 Most methods of calculating modified reserves involve using a modified premium to calculate the reserves. In general, these modified premiums are non-level , consisting of two (or more) components: First Year Premiums , \\(\\alpha\\) Subsequent Premiums , \\(\\beta\\) Where \\(\\beta \\gt P \\gt \\alpha\\) Since \\(\\beta \\gt P\\) , the insurer recognises more premiums for the same amount benefit, resulting in a smaller reserve . This allows the insurer to use their capital more productively. \\[ {}_{t}v^{\\text{Modified}} < {}_{t}v^{\\text{Net}} \\] The real reason is due to acquisition expenses, but im not sure how to explain There are many methods of calculating modified reserves, but for the purposes of this exam, we will only be considering the Full Preliminary Term method. The policy can be thought of as being split into two components: One year TA with single premiums of \\(\\alpha\\) The original policy issued one year later with one less year of coverage with recurring premiums of \\(\\beta\\) For the one year TA, following the equivalence principle approach, \\[ \\begin{aligned} {}_{0}V^{\\text{FPT}} &= 0 \\\\ \\alpha &= A^{1}_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] For the original policy issued one year later with one year less of coverage , following the equivalence principle approach, \\[ \\begin{aligned} {}_{1}V^{\\text{FPT}} &= 0 \\\\ A_{x+1} - \\beta \\ddot{a}_{x+1} &= 0 \\\\ \\beta &= \\frac{A_{x+1}}{\\ddot{a}_{x+1}} \\end{aligned} \\] Special Case: Term & Endowments For Term & Endowment contracts, the one less year of coverage must be accounted for: \\[ \\beta = \\frac{A_{x+1:\\enclose{actuarial}{n-1}}}{\\ddot{a}_{x+1:\\enclose{actuarial}{n-1}}} \\] Thus, the modified reserves at all later times follows the same formula as before, simply using \\(\\beta\\) instead: \\[ \\begin{aligned} {}_{t}V^{\\text{FPT}} &= \\begin{cases} 0, & t = 0 \\\\ 0, & t = 1 \\\\ A_{x+t} - \\beta \\ddot{a}_{x+t}, & t > 1 \\end{cases} \\end{aligned} \\] Thus, the FPT reserves are equivalent to the net premium reserve for a policy that is actually issued one year later with one less year of coverage: \\[ \\therefore {}_{t}V^{\\text{FPT}} = {}_{t-1}V^{\\text{Net}}_{\\text{One year later, one less year}} \\] Interim Reserves \u00b6","title":"Reserves"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#reserves","text":"","title":"Reserves"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#overview","text":"Most life insurance contracts charge level premiums - charging the same amount throughout the lifetime of the policy. When the policyholder is young, the probability of death is small, thus the premiums collected are larger than the expected outflow in the year, leading to a surplus . When the policyholder is older, the probability of death is high, thus the premiums collected are smaller than the expected outflow in the year, leading to a deficit . Given the inevitable deficit, the surplus in the earlier years cannot be recognized as a profit and are instead pooled together and safely invested into an account, from which the deficit in later years will draw from. It is a common misconception to think that each policy has its own account. Following this logic, the insurer would not have enough funds to pay an insured who dies shortly after death. In order to determine the adequacy of the account, insurers will calculate the size of the expected loss of ALL policies and compare it against the (hopefully larger) account value. The expected loss is known as the Reserve of the policy because it represents an amount inside the pooled account that is reserved for paying the benefits of the policy.","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#prospective-approach","text":"There are two approaches to calculate the reserves: Prospective Approach - Based on Present Values of future cashflows Retrospective Approach - Based on Future Values of past cashflows (Not covered) Similar to premiums, there are also two types of reserves, depending on what cashflows are being considered: Net Premium Reserves - Net Premiums and Benefits Gross Premium Reserves - Gross Premiums, Benefits and Expenses While this looks similar to premium calculation, note that the Gross Reserves do NOT include any other cashflows (EG. Profit Margin). This is intuitive, as reserves should only be to meet essential cashflows . Reserves are calculated at the beginning of the policy year : AFTER any year end cashflows from the previous period (Claims & Claim Expenses) BEFORE any beginning of year cashflows from the current period (Premiums & Renewal Expenses) This means that the reserves are the additional amount needed to cover claims from the current year onwards, including the current year. Beginning VS End Recall that the beginning of the policy year is also the end of the previous policy year - avoid confusion by drawing out the timelines. Questions can use either phrasing, so be sure to read very carefully. Let \\({}_{t-1}V\\) represent the reserve at the beginning of policy year \\(t\\) : \\[ \\begin{aligned} {}_{t-1}V &= E(L_{x+(t-1)}) \\\\ &= \\text{PV Benefits}_{x+(t-1)} - \\text{PV Premiums}_{x+(t-1)} \\end{aligned} \\] Note that reserves only consider FUTURE cashflows. For single premium or deferred products, there is no need to consider the premiums after the payment period is over. Similar to premiums, we will only define formulas for the Net Premium Reserves . They can be simplified into an expression involving only the following, which allows it to be easily calculated in situations where limited information is provided: Annuities only Assurances only Annuities and Premiums Assurance and Premiums","title":"Prospective Approach"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#case-1-annuities-only","text":"\\[ \\begin{aligned} {}_{t-1}V &= A_{x+(t-1)} - P \\ddot{a}_{x+(t-1)} \\\\ &= A_{x+(t-1)} - \\frac{A_x}{\\ddot{a}_x} \\cdot \\ddot{a}_{x+(t-1)} \\\\ &= (1-\\delta\\ddot{a}_{x+(t-1)}) - \\frac{1-\\delta\\ddot{a}_{x}}{\\ddot{a}_{x}} \\cdot \\ddot{a}_{x+(t-1)} \\\\ &= 1-\\delta\\ddot{a}_{x+(t-1)} - \\left (\\frac{1}{\\ddot{a}_x}-\\delta \\right) \\cdot \\ddot{a}_{x+(t-1)} \\\\ &= 1-\\delta\\ddot{a}_{x+(t-1)} - \\frac{\\ddot{a}_{x+(t-1)}}{\\ddot{a}_x} + \\delta\\ddot{a}_{x+(t-1)} \\\\ &= 1 - \\frac{\\ddot{a}_{x+(t-1)}}{\\ddot{a}_x} \\end{aligned} \\]","title":"Case 1: Annuities Only"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#case-2-assurances-only","text":"\\[ \\begin{aligned} {}_{t-1}V &= A_{x+(t-1)} - P \\ddot{a}_{x+(t-1)} \\\\ &= A_{x+(t-1)} - \\frac{A_x}{\\ddot{a}_x} \\cdot \\ddot{a}_{x+(t-1)} \\\\ &= A_{x+(t-1)} - \\frac{A_x}{\\frac{1-A_x}{d}} \\cdot \\frac{1-A_{x+(t-1)}}{d} \\\\ &= A_{x+(t-1)} - \\frac{A_x}{1-A_x} \\cdot (1-A_{x+(t-1)}) \\\\ &= \\frac{A_{x+(t-1)} \\cdot (1-A_{x}) - A_x(1-A_{x+(t-1)})}{1-A_x} \\\\ &= \\frac{A_{x+(t-1)} - A_{x}A_{x+(t-1)} - A_x + A_x A_{x+(t-1)}}{1-A_x} \\\\ &= \\frac{A_{x+(t-1)} - A_{x}}{1 - A_{x}} \\end{aligned} \\]","title":"Case 2: Assurances Only"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#case-3-annuities-premiums","text":"\\[ \\begin{aligned} {}_{t}V &= A_{x+(t-1)} - P_x \\cdot \\ddot{a}_{x+(t-1)} \\\\ &= \\ddot{a}_{x+(t-1)} \\cdot \\left(\\frac{A_{x+(t-1)}}{\\ddot{a}_{x+(t-1)}} - P_{x} \\right) \\\\ &= \\ddot{a}_{x+(t-1)} \\cdot \\left(P_{x+(t-1)} - P_{x} \\right) \\end{aligned} \\]","title":"Case 3: Annuities &amp; Premiums"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#case-4-assurances-premiums","text":"\\[ \\begin{aligned} {}_{t}V &= A_{x+(t-1)} - P_x \\cdot \\ddot{a}_{x+(t-1)} \\\\ &= A_{x+(t-1)} \\left(1 - P_x \\cdot \\frac{\\ddot{a}_{x+(t-1)}}{A_{x+(t-1)}} \\right) \\\\ &= A_{x+(t-1)} \\left(1 - P_x \\cdot \\frac{1}{\\frac{A_{x+(t-1)}}{\\ddot{a}_{x+(t-1)}}} \\right) \\\\ &= A_{x+(t-1)} \\left(1 - \\frac{P_x}{P_{x+(t-1)}} \\right) \\end{aligned} \\]","title":"Case 4: Assurances &amp; Premiums"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#reserves-premiums","text":"The calculation of reserves and premiums both involve the expectation of the random variable \\(L\\) : Premiums Reserves \\(E(L)\\) given as 0 \\(E(L)\\) is the target \\(P\\) is the target \\(P\\) is given Calculated at age \\(x\\) Calculated at age \\(x+t\\) If net premium reserves are calculated at the start of the policy ( \\(t=0\\) ), then following the equivalence principle, the reserves will be 0 . This is intuitive, as the premiums are set such that they expect to exactly cover the losses of the policy. However, this is not the case for gross premiums. This is because gross premiums include provisions for other cashflows like profit margins which are not considered during the reserve calculation. Thus, gross premiums collect more than enough to cover the expected losses, resulting in a negative reserve at time 0 . Technically speaking, not all gross premiums include profit margins. If they do not, then the gross premium reserve will also be 0. \\[ \\begin{aligned} {}_{0}V^{Net} &= 0 \\\\ {}_{0}V^{Gross} &\\le 0 \\end{aligned} \\] Note that a newly issued policy for another individual aged \\(x+t\\) uses the exact same \\(L\\) as the reserve, \\(L_{x+t}\\) . This is why some of the expressions previously involve \\(P_{x+t}\\) , which is the premium of this newly issued policy.","title":"Reserves &amp; Premiums"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#red-herrings","text":"Premium and Reserve calculations are similar, so it is easy to get them mixed up with each other. The confusing part is that some questions requires us to compute the premiums ourselves before calculating the reserves. Given how similar the calculations are, this may mistakenly cause us to use the same cashflows for both, when the reserves should be different. Even if the gross premiums are provided, some information exclusively used to compute gross premiums may still be provided as a red herring to intentionally confuse us: Acquisition Expenses - Reserves are usually calculated after inception; no need for it Profit Margin - Reserves DO NOT consider profit margin Premium Frequency - Reserves use annual premiums; monthly premiums may be required or provided Note that the reverse is also possible, where gross premiums are provided but the question actually requires us to calculate net premiums, which can cause us to mistakenly use the gross premium instead.","title":"Red Herrings"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#recursions","text":"Following the same logic as assurances and annuities, net premium reserves can also be recursively expressed as a function of itself: If the policyholder dies, the insurer would have to pay a benefit of 1 at the end of the year If the policyholder survives, the insurer would have to keep a reserve for future payments Premiums are received REGARDLESS of death or survival as it was paid at the start of the year \\[ \\begin{aligned} {}_{t}V^{\\text{Net}} &= - P + \\begin{cases} v, & q_{x+t} \\\\ v \\cdot {}_{t+1}V, & p_{x+t} \\end{cases} \\\\ \\\\ \\therefore {}_{t}V^{\\text{Net}} &= - P + q_{x+t} \\cdot v + p_{x+t} \\cdot v \\cdot {}_{t+1}V \\end{aligned} \\] Gross premium reserves are similar as any expenses are always paid at the beginning of the year with premiums: \\[ {}_{t}V^{\\text{Gross}} = (E-G) + q_{x+t} \\cdot v + p_{x+t} \\cdot v \\cdot {}_{t+1}V \\]","title":"Recursions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#modified-net-premium-reserves","text":"The method and basis for calculating the reserve is based on the prevailing regulation at that time and region. In the US, reserves are calculated using net premiums due to its simplicity at the cost of some accuracy loss. However, net premium reserves tend to be higher than gross premium reserves . This is due to the negative expense reserves, as the excess expense premiums help to offset the future benefits as well. Since net premiums reserves do not recognize expenses, they do not have this benefit! \\[ \\begin{aligned} {}_{t}v^{\\text{Net}} &= {}_{t}v^{\\text{Gross}} + \\underbrace{{}_{t}v^{\\text{Expense}}}_{\\text{-ve}} \\\\ {}_{t}v^{\\text{Net}} &\\gt {}_{t}v^{\\text{Gross}} \\end{aligned} \\] This means that insurers are holding too much reserves under this valuation basis. This is problematic because reserves can only be invested in low risk assets, which typically have lower returns relative to what the capital would have usually be invested in. This incurs an opportunity cost in investment income . To account for this, US regulators allow for insurers to use a modified net premium reserve instead that implicitly accounts for expenses while retaining the simplicity of a net premium approach.","title":"Modified Net Premium Reserves"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#expense-reserves","text":"Following expense premiums, the difference between the gross and net premium reserves represent the additional amount needed to cover the expenses, known as the Expense Reserves . \\[ \\begin{aligned} {}_{t}V^{\\text{Expense}} &= {}_{t}V^{\\text{Gross}} - {}_{t}V^{\\text{Net}} \\\\ &= \\text{EPV(Benefits)} + \\text{EPV(Expenses)} - \\text{EPV(Gross Premium)} - \\text{EPV(Benfits)} + \\text{EPV(Net Premiums)} \\\\ &= \\text{EPV(Expenses)} - (\\text{EPV(Gross Premium)} - \\text{EPV(Net Premiums)}) \\\\ &= \\text{EPV(Expenses)} - \\text{EPV(Expense Premium)} \\end{aligned} \\] The expense reserve is 0 at time 0 . However, the expense reserve is typically negative for all other years. This is because the expense premiums are typically smaller than the acquisition expenses , resulting in a positive loss in the first year, known as the New Business Strain . On the other hand, expense premiums are typically higher than the renewal expenses , resulting in a negative loss in all subsequent years, recovering the initial loss over time. This is known as the Deferred Acquisition Cost as the recovery of the acquisition expenses are deferred to all future years. At time 1, all future expense premiums will be larger than the corresponding renewal expenses, resulting in a negative expense reserve from that time onwards: \\[ \\begin{aligned} \\text{Expense Premium} &\\gt \\text{Renewal Expense} \\\\ \\text{EPV(Expense Premium)} &\\gt \\text{EPV(Renewal Expense)} \\\\ {}_{t}V^{\\text{Expense}} < 0 \\end{aligned} \\]","title":"Expense Reserves"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#full-preliminary-term","text":"Most methods of calculating modified reserves involve using a modified premium to calculate the reserves. In general, these modified premiums are non-level , consisting of two (or more) components: First Year Premiums , \\(\\alpha\\) Subsequent Premiums , \\(\\beta\\) Where \\(\\beta \\gt P \\gt \\alpha\\) Since \\(\\beta \\gt P\\) , the insurer recognises more premiums for the same amount benefit, resulting in a smaller reserve . This allows the insurer to use their capital more productively. \\[ {}_{t}v^{\\text{Modified}} < {}_{t}v^{\\text{Net}} \\] The real reason is due to acquisition expenses, but im not sure how to explain There are many methods of calculating modified reserves, but for the purposes of this exam, we will only be considering the Full Preliminary Term method. The policy can be thought of as being split into two components: One year TA with single premiums of \\(\\alpha\\) The original policy issued one year later with one less year of coverage with recurring premiums of \\(\\beta\\) For the one year TA, following the equivalence principle approach, \\[ \\begin{aligned} {}_{0}V^{\\text{FPT}} &= 0 \\\\ \\alpha &= A^{1}_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] For the original policy issued one year later with one year less of coverage , following the equivalence principle approach, \\[ \\begin{aligned} {}_{1}V^{\\text{FPT}} &= 0 \\\\ A_{x+1} - \\beta \\ddot{a}_{x+1} &= 0 \\\\ \\beta &= \\frac{A_{x+1}}{\\ddot{a}_{x+1}} \\end{aligned} \\] Special Case: Term & Endowments For Term & Endowment contracts, the one less year of coverage must be accounted for: \\[ \\beta = \\frac{A_{x+1:\\enclose{actuarial}{n-1}}}{\\ddot{a}_{x+1:\\enclose{actuarial}{n-1}}} \\] Thus, the modified reserves at all later times follows the same formula as before, simply using \\(\\beta\\) instead: \\[ \\begin{aligned} {}_{t}V^{\\text{FPT}} &= \\begin{cases} 0, & t = 0 \\\\ 0, & t = 1 \\\\ A_{x+t} - \\beta \\ddot{a}_{x+t}, & t > 1 \\end{cases} \\end{aligned} \\] Thus, the FPT reserves are equivalent to the net premium reserve for a policy that is actually issued one year later with one less year of coverage: \\[ \\therefore {}_{t}V^{\\text{FPT}} = {}_{t-1}V^{\\text{Net}}_{\\text{One year later, one less year}} \\]","title":"Full Preliminary Term"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#interim-reserves","text":"","title":"Interim Reserves"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/","text":"Model Estimation \u00b6 Overview \u00b6 Previously, mortality rates were obtained through some parametric distribution. However, this requires us to make an assumption about the population, which may not always hold true. Thus, a better method would be to study the underlying population directly to estimate the mortality rates. Complete Data \u00b6 Consider a study where a group of individuals of the same age are observed till they die. The number of years taken for each individual to die are recorded as observations in the study. Each study has a total of \\(n\\) individuals starting at age \\(x\\) , thus the observations are denoted as \\(t_1, t_2, ..., t_n\\) . Individual Data \u00b6 If each indvidual has an observation tagged to them (similar to what was described earlier), then Individual Data is provided. Equal probability is assigned to each observation, thus the probability of an individual aged \\(x\\) surviving \\(k\\) years is simply the ratio of the number of observations larger than \\(k\\) to the total number of observations: \\[ {}_{t}\\hat{p}_x = \\frac{\\text{# of individuals survive past k}}{n} \\] Grouped Data \u00b6 If the population of the study is large, then the data is usually Grouped into ranges for convenience. The number of observations within each range is also provided. Ranges are concisely expressed using the bracket notation - () denotes that the bound is exlcusive while [] denotes that it is inclusive. For instance, (15, 20] denoted that the time to death is strictly larger than 15 but less than or equal to 20; \\(15 \\lt 5 \\le 20\\) . If \\(k\\) is the boundary of any of the ranges , then the probability of surviving \\(k\\) years is calculated the same way as the individual data - it is the ratio of the number of observations to the total number: \\[ {}_{t}\\hat{p}_x = \\frac{\\text{# of observations larger than k}}{n} \\] However, if \\(k\\) is within the range, by assuming that values within the range are uniformly distributed , then the probability can be linearly interpolated from the probabilities at the boundary: Consider the range \\((a,b]\\) : \\[ {}_{t}\\hat{p}_x = \\frac{k-a}{b-a} \\cdot \\hat{S}_x(a) + \\frac{b-k}{b-a} \\cdot \\hat{S}_x(b) \\] The interpolation follows the same intuition as the fractional age assumption, where the uniform assumption results in a linear graph of probabilities: Incomplete Data \u00b6 In practice, due to various constraints, it is impossible to observe an entire group of individuals till they die. Thus, for some observations, the recorded time to death is not the exact time to death , known as Incomplete Data . Censoring \u00b6 Censoring refers to when the value of the observation is only partially known . For instance, if the study stops prematurely, the time to deaths for the individuals alive will be recorded at the time the study ends. However, in reality, they die at some future time, resulting in a larger time to death than what was recorded . Right Censored - Actual value is larger than the recorded value Left Censored - Actual value is smaller than the recorded value Naturally, since time to death is an increasing function, the only Right Censored data is relevant in an actuarial context Generally speaking, there are two types for censored data: Type 1 Censoring - Study is stopped at a pre-determined time Type 2 Censoring - Study is stopped a certain number of deaths The remaining individuals still in the study at the point of stopping are right censored Censored data are usually denoted by a '+' superscript. Interval censoring is grouped data, random censoring Truncation \u00b6 Truncation refers to when the sample does not contain certain observations outside of some range. This is not to be confused with its mathematical definition of rounding. For instance, consider a study that wishes to study the mortality for people aged 50. However, only people aged 52 are able to be found. Naturally, then the times to death from age 50 for these individuals will always be at least 2 ; it will never be smaller than 2 . Right Truncated - No observations with values larger than cutoff Left Truncated - No observations with values smaller than cutoff Naturally, since time to death is an increasing function, the only Left Truncated data is relevant in an actuarial context Truncation and Censoring are often confused with one another as both result in observations within a given range. Truncation is when observations outside the range are not recorded at all while Censoring still records the observations , with the caveat that it is a floor/ceiling. Kaplan Meier Estimation \u00b6 Consider a study of 5 individuals: One individual dies at \\(t=0.8\\) and \\(t=1.4\\) 5 more individuals join the study at \\(t=1\\) ; Left Truncated at \\(t=1\\) 3 individual leave the study at \\(t=0.5\\) ; Right Censored at \\(t=0.5\\) The information is more often concisely portrayed in a table format: \\(t\\) represents the time of death \\(d\\) represents the number of deaths \\(r\\) represents the risk set RIGHT BEFORE death; simply known as the sample size This is problematic as the sample size changes throughout the study. However, the Kaplan Meier method accounts for this by calculating the survival probability as the product of the survival probabilities for all previous intervals between deaths : \\[ {}_{t}\\hat{p}_x = \\prod \\left(1-\\frac{d_j}{r_j} \\right) \\] Given the above example, the resulting Kaplan Meier estimation would be: \\[ \\begin{aligned} {}_{1.4}\\hat{p}_0 &= P(t > 0.8) * P(t > 1.4 | x > 0.8) \\\\ &= \\left (1 - \\frac{1}{2} \\right) * \\left (1 - \\frac{1}{6} \\right) \\\\ &= \\left (\\frac{1}{2} \\right) * \\left (\\frac{5}{6} \\right) \\\\ &= \\frac{5}{12} \\end{aligned} \\] This results in a constant probability between deaths . Thus, the focus is on the times of death ; the entries and exits from the study are only used to change the sample size. Nelson Aelen Estimation \u00b6 Recall that the survival probability can be expressed as a function of the Force of Mortality: \\[ {}_{t}\\hat{p}_x = e^{- \\int^t_0 \\mu_{x+t}} \\] The Nelson Aelen method estimates the Cumulative Force of Mortality (the exponent of the expression), which can be then used to compute the survival probability. It is an extremely common mistake to take the result of the Nelson Aelen estimation as the survival probability. The nelson aelen estimator is computed as the sum of the death probabilities for all previous intervals between deaths : \\[ \\int^t_0 \\mu_{x+t} = \\sum \\frac{d_j}{r_j} \\] Given the previous example, the resulting Nelson Aelen estimation would be: \\[ \\begin{aligned} \\int^t_0 \\mu_{x+1.4} &= P(t < 0.8) + P(t < 1.4 | x > 0.8) \\\\ &= \\left (\\frac{1}{2} \\right) + \\left (\\frac{1}{6} \\right) \\\\ &= \\frac{2}{3} \\\\ \\\\ {}_{1.4}\\hat{p}_x &= e^{-\\frac{2}{3}} \\\\ &= 0.51171 \\end{aligned} \\] Notice that the two values are not too far off from one another. Kaplan Meier Nelson Aelen Estimates Survival Probability Estimates Cumulative Force of Mortality Uses Survival probabilities Uses Death probabilities Maximum Likelihood Estimation \u00b6 Unlike the Nelson Aelen method, the MLE DIRECTLY estimates the force of mortality , which can be then used to compute survival probabilities. One key concept is the idea of Central Exposed to Risk ( \\(E^c_x\\) ), which is a measure of how long the individual has been alive in the study at the time. Consider the following example to estimate the force of mortality at age 51 : Firstly, we need to determine if and when the individuals turn 51: The date they turn 51 is used as the starting point for the \\(E^c_x\\) . The ending point is based on the year that they turn 51 and is one of the following: Date of death - Died during the year Date of withdrawal - Left during the year Date they turn 52 - Survived the entire year From this, \\(E^c_x\\) is calculated in years : Using the CER, the force of mortality can be determined: \\[ \\hat{\\mu}_x = \\frac{d_x}{E^c_x} \\] Assuming that this force is constant , the resulting survival probability is calculated as: \\[ {}_{t}\\hat{p}_x = e^{-\\hat{\\mu}_x} \\] Statistical Inference \u00b6 Given that all the above methods are estimates of the survival probability, the estimated values will change depending on the underlying sample. Thus, it is important to consider the variance of the estimator as well across various different samples. Complete Variance \u00b6 Each individual is either dead or or alive - thus, the number of people who survive past the specified time can be modelled using a Bernoulli Random variable . Recall that the variance of the bernoulli distribution is: \\[ Var(\\text{# of individuals survive past t}) = n \\cdot {}_{t}p_x \\cdot (1-{}_{t}p_x) \\] Thus, the variance of the \\[ \\begin{aligned} Var({}_{t}\\hat{p}_x) &= Var \\left(\\frac{\\text{# of individuals survive past k}}{n} \\right) \\\\ &= \\frac{1}{n^2} Var(\\text{# of individuals survive past t}) \\\\ &= \\frac{1}{n^2} n \\cdot {}_{t}p_x \\cdot (1-{}_{t}p_x) \\\\ &= \\frac{{}_{t}p_x \\cdot (1-{}_{t}p_x)}{n} \\\\ \\end{aligned} \\] Since the actual survival probability is not known, it is often substituted with the estimate: \\[ Var({}_{t}\\hat{p}_x) = \\frac{{}_{t}\\hat{p}_x \\cdot (1-{}_{t}\\hat{p}_x)}{n} \\] Note that the variance of the estimated death probability is identical to that of the survival probability: \\[ \\begin{aligned} Var({}_{t}\\hat{q}_x) &= Var(1 - {}_{t}\\hat{p}_x) \\\\ &= Var({}_{t}\\hat{p}_x) \\\\ \\end{aligned} \\] Incomplete Variance \u00b6 Due to the complexity of calculating the variance under these methods, the formula for variances are provided in the formula sheets. Kaplan Meier Variance : \\[ Var({}_{t}\\hat{p}_x) = ({}_{t}\\hat{p}_x)^2 \\sum \\frac{d_j}{r_j (r_j - d_j)} \\] Nelson Aelen Variance : \\[ Var({}_{t}\\hat{p}_x) = ({}_{t}\\hat{p}_x)^2 \\sum \\frac{d_j (r_j - d_j)}{r^3_j} \\] Maximum Likelihood Variance : \\[ Var({}_{t}\\hat{p}_x) = ({}_{t}\\hat{p}_x)^2 \\cdot \\frac{d_x}{\\left( E^c_x \\right)^2} \\] Confidence Intervals \u00b6 Linear Log Transformed","title":"Model Estimation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#model-estimation","text":"","title":"Model Estimation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#overview","text":"Previously, mortality rates were obtained through some parametric distribution. However, this requires us to make an assumption about the population, which may not always hold true. Thus, a better method would be to study the underlying population directly to estimate the mortality rates.","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#complete-data","text":"Consider a study where a group of individuals of the same age are observed till they die. The number of years taken for each individual to die are recorded as observations in the study. Each study has a total of \\(n\\) individuals starting at age \\(x\\) , thus the observations are denoted as \\(t_1, t_2, ..., t_n\\) .","title":"Complete Data"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#individual-data","text":"If each indvidual has an observation tagged to them (similar to what was described earlier), then Individual Data is provided. Equal probability is assigned to each observation, thus the probability of an individual aged \\(x\\) surviving \\(k\\) years is simply the ratio of the number of observations larger than \\(k\\) to the total number of observations: \\[ {}_{t}\\hat{p}_x = \\frac{\\text{# of individuals survive past k}}{n} \\]","title":"Individual Data"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#grouped-data","text":"If the population of the study is large, then the data is usually Grouped into ranges for convenience. The number of observations within each range is also provided. Ranges are concisely expressed using the bracket notation - () denotes that the bound is exlcusive while [] denotes that it is inclusive. For instance, (15, 20] denoted that the time to death is strictly larger than 15 but less than or equal to 20; \\(15 \\lt 5 \\le 20\\) . If \\(k\\) is the boundary of any of the ranges , then the probability of surviving \\(k\\) years is calculated the same way as the individual data - it is the ratio of the number of observations to the total number: \\[ {}_{t}\\hat{p}_x = \\frac{\\text{# of observations larger than k}}{n} \\] However, if \\(k\\) is within the range, by assuming that values within the range are uniformly distributed , then the probability can be linearly interpolated from the probabilities at the boundary: Consider the range \\((a,b]\\) : \\[ {}_{t}\\hat{p}_x = \\frac{k-a}{b-a} \\cdot \\hat{S}_x(a) + \\frac{b-k}{b-a} \\cdot \\hat{S}_x(b) \\] The interpolation follows the same intuition as the fractional age assumption, where the uniform assumption results in a linear graph of probabilities:","title":"Grouped Data"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#incomplete-data","text":"In practice, due to various constraints, it is impossible to observe an entire group of individuals till they die. Thus, for some observations, the recorded time to death is not the exact time to death , known as Incomplete Data .","title":"Incomplete Data"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#censoring","text":"Censoring refers to when the value of the observation is only partially known . For instance, if the study stops prematurely, the time to deaths for the individuals alive will be recorded at the time the study ends. However, in reality, they die at some future time, resulting in a larger time to death than what was recorded . Right Censored - Actual value is larger than the recorded value Left Censored - Actual value is smaller than the recorded value Naturally, since time to death is an increasing function, the only Right Censored data is relevant in an actuarial context Generally speaking, there are two types for censored data: Type 1 Censoring - Study is stopped at a pre-determined time Type 2 Censoring - Study is stopped a certain number of deaths The remaining individuals still in the study at the point of stopping are right censored Censored data are usually denoted by a '+' superscript. Interval censoring is grouped data, random censoring","title":"Censoring"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#truncation","text":"Truncation refers to when the sample does not contain certain observations outside of some range. This is not to be confused with its mathematical definition of rounding. For instance, consider a study that wishes to study the mortality for people aged 50. However, only people aged 52 are able to be found. Naturally, then the times to death from age 50 for these individuals will always be at least 2 ; it will never be smaller than 2 . Right Truncated - No observations with values larger than cutoff Left Truncated - No observations with values smaller than cutoff Naturally, since time to death is an increasing function, the only Left Truncated data is relevant in an actuarial context Truncation and Censoring are often confused with one another as both result in observations within a given range. Truncation is when observations outside the range are not recorded at all while Censoring still records the observations , with the caveat that it is a floor/ceiling.","title":"Truncation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#kaplan-meier-estimation","text":"Consider a study of 5 individuals: One individual dies at \\(t=0.8\\) and \\(t=1.4\\) 5 more individuals join the study at \\(t=1\\) ; Left Truncated at \\(t=1\\) 3 individual leave the study at \\(t=0.5\\) ; Right Censored at \\(t=0.5\\) The information is more often concisely portrayed in a table format: \\(t\\) represents the time of death \\(d\\) represents the number of deaths \\(r\\) represents the risk set RIGHT BEFORE death; simply known as the sample size This is problematic as the sample size changes throughout the study. However, the Kaplan Meier method accounts for this by calculating the survival probability as the product of the survival probabilities for all previous intervals between deaths : \\[ {}_{t}\\hat{p}_x = \\prod \\left(1-\\frac{d_j}{r_j} \\right) \\] Given the above example, the resulting Kaplan Meier estimation would be: \\[ \\begin{aligned} {}_{1.4}\\hat{p}_0 &= P(t > 0.8) * P(t > 1.4 | x > 0.8) \\\\ &= \\left (1 - \\frac{1}{2} \\right) * \\left (1 - \\frac{1}{6} \\right) \\\\ &= \\left (\\frac{1}{2} \\right) * \\left (\\frac{5}{6} \\right) \\\\ &= \\frac{5}{12} \\end{aligned} \\] This results in a constant probability between deaths . Thus, the focus is on the times of death ; the entries and exits from the study are only used to change the sample size.","title":"Kaplan Meier Estimation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#nelson-aelen-estimation","text":"Recall that the survival probability can be expressed as a function of the Force of Mortality: \\[ {}_{t}\\hat{p}_x = e^{- \\int^t_0 \\mu_{x+t}} \\] The Nelson Aelen method estimates the Cumulative Force of Mortality (the exponent of the expression), which can be then used to compute the survival probability. It is an extremely common mistake to take the result of the Nelson Aelen estimation as the survival probability. The nelson aelen estimator is computed as the sum of the death probabilities for all previous intervals between deaths : \\[ \\int^t_0 \\mu_{x+t} = \\sum \\frac{d_j}{r_j} \\] Given the previous example, the resulting Nelson Aelen estimation would be: \\[ \\begin{aligned} \\int^t_0 \\mu_{x+1.4} &= P(t < 0.8) + P(t < 1.4 | x > 0.8) \\\\ &= \\left (\\frac{1}{2} \\right) + \\left (\\frac{1}{6} \\right) \\\\ &= \\frac{2}{3} \\\\ \\\\ {}_{1.4}\\hat{p}_x &= e^{-\\frac{2}{3}} \\\\ &= 0.51171 \\end{aligned} \\] Notice that the two values are not too far off from one another. Kaplan Meier Nelson Aelen Estimates Survival Probability Estimates Cumulative Force of Mortality Uses Survival probabilities Uses Death probabilities","title":"Nelson Aelen Estimation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#maximum-likelihood-estimation","text":"Unlike the Nelson Aelen method, the MLE DIRECTLY estimates the force of mortality , which can be then used to compute survival probabilities. One key concept is the idea of Central Exposed to Risk ( \\(E^c_x\\) ), which is a measure of how long the individual has been alive in the study at the time. Consider the following example to estimate the force of mortality at age 51 : Firstly, we need to determine if and when the individuals turn 51: The date they turn 51 is used as the starting point for the \\(E^c_x\\) . The ending point is based on the year that they turn 51 and is one of the following: Date of death - Died during the year Date of withdrawal - Left during the year Date they turn 52 - Survived the entire year From this, \\(E^c_x\\) is calculated in years : Using the CER, the force of mortality can be determined: \\[ \\hat{\\mu}_x = \\frac{d_x}{E^c_x} \\] Assuming that this force is constant , the resulting survival probability is calculated as: \\[ {}_{t}\\hat{p}_x = e^{-\\hat{\\mu}_x} \\]","title":"Maximum Likelihood Estimation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#statistical-inference","text":"Given that all the above methods are estimates of the survival probability, the estimated values will change depending on the underlying sample. Thus, it is important to consider the variance of the estimator as well across various different samples.","title":"Statistical Inference"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#complete-variance","text":"Each individual is either dead or or alive - thus, the number of people who survive past the specified time can be modelled using a Bernoulli Random variable . Recall that the variance of the bernoulli distribution is: \\[ Var(\\text{# of individuals survive past t}) = n \\cdot {}_{t}p_x \\cdot (1-{}_{t}p_x) \\] Thus, the variance of the \\[ \\begin{aligned} Var({}_{t}\\hat{p}_x) &= Var \\left(\\frac{\\text{# of individuals survive past k}}{n} \\right) \\\\ &= \\frac{1}{n^2} Var(\\text{# of individuals survive past t}) \\\\ &= \\frac{1}{n^2} n \\cdot {}_{t}p_x \\cdot (1-{}_{t}p_x) \\\\ &= \\frac{{}_{t}p_x \\cdot (1-{}_{t}p_x)}{n} \\\\ \\end{aligned} \\] Since the actual survival probability is not known, it is often substituted with the estimate: \\[ Var({}_{t}\\hat{p}_x) = \\frac{{}_{t}\\hat{p}_x \\cdot (1-{}_{t}\\hat{p}_x)}{n} \\] Note that the variance of the estimated death probability is identical to that of the survival probability: \\[ \\begin{aligned} Var({}_{t}\\hat{q}_x) &= Var(1 - {}_{t}\\hat{p}_x) \\\\ &= Var({}_{t}\\hat{p}_x) \\\\ \\end{aligned} \\]","title":"Complete Variance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#incomplete-variance","text":"Due to the complexity of calculating the variance under these methods, the formula for variances are provided in the formula sheets. Kaplan Meier Variance : \\[ Var({}_{t}\\hat{p}_x) = ({}_{t}\\hat{p}_x)^2 \\sum \\frac{d_j}{r_j (r_j - d_j)} \\] Nelson Aelen Variance : \\[ Var({}_{t}\\hat{p}_x) = ({}_{t}\\hat{p}_x)^2 \\sum \\frac{d_j (r_j - d_j)}{r^3_j} \\] Maximum Likelihood Variance : \\[ Var({}_{t}\\hat{p}_x) = ({}_{t}\\hat{p}_x)^2 \\cdot \\frac{d_x}{\\left( E^c_x \\right)^2} \\]","title":"Incomplete Variance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#confidence-intervals","text":"Linear Log Transformed","title":"Confidence Intervals"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/","text":"Review of Probability Theory \u00b6 Basic Probability \u00b6 Probability is the study of Experiments whose results cannot be predicted with certainty. The result of such an experiment is known as its Outcome . The Sample Space \\(\\left(\\Omega \\right)\\) is the set of ALL possible outcomes from an experiment. The Event Space \\((E)\\) is a subset of the sample space, representing only the outcomes that we are interested in studying. Conversely, its Complement \\((E^c)\\) is the set of all OTHER outcomes not inside \\(E\\) . The probability of the event occuring is the ratio of the number of elements in the event to the sample space. It is a measure of the chance that the outcome of the experiment is inside the event space. \\[ P(E) = \\frac{n(E)}{n\\left(\\Omega \\right)} \\] Consider the probability of rolling an odd number on a standard dice: Experiment - Rolling a dice Outcome - The number showed on the dice Sample Space - \\({1, 2, 3, 4, 5, 6}\\) Event Space - \\({1, 3, 5}\\) Complement - \\({2, 4, 6}\\) Probability of Event - \\(\\frac{3}{6}\\) Probability of Complement - \\(\\frac{3}{6}\\) Within the same experiment, there may be multiple events of interest. For any two events A and B, its Union \\((A \\cup B)\\) is the set with outcomes that are either in A or B while their Intersection \\((A \\cap B)\\) is the set with outcomes that are in BOTH A and B . If both A and B have no outcomes in common \\((A \\cap B = \\emptyset)\\) , then they are said to be Mutually Exclusive . Naturally, an event and its complement are always mutually exclusive. Warning The following seems intuitive , but is actually a common mistake: \\[ (A \\cap B)^c \\ne A^c \\cap B^c \\] This is properly explained through De-morgans Law : It can be easily remembered by applying the complement to all components of the expression, including the intersection/union symbol : \\[ \\begin{aligned} \\cap^c &= \\cup \\\\ \\cup^c &= \\cap \\end{aligned} \\] Probability Axioms \u00b6 Axiom 1 states that all probabilities must be non-negative : \\[ P(E) \\ge 0 \\] Axiom 2 states that probability of the Sample Space is exactly equal to 1: \\[ P(\\Omega) = 1 \\] Axiom 3 states the probability of a union of mutually exclusive events is equal to the sum of their probabilities, known as Countable Additivity . \\[ \\begin{aligned} A \\cap B &= \\emptyset \\\\ P(A \\cup B) &= P(A) + P(B) \\end{aligned} \\] Based on these axioms, several other important properties can also be deduced: Monoticity : \\(A \\subset B \\rightarrow P(A) \\le P(B)\\) Empty Set : \\(P(\\emptyset) = 0\\) Complement Rule : \\(P(E^c) = 1 - P(E)\\) Numeric Bound : \\(0 \\le P(E) \\le 1\\) Sum Rule : \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) Conditional Probability \u00b6 Conditional Probabilities are denoted by \\(P(A|B)\\) , which is the probability of event A occuring given that event B has already occurred . The intuition is best understood by considering the following - Given that event B has already occured, what is the probability that event A also occurs? The event space is \\(A \\cap B\\) , as we are interested in the probability that both A and B occur. However, since event B has already occured, the sample space is no longer all possible outcomes but rather the event space for B! \\[ \\begin{aligned} P(A|B) &= \\frac{n(A \\cap B)}{n(B)} \\\\ &= \\frac{\\frac{n(A \\cap B)}{n(\\Omega)}}{\\frac{n(B)}{n(\\Omega)}} \\\\ &= \\frac{P(A \\cap B)}{P(B)} \\end{aligned} \\] Following this expression, the probability of an intersection of two events is given by: \\[ P(A \\cap B) = P(A|B) \\cdot P(B) = P(B|A) \\cdot P(A) \\] Most experiments involving conditional probabilities are multi-staged experiments, which are best visualized using Probability Trees : Instead of calculating conditional probabilities from scratch, some questions provide the conditional probability \\(P(A|B)\\) (or the components to do so!) and ask us to find the reverse - \\(P(B|A)\\) . \\[ P(B|A) = \\frac{P(B \\cap A)}{P(A)} \\] The formula is the same as before, but the issue is that the unconditional probability of event A is usually not given. This problem is accounted for in Bayes Theorem : \\[ \\begin{aligned} A &= (A \\cap B) + (A \\cap B^c) \\\\ P(A) &= P(A \\cap B) + P(A \\cap B^c) \\\\ P(A) &= P(A|B) \\cdot P(B) + P(A|B^c) \\cdot P(B^c) \\\\ \\\\ \\therefore P(B|A) &= \\frac{P(A|B) \\cdot P(B)}{P(A|B) \\cdot P(B) + P(A|B^c) \\cdot P(B^c)} \\end{aligned} \\] Note that if the Conditional Probability of A given B is the same as the unconditional probability of A, then events A and B are independent ; B has no effect on A. Thus, the probability of an intersection of two independent events is simply their product: \\[ \\begin{aligned} P(A|B) &= \\frac{P(A \\cap B)}{P(B)} \\\\ P(A) &= \\frac{P(A \\cap B)}{P(B)} \\\\ P(A \\cap B) &= P(A) \\cdot P(B) \\end{aligned} \\] Random Variables \u00b6 Unlike rolling a dice, the outcome of most experiments are non-numeric , which makes them hard to work with. For instance, the outcomes of a coin toss are \"Heads\" and \"Tails\". A Random Variable is a many to one function that maps each outcome to a single real number. Each outcome must have only one corresponding number, but different outcomes can have the same value. Note Although the mapping is deterministic, the underlying experiment is still random which is why it is still a \"random\" variable. The range of possible values that the random variable can take is known as its Support . They are broadly categorized based on its support: Discrete Continuous Countable Support Uncountable Support 1, 2, 3, 4, ... 1, 1.1, 1.01, 1.001, ... Random variables are denoted using upper case letters (X, Y, Z) while their corresponding values are denoted using lower case letters (x, y, z) and their appropriate subscripts . The notation \\(X(s) = x_1\\) denotes that the random variable \\(X\\) maps the outcome \\(s\\) to the value of \\(x_1\\) . Thus, the corresponding probability is denoted by \\(P(X = x_1)\\) . Probability Distributions \u00b6 Similar to how a random variable maps the outcomes to a real number, a Probability Distribution is a function that maps the outcomes to its probability of occurrence . For Discrete Random Variables , their distribution is described using a Probability Mass Function (PMF). The PMF provides the probability that the random variable is exactly equal to some value \\((X = x_1)\\) . It is typically denoted in lower case and sometimes includes a subscript of the random variable when working with multiple to distinguish them from one another. \\[ \\begin{aligned} P(X = a) &= p(a) \\\\ \\\\ P(X = a) &= p_X(a) \\\\ P(Y = a) &= p_Y(a) \\end{aligned} \\] Since it is a probability measure, the sum of the PMF over the support of the random variable must be equal to 1 (Probability Axiom). \\[ \\sum_{x \\in \\text{Support}} p(x) = 1 \\] PMFs can be represented in three main ways - Functions, Tables or Histograms. For Continuous Random Variables , their distribution is described using a Probability Density Function (PDF). The PDF is a non-negative function where the area under it provides the probability that the random variable takes on some range of values \\((a \\le X \\le b)\\) . Similarly, it is typically denoted in lower case and includes a subscript when working with multiple random variables: \\[ \\begin{aligned} P(a \\le X \\le b) = \\int^b_a f(x) \\\\ \\\\ P(a \\le X \\le b) = \\int^b_a f_X(x) \\\\ P(a \\le Y \\le b) = \\int^b_a f_Y(y) \\end{aligned} \\] Similarly, since the area is a probability measure, the total area under the graph must be equal to 1 : \\[ P(-\\infty \\le X \\le \\infty) = \\int^{\\infty}_{-\\infty} f(x) = 1 \\] Note \\(\\infty\\) is used as a catch all for the upper and lower bound of the random variable. If the actual bounds are known, then using them instead is more appropriate. Additionally, note that the probability of a specific value for a continuous RV is 0. This is because there is an infinite number of possible values , thus the probability of a specific value (EG. 1.45679383920) is infinitely small such that it is assumed to be 0. \\[ P(X = a) = \\int^{a}_{a} f(x) = 0 \\] The Cumulative Distribution Function (CDF) is the probability that the random variable is less than or equal to some value \\(X \\le t\\) . It is typically denoted in upper case to distinguish it from the PDF and includes subscripts as well when working with multiple random variables. \\[ \\begin{aligned} F(t) &= P(X < t) \\\\ \\\\ F_X(t) &= P(X < t) \\\\ F_Y(t) &= P(Y < t) \\end{aligned} \\] For discrete variables, the CDF is the sum of all probabilities before the specified value . Following this, the difference of consecutive CDFs allows us to obtain the PMFs at that value: \\[ \\begin{aligned} F(t) &= \\sum_{x \\le t} p(x) \\\\ p(x_i) &= F(x_i) - F(x_{i-1}) \\end{aligned} \\] For continuous variables, the CDF is the integral from the lower bound to the specified value . However, instead of integrating with respect to an actual value, it is better to integrate with respect to a dummy variable \\(t\\) to obtain a general expression for the CDF , allowing it to be easily calculated for any value. Although the CDF is very useful, it can only be used to calculate probabilities starting from the lower bound . When probabilities starting from other ranges are needed, the PDF can be obtained from the CDF by differentiating it and then re-integrating with different limits. \\[ \\begin{aligned} F(t) &= \\int^{t}_{-\\infty} f(x) dx \\\\ \\\\ F(t) &= \\int^{t}_{-\\infty} f(t) dt \\\\ f(t) &= F'(t) \\end{aligned} \\] Moments \u00b6 The Moments of a distribution are quantities that describe characteristics of the distribution . Raw Moments are calculated with respect to the origin . The n-th raw moment is calculated as the following: \\[ \\begin{aligned} E(X^n) &= \\int x^n \\cdot f(x) dx \\\\ \\mu'_k &= \\sum x^n \\cdot p(x) \\end{aligned} \\] The first raw moment is known as the Mean , which is a measure of the Centrality of the distribution. It is commonly denoted as \\(\\mu\\) , without any super or subscripts. It has several important properties (more at the section on transformation): \\(E(c) = c\\) \\(E(X + Y) = E(X) + E(Y)\\) Central Moments are calculated with respect to the mean . The n-th central moment is calculated as the following: \\[ \\begin{aligned} E[(X - \\mu)^n] &= \\int (x - \\mu)^n \\cdot f(x) dx \\\\ \\mu_k&= \\sum (x - \\mu)^n \\cdot p(x) \\end{aligned} \\] The second central moment is known as the Variance , which is a measure of the Spread of the distribution about the mean. Since calculating central moments directly is complicated, it can be simplified to an expression involving raw moments: \\[ \\begin{aligned} Var(X) &= E[(X - \\mu)^2] \\\\ &= E(X^2 - 2\\mu X + \\mu^2) \\\\ &= E(X^2) - 2\\mu^2 + \\mu^2 \\\\ &= E(X^2) - \\mu^2 \\\\ &= E(X^2) - [E(X)]^2 \\end{aligned} \\] It has several important properties (more at the section on transformation): \\(Var(c) = 0\\) \\(Var(X+b) = Var(X) + Var (Y) + 2 \\cdot Cov(X,Y)\\) However, one problem with variance is that it uses squared units, which makes it hard to intepret. Thus, the squareroot of the variance is used instead, known as the Standard Deviation . \\[ \\sigma = \\sqrt{Var(X)} \\] Similarly, standard deviation cannot be used to compare data with different units . Thus, the Coefficient of Variation is used instead, which is a unitless measure of the spread of the distribution. \\[ CV(X) = \\frac{\\sigma}{\\mu} \\] The third central moment is Skewness , which is a measure of the symmetry of distribution about the mean. Being left/right skewed means that the distribution has a \"longer tail\" on that side, which implies that values on the opposite side are more likely to occur . Note Skewness is also sometimes referred to as being Positively or Negatively Skewed . An easy way to remember is that positive values occur to the right of the origin, hence is the same as being right skewed; vice-versa. \\[ \\begin{aligned} \\text{Skewness} &= \\frac{E[(X - \\mu)^3]}{\\sigma^3} \\\\ &= \\frac{E[(X - \\mu)^3]}{(\\sigma^2)^\\frac{3}{2}} \\\\ &= \\frac{E(X^3) - 3 E(X^2) \\cdot E(X) + 2 [E(X)]^3}{(E(X^2) - [E(X)]^2)^\\frac{3}{2}} \\end{aligned} \\] The fourth central moment is Kurtosis , which is a measure of the flatness of the distribution, typically with respect to the normal distribution. It is indicative of the likelihood of producing extreme values (outliers). The normal distribution has a kurtosis of 3. If a distribution has a kurtosis greater than 3, then it is flatter and hence more likely to produce outliers as compared to the normal distribution. \\[ \\begin{aligned} \\text{Kurtosis} &= \\frac{E[(X - \\mu)^4]}{\\sigma^4} \\\\ &= \\frac{E[(X - \\mu)^4]}{(\\sigma^2)^2} \\\\ &= \\frac{E(X^4) - 4 E(X^3) \\cdot E(X) + 6 E(X^2) \\cdot [E(X)]^2 - 3 [E(X)]^4}{(E(X^2) - [E(X)]^2)^2} \\end{aligned} \\] Statistical Metrics \u00b6 Apart from the moment of the distribution, there are some other Statistical Metrics that provide useful information. The Median is the value of the random variable that seperates the upper and lower half of the probability distribution. For discrete variables, the median \\(M\\) is the smallest value such that \\(P(X \\le M) \\ge 0.5\\) and \\(P(X \\ge M) \\ge 0.5\\) . For continuous variables, the median \\(M\\) is found by solving \\(F(M) = 0.5\\) . The key difference is that the continuous median is the value that exactly seperates the distribution while the discrete one approximately splits it, depending on the PMF. The Mode is the value of the random variable that maximises the PMF or PDF. It is the most likely outcome of the experiment (loosely speaking for continuous variables). The Percentile is the value of the random variable below which a certain percentage of observations fall . For instance, the 85 th percentile is the value below which 85% of the observations fall. Let \\(p\\) be the percentage of observations. The 100p-th percentile for a discrete variable is the smallest value \\(a\\) such that \\(P(X \\lt a) \\le p \\lt P(X \\le a)\\) . Similar as before, for continuous variables, the percentile is found by solving \\(F(a) = p\\) . Note 100p-th looks strange because p is a percentage. For instance, if \\(p\\) is 0.85, then 100p is 85, representing the 85-th percentile. Also, the methods for Percentiles and Medians look similar because the median is simply the 50 th percentile! The 25 th , 50 th & 75 th percentile are known as the first, second & third Quartiles \\((q)\\) respectively. The difference between the 3 rd and 1 st quartile is known as the Inter Quartile Range . \\[ IQR = q_3 - q_1 \\] Shifting, Scaling & Transformation \u00b6 An existing random variable \\(X\\) can be adjusted in order to make a new random variable \\(Y\\) . If a constant \\(a\\) has been multiplied to the random variable, then it has been Scaled by \\(a\\) : \\[ \\begin{aligned} Y &= aX \\\\ \\\\ F_Y(y) &= P(Y \\le y) \\\\ &= P(aX \\le y) \\\\ &= P \\left(X \\le \\frac{y}{a} \\right) \\\\ &= F_X \\left(\\frac{y}{a} \\right) \\\\ \\\\ f_Y(y) &= \\frac{d}{dy} F_Y(y) \\\\ &= \\frac{1}{a} \\cdot f_X \\left(\\frac{y}{a} \\right) \\end{aligned} \\] If a constant \\(b\\) is added to the random variable instead, then it has been Shifted by \\(b\\) : \\[ \\begin{aligned} Y &= X + b \\\\ \\\\ F_Y(y) &= P(Y \\le y) \\\\ &= P(X + b \\le y) \\\\ &= P(X \\le y - b) \\\\ &= F_X (y - b) \\\\ \\\\ f_Y(y) &= \\frac{d}{dy} F_Y(y) \\\\ &= f_X (y - b) \\end{aligned} \\] For both scaling and transformation, the expectation and variance can be easily determined if that of the original is known as well: \\[ \\begin{aligned} E(cX) &= c \\cdot E(X) \\\\ E(X+c) &= E(X) + E(c) \\\\ &= E(X) + c \\\\ Var(cX) &= c^2 \\cdot Var(x) \\\\ Var(X+c) &= Var(X) + Var(c) \\\\ &= Var(X) \\end{aligned} \\] If the random variable has been raised by a power of \\(\\frac{1}{c}\\) where \\(c \\ne 1\\) , then it has been Power Transformed : \\[ \\begin{aligned} Y &= X^{\\frac{1}{c}} \\\\ \\\\ F_Y(y) &= P(Y \\le y) \\\\ &= P(X^{\\frac{1}{c}} \\le y) \\\\ &= P(X \\le y^c) \\\\ &= F_X (y^c) \\\\ \\\\ f_Y(y) &= \\frac{d}{dy} F_Y(y) \\\\ &= cy^{c-1}\\cdot f_X (y - b) \\end{aligned} \\] If the random variable has been exponentiated , then it has also been Exponential Transformed : \\[ \\begin{aligned} Y &= e^X \\\\ \\\\ F_Y(y) &= P(Y \\le y) \\\\ &= P(e^X \\le y) \\\\ &= P(X \\le \\ln y) \\\\ &= F_X (\\ln y) \\\\ \\\\ f_Y(y) &= \\frac{d}{dy} F_Y(y) \\\\ &= \\frac{1}{y} \\cdot f_X (\\ln y) \\end{aligned} \\] For both types of transformations, there is no simple method of determining the mean and variance. The various raw moments must be manually determined via integration . Generating Functions \u00b6 Another characteristic PGF and MGF uniquely identify the distribution Moment Generating Function \u00b6 Both discrete and continuous \\[ \\begin{aligned} M_x(t) &= E(e^{tX}) \\\\ &= \\sum e^{tx} \\cdot p(x) \\end{aligned} \\] As its name suggests, the MGF can be used to calculate the raw moments of the distribution. To obtain the k-th raw moment , Differentiate the MGF k times Evaluate the expression at \\(t=0\\) $$ $$ However, the main benefit of MGFs is that they uniquely identify a distribution. If two random variables have the same MGF , then they have the same distribution . This becomes especially useful when dealing with linear combinations of INDEPENDENT random variables . By determining the MGF of the combination, its exact distribution can be determined. \\[ \\begin{aligned} Y &= X_1 + X_2 + ... + X_k \\\\ \\\\ M_Y(t) &= E(e^{tY}) \\\\ &= E(e^{t(X_1 + X_2 + ... + X_k)}) \\\\ &= E(e^{tX_1} \\cdot e^{tX_2} \\cdot ... \\cdot e^{tX_k}) \\\\ &= \\prod E(e^{tx}) \\\\ &= \\prod M_x(t) \\end{aligned} \\] Probability Generating Function \u00b6 Discrete only \\[ \\begin{aligned} P(t) &= E(t^x) \\\\ &= \\sum t^X \\cdot p(x) \\end{aligned} \\] As its name suggests, the PGF can be used to calculate the individual probabilities of the distribution. To obtain the probability of the k-th value , Differentiate the PGF k times Divide the expression by k factorial Evaluate the expression at \\(t=0\\) $$ $$ Additionally moments $$ \\begin{aligned} \\end{aligned} $$ Uniquely identifies distribution Conditional Distributions \u00b6 The concept of conditional probabilities can be extended to Random Variables as well. In particular, one random variable can be conditional on another random variable, resulting in a Conditional Distribution \\((X|Y)\\) . Most problems require us to find the Unconditional Distribution given only the conditional distributions: \\(X\\) is the random variable denoting the test grades (A, B, C) \\(Y\\) is the random variable denoting the gender (M, F) The teacher would like to find the overall distribution of test scores \\((X)\\) , but only has the conditional distribution of the scores of the students for each gender \\((X|Y)\\) and the proportion of the Genders \\((Y)\\) . The Law of Total Probability can be used to determine the unconditional probability of \\(X\\) : \\[ \\begin{aligned} P(X = a) &= E_Y[P(X = a|Y)] \\\\ &= \\sum_{y} P(X = a | y) \\cdot p(Y = y) \\end{aligned} \\] Note that this is equivalent to adding up the final probabilities from the relevant branches from a probability tree: Naturally, this also means that the unconditional CDF can be obtained in similar fashion: \\[ \\begin{aligned} F(a) &= E_Y[F(a)] \\\\ &= \\sum_{y} F(a) \\cdot p(Y = y) \\end{aligned} \\] Following the same logic, the Law of Total Expectation can be used to determine the unconditional expectation of \\(x\\) : \\[ \\begin{aligned} E(X) &= E_Y[E_X(X|Y)] \\\\ &= \\sum_{y} E_X(X|Y) \\cdot p(Y = y) \\end{aligned} \\] The Law of Total Variance can be used to determine the unconditional variance of \\(x\\) . However, unlike the previous two, it is NOT simply the expectation of the conditional variance: \\[ \\begin{aligned} Var (X) &= E_Y [Var_X(X|Y)] + Var_Y[E_X(X|Y)] \\\\ \\\\ E_Y [Var_X(X|Y)] &= \\sum_{y} Var_X(X|Y) \\cdot p(Y = y) \\\\ \\\\ Var_Y[E_X(X|Y)] &= E_Y[E_X(X|Y)^2] - (E_Y[E_X(X|Y)])^2 \\\\ &= \\sum_{y} E_X(X|Y)^2 \\cdot p(Y = y) - \\sum_{y} E_X(X|Y) \\cdot p(Y = y) \\end{aligned} \\] Alternatively, the Unconditional Variance can be directly calculated using the typical formula of \\(E(X^2) - [E(X)]^2\\) , where the two unconditional expectations are calculated using the law of total expectation. Alternatively once more, if the conditional distribution \\(Y\\) only has two outcomes, then the Bernoulli Shortcut (covered in a later section) can be used to quickly compute the value of \\(Var_Y[E_X(X|Y)]\\) . Note The discrete case was shown in this section due to its simplicity. All the same concepts apply to the continuous variables as well - simply replace the summation & PMFs with integrals and PDFs. Mixture Distributions \u00b6 A mixture distribution is a distribution whose values can be intepreted as being derived from an underlying set of other random variables . In an insurance context, a Homeowners Insurance claim could be from a fire, burglary or liability accident. To model it, we could use a mixture that is made up of the basic distributions used to individually model each type of accident. If the mixture contains a countable number of other distributions, then it is known as a Discrete Mixture . Otherwise, it is known as a Continuous Mixture . Warning It is a common mistake to think that a discrete mixture is only made up of discrete distributions, vice-versa. Any type of distribution can be included in a mixture; the classification is based on the number of distributions. The random variable \\(X\\) is a k-point mixture if its probability functions can be expressed as the weighted average of the probability functions of the \\(k\\) distributions \\(X_1, X_2, ... X_k\\) : \\[ \\begin{aligned} F_X(x) &= w_1 \\cdot F_{X_1}(x) + w_2 \\cdot F_{X_2}(x) + ... + w_k \\cdot F_{X_k}(x) \\\\ f_X(x) &= w_1 \\cdot f_{X_1}(x) + w_2 \\cdot f_{X_2}(x) + ... + w_k \\cdot f_{X_k}(x) \\end{aligned} \\] Note For this exam, questions will usually only use 2 or 3 point mixtures . \\(w\\) represents the mixing weights , such that \\(w_1 + w_2 + ... + w_k = 1\\) . It can be intepreted that \\(Y\\) follows the distribution of \\(X_1\\) \\(100w_1 \\%\\) of the time, follows the distribution of \\(X_2\\) \\(100w_2 \\%\\) of the time etc. Warning Another common mistake is confusing mixtures with Linear Combinations of random variables: \\[ X \\ne w_1 X_1 + w_2 X_2 + ... + w_k X_k \\] In a linear combination, \\(X\\) neither follows the distribution of any of the \\(X_k\\) . Furthermore, since \\(w_k\\) are not weights, they can be any real number and do not need to sum to 1 . The mixing weights can also be thought of as Discrete Probabilities that come from a random variable \\(Y\\) with the support \\(\\set{1, 2, ..., k}\\) . Thus, we can think of the overall mixture \\(X\\) as an unconditional distribution while each of the underlying distributions are conditional distributions \\(X|Y\\) . This allows us to make use of the all the previous results from the conditional distributions: \\[ \\begin{aligned} P(X = a) &= E_Y[P(X = a|Y)] \\\\ F(a) &= E_Y[F(a)] \\\\ E(X) &= E_Y[E_X(X|Y)] \\\\ Var (X) &= E_Y [Var_X(X|Y)] + Var_Y[E_X(X|Y)] \\end{aligned} \\]","title":"Review of Probability Theory"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#review-of-probability-theory","text":"","title":"Review of Probability Theory"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#basic-probability","text":"Probability is the study of Experiments whose results cannot be predicted with certainty. The result of such an experiment is known as its Outcome . The Sample Space \\(\\left(\\Omega \\right)\\) is the set of ALL possible outcomes from an experiment. The Event Space \\((E)\\) is a subset of the sample space, representing only the outcomes that we are interested in studying. Conversely, its Complement \\((E^c)\\) is the set of all OTHER outcomes not inside \\(E\\) . The probability of the event occuring is the ratio of the number of elements in the event to the sample space. It is a measure of the chance that the outcome of the experiment is inside the event space. \\[ P(E) = \\frac{n(E)}{n\\left(\\Omega \\right)} \\] Consider the probability of rolling an odd number on a standard dice: Experiment - Rolling a dice Outcome - The number showed on the dice Sample Space - \\({1, 2, 3, 4, 5, 6}\\) Event Space - \\({1, 3, 5}\\) Complement - \\({2, 4, 6}\\) Probability of Event - \\(\\frac{3}{6}\\) Probability of Complement - \\(\\frac{3}{6}\\) Within the same experiment, there may be multiple events of interest. For any two events A and B, its Union \\((A \\cup B)\\) is the set with outcomes that are either in A or B while their Intersection \\((A \\cap B)\\) is the set with outcomes that are in BOTH A and B . If both A and B have no outcomes in common \\((A \\cap B = \\emptyset)\\) , then they are said to be Mutually Exclusive . Naturally, an event and its complement are always mutually exclusive. Warning The following seems intuitive , but is actually a common mistake: \\[ (A \\cap B)^c \\ne A^c \\cap B^c \\] This is properly explained through De-morgans Law : It can be easily remembered by applying the complement to all components of the expression, including the intersection/union symbol : \\[ \\begin{aligned} \\cap^c &= \\cup \\\\ \\cup^c &= \\cap \\end{aligned} \\]","title":"Basic Probability"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#probability-axioms","text":"Axiom 1 states that all probabilities must be non-negative : \\[ P(E) \\ge 0 \\] Axiom 2 states that probability of the Sample Space is exactly equal to 1: \\[ P(\\Omega) = 1 \\] Axiom 3 states the probability of a union of mutually exclusive events is equal to the sum of their probabilities, known as Countable Additivity . \\[ \\begin{aligned} A \\cap B &= \\emptyset \\\\ P(A \\cup B) &= P(A) + P(B) \\end{aligned} \\] Based on these axioms, several other important properties can also be deduced: Monoticity : \\(A \\subset B \\rightarrow P(A) \\le P(B)\\) Empty Set : \\(P(\\emptyset) = 0\\) Complement Rule : \\(P(E^c) = 1 - P(E)\\) Numeric Bound : \\(0 \\le P(E) \\le 1\\) Sum Rule : \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)","title":"Probability Axioms"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#conditional-probability","text":"Conditional Probabilities are denoted by \\(P(A|B)\\) , which is the probability of event A occuring given that event B has already occurred . The intuition is best understood by considering the following - Given that event B has already occured, what is the probability that event A also occurs? The event space is \\(A \\cap B\\) , as we are interested in the probability that both A and B occur. However, since event B has already occured, the sample space is no longer all possible outcomes but rather the event space for B! \\[ \\begin{aligned} P(A|B) &= \\frac{n(A \\cap B)}{n(B)} \\\\ &= \\frac{\\frac{n(A \\cap B)}{n(\\Omega)}}{\\frac{n(B)}{n(\\Omega)}} \\\\ &= \\frac{P(A \\cap B)}{P(B)} \\end{aligned} \\] Following this expression, the probability of an intersection of two events is given by: \\[ P(A \\cap B) = P(A|B) \\cdot P(B) = P(B|A) \\cdot P(A) \\] Most experiments involving conditional probabilities are multi-staged experiments, which are best visualized using Probability Trees : Instead of calculating conditional probabilities from scratch, some questions provide the conditional probability \\(P(A|B)\\) (or the components to do so!) and ask us to find the reverse - \\(P(B|A)\\) . \\[ P(B|A) = \\frac{P(B \\cap A)}{P(A)} \\] The formula is the same as before, but the issue is that the unconditional probability of event A is usually not given. This problem is accounted for in Bayes Theorem : \\[ \\begin{aligned} A &= (A \\cap B) + (A \\cap B^c) \\\\ P(A) &= P(A \\cap B) + P(A \\cap B^c) \\\\ P(A) &= P(A|B) \\cdot P(B) + P(A|B^c) \\cdot P(B^c) \\\\ \\\\ \\therefore P(B|A) &= \\frac{P(A|B) \\cdot P(B)}{P(A|B) \\cdot P(B) + P(A|B^c) \\cdot P(B^c)} \\end{aligned} \\] Note that if the Conditional Probability of A given B is the same as the unconditional probability of A, then events A and B are independent ; B has no effect on A. Thus, the probability of an intersection of two independent events is simply their product: \\[ \\begin{aligned} P(A|B) &= \\frac{P(A \\cap B)}{P(B)} \\\\ P(A) &= \\frac{P(A \\cap B)}{P(B)} \\\\ P(A \\cap B) &= P(A) \\cdot P(B) \\end{aligned} \\]","title":"Conditional Probability"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#random-variables","text":"Unlike rolling a dice, the outcome of most experiments are non-numeric , which makes them hard to work with. For instance, the outcomes of a coin toss are \"Heads\" and \"Tails\". A Random Variable is a many to one function that maps each outcome to a single real number. Each outcome must have only one corresponding number, but different outcomes can have the same value. Note Although the mapping is deterministic, the underlying experiment is still random which is why it is still a \"random\" variable. The range of possible values that the random variable can take is known as its Support . They are broadly categorized based on its support: Discrete Continuous Countable Support Uncountable Support 1, 2, 3, 4, ... 1, 1.1, 1.01, 1.001, ... Random variables are denoted using upper case letters (X, Y, Z) while their corresponding values are denoted using lower case letters (x, y, z) and their appropriate subscripts . The notation \\(X(s) = x_1\\) denotes that the random variable \\(X\\) maps the outcome \\(s\\) to the value of \\(x_1\\) . Thus, the corresponding probability is denoted by \\(P(X = x_1)\\) .","title":"Random Variables"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#probability-distributions","text":"Similar to how a random variable maps the outcomes to a real number, a Probability Distribution is a function that maps the outcomes to its probability of occurrence . For Discrete Random Variables , their distribution is described using a Probability Mass Function (PMF). The PMF provides the probability that the random variable is exactly equal to some value \\((X = x_1)\\) . It is typically denoted in lower case and sometimes includes a subscript of the random variable when working with multiple to distinguish them from one another. \\[ \\begin{aligned} P(X = a) &= p(a) \\\\ \\\\ P(X = a) &= p_X(a) \\\\ P(Y = a) &= p_Y(a) \\end{aligned} \\] Since it is a probability measure, the sum of the PMF over the support of the random variable must be equal to 1 (Probability Axiom). \\[ \\sum_{x \\in \\text{Support}} p(x) = 1 \\] PMFs can be represented in three main ways - Functions, Tables or Histograms. For Continuous Random Variables , their distribution is described using a Probability Density Function (PDF). The PDF is a non-negative function where the area under it provides the probability that the random variable takes on some range of values \\((a \\le X \\le b)\\) . Similarly, it is typically denoted in lower case and includes a subscript when working with multiple random variables: \\[ \\begin{aligned} P(a \\le X \\le b) = \\int^b_a f(x) \\\\ \\\\ P(a \\le X \\le b) = \\int^b_a f_X(x) \\\\ P(a \\le Y \\le b) = \\int^b_a f_Y(y) \\end{aligned} \\] Similarly, since the area is a probability measure, the total area under the graph must be equal to 1 : \\[ P(-\\infty \\le X \\le \\infty) = \\int^{\\infty}_{-\\infty} f(x) = 1 \\] Note \\(\\infty\\) is used as a catch all for the upper and lower bound of the random variable. If the actual bounds are known, then using them instead is more appropriate. Additionally, note that the probability of a specific value for a continuous RV is 0. This is because there is an infinite number of possible values , thus the probability of a specific value (EG. 1.45679383920) is infinitely small such that it is assumed to be 0. \\[ P(X = a) = \\int^{a}_{a} f(x) = 0 \\] The Cumulative Distribution Function (CDF) is the probability that the random variable is less than or equal to some value \\(X \\le t\\) . It is typically denoted in upper case to distinguish it from the PDF and includes subscripts as well when working with multiple random variables. \\[ \\begin{aligned} F(t) &= P(X < t) \\\\ \\\\ F_X(t) &= P(X < t) \\\\ F_Y(t) &= P(Y < t) \\end{aligned} \\] For discrete variables, the CDF is the sum of all probabilities before the specified value . Following this, the difference of consecutive CDFs allows us to obtain the PMFs at that value: \\[ \\begin{aligned} F(t) &= \\sum_{x \\le t} p(x) \\\\ p(x_i) &= F(x_i) - F(x_{i-1}) \\end{aligned} \\] For continuous variables, the CDF is the integral from the lower bound to the specified value . However, instead of integrating with respect to an actual value, it is better to integrate with respect to a dummy variable \\(t\\) to obtain a general expression for the CDF , allowing it to be easily calculated for any value. Although the CDF is very useful, it can only be used to calculate probabilities starting from the lower bound . When probabilities starting from other ranges are needed, the PDF can be obtained from the CDF by differentiating it and then re-integrating with different limits. \\[ \\begin{aligned} F(t) &= \\int^{t}_{-\\infty} f(x) dx \\\\ \\\\ F(t) &= \\int^{t}_{-\\infty} f(t) dt \\\\ f(t) &= F'(t) \\end{aligned} \\]","title":"Probability Distributions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#moments","text":"The Moments of a distribution are quantities that describe characteristics of the distribution . Raw Moments are calculated with respect to the origin . The n-th raw moment is calculated as the following: \\[ \\begin{aligned} E(X^n) &= \\int x^n \\cdot f(x) dx \\\\ \\mu'_k &= \\sum x^n \\cdot p(x) \\end{aligned} \\] The first raw moment is known as the Mean , which is a measure of the Centrality of the distribution. It is commonly denoted as \\(\\mu\\) , without any super or subscripts. It has several important properties (more at the section on transformation): \\(E(c) = c\\) \\(E(X + Y) = E(X) + E(Y)\\) Central Moments are calculated with respect to the mean . The n-th central moment is calculated as the following: \\[ \\begin{aligned} E[(X - \\mu)^n] &= \\int (x - \\mu)^n \\cdot f(x) dx \\\\ \\mu_k&= \\sum (x - \\mu)^n \\cdot p(x) \\end{aligned} \\] The second central moment is known as the Variance , which is a measure of the Spread of the distribution about the mean. Since calculating central moments directly is complicated, it can be simplified to an expression involving raw moments: \\[ \\begin{aligned} Var(X) &= E[(X - \\mu)^2] \\\\ &= E(X^2 - 2\\mu X + \\mu^2) \\\\ &= E(X^2) - 2\\mu^2 + \\mu^2 \\\\ &= E(X^2) - \\mu^2 \\\\ &= E(X^2) - [E(X)]^2 \\end{aligned} \\] It has several important properties (more at the section on transformation): \\(Var(c) = 0\\) \\(Var(X+b) = Var(X) + Var (Y) + 2 \\cdot Cov(X,Y)\\) However, one problem with variance is that it uses squared units, which makes it hard to intepret. Thus, the squareroot of the variance is used instead, known as the Standard Deviation . \\[ \\sigma = \\sqrt{Var(X)} \\] Similarly, standard deviation cannot be used to compare data with different units . Thus, the Coefficient of Variation is used instead, which is a unitless measure of the spread of the distribution. \\[ CV(X) = \\frac{\\sigma}{\\mu} \\] The third central moment is Skewness , which is a measure of the symmetry of distribution about the mean. Being left/right skewed means that the distribution has a \"longer tail\" on that side, which implies that values on the opposite side are more likely to occur . Note Skewness is also sometimes referred to as being Positively or Negatively Skewed . An easy way to remember is that positive values occur to the right of the origin, hence is the same as being right skewed; vice-versa. \\[ \\begin{aligned} \\text{Skewness} &= \\frac{E[(X - \\mu)^3]}{\\sigma^3} \\\\ &= \\frac{E[(X - \\mu)^3]}{(\\sigma^2)^\\frac{3}{2}} \\\\ &= \\frac{E(X^3) - 3 E(X^2) \\cdot E(X) + 2 [E(X)]^3}{(E(X^2) - [E(X)]^2)^\\frac{3}{2}} \\end{aligned} \\] The fourth central moment is Kurtosis , which is a measure of the flatness of the distribution, typically with respect to the normal distribution. It is indicative of the likelihood of producing extreme values (outliers). The normal distribution has a kurtosis of 3. If a distribution has a kurtosis greater than 3, then it is flatter and hence more likely to produce outliers as compared to the normal distribution. \\[ \\begin{aligned} \\text{Kurtosis} &= \\frac{E[(X - \\mu)^4]}{\\sigma^4} \\\\ &= \\frac{E[(X - \\mu)^4]}{(\\sigma^2)^2} \\\\ &= \\frac{E(X^4) - 4 E(X^3) \\cdot E(X) + 6 E(X^2) \\cdot [E(X)]^2 - 3 [E(X)]^4}{(E(X^2) - [E(X)]^2)^2} \\end{aligned} \\]","title":"Moments"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#statistical-metrics","text":"Apart from the moment of the distribution, there are some other Statistical Metrics that provide useful information. The Median is the value of the random variable that seperates the upper and lower half of the probability distribution. For discrete variables, the median \\(M\\) is the smallest value such that \\(P(X \\le M) \\ge 0.5\\) and \\(P(X \\ge M) \\ge 0.5\\) . For continuous variables, the median \\(M\\) is found by solving \\(F(M) = 0.5\\) . The key difference is that the continuous median is the value that exactly seperates the distribution while the discrete one approximately splits it, depending on the PMF. The Mode is the value of the random variable that maximises the PMF or PDF. It is the most likely outcome of the experiment (loosely speaking for continuous variables). The Percentile is the value of the random variable below which a certain percentage of observations fall . For instance, the 85 th percentile is the value below which 85% of the observations fall. Let \\(p\\) be the percentage of observations. The 100p-th percentile for a discrete variable is the smallest value \\(a\\) such that \\(P(X \\lt a) \\le p \\lt P(X \\le a)\\) . Similar as before, for continuous variables, the percentile is found by solving \\(F(a) = p\\) . Note 100p-th looks strange because p is a percentage. For instance, if \\(p\\) is 0.85, then 100p is 85, representing the 85-th percentile. Also, the methods for Percentiles and Medians look similar because the median is simply the 50 th percentile! The 25 th , 50 th & 75 th percentile are known as the first, second & third Quartiles \\((q)\\) respectively. The difference between the 3 rd and 1 st quartile is known as the Inter Quartile Range . \\[ IQR = q_3 - q_1 \\]","title":"Statistical Metrics"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#shifting-scaling-transformation","text":"An existing random variable \\(X\\) can be adjusted in order to make a new random variable \\(Y\\) . If a constant \\(a\\) has been multiplied to the random variable, then it has been Scaled by \\(a\\) : \\[ \\begin{aligned} Y &= aX \\\\ \\\\ F_Y(y) &= P(Y \\le y) \\\\ &= P(aX \\le y) \\\\ &= P \\left(X \\le \\frac{y}{a} \\right) \\\\ &= F_X \\left(\\frac{y}{a} \\right) \\\\ \\\\ f_Y(y) &= \\frac{d}{dy} F_Y(y) \\\\ &= \\frac{1}{a} \\cdot f_X \\left(\\frac{y}{a} \\right) \\end{aligned} \\] If a constant \\(b\\) is added to the random variable instead, then it has been Shifted by \\(b\\) : \\[ \\begin{aligned} Y &= X + b \\\\ \\\\ F_Y(y) &= P(Y \\le y) \\\\ &= P(X + b \\le y) \\\\ &= P(X \\le y - b) \\\\ &= F_X (y - b) \\\\ \\\\ f_Y(y) &= \\frac{d}{dy} F_Y(y) \\\\ &= f_X (y - b) \\end{aligned} \\] For both scaling and transformation, the expectation and variance can be easily determined if that of the original is known as well: \\[ \\begin{aligned} E(cX) &= c \\cdot E(X) \\\\ E(X+c) &= E(X) + E(c) \\\\ &= E(X) + c \\\\ Var(cX) &= c^2 \\cdot Var(x) \\\\ Var(X+c) &= Var(X) + Var(c) \\\\ &= Var(X) \\end{aligned} \\] If the random variable has been raised by a power of \\(\\frac{1}{c}\\) where \\(c \\ne 1\\) , then it has been Power Transformed : \\[ \\begin{aligned} Y &= X^{\\frac{1}{c}} \\\\ \\\\ F_Y(y) &= P(Y \\le y) \\\\ &= P(X^{\\frac{1}{c}} \\le y) \\\\ &= P(X \\le y^c) \\\\ &= F_X (y^c) \\\\ \\\\ f_Y(y) &= \\frac{d}{dy} F_Y(y) \\\\ &= cy^{c-1}\\cdot f_X (y - b) \\end{aligned} \\] If the random variable has been exponentiated , then it has also been Exponential Transformed : \\[ \\begin{aligned} Y &= e^X \\\\ \\\\ F_Y(y) &= P(Y \\le y) \\\\ &= P(e^X \\le y) \\\\ &= P(X \\le \\ln y) \\\\ &= F_X (\\ln y) \\\\ \\\\ f_Y(y) &= \\frac{d}{dy} F_Y(y) \\\\ &= \\frac{1}{y} \\cdot f_X (\\ln y) \\end{aligned} \\] For both types of transformations, there is no simple method of determining the mean and variance. The various raw moments must be manually determined via integration .","title":"Shifting, Scaling &amp; Transformation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#generating-functions","text":"Another characteristic PGF and MGF uniquely identify the distribution","title":"Generating Functions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#moment-generating-function","text":"Both discrete and continuous \\[ \\begin{aligned} M_x(t) &= E(e^{tX}) \\\\ &= \\sum e^{tx} \\cdot p(x) \\end{aligned} \\] As its name suggests, the MGF can be used to calculate the raw moments of the distribution. To obtain the k-th raw moment , Differentiate the MGF k times Evaluate the expression at \\(t=0\\) $$ $$ However, the main benefit of MGFs is that they uniquely identify a distribution. If two random variables have the same MGF , then they have the same distribution . This becomes especially useful when dealing with linear combinations of INDEPENDENT random variables . By determining the MGF of the combination, its exact distribution can be determined. \\[ \\begin{aligned} Y &= X_1 + X_2 + ... + X_k \\\\ \\\\ M_Y(t) &= E(e^{tY}) \\\\ &= E(e^{t(X_1 + X_2 + ... + X_k)}) \\\\ &= E(e^{tX_1} \\cdot e^{tX_2} \\cdot ... \\cdot e^{tX_k}) \\\\ &= \\prod E(e^{tx}) \\\\ &= \\prod M_x(t) \\end{aligned} \\]","title":"Moment Generating Function"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#probability-generating-function","text":"Discrete only \\[ \\begin{aligned} P(t) &= E(t^x) \\\\ &= \\sum t^X \\cdot p(x) \\end{aligned} \\] As its name suggests, the PGF can be used to calculate the individual probabilities of the distribution. To obtain the probability of the k-th value , Differentiate the PGF k times Divide the expression by k factorial Evaluate the expression at \\(t=0\\) $$ $$ Additionally moments $$ \\begin{aligned} \\end{aligned} $$ Uniquely identifies distribution","title":"Probability Generating Function"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#conditional-distributions","text":"The concept of conditional probabilities can be extended to Random Variables as well. In particular, one random variable can be conditional on another random variable, resulting in a Conditional Distribution \\((X|Y)\\) . Most problems require us to find the Unconditional Distribution given only the conditional distributions: \\(X\\) is the random variable denoting the test grades (A, B, C) \\(Y\\) is the random variable denoting the gender (M, F) The teacher would like to find the overall distribution of test scores \\((X)\\) , but only has the conditional distribution of the scores of the students for each gender \\((X|Y)\\) and the proportion of the Genders \\((Y)\\) . The Law of Total Probability can be used to determine the unconditional probability of \\(X\\) : \\[ \\begin{aligned} P(X = a) &= E_Y[P(X = a|Y)] \\\\ &= \\sum_{y} P(X = a | y) \\cdot p(Y = y) \\end{aligned} \\] Note that this is equivalent to adding up the final probabilities from the relevant branches from a probability tree: Naturally, this also means that the unconditional CDF can be obtained in similar fashion: \\[ \\begin{aligned} F(a) &= E_Y[F(a)] \\\\ &= \\sum_{y} F(a) \\cdot p(Y = y) \\end{aligned} \\] Following the same logic, the Law of Total Expectation can be used to determine the unconditional expectation of \\(x\\) : \\[ \\begin{aligned} E(X) &= E_Y[E_X(X|Y)] \\\\ &= \\sum_{y} E_X(X|Y) \\cdot p(Y = y) \\end{aligned} \\] The Law of Total Variance can be used to determine the unconditional variance of \\(x\\) . However, unlike the previous two, it is NOT simply the expectation of the conditional variance: \\[ \\begin{aligned} Var (X) &= E_Y [Var_X(X|Y)] + Var_Y[E_X(X|Y)] \\\\ \\\\ E_Y [Var_X(X|Y)] &= \\sum_{y} Var_X(X|Y) \\cdot p(Y = y) \\\\ \\\\ Var_Y[E_X(X|Y)] &= E_Y[E_X(X|Y)^2] - (E_Y[E_X(X|Y)])^2 \\\\ &= \\sum_{y} E_X(X|Y)^2 \\cdot p(Y = y) - \\sum_{y} E_X(X|Y) \\cdot p(Y = y) \\end{aligned} \\] Alternatively, the Unconditional Variance can be directly calculated using the typical formula of \\(E(X^2) - [E(X)]^2\\) , where the two unconditional expectations are calculated using the law of total expectation. Alternatively once more, if the conditional distribution \\(Y\\) only has two outcomes, then the Bernoulli Shortcut (covered in a later section) can be used to quickly compute the value of \\(Var_Y[E_X(X|Y)]\\) . Note The discrete case was shown in this section due to its simplicity. All the same concepts apply to the continuous variables as well - simply replace the summation & PMFs with integrals and PDFs.","title":"Conditional Distributions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#mixture-distributions","text":"A mixture distribution is a distribution whose values can be intepreted as being derived from an underlying set of other random variables . In an insurance context, a Homeowners Insurance claim could be from a fire, burglary or liability accident. To model it, we could use a mixture that is made up of the basic distributions used to individually model each type of accident. If the mixture contains a countable number of other distributions, then it is known as a Discrete Mixture . Otherwise, it is known as a Continuous Mixture . Warning It is a common mistake to think that a discrete mixture is only made up of discrete distributions, vice-versa. Any type of distribution can be included in a mixture; the classification is based on the number of distributions. The random variable \\(X\\) is a k-point mixture if its probability functions can be expressed as the weighted average of the probability functions of the \\(k\\) distributions \\(X_1, X_2, ... X_k\\) : \\[ \\begin{aligned} F_X(x) &= w_1 \\cdot F_{X_1}(x) + w_2 \\cdot F_{X_2}(x) + ... + w_k \\cdot F_{X_k}(x) \\\\ f_X(x) &= w_1 \\cdot f_{X_1}(x) + w_2 \\cdot f_{X_2}(x) + ... + w_k \\cdot f_{X_k}(x) \\end{aligned} \\] Note For this exam, questions will usually only use 2 or 3 point mixtures . \\(w\\) represents the mixing weights , such that \\(w_1 + w_2 + ... + w_k = 1\\) . It can be intepreted that \\(Y\\) follows the distribution of \\(X_1\\) \\(100w_1 \\%\\) of the time, follows the distribution of \\(X_2\\) \\(100w_2 \\%\\) of the time etc. Warning Another common mistake is confusing mixtures with Linear Combinations of random variables: \\[ X \\ne w_1 X_1 + w_2 X_2 + ... + w_k X_k \\] In a linear combination, \\(X\\) neither follows the distribution of any of the \\(X_k\\) . Furthermore, since \\(w_k\\) are not weights, they can be any real number and do not need to sum to 1 . The mixing weights can also be thought of as Discrete Probabilities that come from a random variable \\(Y\\) with the support \\(\\set{1, 2, ..., k}\\) . Thus, we can think of the overall mixture \\(X\\) as an unconditional distribution while each of the underlying distributions are conditional distributions \\(X|Y\\) . This allows us to make use of the all the previous results from the conditional distributions: \\[ \\begin{aligned} P(X = a) &= E_Y[P(X = a|Y)] \\\\ F(a) &= E_Y[F(a)] \\\\ E(X) &= E_Y[E_X(X|Y)] \\\\ Var (X) &= E_Y [Var_X(X|Y)] + Var_Y[E_X(X|Y)] \\end{aligned} \\]","title":"Mixture Distributions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/","text":"Frequency Models \u00b6 Frequency Distributions \u00b6 The number of claims is usually known as the Loss Frequency and is typically modelled with a discrete distribution. Let \\(N\\) be the random variable denoting the number of claims. This section will cover all the relevant frequency distributions, but note that there is no need to memorize anything as they are all provided in the formula sheet. Poisson Distribution \u00b6 The Poisson Distribution is used to count the number of random events in a specified unit of space or time . It only has one parameter \\(\\lambda\\) , the mean number of occurrences in the specified unit of space or time. The key property is that its mean and variance are both equal to \\(\\lambda\\) . \\[ \\begin{aligned} N &\\sim \\text{Poisson}(\\lambda) \\\\ \\\\ p_n &= \\frac{e^{-\\lambda} \\cdot \\lambda^k}{k!} \\\\ \\\\ E(N) &= \\lambda \\\\ Var (N) &= \\lambda \\end{aligned} \\] The sum of \\(k\\) independent poisson random variables is still poisson : \\[ \\begin{aligned} N_n &\\sim \\text{Poisson}(\\lambda_k) \\\\ N &= N_1 + N_2 + ... + N_k \\\\ \\therefore N &\\sim \\text{Poisson} (\\lambda_1 + \\lambda_2 + ... + \\lambda_k) \\end{aligned} \\] Bernoulli Distribution \u00b6 The Bernoulli Distribution is used to determine the outcome of a Bernoulli Trial . They are experiments with only two possible outcomes . For a Standard Bernoulli Distribution , the two outcomes are Successes and Failures , denoted by 1 and 0 respectively: \\[ \\begin{aligned} X &\\sim \\text{Bernoulli} (q) \\\\ \\\\ p_x &= \\begin{cases} \\text{Success } (1),& \\text{Probability} = q, \\\\ \\text{Failure } (0),& \\text{Probability} = 1-q \\end{cases} \\\\ \\\\ E(X) &= q \\\\ Var (X) &= q(1-q) \\end{aligned} \\] However, the Bernoulli Shortcut generalizes this for any two mutually exclusive outcomes , denoted by \\(a\\) and \\(b\\) respectively: \\[ \\begin{aligned} Y &= (a-b)X + b \\\\ \\\\ Y &= \\begin{cases} \\text{Outcome 1 } (a),& \\text{Probability} = q, \\\\ \\text{Outcome 2 } (b),& \\text{Probability} = 1-q \\end{cases} \\\\ \\\\ E(Y) &= E[(a-b)X + b] \\\\ &= (a-b)q + b \\\\ \\\\ Var (Y) &= Var[(a-b)X + b] \\\\ &= (a-b)^2 \\cdot q(1-q) \\end{aligned} \\] Note that there is no need to memorize the mean and variance for the bernoulli shortcut - they can be easily derived from the standard distribution. Binomial Distribution \u00b6 The Binomial Distribution is used to count the number of successes out of a fixed number of independent Standard Bernoulli Trials . It has two parameters : The number of independent trials, \\(m\\) The probability of success for each trial, \\(q\\) \\[ \\begin{aligned} N &\\sim \\text{Binomial} (m, q) \\\\ \\\\ p_n &= {m \\choose k} q^k (1-q)^{1-k} \\\\ \\\\ E(N) &= nq \\\\ Var (N) &= nq(1-q) \\end{aligned} \\] The binomial distribution is actually the sum of \\(m\\) independent standard bernoulli variables with the same probability : \\[ \\begin{aligned} X_k &\\sim \\text{Bernoulli} (q) \\\\ N &= X_1 + X_2 + ... + X_m \\\\ \\therefore N &\\sim \\text{Binomial} (m, q) \\end{aligned} \\] Note A bernoulli distribution is simply a binomial distribution with \\(m=1\\) . Thus, the sum of \\(k\\) independent binomial variables with the same probability will still binomial : \\[ \\begin{aligned} N_k &\\sim \\text{Binomial}(m, q) \\\\ N &= N_1 + N_2 + ... + N_k \\\\ \\therefore N &\\sim \\text{Binomial} (m_1 + m_2 + ... + m_k, q) \\end{aligned} \\] This is intuitive, because the sum of binomial variables is actually the sum of even more bernoulli variables , which as shown previously, will follow the binomial distribution. Geometric Distribution \u00b6 The Geometric Distribution can be understood in one of two ways, with respect to a sequence of independent bernoulli trials : Number of trials needed to get the first success , \\(N \\in \\set{1, 2, 3, ...}\\) Number of failures needed to get the first success , \\(F \\in \\set{0, 1, 2, ...}\\) If not explicitly stated, the two intepretations can be distinguished from their supports - there must be at least 1 trial but there can be 0 failures. It is useful to remember just one intepretation and understand how they are related: \\[ F = N-1 \\] The second intepretation is preferred , since that is what is provided in the formula sheet. Regardless, the Geometric distribution only has one parameter , the probability of success: \\[ \\begin{aligned} N &\\sim \\text{Geom} (q) \\\\ \\\\ p_k &= (1-q)^{k} \\cdot q \\\\ \\\\ E(N) &= \\frac{1-p}{p} \\\\ Var (N) &= \\frac{1-p}{p^2} \\end{aligned} \\] The key property is that it is Memoryless . Intuitively, the geometric distribution is \"waiting\" for the first success to occur. However, it does not matter how many failures have occurred , the probability of the first success occuring in another \\(k\\) failures is independent of the history of the process . The distribution \"forgets\" (memoryless) the current state that the process is in. \\[ P(N > m + n | N \\ge m) = P(N > n) \\] Negative Binomial Distribution \u00b6 The Negative Binomial Distribution is used to count the number of failures to get a fixed number of successes , with respect to a sequence of independent bernoulli trials. It has two parameters: The number of successes, \\(r\\) The probability of success, \\(q\\) \\[ \\begin{aligned} N &\\sim \\text{Negative Binomial} (r, q) \\\\ \\\\ p_n &= {{k+r-1} \\choose {k}} (1-p)^k p^{r} \\\\ \\\\ E(N) &= \\frac{r(1-p)}{p} \\\\ Var (N) &= \\frac{r(1-p)}{p^2} \\end{aligned} \\] Note The distribution is also referred to as Pascals Distribution , but is most commonly called the \"negative\" binomial because it uses a negative integer in the binomial coefficient : \\[ \\begin{aligned} {n \\choose k} &= \\frac{n!}{k! \\cdot (n-k)!} \\\\ \\\\ {{k+r-1} \\choose {k}} &= \\frac{(k+r-1)!}{k! \\cdot (r-1)!} \\\\ &= \\frac{(k+r-1) \\cdot (k+r-2) \\cdot \\dots \\cdot (r)}{k!} \\\\ &= (-1)^k \\cdot \\frac{(-r) \\cdot (-r-1) \\cdot \\dots \\cdot (-r-k+1)}{k!} \\\\ &= (-1)^k {-r \\choose k} \\end{aligned} \\] The negative binomial distribution is actually the sum of \\(r\\) i.i.d. geometric variables with the same probability : \\[ \\begin{aligned} X_k &\\sim \\text{Geom} (q) \\\\ N &= X_1 + X_2 + ... + X_r \\\\ \\therefore N &\\sim \\text{Negative Binomial} (r, q) \\end{aligned} \\] Note A geometric distribution is simply a negative binomial distribution with \\(r=1\\) . Thus, following the same logic as before, the sum of \\(k\\) independent negative binomial variables with the same probability will still be negative binomial : \\[ \\begin{aligned} N_k &\\sim \\text{Negative Binomial}(r, q) \\\\ N &= N_1 + N_2 + ... + N_k \\\\ \\therefore N &\\sim \\text{Negative Binomial} (r_1 + r_2 + ... + r_k, q) \\end{aligned} \\] Poisson Gamma Mixture \u00b6 TBC Re-parameterized, refer to wikipedia Odds vs Probabiliyu (a, b, 0) Class \u00b6 ONLY the distributions discussed above fall into the (a, b, 0) class of distributions, as their PMFs follow the same recursion : \\[ \\begin{aligned} \\frac{p_{n}}{p_{n-1}} &= a + \\frac{b}{n} \\\\ p_n &= \\left(a + \\frac{b}{n} \\right) p_{n-1} \\end{aligned} \\] \\(a\\) and \\(b\\) are constants that are unique to each distribution while the \"0\" comes from the fact that the recursion starts from \\(n-1=0\\) . Notice that each distribution has a different signs for \\(a\\) . Thus, given the recursive equation, the underlying distribution can be determined based on the sign of \\(a\\) . Choosing Distributions \u00b6 Given a sample of data, we need to decide which distribution best represents it. The general idea is that each distribution has some unique characteristics - if the dataset matches those characteristics, then the distribution should be used. The first method involves the Mean and Variance of the distribution. Notice that for all three distributions, the mean and variance have a different relative size to one another . Thus, by comparing the Sample Mean and Unbiased Sample Variance of the dataset, it can be matched to the corresponding distribution. Warning It is almost impossible (whether in practice or theory) to find a sample mean and variance that is exactly equal to one another. Generally speaking, if they are sufficiently close together, then they can be assumed to be equal. However, being \"sufficiently close together\" is arbitrary. Thus, a better way would be to compare the ratio of the two values - if the ratio is around 1.00 , then it can be concluded that the two values are equal . The second method involves the sign of \\(a\\) in the recursion process. Since all three distributions have different signs for \\(a\\) , their probabilities will move differently as \\(n\\) increases. Thus, by observing how the probability of the sample changes, it can be matched to the corresponding distribution. However, we must first multiply the ratio of the probabilities by \\(n\\) in order to obtain a smooth linear line that clearly exhibits the relationships: \\[ \\begin{aligned} \\frac{p_{n}}{p_{n-1}} &= a + \\frac{b}{n} \\\\ \\frac{n \\cdot p_{n}}{p_{n-1}} &= an + b \\end{aligned} \\] (a, b, 1) Class \u00b6 When using the (a, b, 0) class of distributions to model experiments, one common problem is that the probability at \\(n=0\\) does not match experience . Thus, there is a need to modify the distribution such that the probability at 0 is at the desired level . This modification is known as the Zero-Modification . Note If the desired level is 0, then the modification is known as a Zero-Truncation , which is a special case of the zero-modification. This is because this truncates (removes) the possibility of 0 from the random variable. This is done by directly changing the probability at 0 to the desired level. However, this modification alone would cause the sum of probabilities to deviate from 1, thus the probabilities at all other levels of the support must be scaled such that they sum to 1 . Let \\(N^M\\) be the zero-modified distribution and \\(p^{M}_{n}\\) be its PMF. \\[ p^{M}_{n} = \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_n \\] If \\(N\\) is a poisson random variable, then \\(N^M\\) is known as a Zero-Modified Poisson Random Variable . However, it is important to note that \\(N^M\\) does NOT follow a poisson distribution - it has its own unique distribution. Thus, by applying the previous result, the moments of the new distribution can be calculated: \\[ \\begin{aligned} E \\left(N^M \\right) &= 0 \\cdot p^{M}_{0} + 1 \\cdot p^{M}_{1} + 2 \\cdot p^{M}_{2} + ... \\\\ &= 0 \\cdot \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_0 + 1 \\cdot \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_1 + 2 \\cdot \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_2 + ... \\\\ &= \\frac{1-p^{M}_{0}}{1-p_{0}} (0 \\cdot p_0 + 1 \\cdot p_1 + 2 \\cdot p_2 + ...) \\\\ &= \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot E(N) \\\\ \\\\ \\therefore E \\left[\\left(N^M \\right)^k \\right] &= \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot E \\left[N^k \\right] \\end{aligned} \\] Note that this result ONLY applies to raw moments . In order to calculate the central moments, express them in terms of raw moments and then calculate them from the above conversion. Even after modification, the PMFs can still follow the same recursion: \\[ \\begin{aligned} \\frac{p^{M}_{n}}{p^{M}_{n-1}} &= \\frac{\\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_{n}}{\\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_{n-1}} \\\\ &= \\frac{p_{n}}{p_{n-1}} \\\\ &= a + \\frac{b}{n} \\end{aligned} \\] However, since the distribution is zero-modified, the recursion can only start from \\(k-1=1\\) , which is why they are known as the (a, b, 1) class of distributions. The domain of the recursion is what allows us to distinguish the two classes of distributions. Warning There are only 4 distributions under the (a, b, 0) class: Poisson, Binomial, Geometric & Negative Binomial. It is seemingly intuitive to think that their 0-modified versions are the only members of the (a, b, 1) class. However, the Logarithmic Distribution is also a member of the (a, b, 1).","title":"Frequency Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/#frequency-models","text":"","title":"Frequency Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/#frequency-distributions","text":"The number of claims is usually known as the Loss Frequency and is typically modelled with a discrete distribution. Let \\(N\\) be the random variable denoting the number of claims. This section will cover all the relevant frequency distributions, but note that there is no need to memorize anything as they are all provided in the formula sheet.","title":"Frequency Distributions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/#poisson-distribution","text":"The Poisson Distribution is used to count the number of random events in a specified unit of space or time . It only has one parameter \\(\\lambda\\) , the mean number of occurrences in the specified unit of space or time. The key property is that its mean and variance are both equal to \\(\\lambda\\) . \\[ \\begin{aligned} N &\\sim \\text{Poisson}(\\lambda) \\\\ \\\\ p_n &= \\frac{e^{-\\lambda} \\cdot \\lambda^k}{k!} \\\\ \\\\ E(N) &= \\lambda \\\\ Var (N) &= \\lambda \\end{aligned} \\] The sum of \\(k\\) independent poisson random variables is still poisson : \\[ \\begin{aligned} N_n &\\sim \\text{Poisson}(\\lambda_k) \\\\ N &= N_1 + N_2 + ... + N_k \\\\ \\therefore N &\\sim \\text{Poisson} (\\lambda_1 + \\lambda_2 + ... + \\lambda_k) \\end{aligned} \\]","title":"Poisson Distribution"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/#bernoulli-distribution","text":"The Bernoulli Distribution is used to determine the outcome of a Bernoulli Trial . They are experiments with only two possible outcomes . For a Standard Bernoulli Distribution , the two outcomes are Successes and Failures , denoted by 1 and 0 respectively: \\[ \\begin{aligned} X &\\sim \\text{Bernoulli} (q) \\\\ \\\\ p_x &= \\begin{cases} \\text{Success } (1),& \\text{Probability} = q, \\\\ \\text{Failure } (0),& \\text{Probability} = 1-q \\end{cases} \\\\ \\\\ E(X) &= q \\\\ Var (X) &= q(1-q) \\end{aligned} \\] However, the Bernoulli Shortcut generalizes this for any two mutually exclusive outcomes , denoted by \\(a\\) and \\(b\\) respectively: \\[ \\begin{aligned} Y &= (a-b)X + b \\\\ \\\\ Y &= \\begin{cases} \\text{Outcome 1 } (a),& \\text{Probability} = q, \\\\ \\text{Outcome 2 } (b),& \\text{Probability} = 1-q \\end{cases} \\\\ \\\\ E(Y) &= E[(a-b)X + b] \\\\ &= (a-b)q + b \\\\ \\\\ Var (Y) &= Var[(a-b)X + b] \\\\ &= (a-b)^2 \\cdot q(1-q) \\end{aligned} \\] Note that there is no need to memorize the mean and variance for the bernoulli shortcut - they can be easily derived from the standard distribution.","title":"Bernoulli Distribution"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/#binomial-distribution","text":"The Binomial Distribution is used to count the number of successes out of a fixed number of independent Standard Bernoulli Trials . It has two parameters : The number of independent trials, \\(m\\) The probability of success for each trial, \\(q\\) \\[ \\begin{aligned} N &\\sim \\text{Binomial} (m, q) \\\\ \\\\ p_n &= {m \\choose k} q^k (1-q)^{1-k} \\\\ \\\\ E(N) &= nq \\\\ Var (N) &= nq(1-q) \\end{aligned} \\] The binomial distribution is actually the sum of \\(m\\) independent standard bernoulli variables with the same probability : \\[ \\begin{aligned} X_k &\\sim \\text{Bernoulli} (q) \\\\ N &= X_1 + X_2 + ... + X_m \\\\ \\therefore N &\\sim \\text{Binomial} (m, q) \\end{aligned} \\] Note A bernoulli distribution is simply a binomial distribution with \\(m=1\\) . Thus, the sum of \\(k\\) independent binomial variables with the same probability will still binomial : \\[ \\begin{aligned} N_k &\\sim \\text{Binomial}(m, q) \\\\ N &= N_1 + N_2 + ... + N_k \\\\ \\therefore N &\\sim \\text{Binomial} (m_1 + m_2 + ... + m_k, q) \\end{aligned} \\] This is intuitive, because the sum of binomial variables is actually the sum of even more bernoulli variables , which as shown previously, will follow the binomial distribution.","title":"Binomial Distribution"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/#geometric-distribution","text":"The Geometric Distribution can be understood in one of two ways, with respect to a sequence of independent bernoulli trials : Number of trials needed to get the first success , \\(N \\in \\set{1, 2, 3, ...}\\) Number of failures needed to get the first success , \\(F \\in \\set{0, 1, 2, ...}\\) If not explicitly stated, the two intepretations can be distinguished from their supports - there must be at least 1 trial but there can be 0 failures. It is useful to remember just one intepretation and understand how they are related: \\[ F = N-1 \\] The second intepretation is preferred , since that is what is provided in the formula sheet. Regardless, the Geometric distribution only has one parameter , the probability of success: \\[ \\begin{aligned} N &\\sim \\text{Geom} (q) \\\\ \\\\ p_k &= (1-q)^{k} \\cdot q \\\\ \\\\ E(N) &= \\frac{1-p}{p} \\\\ Var (N) &= \\frac{1-p}{p^2} \\end{aligned} \\] The key property is that it is Memoryless . Intuitively, the geometric distribution is \"waiting\" for the first success to occur. However, it does not matter how many failures have occurred , the probability of the first success occuring in another \\(k\\) failures is independent of the history of the process . The distribution \"forgets\" (memoryless) the current state that the process is in. \\[ P(N > m + n | N \\ge m) = P(N > n) \\]","title":"Geometric Distribution"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/#negative-binomial-distribution","text":"The Negative Binomial Distribution is used to count the number of failures to get a fixed number of successes , with respect to a sequence of independent bernoulli trials. It has two parameters: The number of successes, \\(r\\) The probability of success, \\(q\\) \\[ \\begin{aligned} N &\\sim \\text{Negative Binomial} (r, q) \\\\ \\\\ p_n &= {{k+r-1} \\choose {k}} (1-p)^k p^{r} \\\\ \\\\ E(N) &= \\frac{r(1-p)}{p} \\\\ Var (N) &= \\frac{r(1-p)}{p^2} \\end{aligned} \\] Note The distribution is also referred to as Pascals Distribution , but is most commonly called the \"negative\" binomial because it uses a negative integer in the binomial coefficient : \\[ \\begin{aligned} {n \\choose k} &= \\frac{n!}{k! \\cdot (n-k)!} \\\\ \\\\ {{k+r-1} \\choose {k}} &= \\frac{(k+r-1)!}{k! \\cdot (r-1)!} \\\\ &= \\frac{(k+r-1) \\cdot (k+r-2) \\cdot \\dots \\cdot (r)}{k!} \\\\ &= (-1)^k \\cdot \\frac{(-r) \\cdot (-r-1) \\cdot \\dots \\cdot (-r-k+1)}{k!} \\\\ &= (-1)^k {-r \\choose k} \\end{aligned} \\] The negative binomial distribution is actually the sum of \\(r\\) i.i.d. geometric variables with the same probability : \\[ \\begin{aligned} X_k &\\sim \\text{Geom} (q) \\\\ N &= X_1 + X_2 + ... + X_r \\\\ \\therefore N &\\sim \\text{Negative Binomial} (r, q) \\end{aligned} \\] Note A geometric distribution is simply a negative binomial distribution with \\(r=1\\) . Thus, following the same logic as before, the sum of \\(k\\) independent negative binomial variables with the same probability will still be negative binomial : \\[ \\begin{aligned} N_k &\\sim \\text{Negative Binomial}(r, q) \\\\ N &= N_1 + N_2 + ... + N_k \\\\ \\therefore N &\\sim \\text{Negative Binomial} (r_1 + r_2 + ... + r_k, q) \\end{aligned} \\]","title":"Negative Binomial Distribution"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/#poisson-gamma-mixture","text":"TBC Re-parameterized, refer to wikipedia Odds vs Probabiliyu","title":"Poisson Gamma Mixture"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/#a-b-0-class","text":"ONLY the distributions discussed above fall into the (a, b, 0) class of distributions, as their PMFs follow the same recursion : \\[ \\begin{aligned} \\frac{p_{n}}{p_{n-1}} &= a + \\frac{b}{n} \\\\ p_n &= \\left(a + \\frac{b}{n} \\right) p_{n-1} \\end{aligned} \\] \\(a\\) and \\(b\\) are constants that are unique to each distribution while the \"0\" comes from the fact that the recursion starts from \\(n-1=0\\) . Notice that each distribution has a different signs for \\(a\\) . Thus, given the recursive equation, the underlying distribution can be determined based on the sign of \\(a\\) .","title":"(a, b, 0) Class"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/#choosing-distributions","text":"Given a sample of data, we need to decide which distribution best represents it. The general idea is that each distribution has some unique characteristics - if the dataset matches those characteristics, then the distribution should be used. The first method involves the Mean and Variance of the distribution. Notice that for all three distributions, the mean and variance have a different relative size to one another . Thus, by comparing the Sample Mean and Unbiased Sample Variance of the dataset, it can be matched to the corresponding distribution. Warning It is almost impossible (whether in practice or theory) to find a sample mean and variance that is exactly equal to one another. Generally speaking, if they are sufficiently close together, then they can be assumed to be equal. However, being \"sufficiently close together\" is arbitrary. Thus, a better way would be to compare the ratio of the two values - if the ratio is around 1.00 , then it can be concluded that the two values are equal . The second method involves the sign of \\(a\\) in the recursion process. Since all three distributions have different signs for \\(a\\) , their probabilities will move differently as \\(n\\) increases. Thus, by observing how the probability of the sample changes, it can be matched to the corresponding distribution. However, we must first multiply the ratio of the probabilities by \\(n\\) in order to obtain a smooth linear line that clearly exhibits the relationships: \\[ \\begin{aligned} \\frac{p_{n}}{p_{n-1}} &= a + \\frac{b}{n} \\\\ \\frac{n \\cdot p_{n}}{p_{n-1}} &= an + b \\end{aligned} \\]","title":"Choosing Distributions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/#a-b-1-class","text":"When using the (a, b, 0) class of distributions to model experiments, one common problem is that the probability at \\(n=0\\) does not match experience . Thus, there is a need to modify the distribution such that the probability at 0 is at the desired level . This modification is known as the Zero-Modification . Note If the desired level is 0, then the modification is known as a Zero-Truncation , which is a special case of the zero-modification. This is because this truncates (removes) the possibility of 0 from the random variable. This is done by directly changing the probability at 0 to the desired level. However, this modification alone would cause the sum of probabilities to deviate from 1, thus the probabilities at all other levels of the support must be scaled such that they sum to 1 . Let \\(N^M\\) be the zero-modified distribution and \\(p^{M}_{n}\\) be its PMF. \\[ p^{M}_{n} = \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_n \\] If \\(N\\) is a poisson random variable, then \\(N^M\\) is known as a Zero-Modified Poisson Random Variable . However, it is important to note that \\(N^M\\) does NOT follow a poisson distribution - it has its own unique distribution. Thus, by applying the previous result, the moments of the new distribution can be calculated: \\[ \\begin{aligned} E \\left(N^M \\right) &= 0 \\cdot p^{M}_{0} + 1 \\cdot p^{M}_{1} + 2 \\cdot p^{M}_{2} + ... \\\\ &= 0 \\cdot \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_0 + 1 \\cdot \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_1 + 2 \\cdot \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_2 + ... \\\\ &= \\frac{1-p^{M}_{0}}{1-p_{0}} (0 \\cdot p_0 + 1 \\cdot p_1 + 2 \\cdot p_2 + ...) \\\\ &= \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot E(N) \\\\ \\\\ \\therefore E \\left[\\left(N^M \\right)^k \\right] &= \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot E \\left[N^k \\right] \\end{aligned} \\] Note that this result ONLY applies to raw moments . In order to calculate the central moments, express them in terms of raw moments and then calculate them from the above conversion. Even after modification, the PMFs can still follow the same recursion: \\[ \\begin{aligned} \\frac{p^{M}_{n}}{p^{M}_{n-1}} &= \\frac{\\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_{n}}{\\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_{n-1}} \\\\ &= \\frac{p_{n}}{p_{n-1}} \\\\ &= a + \\frac{b}{n} \\end{aligned} \\] However, since the distribution is zero-modified, the recursion can only start from \\(k-1=1\\) , which is why they are known as the (a, b, 1) class of distributions. The domain of the recursion is what allows us to distinguish the two classes of distributions. Warning There are only 4 distributions under the (a, b, 0) class: Poisson, Binomial, Geometric & Negative Binomial. It is seemingly intuitive to think that their 0-modified versions are the only members of the (a, b, 1) class. However, the Logarithmic Distribution is also a member of the (a, b, 1).","title":"(a, b, 1) Class"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/","text":"Severity Models \u00b6 Severity Distributions \u00b6 The size of the loss is usually known as Loss Severity and is typically modelled with a continuous distribution. Let \\(X\\) be the random variable denoting the size of the loss. This section will cover most of the relevant severity distributions, but similarly note that there is no need to memorize anything as they are all provided in the formula sheet. Normal \u00b6 Pareto \u00b6 Gamma \u00b6 Beta \u00b6 Role of Parameters \u00b6 Policy Features \u00b6 As mentioned previously, most insurance policies have some coverage modifications such that the claim payments are often not equal to the size of the losses . Thus, let \\(Y\\) be the random variable denoting the claim amount that will be paid by the insurer. Deductibles \u00b6 The first type of coverage modification is known as a Deductible , denoted by \\(d\\) . If the loss amount does not exceed the deductible, then the insurance will pay out nothing . However, if it does exceed the deductible , then the insurance will pay out the excess above the deductible . Note that if the loss is equal to the deductible , then still nothing is paid out because there is no excess above the deductible. \\[ \\begin{aligned} Y &= \\begin{cases} 0,& X \\le d\\\\ X - d,& X \\gt d \\end{cases} \\end{aligned} \\] Notice that \\(Y\\) is a mixed random variable: For \\(X \\le d\\) , it is discrete variable with support \\(\\set{0}\\) For \\(X \\gt d\\) , it is a continuous variable with support \\((x-d, \\infty]\\) Limits \u00b6 The second type of coverage modification is known as Limit, denoted by \\(u\\) . Coinsurance \u00b6 Inflation \u00b6 Scaling & Transformation \u00b6 Method of creating new distributions from existing ones? Scaling Multiply RV by a constant Y = cX Fy","title":"Severity Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#severity-models","text":"","title":"Severity Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#severity-distributions","text":"The size of the loss is usually known as Loss Severity and is typically modelled with a continuous distribution. Let \\(X\\) be the random variable denoting the size of the loss. This section will cover most of the relevant severity distributions, but similarly note that there is no need to memorize anything as they are all provided in the formula sheet.","title":"Severity Distributions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#normal","text":"","title":"Normal"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#pareto","text":"","title":"Pareto"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#gamma","text":"","title":"Gamma"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#beta","text":"","title":"Beta"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#role-of-parameters","text":"","title":"Role of Parameters"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#policy-features","text":"As mentioned previously, most insurance policies have some coverage modifications such that the claim payments are often not equal to the size of the losses . Thus, let \\(Y\\) be the random variable denoting the claim amount that will be paid by the insurer.","title":"Policy Features"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#deductibles","text":"The first type of coverage modification is known as a Deductible , denoted by \\(d\\) . If the loss amount does not exceed the deductible, then the insurance will pay out nothing . However, if it does exceed the deductible , then the insurance will pay out the excess above the deductible . Note that if the loss is equal to the deductible , then still nothing is paid out because there is no excess above the deductible. \\[ \\begin{aligned} Y &= \\begin{cases} 0,& X \\le d\\\\ X - d,& X \\gt d \\end{cases} \\end{aligned} \\] Notice that \\(Y\\) is a mixed random variable: For \\(X \\le d\\) , it is discrete variable with support \\(\\set{0}\\) For \\(X \\gt d\\) , it is a continuous variable with support \\((x-d, \\infty]\\)","title":"Deductibles"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#limits","text":"The second type of coverage modification is known as Limit, denoted by \\(u\\) .","title":"Limits"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#coinsurance","text":"","title":"Coinsurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#inflation","text":"","title":"Inflation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#scaling-transformation","text":"Method of creating new distributions from existing ones? Scaling Multiply RV by a constant Y = cX Fy","title":"Scaling &amp; Transformation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/4.%20Aggregate%20Models/","text":"Aggregate Models \u00b6","title":"Aggregate Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/4.%20Aggregate%20Models/#aggregate-models","text":"","title":"Aggregate Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/Credibility/","text":"","title":"Credibility"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/Loss%20Reserving/","text":"","title":"Loss Reserving"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/Maximum%20Likelihood%20Estimation/","text":"","title":"Maximum Likelihood Estimation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/Ratemaking/","text":"","title":"Ratemaking"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/","text":"Linear Regression \u00b6 Population Regression Model \u00b6 Regression is a statistical model that relates a Dependent Variable (DV) to one or more Independent Variables (IV). The dependent variable is regressed on to the independent variable. They are fundamentally a function of the independent variables and several Regression Parameters , \\(\\beta\\) . The functional form of the regression is based on the relationship between the variables. The goal is to use a model that best captures the relationship between the variables. \\[ Y = f(X, \\beta) \\] Independent Variable(s) Dependent Variables Variable used to make predictions Variable being predicted Free to change the value Depends on the value of indepenent variable Deterministic Random Variable Denoted as \\(X\\) Denoted as \\(Y\\) To be precise, for every set of IVs, the DV has a Conditional Distribution dependent on the given IV. For instance, the the \\(Y\\) could take any possible value (Marginal Distribution), but given these set of \\(X\\) , the possible values can be narrowed down to a certain range (conditional distribution). \\[ \\displaylines{ Y \\sim Distribution \\\\ Y|X \\sim Conditional~Distribution} \\] Thus, the output of the regression model is actually the Expected Value of the conditional distribution, \\(E(Y|X)\\) , for every possible \\(X\\) . \\[ E(Y|X) = f(X, \\beta) \\] The actual observations are unlikely to be exactly equal to its expectation, thus there is a difference between an observation and the corresponding regression output. It known as the Random Error Term which accounts for all other factors that affect the DV that are not captured in the regression. This means that the relationship between the \\(Y\\) and \\(X\\) is only approximate , as the true relationship is probably different due to the possibility of unaccounted variables. Note that the sign of the errors are significant - positive implies the actual value lies above the regression output while negative implies it lies below. \\[ \\varepsilon_i = y_i - f(x_i, \\beta) \\] This means that \\(Y\\) (not its expectation!) can be expressed as a sum of the regression model and the error terms: \\[ y_i = f(x_i,\\beta) + \\varepsilon_i \\] The Regression is known as the Systematic component as it is shared among all observations The Error is known as the Non-Systematic component as it is unique to each observation Sample Regression Model \u00b6 In practice, the population is unobservable hence it is impossible to construct the population regression model. Instead, a regression model is constructed from a sample instead, which aims to estimate the population model. \\[ \\hat{y} = f(X,\\hat{\\beta}) \\] Similarly, the output of this model can be compared to the actual observations. However, the resulting difference is known as the Residual of the model, which like all the other components, is an estimate for the Error term. \\[ \\hat{\\varepsilon_i} = y_i - \\hat{y_i} \\] There are several different methods to estimate the regression parameters, but they usually involve minimizing the residuals of the model, such that the resulting model best fits the given sample, which is why it is also known as the Fitted Regression Model . Hypothesis Testing \u00b6 Once a regression model has been fit, the next step is to determine if the relationship found in the sample is indicative of a relationship in the population. This can be determined through the following two-sided hypothesis test : Null Hypothesis: \\(\\beta_1 = 0\\) Alternative Hypothesis: \\(\\beta_1 \\ne 0\\) Under the null, the regression parameters are assumed to be 0, implying that there is no relationship between \\(Y\\) and \\(X\\) . The test should reject the null , proving that there IS a relationship between the DV and IVs. Prediction \u00b6 Once the best model has been determined, it can be used to make Predictions about future unobserved values. Let these future values be denoted by the subscript \\(*\\) . These unobserved DVs come from the population, thus can be expressed as a function of the population model: \\[ y_* = f(x_*, \\beta) + \\varepsilon_* \\] The corresponding values from the sample regression model is an estimate for this unobserved value: \\[ \\hat{y}_* = f(x_*, \\hat{\\beta}) \\] Like before, the predicted value is unlikely to be exactly equal to the actual value. Thus, the difference between both values can be measured as the Prediction Error : \\[ y_* - \\hat{y_*} = \\varepsilon_* + [f(x_*, \\beta) - f(x_*, \\hat{\\beta})] \\] The prediction error is thus made up of two components : Inherent error present in the DV ( \\(\\varepsilon_*\\) ) Error in estimating the population model ( \\(f(x_*, \\beta) - f(x_*, \\hat{\\beta})\\) ) Based on the distribution of the prediction error, a Prediction Interval at a given confidence level can be calculated to accompany the regression estimate, which is essentially a confidence interval for the predicted value . Note that the prediction intervals will always be wider than confidence intervals . This is because CIs only takes into the account the error in estimating the population model/parameters while PIs take into account the inherent error of the DV as well.","title":"Regression Overview"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#linear-regression","text":"","title":"Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#population-regression-model","text":"Regression is a statistical model that relates a Dependent Variable (DV) to one or more Independent Variables (IV). The dependent variable is regressed on to the independent variable. They are fundamentally a function of the independent variables and several Regression Parameters , \\(\\beta\\) . The functional form of the regression is based on the relationship between the variables. The goal is to use a model that best captures the relationship between the variables. \\[ Y = f(X, \\beta) \\] Independent Variable(s) Dependent Variables Variable used to make predictions Variable being predicted Free to change the value Depends on the value of indepenent variable Deterministic Random Variable Denoted as \\(X\\) Denoted as \\(Y\\) To be precise, for every set of IVs, the DV has a Conditional Distribution dependent on the given IV. For instance, the the \\(Y\\) could take any possible value (Marginal Distribution), but given these set of \\(X\\) , the possible values can be narrowed down to a certain range (conditional distribution). \\[ \\displaylines{ Y \\sim Distribution \\\\ Y|X \\sim Conditional~Distribution} \\] Thus, the output of the regression model is actually the Expected Value of the conditional distribution, \\(E(Y|X)\\) , for every possible \\(X\\) . \\[ E(Y|X) = f(X, \\beta) \\] The actual observations are unlikely to be exactly equal to its expectation, thus there is a difference between an observation and the corresponding regression output. It known as the Random Error Term which accounts for all other factors that affect the DV that are not captured in the regression. This means that the relationship between the \\(Y\\) and \\(X\\) is only approximate , as the true relationship is probably different due to the possibility of unaccounted variables. Note that the sign of the errors are significant - positive implies the actual value lies above the regression output while negative implies it lies below. \\[ \\varepsilon_i = y_i - f(x_i, \\beta) \\] This means that \\(Y\\) (not its expectation!) can be expressed as a sum of the regression model and the error terms: \\[ y_i = f(x_i,\\beta) + \\varepsilon_i \\] The Regression is known as the Systematic component as it is shared among all observations The Error is known as the Non-Systematic component as it is unique to each observation","title":"Population Regression Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#sample-regression-model","text":"In practice, the population is unobservable hence it is impossible to construct the population regression model. Instead, a regression model is constructed from a sample instead, which aims to estimate the population model. \\[ \\hat{y} = f(X,\\hat{\\beta}) \\] Similarly, the output of this model can be compared to the actual observations. However, the resulting difference is known as the Residual of the model, which like all the other components, is an estimate for the Error term. \\[ \\hat{\\varepsilon_i} = y_i - \\hat{y_i} \\] There are several different methods to estimate the regression parameters, but they usually involve minimizing the residuals of the model, such that the resulting model best fits the given sample, which is why it is also known as the Fitted Regression Model .","title":"Sample Regression Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#hypothesis-testing","text":"Once a regression model has been fit, the next step is to determine if the relationship found in the sample is indicative of a relationship in the population. This can be determined through the following two-sided hypothesis test : Null Hypothesis: \\(\\beta_1 = 0\\) Alternative Hypothesis: \\(\\beta_1 \\ne 0\\) Under the null, the regression parameters are assumed to be 0, implying that there is no relationship between \\(Y\\) and \\(X\\) . The test should reject the null , proving that there IS a relationship between the DV and IVs.","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#prediction","text":"Once the best model has been determined, it can be used to make Predictions about future unobserved values. Let these future values be denoted by the subscript \\(*\\) . These unobserved DVs come from the population, thus can be expressed as a function of the population model: \\[ y_* = f(x_*, \\beta) + \\varepsilon_* \\] The corresponding values from the sample regression model is an estimate for this unobserved value: \\[ \\hat{y}_* = f(x_*, \\hat{\\beta}) \\] Like before, the predicted value is unlikely to be exactly equal to the actual value. Thus, the difference between both values can be measured as the Prediction Error : \\[ y_* - \\hat{y_*} = \\varepsilon_* + [f(x_*, \\beta) - f(x_*, \\hat{\\beta})] \\] The prediction error is thus made up of two components : Inherent error present in the DV ( \\(\\varepsilon_*\\) ) Error in estimating the population model ( \\(f(x_*, \\beta) - f(x_*, \\hat{\\beta})\\) ) Based on the distribution of the prediction error, a Prediction Interval at a given confidence level can be calculated to accompany the regression estimate, which is essentially a confidence interval for the predicted value . Note that the prediction intervals will always be wider than confidence intervals . This is because CIs only takes into the account the error in estimating the population model/parameters while PIs take into account the inherent error of the DV as well.","title":"Prediction"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/10.%20Clustering/","text":"","title":"10. Clustering"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/","text":"Simple Linear Regression \u00b6 Simple Linear Regression (SLR) assumes a Linear Relationship between a Numeric DV and single continuous quantitative IV. The model is considered to be simple because it only contains a single independent variable. \\[ E(Y|X) = \\beta_0 + \\beta_1 X \\] \\(\\beta_0\\) \\(\\beta_1\\) Expected value of \\(Y\\) when \\(X = 0\\) Change in the expected value of \\(Y\\) given a one unit increase in \\(X\\) Intercept Parameter Slope Parameter Each observation can also be expressed as sum of the regression and its error term: \\[ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\] Ordinary Least Squares \u00b6 SLR parameters are estimated using the Ordinary Least Squares method, which minimizes the Sum of Squared Residuals of the fitted model. It is commonly referred to as the Residual Sum Squared (RSS). \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 \\] The minimization is solved through calculus by setting the partial derivatives of the RSS to 0: For the intercept parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_0} RSS &= 0 \\\\ -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum y_i - \\sum \\hat{\\beta}_0 - \\sum \\hat{\\beta}_1 x &= 0 \\\\ n\\bar{y} -n\\hat{\\beta}_0 - n\\hat{\\beta}_1 \\bar{x} &= 0 \\\\ \\end{aligned} \\] \\[\\therefore \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\] For the slope parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_1} RSS &= 0 \\\\ -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum (y_i x_i) - \\hat{\\beta}_0 \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} + n\\hat{\\beta}_1 \\bar{x}^2 - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} &= \\hat{\\beta}_1 \\sum (x^2_i) - n\\bar{x}^2 \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 = \\frac{\\sum (x_i y_i) - n\\bar{x}\\bar{y}}{\\sum (x_i^2) - n\\bar{x}^2} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} = r * \\frac{s_y}{s_x} \\] This results in the following fitted regression model, which can be graphically expressed as a Regression Line : \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] Note that it should \\(\\hat{\\varepsilon}\\) in the above image, not \\(e_i\\) . OLS Properties \u00b6 By re-arranging the formula for \\(\\hat{\\beta}_0\\) , we can show that \\((\\bar{x}, \\bar{y})\\) always lies on the fitted regression model: \\[ \\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x} \\] Additionally, since the parameters are estimated through minimization, the resulting model must always fulfil the two first order conditions . The model thus has \\(n-2\\) degrees of freedom to reflect these \"constraints\". \\(\\beta_0\\) FOC \\(\\beta_1\\) FOC \\(\\frac{\\partial}{\\partial \\beta_0} = -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\frac{\\partial}{\\partial \\beta_1} = -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\sum \\hat{\\varepsilon_i} = 0\\) \\(\\sum x_i \\hat{\\varepsilon_i} = 0\\) Residuals are negatively correlated Residuals and Independendent variables are uncorrelated Using the above results, we can also show the following that the mean of the regression outputs is equal to the mean of the population: \\[ \\begin{aligned} \\hat{\\varepsilon_i} &= y_i - \\hat{y_i} \\\\ \\sum \\hat{\\varepsilon_i} &= \\sum y_i - \\hat{y_i} \\\\ 0 &= n\\bar{y} - n\\bar{\\hat{y}} \\\\ \\bar{\\hat{y}} &= \\bar{y} \\\\ \\end{aligned} \\] Goodness of Fit \u00b6 Ideally, the regression should fit the sample closely, having as small as residuals as possible . The size of all the residuals in the model can be summarized through the RSS. The lower the RSS, the better the fit of the model. Recall that the residuals naturally sum to 0 under OLS - the residuals are thus squared to remove the sign so that they can be summed together. \\[ RSS = \\sum (y_i - \\hat{y})^2 \\] However, the SSR on its own is hard to intepret as there is no indication of how low or high it actually is. Thus, the Total Sum of Squares (TSS) can be used as a benchmark for the RSS as it is at least equal to or higher than the RSS. The TSS represents the RSS for a Null Regression - a model with containing only the intercept parameter . The output of this regression is always the sample mean \\(\\bar{y}\\) , which is used for the computation of its residuals. It represents the worst possible model which thus has the highest possible RSS . The lower the RSS compared to the TSS, the better the fit of the model. \\[ TSS = \\sum (y_i - \\bar{y}) \\] Null Model \u00b6 Consider a regression with only the intercept; \\(\\beta_1 = 0\\) . It is known as the Null Model as there are no independent variables used. \\[ y = \\beta_0 \\] We can estimate \\(\\hat{\\beta_0}\\) using OLS, which results in the following result: \\[ \\begin{aligned} -2 \\sum (y_i - \\hat{\\beta_0}) &= 0 \\\\ n \\bar{y} - n \\hat{\\beta_0} &= 0 \\\\ \\hat{\\beta_0} &= \\bar{y} \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{y} = \\bar{y} \\] Sum of Squares \u00b6 The TSS can be further decomposed into two more parts for analysis: \\[ \\begin{aligned} TSS &= \\sum (y_i - \\bar{y})^2 \\\\ TSS &= \\sum[(y_i - \\hat{y}) + (\\hat{y}-\\bar{y})]^2 \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 2 \\sum((y_i - \\hat{y})(\\hat{y}-\\bar{y})) \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 0 \\\\ TSS &= RSS + RegSS \\end{aligned} \\] Residual SS (RSS) Regression SS (RegSS) \\(\\sum(\\hat{y}-\\bar{y})^2\\) \\(\\sum(y_i - \\hat{y})^2\\) Variation of the observed values about the regression Variation of the regression output about the sample mean Variation explained by the regression Variation unexplained by the regression Note that it can also be expressed in terms of the Slope Parameter : \\[ \\begin{align} RegSS &= \\sum(\\hat{y}-\\bar{y})^2 \\\\ &= \\sum(\\hat{\\beta}_0 + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum(\\bar{y} - \\beta_1 \\bar{x} + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum[\\hat{\\beta}_1 (x_i - \\bar{x})]^2 \\\\ &= \\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2 \\\\ \\end{align} \\] The Coefficient of Determination \\(R^2\\) can also be used to demonstrate goodness of fit. It measures the proportion of variation explained by the regression model: \\[ R^2 = \\frac{RegSS}{TSS} = 1 - \\frac{RSS}{TSS} \\] Building off the above expression, it can also be expressed in terms of the Sample Correlation : \\[ \\begin{align} R^2 &= \\frac{\\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\\right)^2 \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^4} \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x}) \\sum (y_i - \\bar{y})}\\right)^2 \\\\ &= r_{y,x}^2 \\end{align} \\] Degrees of Freedom \u00b6 The TSS is based on the naive model with only the intercept parameter, thus, it is subject to the single contraint of all residuals summing to 0. The TSS thus has \\(n-1\\) degrees of freedom . The RSS is based on the SLR with both the intercept and slope parameter, thus it is subject to an additional constraint of the sumproduct of all residuals and independent variables being 0. The RSS thus has \\(n-2\\) degrees of freedom. The sum of the RSS and RegSS is equal to the TSS, thus the sum of their degrees of freedom must also be equal to that of the TSS. By working backwards, the RegSS thus has only \\(1\\) degree of freedom . Mean Squared \u00b6 The division of any Sum of Square (TSS, RSS, RegSS) by its Degrees of Freedom is known as the Mean Squared (MS), which is a measure of its average variance . The MS of the TSS is the Unbiased Estimator for Population Variance , which is why this process is known as the Analysis of Variance , as it decomposes the variance of \\(Y\\) into its constituent components: \\[ s = \\frac{TSS}{n-1} \\] The MS of the RSS is known the Mean Squared Residuals , often also referred to as the Mean Squared Error , as it is an estimate for the population variance of the error \\(\\sigma^2\\) : \\[ MS_{\\text{Residuals}} = \\frac{RSS}{n-2} \\] The MS of the RegSS is known as the Mean Squared Regression , which represents the proportion of variance explained per \\(X\\) used. \\[ MS_{\\text{Regression}} = \\frac{RegSS}{1} \\] F Statistic \u00b6 The ANOVA parameters can be used to conduct a hypothesis test to determine if there is in fact a relationship between \\(X\\) and \\(Y\\) : \\(H_0\\) : \\(\\beta_1 = 0\\) \\(H_1\\) : \\(\\beta_1 \\ne 0\\) Under the null hypothesis, there should be no difference between the assumed model and a null model as both only contain the intercept parameter, thus \\(TSS = RSS\\) , where \\(RegSS = 0\\) . Thus, the F-statistic is testing for the equality of variance between the TSS and RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_1 \\ne 0\\) . The F-statistic can be constructed using the sum of squares: \\[ \\begin{aligned} F &= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &= \\frac{RegSS/1}{RSS/(n-2)} \\\\ &= \\frac{(TSS - RSS)/1}{RSS/(n-2)} \\\\ &= (n-2) * \\frac{R^2}{1-R^2}, \\text{divide both by TSS} \\end{aligned} \\] Similar to the variance test, it can be shown that under the null, this test-statistic follows an F distribution with \\(1\\) and \\(n-2\\) degrees of freedom: \\[ \\begin{aligned} T &= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &= \\frac{\\sigma^2_{RegSS} \\frac{MS_{Reg}}{\\sigma^2_{RegSS}}}{\\sigma^2_{RSS} \\frac{MS_{RSS}}{\\sigma^2_{RSS}}} \\\\ &= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\frac{1 * MS_{Reg}}{\\sigma^2_{RegSS}} * \\frac{1}{1}}{\\frac{(n-2) * MS_{RegSS}}{\\sigma^2_{RegSS}}* \\frac{1}{n-2}} \\\\ &= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\chi_1}{\\chi_{n-2}} * (n-2) \\\\ &= 1 * F_{1, n-2} * (n-2) \\\\ &= F_{1, n-2} * (n-2) \\end{aligned} \\] \\[ \\therefore F \\sim F_{1, n-2} \\] ANOVA Table \u00b6 All the above information is then summarized in a table for convenience, known as the ANOVA Table : Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(1\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-2\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) - Statistical Inference \u00b6 Sampling Distributions \u00b6 Since the errors are assumed to be normally distributed, then \\(Y\\) is assumed to be normally distributed as well. Since \\(Y\\) is a linear combination of the regression parameters, then the parameters (& their estimates) are normally distributed as well. Both estimates can be expressed in another form that makes it more convenient to find their expectation & variances. \\[ \\begin{aligned} \\hat{\\beta}_1 &= \\frac{\\sum [(x_i - \\bar{x})(y_i - \\bar{y})]}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})y_i}{\\sum (x^2_i - \\bar{x})} - \\frac{\\bar{y} \\sum (x_i - \\bar{x})}{\\sum (x^2_i - \\bar{x})} \\\\ &= \\sum \\frac{(x_i - \\bar{x})}{(x^2_i - \\bar{x})}* y_i - 0 \\\\ &= \\sum w_i * y_i \\\\ \\\\ \\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\ &= \\frac{1}{n} \\sum y_i - \\bar{x} \\sum w_i * y_i \\\\ &= \\sum y_i (\\frac{1}{n} - \\bar{x}w_i) \\end{aligned} \\] \\(w_i\\) is a sort of \"weight\" parameter of the sum of squares. It has three interesting properties that makes it useful: \\[ \\begin{aligned} \\sum w_i &= \\frac{\\sum (x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{n\\bar{x}-n\\bar{x}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{0}{\\sum (x^2_i - \\bar{x})} \\\\ &= 0 \\\\ \\\\ \\sum w_i x_i &= \\frac{\\sum x_i(x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x^2_i - \\bar{x} \\sum x_i)}{\\sum x^2_i - 2\\bar{x}\\sum x_i + \\sum \\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - \\bar{x}(n \\bar{x})}{\\sum x^2_i - 2\\bar{x}(n\\bar{x}) + n\\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - 2n\\bar{x}^2 + n\\bar{x}^2} \\\\ &=\\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - n\\bar{x}^2} \\\\ &= 1 \\\\ \\\\ \\sum w_i^2 &= \\frac{\\sum (x_i - \\bar{x})^2}{\\sum (x_i - \\bar{x})^4} \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i) &= n(\\frac{1}{n}) - \\bar{x} \\sum w_i \\\\ &= 1 - 0 \\\\ &= 1 \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i &= \\frac{1}{n} \\sum x_i - \\bar{x} \\sum w_i x_i \\\\ &= \\frac{1}{n} (n\\bar{x}) - \\bar{x} (1) \\\\ &= \\bar{x} - \\bar{x} \\\\ &= 0 \\end{aligned} \\] Using this, the Expectation & Variance can be determined: \\[ \\begin{aligned} E(\\hat{\\beta}_1) &= \\sum w_i E(y_i) \\\\ &= \\sum w_i E(\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum w_i + \\beta_1 \\sum w_i x_i \\\\ &= \\beta_0 (0) + \\beta_1 (1) \\\\ &= \\beta_1\\\\ \\\\ Var(\\hat{\\beta}_1) &= Var(\\sum w_i y_i) \\\\ &= \\sum w_i^2 Var (y_i) \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2} \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 \\sim N(\\beta_1, \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2}) \\] \\[ \\begin{aligned} E(\\hat{\\beta}_0) &= \\sum (\\frac{1}{n} - \\bar{x}w_i) E(y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i) (\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum (\\frac{1}{n} - \\bar{x}w_i) + \\beta_1 \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i \\\\ &= \\beta_0 (1) + \\beta_1 (0) \\\\ &= \\beta_0 \\\\ \\\\ Var(\\hat{\\beta}_0) &= Var(\\sum (\\frac{1}{n} - \\bar{x}w_i)y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i)^2 Var (y_i) \\\\ &= \\sigma^2 \\sum (\\frac{1}{n^2} -\\frac{2\\bar{x}w_i}{n} + \\bar{x}^2 w_i^2) \\\\ &= \\sigma^2 (\\sum \\frac{1}{n^2} - \\frac{2\\bar{x}}{n} \\sum w_i + \\bar{x}^2 \\sum w_i^2) \\\\ &= \\sigma^2 [n(\\frac{1}{n^2}) - \\frac{2\\bar{x}}{n} (0) + \\bar{x}^2 (\\frac{1}{\\sum (x_i - \\bar{x})^2})] \\\\ &= \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2}) \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_0 \\sim N(\\beta_0, \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2})) \\] Hypothesis Testing \u00b6 Since the regression parameters are normally distributed, a z-statistic can also be used to conduct the tests. However, since the population variance is not known, a t-statistic is used instead: \\[ \\begin{aligned} t &= \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\end{aligned} \\] Since the population variance is estimated by the MSE which has \\(n-2\\) degrees of freedom, the corresponding chi-squared and hence t-distribution has \\(n-2\\) degrees of freedom as well. \\[ \\begin{aligned} \\hat{Var}(\\hat{\\beta_1}) &= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} * \\frac{\\frac{1}{n-1}}{\\frac{1}{n-1}} \\\\ &= \\frac{MS_{RSS}}{(n-1) s^2} \\end{aligned} \\] \\[ t \\sim t_{n-2} \\] Since the square of the t-statistic is the F-statistic, both are equivalent ways of doing so and will always lead to the same conclusions. \\[ t^2 \\sim F_{1, n-2} \\] Confidence Intervals \u00b6 Since the distribution of t-statistic is known, it can be used to determine the confidence interval of the estimate: \\[ P\\left(\\text{Margin of Error} < \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} < \\text{Margin of Error}\\right) = 1 - \\alpha \\] \\[ \\text{Confidence Interval} = \\hat{\\beta}_1 \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_1}} \\] Prediction Intervals \u00b6 Consider the Prediction Error of the SLR model: \\[ y_* - \\hat{y_*} = \\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\] Since both \\(y_*\\) and \\(\\hat{y_*}\\) are normally distributed, the prediction errors are normally distributed as well: \\[ \\begin{aligned} E(y_* - \\hat{y_*}) &= E[\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)]] \\\\ &= 0 + \\beta_0 + \\beta_1 E(x_*) - \\beta_0 - \\beta_1 E(x_*) \\\\ &= 0 \\\\ \\\\ Var(y_* - \\hat{y_*}) &= Var(\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= Var(\\varepsilon_*) + Var[(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= ... \\\\ &= \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}] \\end{aligned} \\] \\[ \\therefore y_* - \\hat{y_*} \\sim N\\left(0, \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}]\\right) \\] Similar to before, since the population variance is unknown, it can be approximated using the MSE. Thus, a t-statistic constructed from this sample will have a t-distribution , allowing the following prediction interval to be calculated: \\[ \\text{Prediction Interval} = \\hat{y}_* \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\] Notice that the standard error of the prediction interval increases as \\(x_*\\) moves further away \\(\\bar{x}\\) , indicating that the predictions become less accurate for those values.","title":"Simple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#simple-linear-regression","text":"Simple Linear Regression (SLR) assumes a Linear Relationship between a Numeric DV and single continuous quantitative IV. The model is considered to be simple because it only contains a single independent variable. \\[ E(Y|X) = \\beta_0 + \\beta_1 X \\] \\(\\beta_0\\) \\(\\beta_1\\) Expected value of \\(Y\\) when \\(X = 0\\) Change in the expected value of \\(Y\\) given a one unit increase in \\(X\\) Intercept Parameter Slope Parameter Each observation can also be expressed as sum of the regression and its error term: \\[ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\]","title":"Simple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#ordinary-least-squares","text":"SLR parameters are estimated using the Ordinary Least Squares method, which minimizes the Sum of Squared Residuals of the fitted model. It is commonly referred to as the Residual Sum Squared (RSS). \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 \\] The minimization is solved through calculus by setting the partial derivatives of the RSS to 0: For the intercept parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_0} RSS &= 0 \\\\ -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum y_i - \\sum \\hat{\\beta}_0 - \\sum \\hat{\\beta}_1 x &= 0 \\\\ n\\bar{y} -n\\hat{\\beta}_0 - n\\hat{\\beta}_1 \\bar{x} &= 0 \\\\ \\end{aligned} \\] \\[\\therefore \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\] For the slope parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_1} RSS &= 0 \\\\ -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum (y_i x_i) - \\hat{\\beta}_0 \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} + n\\hat{\\beta}_1 \\bar{x}^2 - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} &= \\hat{\\beta}_1 \\sum (x^2_i) - n\\bar{x}^2 \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 = \\frac{\\sum (x_i y_i) - n\\bar{x}\\bar{y}}{\\sum (x_i^2) - n\\bar{x}^2} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} = r * \\frac{s_y}{s_x} \\] This results in the following fitted regression model, which can be graphically expressed as a Regression Line : \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] Note that it should \\(\\hat{\\varepsilon}\\) in the above image, not \\(e_i\\) .","title":"Ordinary Least Squares"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#ols-properties","text":"By re-arranging the formula for \\(\\hat{\\beta}_0\\) , we can show that \\((\\bar{x}, \\bar{y})\\) always lies on the fitted regression model: \\[ \\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x} \\] Additionally, since the parameters are estimated through minimization, the resulting model must always fulfil the two first order conditions . The model thus has \\(n-2\\) degrees of freedom to reflect these \"constraints\". \\(\\beta_0\\) FOC \\(\\beta_1\\) FOC \\(\\frac{\\partial}{\\partial \\beta_0} = -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\frac{\\partial}{\\partial \\beta_1} = -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\sum \\hat{\\varepsilon_i} = 0\\) \\(\\sum x_i \\hat{\\varepsilon_i} = 0\\) Residuals are negatively correlated Residuals and Independendent variables are uncorrelated Using the above results, we can also show the following that the mean of the regression outputs is equal to the mean of the population: \\[ \\begin{aligned} \\hat{\\varepsilon_i} &= y_i - \\hat{y_i} \\\\ \\sum \\hat{\\varepsilon_i} &= \\sum y_i - \\hat{y_i} \\\\ 0 &= n\\bar{y} - n\\bar{\\hat{y}} \\\\ \\bar{\\hat{y}} &= \\bar{y} \\\\ \\end{aligned} \\]","title":"OLS Properties"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#goodness-of-fit","text":"Ideally, the regression should fit the sample closely, having as small as residuals as possible . The size of all the residuals in the model can be summarized through the RSS. The lower the RSS, the better the fit of the model. Recall that the residuals naturally sum to 0 under OLS - the residuals are thus squared to remove the sign so that they can be summed together. \\[ RSS = \\sum (y_i - \\hat{y})^2 \\] However, the SSR on its own is hard to intepret as there is no indication of how low or high it actually is. Thus, the Total Sum of Squares (TSS) can be used as a benchmark for the RSS as it is at least equal to or higher than the RSS. The TSS represents the RSS for a Null Regression - a model with containing only the intercept parameter . The output of this regression is always the sample mean \\(\\bar{y}\\) , which is used for the computation of its residuals. It represents the worst possible model which thus has the highest possible RSS . The lower the RSS compared to the TSS, the better the fit of the model. \\[ TSS = \\sum (y_i - \\bar{y}) \\]","title":"Goodness of Fit"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#null-model","text":"Consider a regression with only the intercept; \\(\\beta_1 = 0\\) . It is known as the Null Model as there are no independent variables used. \\[ y = \\beta_0 \\] We can estimate \\(\\hat{\\beta_0}\\) using OLS, which results in the following result: \\[ \\begin{aligned} -2 \\sum (y_i - \\hat{\\beta_0}) &= 0 \\\\ n \\bar{y} - n \\hat{\\beta_0} &= 0 \\\\ \\hat{\\beta_0} &= \\bar{y} \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{y} = \\bar{y} \\]","title":"Null Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#sum-of-squares","text":"The TSS can be further decomposed into two more parts for analysis: \\[ \\begin{aligned} TSS &= \\sum (y_i - \\bar{y})^2 \\\\ TSS &= \\sum[(y_i - \\hat{y}) + (\\hat{y}-\\bar{y})]^2 \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 2 \\sum((y_i - \\hat{y})(\\hat{y}-\\bar{y})) \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 0 \\\\ TSS &= RSS + RegSS \\end{aligned} \\] Residual SS (RSS) Regression SS (RegSS) \\(\\sum(\\hat{y}-\\bar{y})^2\\) \\(\\sum(y_i - \\hat{y})^2\\) Variation of the observed values about the regression Variation of the regression output about the sample mean Variation explained by the regression Variation unexplained by the regression Note that it can also be expressed in terms of the Slope Parameter : \\[ \\begin{align} RegSS &= \\sum(\\hat{y}-\\bar{y})^2 \\\\ &= \\sum(\\hat{\\beta}_0 + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum(\\bar{y} - \\beta_1 \\bar{x} + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum[\\hat{\\beta}_1 (x_i - \\bar{x})]^2 \\\\ &= \\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2 \\\\ \\end{align} \\] The Coefficient of Determination \\(R^2\\) can also be used to demonstrate goodness of fit. It measures the proportion of variation explained by the regression model: \\[ R^2 = \\frac{RegSS}{TSS} = 1 - \\frac{RSS}{TSS} \\] Building off the above expression, it can also be expressed in terms of the Sample Correlation : \\[ \\begin{align} R^2 &= \\frac{\\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\\right)^2 \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^4} \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x}) \\sum (y_i - \\bar{y})}\\right)^2 \\\\ &= r_{y,x}^2 \\end{align} \\]","title":"Sum of Squares"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#degrees-of-freedom","text":"The TSS is based on the naive model with only the intercept parameter, thus, it is subject to the single contraint of all residuals summing to 0. The TSS thus has \\(n-1\\) degrees of freedom . The RSS is based on the SLR with both the intercept and slope parameter, thus it is subject to an additional constraint of the sumproduct of all residuals and independent variables being 0. The RSS thus has \\(n-2\\) degrees of freedom. The sum of the RSS and RegSS is equal to the TSS, thus the sum of their degrees of freedom must also be equal to that of the TSS. By working backwards, the RegSS thus has only \\(1\\) degree of freedom .","title":"Degrees of Freedom"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#mean-squared","text":"The division of any Sum of Square (TSS, RSS, RegSS) by its Degrees of Freedom is known as the Mean Squared (MS), which is a measure of its average variance . The MS of the TSS is the Unbiased Estimator for Population Variance , which is why this process is known as the Analysis of Variance , as it decomposes the variance of \\(Y\\) into its constituent components: \\[ s = \\frac{TSS}{n-1} \\] The MS of the RSS is known the Mean Squared Residuals , often also referred to as the Mean Squared Error , as it is an estimate for the population variance of the error \\(\\sigma^2\\) : \\[ MS_{\\text{Residuals}} = \\frac{RSS}{n-2} \\] The MS of the RegSS is known as the Mean Squared Regression , which represents the proportion of variance explained per \\(X\\) used. \\[ MS_{\\text{Regression}} = \\frac{RegSS}{1} \\]","title":"Mean Squared"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#f-statistic","text":"The ANOVA parameters can be used to conduct a hypothesis test to determine if there is in fact a relationship between \\(X\\) and \\(Y\\) : \\(H_0\\) : \\(\\beta_1 = 0\\) \\(H_1\\) : \\(\\beta_1 \\ne 0\\) Under the null hypothesis, there should be no difference between the assumed model and a null model as both only contain the intercept parameter, thus \\(TSS = RSS\\) , where \\(RegSS = 0\\) . Thus, the F-statistic is testing for the equality of variance between the TSS and RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_1 \\ne 0\\) . The F-statistic can be constructed using the sum of squares: \\[ \\begin{aligned} F &= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &= \\frac{RegSS/1}{RSS/(n-2)} \\\\ &= \\frac{(TSS - RSS)/1}{RSS/(n-2)} \\\\ &= (n-2) * \\frac{R^2}{1-R^2}, \\text{divide both by TSS} \\end{aligned} \\] Similar to the variance test, it can be shown that under the null, this test-statistic follows an F distribution with \\(1\\) and \\(n-2\\) degrees of freedom: \\[ \\begin{aligned} T &= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &= \\frac{\\sigma^2_{RegSS} \\frac{MS_{Reg}}{\\sigma^2_{RegSS}}}{\\sigma^2_{RSS} \\frac{MS_{RSS}}{\\sigma^2_{RSS}}} \\\\ &= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\frac{1 * MS_{Reg}}{\\sigma^2_{RegSS}} * \\frac{1}{1}}{\\frac{(n-2) * MS_{RegSS}}{\\sigma^2_{RegSS}}* \\frac{1}{n-2}} \\\\ &= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\chi_1}{\\chi_{n-2}} * (n-2) \\\\ &= 1 * F_{1, n-2} * (n-2) \\\\ &= F_{1, n-2} * (n-2) \\end{aligned} \\] \\[ \\therefore F \\sim F_{1, n-2} \\]","title":"F Statistic"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#anova-table","text":"All the above information is then summarized in a table for convenience, known as the ANOVA Table : Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(1\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-2\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) -","title":"ANOVA Table"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#statistical-inference","text":"","title":"Statistical Inference"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#sampling-distributions","text":"Since the errors are assumed to be normally distributed, then \\(Y\\) is assumed to be normally distributed as well. Since \\(Y\\) is a linear combination of the regression parameters, then the parameters (& their estimates) are normally distributed as well. Both estimates can be expressed in another form that makes it more convenient to find their expectation & variances. \\[ \\begin{aligned} \\hat{\\beta}_1 &= \\frac{\\sum [(x_i - \\bar{x})(y_i - \\bar{y})]}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})y_i}{\\sum (x^2_i - \\bar{x})} - \\frac{\\bar{y} \\sum (x_i - \\bar{x})}{\\sum (x^2_i - \\bar{x})} \\\\ &= \\sum \\frac{(x_i - \\bar{x})}{(x^2_i - \\bar{x})}* y_i - 0 \\\\ &= \\sum w_i * y_i \\\\ \\\\ \\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\ &= \\frac{1}{n} \\sum y_i - \\bar{x} \\sum w_i * y_i \\\\ &= \\sum y_i (\\frac{1}{n} - \\bar{x}w_i) \\end{aligned} \\] \\(w_i\\) is a sort of \"weight\" parameter of the sum of squares. It has three interesting properties that makes it useful: \\[ \\begin{aligned} \\sum w_i &= \\frac{\\sum (x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{n\\bar{x}-n\\bar{x}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{0}{\\sum (x^2_i - \\bar{x})} \\\\ &= 0 \\\\ \\\\ \\sum w_i x_i &= \\frac{\\sum x_i(x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x^2_i - \\bar{x} \\sum x_i)}{\\sum x^2_i - 2\\bar{x}\\sum x_i + \\sum \\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - \\bar{x}(n \\bar{x})}{\\sum x^2_i - 2\\bar{x}(n\\bar{x}) + n\\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - 2n\\bar{x}^2 + n\\bar{x}^2} \\\\ &=\\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - n\\bar{x}^2} \\\\ &= 1 \\\\ \\\\ \\sum w_i^2 &= \\frac{\\sum (x_i - \\bar{x})^2}{\\sum (x_i - \\bar{x})^4} \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i) &= n(\\frac{1}{n}) - \\bar{x} \\sum w_i \\\\ &= 1 - 0 \\\\ &= 1 \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i &= \\frac{1}{n} \\sum x_i - \\bar{x} \\sum w_i x_i \\\\ &= \\frac{1}{n} (n\\bar{x}) - \\bar{x} (1) \\\\ &= \\bar{x} - \\bar{x} \\\\ &= 0 \\end{aligned} \\] Using this, the Expectation & Variance can be determined: \\[ \\begin{aligned} E(\\hat{\\beta}_1) &= \\sum w_i E(y_i) \\\\ &= \\sum w_i E(\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum w_i + \\beta_1 \\sum w_i x_i \\\\ &= \\beta_0 (0) + \\beta_1 (1) \\\\ &= \\beta_1\\\\ \\\\ Var(\\hat{\\beta}_1) &= Var(\\sum w_i y_i) \\\\ &= \\sum w_i^2 Var (y_i) \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2} \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 \\sim N(\\beta_1, \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2}) \\] \\[ \\begin{aligned} E(\\hat{\\beta}_0) &= \\sum (\\frac{1}{n} - \\bar{x}w_i) E(y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i) (\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum (\\frac{1}{n} - \\bar{x}w_i) + \\beta_1 \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i \\\\ &= \\beta_0 (1) + \\beta_1 (0) \\\\ &= \\beta_0 \\\\ \\\\ Var(\\hat{\\beta}_0) &= Var(\\sum (\\frac{1}{n} - \\bar{x}w_i)y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i)^2 Var (y_i) \\\\ &= \\sigma^2 \\sum (\\frac{1}{n^2} -\\frac{2\\bar{x}w_i}{n} + \\bar{x}^2 w_i^2) \\\\ &= \\sigma^2 (\\sum \\frac{1}{n^2} - \\frac{2\\bar{x}}{n} \\sum w_i + \\bar{x}^2 \\sum w_i^2) \\\\ &= \\sigma^2 [n(\\frac{1}{n^2}) - \\frac{2\\bar{x}}{n} (0) + \\bar{x}^2 (\\frac{1}{\\sum (x_i - \\bar{x})^2})] \\\\ &= \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2}) \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_0 \\sim N(\\beta_0, \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2})) \\]","title":"Sampling Distributions"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#hypothesis-testing","text":"Since the regression parameters are normally distributed, a z-statistic can also be used to conduct the tests. However, since the population variance is not known, a t-statistic is used instead: \\[ \\begin{aligned} t &= \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\end{aligned} \\] Since the population variance is estimated by the MSE which has \\(n-2\\) degrees of freedom, the corresponding chi-squared and hence t-distribution has \\(n-2\\) degrees of freedom as well. \\[ \\begin{aligned} \\hat{Var}(\\hat{\\beta_1}) &= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} * \\frac{\\frac{1}{n-1}}{\\frac{1}{n-1}} \\\\ &= \\frac{MS_{RSS}}{(n-1) s^2} \\end{aligned} \\] \\[ t \\sim t_{n-2} \\] Since the square of the t-statistic is the F-statistic, both are equivalent ways of doing so and will always lead to the same conclusions. \\[ t^2 \\sim F_{1, n-2} \\]","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#confidence-intervals","text":"Since the distribution of t-statistic is known, it can be used to determine the confidence interval of the estimate: \\[ P\\left(\\text{Margin of Error} < \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} < \\text{Margin of Error}\\right) = 1 - \\alpha \\] \\[ \\text{Confidence Interval} = \\hat{\\beta}_1 \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_1}} \\]","title":"Confidence Intervals"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#prediction-intervals","text":"Consider the Prediction Error of the SLR model: \\[ y_* - \\hat{y_*} = \\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\] Since both \\(y_*\\) and \\(\\hat{y_*}\\) are normally distributed, the prediction errors are normally distributed as well: \\[ \\begin{aligned} E(y_* - \\hat{y_*}) &= E[\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)]] \\\\ &= 0 + \\beta_0 + \\beta_1 E(x_*) - \\beta_0 - \\beta_1 E(x_*) \\\\ &= 0 \\\\ \\\\ Var(y_* - \\hat{y_*}) &= Var(\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= Var(\\varepsilon_*) + Var[(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= ... \\\\ &= \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}] \\end{aligned} \\] \\[ \\therefore y_* - \\hat{y_*} \\sim N\\left(0, \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}]\\right) \\] Similar to before, since the population variance is unknown, it can be approximated using the MSE. Thus, a t-statistic constructed from this sample will have a t-distribution , allowing the following prediction interval to be calculated: \\[ \\text{Prediction Interval} = \\hat{y}_* \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\] Notice that the standard error of the prediction interval increases as \\(x_*\\) moves further away \\(\\bar{x}\\) , indicating that the predictions become less accurate for those values.","title":"Prediction Intervals"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/","text":"Multiple Linear Regression \u00b6 The natural extension of the SLR model is to include more than one independent variable , which thus results in the more generalized Multiple Linear Regression (MLR) model. \\[ E(Y|X_1, ... X_p) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_{p}X_{p} \\] Unlike the SLR which studies how each individual IV influences the DV, the goal of the MLR model is to study how all the IVs operate together to influence the DV. \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) when \\(X_1 = X_2 = ... = 0\\) Change in \\(E(Y)\\) given a one unit increase in \\(X_j\\) , holding all other \\(X\\) 's constant Intercept Parameter \"Slope\" Parameter For avoidance of doubt, the subscript \\(i\\) will be used to denote observations while \\(j\\) will be used to denote independent variables. Every observation can also be expressed as the sum of the regression and the error term. However, due to the multi-dimensional nature of the model, it is commonly expressed in matrix notation: \\[ \\begin{aligned} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} &= \\begin{pmatrix} 1 & x_{11} & x_{12} & ... & x_{1p} \\\\ 1 & x_{21} & x_{22} & ... & x_{2p} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & ... & x_{np} \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix} + \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix} \\\\ \\boldsymbol{y} &= \\boldsymbol{X\\beta + \\varepsilon} \\end{aligned} \\] Ordinary Least Squares \u00b6 Similar to the SLR model, the regression parameters can be found by minimizing the sum of squared residuals: \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_p x_ip)]^2 \\] There are \\(p+1\\) FOC equations to solve through the minimization, with the additional one reflecting the intercept parameter. It is difficult to algebraically solve this system of equations, thus there is no closed form tsolution for each individual paramater. Instead, here is a vector solution for all of the parameters: \\[ \\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\hat{\\beta_0} \\\\ \\hat{\\beta_1} \\\\ \\vdots \\\\ \\hat{\\beta_p} \\end{pmatrix} = (\\boldsymbol{X'X})^{-1}\\boldsymbol{X'y} \\] Note that since there are \\(p+1\\) equations that must be solved, the model has \\(n-p+1\\) degrees of freedom. Following the same logic, there must be at least \\(p+1\\) observations in order to solve the equations and hence construct the model. This results in the following fitted regression model, which can be graphically expressed as a Regression Plane : \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_P x_ip \\] Manual Computation \u00b6 The tricky part is that \\((\\boldsymbol{X'X})^{-1}\\) is hard to compute by manually , except for the special case where \\(p=1\\) (SLR). Thus, it is likely that the parameters will be provided by the question. If required to compute them manually, then \\((\\boldsymbol{X'X})^{-1}\\) is likely to be provided. The remaining \\(\\boldsymbol{X'y}\\) still needs to be computed and put together to obtain the regression parameters. However, if the model has \\(p=2\\) but no intercept , then \\((\\boldsymbol{X'X})^{-1}\\) is a 2 x 2 Matrix whose inverse can be easily calculated. Similarly, if \\((\\boldsymbol{X'X})^{-1}\\) is a Diagonal Matrix , its inverse can be easily calculated as well. Goodness of Fit \u00b6 The ANOVA for MLR follows the same intuition as the SLR version, adjusted for the new degrees of freedom: Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(p\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-(p+1)\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) - The coefficient of determination still represents the proportion of variance explained by the regression, but has a slightly different formula: \\[ R^2 = \\frac{RegSS}{TSS} = r_{y,\\hat{y}}^2 \\] The multiple IVs of the model are now captured through \\(\\hat{y}\\) instead of \\(x\\) directly. However, the hypothesis test under the MLR is vastly different from the SLR version. Instead of testing if an individual IV is useful, it tests if all the IVs are collectively useful in helping to explain the DV. \\[ \\begin{aligned} H_0 &: \\beta_1 = \\beta_2 = ... = \\beta_p = 0 \\\\ H_1 &: \\text{At least one } \\beta_j \\text{ is non-zero} \\end{aligned} \\] Thus, rejecting the null hypothesis implies that at least one of the IVs used is useful, but does not provide much insight into which of them are useful. Partial F Test \u00b6 A partial F-test can be used to precisely determine which of the IVs are useful in explaining the DV. A regular F test compares the null model with no IVs to the desired model with all the IVs. If the sum of squares are significantly different, then it implies that the additional IVs are jointly useful in explaining \\(Y\\) . The partial F test generalizes this idea. Instead of considering a null model with no IVs, a Reduced Model with a limited number IVs ( \\(q\\) ) is considered instead. Consequently, the desired model is known as the Full Model with all \\(p\\) IVs, where \\(q < p\\) . \\[ \\begin{aligned} H_0: \\beta_{p-q+1} = ... = \\beta_p = 0 \\\\ H_1: \\beta_{p-q+1} = ... = \\beta_p \\ne 0 \\end{aligned} \\] The test is also commonly referred to as the Generalized F test, where the models are referred to as the Restricted and Unrestricted Models. The difference in the RSS between the Full and Reduced Model is known as the Extra Sum of Squares (ExtraSS) . It represents the contribution of the missing variables in explaining the variance of \\(Y\\) . Under the null hypothesis, there should be no difference between the two RSS, and thus \\(ExtraSS = 0\\) . \\[ ExtraSS = RSS_{Reduced} - RSS_{Full} \\] Thus, the Partial F-statistic is testing for the equality of variance between the two RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_{p-q+1} = ... = \\beta_p \\ne 0\\) . \\[ \\begin{aligned} F &= \\frac{MS_{ExtraSS}}{MS_{RSS_{Full}}} \\\\ &= \\frac{ExtraSS/q}{RSS/(n-2)} \\\\ &= \\frac{(RSS_{Reduced} - RSS_{Full})/q}{RSS/(n-2)} \\\\ &= (n-2) * \\frac{(1- R_{Reduced}^2) - (1 - R_{Full}^2)}{1-RSS_{Full}^2}, \\text{divide both by TSS} \\\\ &= (n-2) * \\frac{R_{Full}^2 - R_{Reduced}^2}{1-RSS_{Full}^2} \\end{aligned} \\] Statistical Inference \u00b6 Sampling Distributions \u00b6 Similar to SLR, the regression parameters are normally distributed as well. However, since there are multiple regression parameters, they collectively follow a multivariate normal distribution . \\[ \\hat{\\beta} \\sim N_{p+1}(\\beta, \\sigma^2 (\\boldsymbol{X'X})^{-1}) \\] The variance of the distribution is known as the Variance Covariance Matrix , which provides the covariances between every possible pair of regression parameters. Since the covariance of a variable with itself is its variance, the diagonals are the respective variances of the parameters. Note that the first element of the diagonal is the intercept, thus the variance of the jth IV is the (j+1)th element of the diagonal . \\[ Var(\\hat{\\beta}) = \\begin{pmatrix} Var(\\hat{\\beta}_0) & Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) & ... & Cov(\\hat{\\beta}_0, \\hat{\\beta}_p) \\\\ Cov(\\hat{\\beta}_1, \\hat{\\beta}_0) & Var(\\hat{\\beta}_1) & ... & Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ Cov(\\hat{\\beta}_p, \\hat{\\beta}_0) & Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) & ... & Var(\\hat{\\beta}_p) \\end{pmatrix} \\] Note that the covariances are symmetrical about the diagonal - \\(Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = Cov(\\hat{\\beta}_1, \\hat{\\beta}_0)\\) . Hypothesis Testing \u00b6 Similar to SLR, the t-test can be used to test for the significance of an individual IV, but the intepretation of the test is different from the SLR case. It tests the usefulness of an individual IV in the presence of the other predictors . \\[ \\begin{aligned} H_0: \\beta_j = 0 \\\\ H_1: \\beta_j \\ne 0 \\end{aligned} \\] \\[ t(\\beta_j) = \\frac{\\hat{\\beta_j} - \\beta_j}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\] Recall that the variance of the jth IV si the (j+1)th element of the variance covariance matrix. \\[ t(\\beta_j) \\sim t_{n-p-1} \\] However, this leads to several odd results which needs to be accounted for: Predictor is not significant individually but significant when taken alone . Predictors are not significant individually but significant when taken together . There are now two possible ways to test for the significance of IVs: Conduct a single F-test to test for the joint significance of all IVs Conduct multiple t-tests to test for the joint significance of each IV The problem with the multiple t-test approach lies with the type I error of the tests. For \\(\\alpha = 0.05\\) , the probability of correctly rejecting the null is \\(0.95\\) . Assuming that all the tests are independent, the probability of correctly rejecting all the nulls is \\(0.95^p\\) , which drastically decreases with the number of tests conducted. Given enough predictors, this means that the probability drops to approximately 0, which means that there is bound to be a wrongly rejected null; a type I error is guaranteed even though it was supposed to be limited at a 0.05 chance. The Bonferroni Correction is a method of adjusting \\(\\alpha\\) of each hypothesis test such that the overall type I error is kept at its desired level. However, this has the consequence of increasing the probability of type II errors, which is why it is not popular. The F-test has the advantage of controlling the type I error regardless of the number of predictors , which is why it is preferred for hypothesis testing in the MLR. Confidence Intervals \u00b6 Similar to SLR, the confidence intervals can be constructed using the distribution of the test-statistic: \\[ \\text{Confidence Interval} = \\hat{\\beta}_j \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_j}} \\] Prediction Intervals \u00b6 Unlike in SLR, it is difficult to determine the distribution of the prediction error. Thus, the final result can be found below: \\[ \\begin{aligned} \\text{Prediction Interval} &= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\\\ &= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\sqrt{s^2 [1 + x'_* (\\boldsymbol{X'X})^-1 * x_*]} \\end{aligned} \\] Despite the result looking more complicated, the key takeaway remains the same - the further away \\(x_*\\) is from \\(\\bar{x}\\) , the greater Variations of MLR \u00b6 Qualitative IV \u00b6 The discussion so far has mostly focused on Quantitative IVs, thus this section will explore Qualitative IVs. They can only take values from a list of pre-defined values, known as the Levels of the variable. In a regression context, most qualitative IVs are represented in the form of a Dummy Variable which can only take two possible levels - Yes (1) or No (0). Note that there are other ways of encoding a dummy variable (-1/0/1 etc), but the principles stay large the same. \\[ \\text{Dummy Variable} = \\begin{cases} 1, & \\text{First Level} \\\\ 0, & \\text{Second Level} \\end{cases} \\] In general, \\(n-1\\) dummy variables are needed to represent a qualitative variable with \\(n\\) levels. This is because the status of the last level can be deduced from the other dummy variables . Thus, including a seperate dummy variable for this last level is redundant and will lead to the problem of Collinearity , which will be explored in a later section. For instance, consider four levels (North, East, South & West), represented by the three dummy variables ( \\(N, E , S\\) ). If any of the variables are 1, they represent their respective direction (North, East & South). If all of them are 0, then the direction is the remaining level (West). The last remaining level is often referred to as the Baseline Level as it is the default level of the variable when all other dummies are 0. Any level can be used as the baseline, but the parameters will differ across models with different baselines. \\[ E(Y|X) = \\beta_0 + \\beta_{1, North} x_{North} + \\beta_{2, East} * x_{East} + \\beta_{3, South} * x_{South} \\] \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) at the baseline level Change in \\(E(Y)\\) from the baseline to the chosen level \\(X_1 = X_2 = ... = X_j = 0\\) \\(X_1 = X_2 = ... = 0; X_j = 1\\) Dummy variables are usually used in conjunction with quantitative ones . This essentially creates a \"seperate\" regression model for each of the levels. For the simplest case of one quantiative and one dummy, \\(\\beta_2\\) is the difference in the intercept of the two resulting SLR models. \\[ E(Y|X) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\] \\[ E(Y|X) = \\begin{cases} (\\beta_0 + \\beta_2) + \\beta_1 x_1, & x_2 = 1 \\\\ \\beta_0 + \\beta_1 x_1, & x_2 = 0 \\end{cases} \\] Interaction Model \u00b6 So far, it was assumed that each IV had an independent effect on the DV. However, IVs may interact to produce a joint effect on the DV, where the effect of one IV depends on the value of another IV. For instance, the production of a factory may depend on the number of Machines and Workers. However, the more machines there are, the greater the effect of an additional worker . Thus, this interaction effect can be captured through an Interaction Variable , which is the product of both IVs: \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 \\\\ &= \\beta_0 + (\\beta_1 + \\beta_3 x_2)x_1 + \\beta_2 x_2 \\\\ &= \\beta_0 + \\beta_1 x_1 + (\\beta_2 + \\beta_3 x_1) x_2 \\\\ \\end{aligned} \\] A one unit increase in \\(x_1\\) will increase E(Y) by \\(\\beta_1 + \\beta_3 x_2\\) , which depends on the value of \\(x_2\\) as well, which is is why they interact with each another. Phrased another way, for every one unit increase in \\(x_2\\) , the change in E(Y) from a unit increase in \\(x_1\\) increases by \\(\\beta_3\\) . Something unusual to take note of is that the Interaction Variable tests significant but the constituent variables do not. In this case, it is common practice to retain both the interaction and the consistuent variables in the model. This is practice is known as the Hierarchical Principle . Models containing dummy variables can also have an interaction effect. Building off the example from the previous section, \\(\\beta_2\\) is still the difference in the intercept but with the new \\(\\beta_3\\) being the difference in slopes of the two resulting SLR models. \\[ E(Y|X) = \\begin{cases} (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3)x_1, & x_2 = 1 \\\\ \\beta_0 + \\beta_1 x_1, & x_2 = 0 \\end{cases} \\] Piecewise Model \u00b6 If the DV has an abrupt change in behaviour over different values of the IVs, it can be accounted for the through the use of a Piecewise Regression . The first method of creating a piecewise regression involves the use of an Indicator Function . It is essentially a dummy variable which depends on the value of the other IVs. \\[ z_{\\{x>=c\\}} = \\begin{cases} 0, & x < c \\\\ 1, & x \\ge c \\end{cases} \\] \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 z(x-c) \\\\ &= \\begin{cases} \\beta_0 + \\beta_1 x_1, & x < c \\\\ (\\beta_0 - \\beta_2c) + (\\beta_1 + \\beta_2) x_1, & x \\ge c \\end{cases} \\end{aligned} \\] Note that \\(z(x-c)\\) is treated as a distinct IV and hence can be equivalently expressed as \\(x_2\\) ; the full notation is used here for clarity. \\(c\\) is the value at which the DV abruptly changes in behaviour, known as a Kink in the graph, which continuously connects the two regression lines. The other method is to use an interaction variable instead. Similar to how the interaction variables resulted in the model to \"split\", the model now splits at \\(x = c\\) , resulting in a non-continuous gap. \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2z + \\beta_3 zx \\\\ &= \\begin{cases} \\beta_0 + \\beta_1 x_1, & x < c \\\\ (\\beta_0 - \\beta_2) + (\\beta_1 + \\beta_3) x_1, & x \\ge c \\end{cases} \\end{aligned} \\] Polynomial Model \u00b6 If the relationship between the DV and IV is complex (non-linear), then a Polynomial Regression can be used to better model the relationship between the two. \\[ E(Y|X) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 \\] Note that it is the same IV used in the regression, just with additional powers. Although a polynomial regression may capture the true relationship better, the regression parameters become hard to intepret. The partial derivatives can no longer be intepreted as holding other IVs constant as each IV is dependent on the same quantity, just to different powers. \\[ \\frac{\\partial E(Y|X)}{\\partial x} = \\beta_1 + 2\\beta_2 x + ... + m \\beta_m x^{m-1} \\]","title":"Multiple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#multiple-linear-regression","text":"The natural extension of the SLR model is to include more than one independent variable , which thus results in the more generalized Multiple Linear Regression (MLR) model. \\[ E(Y|X_1, ... X_p) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_{p}X_{p} \\] Unlike the SLR which studies how each individual IV influences the DV, the goal of the MLR model is to study how all the IVs operate together to influence the DV. \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) when \\(X_1 = X_2 = ... = 0\\) Change in \\(E(Y)\\) given a one unit increase in \\(X_j\\) , holding all other \\(X\\) 's constant Intercept Parameter \"Slope\" Parameter For avoidance of doubt, the subscript \\(i\\) will be used to denote observations while \\(j\\) will be used to denote independent variables. Every observation can also be expressed as the sum of the regression and the error term. However, due to the multi-dimensional nature of the model, it is commonly expressed in matrix notation: \\[ \\begin{aligned} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} &= \\begin{pmatrix} 1 & x_{11} & x_{12} & ... & x_{1p} \\\\ 1 & x_{21} & x_{22} & ... & x_{2p} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & ... & x_{np} \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix} + \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix} \\\\ \\boldsymbol{y} &= \\boldsymbol{X\\beta + \\varepsilon} \\end{aligned} \\]","title":"Multiple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#ordinary-least-squares","text":"Similar to the SLR model, the regression parameters can be found by minimizing the sum of squared residuals: \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_p x_ip)]^2 \\] There are \\(p+1\\) FOC equations to solve through the minimization, with the additional one reflecting the intercept parameter. It is difficult to algebraically solve this system of equations, thus there is no closed form tsolution for each individual paramater. Instead, here is a vector solution for all of the parameters: \\[ \\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\hat{\\beta_0} \\\\ \\hat{\\beta_1} \\\\ \\vdots \\\\ \\hat{\\beta_p} \\end{pmatrix} = (\\boldsymbol{X'X})^{-1}\\boldsymbol{X'y} \\] Note that since there are \\(p+1\\) equations that must be solved, the model has \\(n-p+1\\) degrees of freedom. Following the same logic, there must be at least \\(p+1\\) observations in order to solve the equations and hence construct the model. This results in the following fitted regression model, which can be graphically expressed as a Regression Plane : \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_P x_ip \\]","title":"Ordinary Least Squares"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#manual-computation","text":"The tricky part is that \\((\\boldsymbol{X'X})^{-1}\\) is hard to compute by manually , except for the special case where \\(p=1\\) (SLR). Thus, it is likely that the parameters will be provided by the question. If required to compute them manually, then \\((\\boldsymbol{X'X})^{-1}\\) is likely to be provided. The remaining \\(\\boldsymbol{X'y}\\) still needs to be computed and put together to obtain the regression parameters. However, if the model has \\(p=2\\) but no intercept , then \\((\\boldsymbol{X'X})^{-1}\\) is a 2 x 2 Matrix whose inverse can be easily calculated. Similarly, if \\((\\boldsymbol{X'X})^{-1}\\) is a Diagonal Matrix , its inverse can be easily calculated as well.","title":"Manual Computation"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#goodness-of-fit","text":"The ANOVA for MLR follows the same intuition as the SLR version, adjusted for the new degrees of freedom: Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(p\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-(p+1)\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) - The coefficient of determination still represents the proportion of variance explained by the regression, but has a slightly different formula: \\[ R^2 = \\frac{RegSS}{TSS} = r_{y,\\hat{y}}^2 \\] The multiple IVs of the model are now captured through \\(\\hat{y}\\) instead of \\(x\\) directly. However, the hypothesis test under the MLR is vastly different from the SLR version. Instead of testing if an individual IV is useful, it tests if all the IVs are collectively useful in helping to explain the DV. \\[ \\begin{aligned} H_0 &: \\beta_1 = \\beta_2 = ... = \\beta_p = 0 \\\\ H_1 &: \\text{At least one } \\beta_j \\text{ is non-zero} \\end{aligned} \\] Thus, rejecting the null hypothesis implies that at least one of the IVs used is useful, but does not provide much insight into which of them are useful.","title":"Goodness of Fit"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#partial-f-test","text":"A partial F-test can be used to precisely determine which of the IVs are useful in explaining the DV. A regular F test compares the null model with no IVs to the desired model with all the IVs. If the sum of squares are significantly different, then it implies that the additional IVs are jointly useful in explaining \\(Y\\) . The partial F test generalizes this idea. Instead of considering a null model with no IVs, a Reduced Model with a limited number IVs ( \\(q\\) ) is considered instead. Consequently, the desired model is known as the Full Model with all \\(p\\) IVs, where \\(q < p\\) . \\[ \\begin{aligned} H_0: \\beta_{p-q+1} = ... = \\beta_p = 0 \\\\ H_1: \\beta_{p-q+1} = ... = \\beta_p \\ne 0 \\end{aligned} \\] The test is also commonly referred to as the Generalized F test, where the models are referred to as the Restricted and Unrestricted Models. The difference in the RSS between the Full and Reduced Model is known as the Extra Sum of Squares (ExtraSS) . It represents the contribution of the missing variables in explaining the variance of \\(Y\\) . Under the null hypothesis, there should be no difference between the two RSS, and thus \\(ExtraSS = 0\\) . \\[ ExtraSS = RSS_{Reduced} - RSS_{Full} \\] Thus, the Partial F-statistic is testing for the equality of variance between the two RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_{p-q+1} = ... = \\beta_p \\ne 0\\) . \\[ \\begin{aligned} F &= \\frac{MS_{ExtraSS}}{MS_{RSS_{Full}}} \\\\ &= \\frac{ExtraSS/q}{RSS/(n-2)} \\\\ &= \\frac{(RSS_{Reduced} - RSS_{Full})/q}{RSS/(n-2)} \\\\ &= (n-2) * \\frac{(1- R_{Reduced}^2) - (1 - R_{Full}^2)}{1-RSS_{Full}^2}, \\text{divide both by TSS} \\\\ &= (n-2) * \\frac{R_{Full}^2 - R_{Reduced}^2}{1-RSS_{Full}^2} \\end{aligned} \\]","title":"Partial F Test"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#statistical-inference","text":"","title":"Statistical Inference"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#sampling-distributions","text":"Similar to SLR, the regression parameters are normally distributed as well. However, since there are multiple regression parameters, they collectively follow a multivariate normal distribution . \\[ \\hat{\\beta} \\sim N_{p+1}(\\beta, \\sigma^2 (\\boldsymbol{X'X})^{-1}) \\] The variance of the distribution is known as the Variance Covariance Matrix , which provides the covariances between every possible pair of regression parameters. Since the covariance of a variable with itself is its variance, the diagonals are the respective variances of the parameters. Note that the first element of the diagonal is the intercept, thus the variance of the jth IV is the (j+1)th element of the diagonal . \\[ Var(\\hat{\\beta}) = \\begin{pmatrix} Var(\\hat{\\beta}_0) & Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) & ... & Cov(\\hat{\\beta}_0, \\hat{\\beta}_p) \\\\ Cov(\\hat{\\beta}_1, \\hat{\\beta}_0) & Var(\\hat{\\beta}_1) & ... & Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ Cov(\\hat{\\beta}_p, \\hat{\\beta}_0) & Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) & ... & Var(\\hat{\\beta}_p) \\end{pmatrix} \\] Note that the covariances are symmetrical about the diagonal - \\(Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = Cov(\\hat{\\beta}_1, \\hat{\\beta}_0)\\) .","title":"Sampling Distributions"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#hypothesis-testing","text":"Similar to SLR, the t-test can be used to test for the significance of an individual IV, but the intepretation of the test is different from the SLR case. It tests the usefulness of an individual IV in the presence of the other predictors . \\[ \\begin{aligned} H_0: \\beta_j = 0 \\\\ H_1: \\beta_j \\ne 0 \\end{aligned} \\] \\[ t(\\beta_j) = \\frac{\\hat{\\beta_j} - \\beta_j}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\] Recall that the variance of the jth IV si the (j+1)th element of the variance covariance matrix. \\[ t(\\beta_j) \\sim t_{n-p-1} \\] However, this leads to several odd results which needs to be accounted for: Predictor is not significant individually but significant when taken alone . Predictors are not significant individually but significant when taken together . There are now two possible ways to test for the significance of IVs: Conduct a single F-test to test for the joint significance of all IVs Conduct multiple t-tests to test for the joint significance of each IV The problem with the multiple t-test approach lies with the type I error of the tests. For \\(\\alpha = 0.05\\) , the probability of correctly rejecting the null is \\(0.95\\) . Assuming that all the tests are independent, the probability of correctly rejecting all the nulls is \\(0.95^p\\) , which drastically decreases with the number of tests conducted. Given enough predictors, this means that the probability drops to approximately 0, which means that there is bound to be a wrongly rejected null; a type I error is guaranteed even though it was supposed to be limited at a 0.05 chance. The Bonferroni Correction is a method of adjusting \\(\\alpha\\) of each hypothesis test such that the overall type I error is kept at its desired level. However, this has the consequence of increasing the probability of type II errors, which is why it is not popular. The F-test has the advantage of controlling the type I error regardless of the number of predictors , which is why it is preferred for hypothesis testing in the MLR.","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#confidence-intervals","text":"Similar to SLR, the confidence intervals can be constructed using the distribution of the test-statistic: \\[ \\text{Confidence Interval} = \\hat{\\beta}_j \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_j}} \\]","title":"Confidence Intervals"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#prediction-intervals","text":"Unlike in SLR, it is difficult to determine the distribution of the prediction error. Thus, the final result can be found below: \\[ \\begin{aligned} \\text{Prediction Interval} &= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\\\ &= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\sqrt{s^2 [1 + x'_* (\\boldsymbol{X'X})^-1 * x_*]} \\end{aligned} \\] Despite the result looking more complicated, the key takeaway remains the same - the further away \\(x_*\\) is from \\(\\bar{x}\\) , the greater","title":"Prediction Intervals"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#variations-of-mlr","text":"","title":"Variations of MLR"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#qualitative-iv","text":"The discussion so far has mostly focused on Quantitative IVs, thus this section will explore Qualitative IVs. They can only take values from a list of pre-defined values, known as the Levels of the variable. In a regression context, most qualitative IVs are represented in the form of a Dummy Variable which can only take two possible levels - Yes (1) or No (0). Note that there are other ways of encoding a dummy variable (-1/0/1 etc), but the principles stay large the same. \\[ \\text{Dummy Variable} = \\begin{cases} 1, & \\text{First Level} \\\\ 0, & \\text{Second Level} \\end{cases} \\] In general, \\(n-1\\) dummy variables are needed to represent a qualitative variable with \\(n\\) levels. This is because the status of the last level can be deduced from the other dummy variables . Thus, including a seperate dummy variable for this last level is redundant and will lead to the problem of Collinearity , which will be explored in a later section. For instance, consider four levels (North, East, South & West), represented by the three dummy variables ( \\(N, E , S\\) ). If any of the variables are 1, they represent their respective direction (North, East & South). If all of them are 0, then the direction is the remaining level (West). The last remaining level is often referred to as the Baseline Level as it is the default level of the variable when all other dummies are 0. Any level can be used as the baseline, but the parameters will differ across models with different baselines. \\[ E(Y|X) = \\beta_0 + \\beta_{1, North} x_{North} + \\beta_{2, East} * x_{East} + \\beta_{3, South} * x_{South} \\] \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) at the baseline level Change in \\(E(Y)\\) from the baseline to the chosen level \\(X_1 = X_2 = ... = X_j = 0\\) \\(X_1 = X_2 = ... = 0; X_j = 1\\) Dummy variables are usually used in conjunction with quantitative ones . This essentially creates a \"seperate\" regression model for each of the levels. For the simplest case of one quantiative and one dummy, \\(\\beta_2\\) is the difference in the intercept of the two resulting SLR models. \\[ E(Y|X) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\] \\[ E(Y|X) = \\begin{cases} (\\beta_0 + \\beta_2) + \\beta_1 x_1, & x_2 = 1 \\\\ \\beta_0 + \\beta_1 x_1, & x_2 = 0 \\end{cases} \\]","title":"Qualitative IV"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#interaction-model","text":"So far, it was assumed that each IV had an independent effect on the DV. However, IVs may interact to produce a joint effect on the DV, where the effect of one IV depends on the value of another IV. For instance, the production of a factory may depend on the number of Machines and Workers. However, the more machines there are, the greater the effect of an additional worker . Thus, this interaction effect can be captured through an Interaction Variable , which is the product of both IVs: \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 \\\\ &= \\beta_0 + (\\beta_1 + \\beta_3 x_2)x_1 + \\beta_2 x_2 \\\\ &= \\beta_0 + \\beta_1 x_1 + (\\beta_2 + \\beta_3 x_1) x_2 \\\\ \\end{aligned} \\] A one unit increase in \\(x_1\\) will increase E(Y) by \\(\\beta_1 + \\beta_3 x_2\\) , which depends on the value of \\(x_2\\) as well, which is is why they interact with each another. Phrased another way, for every one unit increase in \\(x_2\\) , the change in E(Y) from a unit increase in \\(x_1\\) increases by \\(\\beta_3\\) . Something unusual to take note of is that the Interaction Variable tests significant but the constituent variables do not. In this case, it is common practice to retain both the interaction and the consistuent variables in the model. This is practice is known as the Hierarchical Principle . Models containing dummy variables can also have an interaction effect. Building off the example from the previous section, \\(\\beta_2\\) is still the difference in the intercept but with the new \\(\\beta_3\\) being the difference in slopes of the two resulting SLR models. \\[ E(Y|X) = \\begin{cases} (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3)x_1, & x_2 = 1 \\\\ \\beta_0 + \\beta_1 x_1, & x_2 = 0 \\end{cases} \\]","title":"Interaction Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#piecewise-model","text":"If the DV has an abrupt change in behaviour over different values of the IVs, it can be accounted for the through the use of a Piecewise Regression . The first method of creating a piecewise regression involves the use of an Indicator Function . It is essentially a dummy variable which depends on the value of the other IVs. \\[ z_{\\{x>=c\\}} = \\begin{cases} 0, & x < c \\\\ 1, & x \\ge c \\end{cases} \\] \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 z(x-c) \\\\ &= \\begin{cases} \\beta_0 + \\beta_1 x_1, & x < c \\\\ (\\beta_0 - \\beta_2c) + (\\beta_1 + \\beta_2) x_1, & x \\ge c \\end{cases} \\end{aligned} \\] Note that \\(z(x-c)\\) is treated as a distinct IV and hence can be equivalently expressed as \\(x_2\\) ; the full notation is used here for clarity. \\(c\\) is the value at which the DV abruptly changes in behaviour, known as a Kink in the graph, which continuously connects the two regression lines. The other method is to use an interaction variable instead. Similar to how the interaction variables resulted in the model to \"split\", the model now splits at \\(x = c\\) , resulting in a non-continuous gap. \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2z + \\beta_3 zx \\\\ &= \\begin{cases} \\beta_0 + \\beta_1 x_1, & x < c \\\\ (\\beta_0 - \\beta_2) + (\\beta_1 + \\beta_3) x_1, & x \\ge c \\end{cases} \\end{aligned} \\]","title":"Piecewise Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#polynomial-model","text":"If the relationship between the DV and IV is complex (non-linear), then a Polynomial Regression can be used to better model the relationship between the two. \\[ E(Y|X) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 \\] Note that it is the same IV used in the regression, just with additional powers. Although a polynomial regression may capture the true relationship better, the regression parameters become hard to intepret. The partial derivatives can no longer be intepreted as holding other IVs constant as each IV is dependent on the same quantity, just to different powers. \\[ \\frac{\\partial E(Y|X)}{\\partial x} = \\beta_1 + 2\\beta_2 x + ... + m \\beta_m x^{m-1} \\]","title":"Polynomial Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/","text":"Gauss Markov Theorem \u00b6 Using OLS, the estimated regression parameters will always be unbiased under certain assumptions. The Gauss Markov Theorem extends this, which states that under certain assumptions, the OLS estimators will have the lowest variance among all possible linear unbiased estimators. In a statistics context, they are said to be the most efficient among all other linear unbiased estimators. In a regression context, the OLS estimators are said to be the Best Linear Unbiased Estimators (BLUE). This section will go over the various assumptions for both OLS and Gauss Markov Theorem. It will also cover the diagnostics to determine if the assumptions have been violated. The assumptions needed for OLS and the Gauss Markov theorem are often mixed up with each other as the assumptions needed for OLS are also needed for the theorem. This set of notes makes a clear distinction between the two. OLS Assumptions \u00b6 #1: Linearity \u00b6 Linear regression is a model where the relationship between the DV and IVs are linear. Thus, the regression parameters must be linear , but NOT the DV or IV. This means that the model is still considered a \"Linear Regression\" even after a transformation of the DV and/or IV. \\[ \\begin{aligned} y_i &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\\\ y_i &= \\beta_0 + \\beta_1 x^2 + \\beta_2 x^3 \\\\ \\ln y_i &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\end{aligned} \\] #2: Exogenity \u00b6 Exogenity refers to how a variable comes from outside the model and is thus independent of any other variables within the model . In a regression context, this comes in the form of the errors having a conditional mean of 0 , ensuring that the errors are random and thus not related to the IVs. \\[ E(\\varepsilon_i | x_{ij}) = 0 \\\\ \\] \"Endo\" and \"Exo\" in Greek means \"In\" and \"Out\" respectively, which is how the meaning of the words were derived. There are two key implications of Exogenity: By the Law of Total Expectations, the unconditional expectation of the error is also 0. By the Linearity of Conditional Expectations, the expectation of the product of the Error and IVs is 0. \\[ \\begin{aligned} E(\\varepsilon_i) = 0 \\\\ E(\\varepsilon_i x_{ij}) = 0 \\end{aligned} \\] Following these two implications, it can be shown that the Covariance between the Error and IVs are also 0, which is another consequence of independence (NOT the other way around). \\[ \\begin{aligned} Cov (\\varepsilon_i, x_{ij}) &= E(\\varepsilon_i x_{ij}) - E(\\varepsilon_i) * E(x_{ij}) \\\\ &= 0 - 0 * E(x_{ij}) \\\\ &= 0 \\end{aligned} \\] Without exogenity, the regression parameters would reflect the effect of both the IV and the unmodelled variable within the error term. This causes the OLS estimate to be biased , known as the Omitted Variable Bias . Since the unmodelled variable confounds the results of the regression, it is known as a Confounding Variable . Residual Analysis \u00b6 Since the errors are unobservable, the residuals are used to estimate the errors. If the fitted model is adequate - all relavent IVs are included in the right form, then the residuals should closely resemble the errors and therefore be structureless (random). However, if there are patterns in the residuals , it indicates that there is additional information that can be used to improve the model and thus should be included. Due to the OLS, the correlation between residuals and existing IVs will always be 0 indicating no linear relationship . To check for unmodelled non-linear relationships , a Residual Plot of the IVs against the Residuals can be used. For instance, if the residual plot shows a quadractic pattern (curve), then a quadractic IV should be added into the model. Mathematically, it can be expressed as a function of the existing estimates: \\[ \\begin{aligned} \\hat{y_i} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\varepsilon}_i \\\\ \\hat{\\varepsilon_i} &= \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\text{ (From residual plot)} \\\\ \\hat{y_i} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &= (\\hat{\\beta}_0 + \\hat{\\gamma}_0) + (\\hat{\\beta}_1 + \\hat{\\gamma}_1)x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &= \\hat{\\beta}_0' + \\hat{\\beta}_1'x + \\hat{\\beta}_2' x^2 \\end{aligned} \\] #3: No Perfect Collinearity \u00b6 Collinearity refers to when an IV can be expressed as a linear combination of one or more other IVs . Perfect collinearity is an extreme case where an IV can be perfectly expressed as a combination of another. Technically speaking, Collinearity refers to one to one variable relationship, while Multicollinearity refers to one to many variable relationship, hence \"Multi\". Variable is a multiple of another: \\(x_1 = cx_2\\) Variable differs by a constant from another: \\(x_1 = x_2 \\pm c\\) Variable is an affine transformation of another: Sum of several variables is fixed: Dummy Variable Trap The issue with perfect collinearity is that it affects the linear algebra used to solve for the coefficients (EG. Two equations to solve for three unknowns). There will be no unique solutions - many different values for the coefficients could work equally well. Imperfect Collinearity \u00b6 Imperfect Collinearity is a less extreme case where an IV is highly (but not perfectly) correlated with one or more other IVs. Recall that correlation refers to the extent of a Linear relationship. Non-linear relationships between variables are fine (EG. Polynomial Regression). This means that including the IV does not bring much additional predctive power into the model as its effects are already captured through related predictors and thus can be removed from the model. Unlike in the perfect case, high collinearity does not prevent OLS from finding a solution. However, the intepretation of the variables become complicated: The original intepretation of coefficients \"holding other variables constant\" is no longer true as highly correlated variables tend to move together. Thus, it is hard to seperate the effects of an individual variable . Consequently, OLS has difficulty estimating these coefficients, which could result in weird meaningless estimates ; EG. Large positive coefficient but large negative for its correlated counterpart. This also results in higher standard errors for the coefficients of correlated variables. This reduces the magnitude of t-statistic , which results in more false negatives , failing to reject the null when it should. This results in important variables being omitted from the regression. Technically speaking, there is nothing wrong with collinearity if the purpose of the model is solely for prediction. However, if the purpose of the model was to establish causality, then collinearity poses a problem as it interferes with statistical inference. Detecting Collinearity \u00b6 The simplest way to detect collinearity is through a Scatterplot or Correlation Matrix , which shows the correlations between pairs of variables . A correlation of 0.8 and higher is typically considered high enough where the collinearity becomes problematic. However, the issue is that this method can only detect collinearity between pairs of variable at a time. In order to detect collinearity among three or more variables ( multicollinearity ), then the Variance Inflation Factor (VIF) should be used instead. \\[ VIF = \\frac{1}{1-R^2_j} \\] The VIF is derived from the variance of the regression coefficient. As mentioned previously, under the presence of collinearity, the standard error and hence variance of the coefficient increases (\"inflated\"). The extent of the increase is known as the VIF. \\[ \\hat{Var}(\\hat{\\beta_1}) = \\frac{MS_{RSS}}{(n-1) s^2} * \\frac{1}{1-R^2_j} \\] The \\(R^2_j\\) in the VIF is the coefficient of determination of a model where the jth IV is regressed against all other IVs . A high \\(R^2_j\\) means that the IV is well explained by the other IVs (high correlation), which indicates the presence of collinearity. Generally, a \\(VIF > 10\\) is deemed to have severe collinearity . #4: No Extreme Outliers \u00b6 Outliers are observations with unusual values of the DV relative to fitted regression model. The last OLS assumption is that there are no extreme outliers in the dataset used to create the regression model. Generally, as long as the DV and IVs have a positive and finite Kurtosis , then the probability of such observations occuring are low. Outliers are problematic as OLS is sensitive to outliers . Extreme outliers have large residuals which receive more weight in the optimization process, which causes the resulting model to accomodate it (when it should not), causing the resulting coefficients to be biased. Identifying Outliers \u00b6 By definition, Outliers have unusually large residuals . In order to gauge what is considered a \"large residual\", the residuals are standardized for comparison. Standardization requires knowledge of the sampling distribution of the residuals . Given that the errors have a constant variance of \\(\\sigma^2\\) , the variance of the residuals can be shown to be: \\[ var(\\hat{\\varepsilon}_i) = \\sigma^2 (1-h_{ii}) \\] \\(h_ii\\) is known as the Leverage of the observation, which will be covered in the following section. The sampling distribution can then be determined: \\[ \\hat{\\varepsilon}_i ~ N(0, \\sigma^2 (1-h_{ii})) \\] Thus, the standardized residuals are the raw residuals scaled by their standard error: \\[ \\hat{\\varepsilon}_i^{\\text{standardized}} = \\frac{\\hat{\\varepsilon}_i}{\\sqrt{\\sigma^2 (1-h_{ii})}} \\] In practice, the variance of the errors are unknown, thus it is estimated using the Sample Variance of the residuals instead. Residuals with standardized values of larger than 2 or 3 are considered large and thus can be considered as an outlier. High Leverage Points \u00b6 While outliers are unusual points of the DV, High Leverage observations have unusual values of the IV relative to the majority of the values. It is easy to identify high leverage points when there is only one IV through a scatterplot - simply find the observation that is away from the rest. It becomes much more complicated when there are multiple IVs. The observation's values for each of the IVs could be in the common range of each IV, but unusual when taken collectively : The leverage of the observation can be determined from the MLR: \\[ \\begin{aligned} \\hat{\\boldsymbol{y}} &= \\boldsymbol{X}\\boldsymbol{\\beta} \\\\ &= \\boldsymbol{X}[(X'X)^{-1}y] \\\\ &= \\boldsymbol{H}\\boldsymbol{y} \\end{aligned} \\] \\(\\boldsymbol{H}\\) is known as the Hat Matrix as it puts a hat on y in the notation. The leverage of the \\(ith\\) observation is the \\(ith\\) diagonal element of the matrix, \\(h_{ii}\\) . An observation is considered to have high leverage if its leverage is greater than three times the average leverage: \\[ h_{ii} \\gt 3 \\left(\\frac{p+1}{n}\\right) \\] Influential Points \u00b6 The effect of Outliers and High leverage points can both be summarized into a concept known as Influence . An observation is influential if the exclusion of the observation from the regression leads to significantly differently results. In general the process involves three steps: Fit the original model with all \\(n\\) observations; determine the \\(j-th\\) fitted value \\(\\hat{y}_j\\) Fit an adjusted model with omitting the \\(i-th\\) observation, determine the \\(j-th\\) fitted value \\(\\hat{y}_{j(i)}\\) Calculate the change in the \\(j-th\\) fitted value This process has to be repeated for all fitted values for all observations . Cook's Distance summarizes the effect of the \\(i-th\\) observation on the whole model: \\[ D_i = \\frac{\\sum^n_{j=1} (\\hat{y}_j - \\hat{y}_{j(i)})^2}{(p+1) \\cdot MS_{Residuals}} \\] This method of computation requires \\(n+1\\) datasets - 1 dataset with all the observation and \\(n\\) datasets with the \\(i-th\\) observation omitted. It is also extremely time consuming to have to fit a model to each dataset. An alternative method of determining Cook's Distance is to make use of both the Outliers and Leverage: \\[ D_i = \\frac{1}{p+1} \\cdot (e^{\\text{standardized}})^2 \\cdot \\frac{h_{ii}}{1-h_{ii}} \\] Thus, an observation must be unusual in BOTH the DV and IV in order to be considered influential. Outliers and High leverage points are necessary but not sufficient conditions to be influential. Gauss Markov Assumptions \u00b6 #1: Conditional Homoscedasticity \u00b6 Homoscedasticity refers to the error terms having constant variance while Heteroscedasticity refers to having non-constant variance. \\[ var(\\varepsilon_i | x_i) = \\sigma^2 \\] Under homoscedasticity, the sampling distribution of the estimates are easily derived and thus it can be shown that they are the most efficient estimators. The same cannot be proven under heteroscedasticity. Note that that the OLS estimates are still unbiased; they are just not the most efficient. Identifying Heteroscedasticity \u00b6 Similar to before, if the model is adequate, then the residuals should resemble the errors and have constant variance . Thus, this can be easily determined through a residual plot of the Residuals against the fitted values. If the points are equally spread out about the mean (0) and show no pattern , then homoscedasticity is present. However, if the points show an increasing or decreasing variance (typically in the shape of a funnel ), then heteroscedasticity is present. Alternatively, a hypothesis test can be conducted to determine if heteroscedasticity is present, known as the Bresuch Pagan Test . \\[ \\begin{aligned} H_0 &: \\sigma^2 \\\\ H_1 &: \\sigma^2 + \\boldsymbol{Z\\gamma} \\end{aligned} \\] The test-statistic is computed as follows: Compute the squared standardized residuals from the original model Regress them onto the variables in Z (LOL need to change this part) Compute the RegSS of the new regression \\[ T = \\frac{RegSS}{2} \\] The test-statistic follows a chi-square distribution with \\(q\\) degrees of freedom, where \\(q\\) is the number of variables in \\(\\boldsymbol{Z}\\) . \\[ T \\sim \\chi^2_q \\] Dealing with Heteroscedasticity \u00b6 If prior information is known about the structure of the data, then the most intuitive method would be to incorporate that information into the data. \\[ Var(\\varepsilon_i) = \\frac{\\sigma^2}{w_i} \\] If no prior information is known, then the Heteroscedasticity can be reduced by using a Variance Stabilizing Transformation , such as the Log or Squareroot . Note that since they require positive data, a constant can be added to each term before the transformation to ensure that the values are positive. It is out of the scope for this set of notes to show why these help to stabilize variance. Alternatively, if there is only mild heteroscedasticity in the data, then OLS can be used but with an adjustment to the standard errors of the coefficients, known as heteroscedastic-robust standard errors . Due to complexity of the computations, it will not be covered in this set of notes. However, the general idea is that an weighted estimate of the variance covariance matrix is computed and the standard errors are computed from there. #2: No Serial Correlation \u00b6 If errors are correlated with one another, it is known as Serial Correlation or Autocorrelation . It implies that there are other unmodelled factors that can be used for prediction, which would imply the current model specification to be wrong. Thus, for the SLR model to be true, the errors must be independent of one another . \\[ Cov(\\varepsilon_i,\\varepsilon_j) = 0 \\] Confidence Intervals and PI are narrower than it should be > 95% PI is actually < 95%> P values lower > Appear statisticlaly significant when they shld not be Time series tends to have errors that are positively correlated, which is why it has its own dedicated section No Serial Correlation > Outcome of zero conditional mean, but most likely in time series data Error Distribution \u00b6 Although not needed for OLS estimation or Guass Markov, the errors of the regression are usually assumed to be normally distributed . \\[ \\varepsilon \\sim N(0, \\sigma^2) \\] If the errors are normally distributed, then it follows that \\(\\beta\\) is normally distributed as well since they are linear and additive. This greatly eases the computation needed to determine the sampling distribution for statistical inference. Q-Q Plots \u00b6 Since the errors are normally distributed, the residuals should be normally distributed as well. This can be verified using a Quantile-Quantile Plot (QQ Plot) , which compares the quantiles of two distributions. The first distribution is plotted on the x-axis while the second on the y-axis. If the quantiles are the same (same distribution), then the points should lie on \\(y = x\\) , the 45 degree line.","title":"Gauss Markov Theorem"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#gauss-markov-theorem","text":"Using OLS, the estimated regression parameters will always be unbiased under certain assumptions. The Gauss Markov Theorem extends this, which states that under certain assumptions, the OLS estimators will have the lowest variance among all possible linear unbiased estimators. In a statistics context, they are said to be the most efficient among all other linear unbiased estimators. In a regression context, the OLS estimators are said to be the Best Linear Unbiased Estimators (BLUE). This section will go over the various assumptions for both OLS and Gauss Markov Theorem. It will also cover the diagnostics to determine if the assumptions have been violated. The assumptions needed for OLS and the Gauss Markov theorem are often mixed up with each other as the assumptions needed for OLS are also needed for the theorem. This set of notes makes a clear distinction between the two.","title":"Gauss Markov Theorem"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#ols-assumptions","text":"","title":"OLS Assumptions"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#1-linearity","text":"Linear regression is a model where the relationship between the DV and IVs are linear. Thus, the regression parameters must be linear , but NOT the DV or IV. This means that the model is still considered a \"Linear Regression\" even after a transformation of the DV and/or IV. \\[ \\begin{aligned} y_i &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\\\ y_i &= \\beta_0 + \\beta_1 x^2 + \\beta_2 x^3 \\\\ \\ln y_i &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\end{aligned} \\]","title":"#1: Linearity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#2-exogenity","text":"Exogenity refers to how a variable comes from outside the model and is thus independent of any other variables within the model . In a regression context, this comes in the form of the errors having a conditional mean of 0 , ensuring that the errors are random and thus not related to the IVs. \\[ E(\\varepsilon_i | x_{ij}) = 0 \\\\ \\] \"Endo\" and \"Exo\" in Greek means \"In\" and \"Out\" respectively, which is how the meaning of the words were derived. There are two key implications of Exogenity: By the Law of Total Expectations, the unconditional expectation of the error is also 0. By the Linearity of Conditional Expectations, the expectation of the product of the Error and IVs is 0. \\[ \\begin{aligned} E(\\varepsilon_i) = 0 \\\\ E(\\varepsilon_i x_{ij}) = 0 \\end{aligned} \\] Following these two implications, it can be shown that the Covariance between the Error and IVs are also 0, which is another consequence of independence (NOT the other way around). \\[ \\begin{aligned} Cov (\\varepsilon_i, x_{ij}) &= E(\\varepsilon_i x_{ij}) - E(\\varepsilon_i) * E(x_{ij}) \\\\ &= 0 - 0 * E(x_{ij}) \\\\ &= 0 \\end{aligned} \\] Without exogenity, the regression parameters would reflect the effect of both the IV and the unmodelled variable within the error term. This causes the OLS estimate to be biased , known as the Omitted Variable Bias . Since the unmodelled variable confounds the results of the regression, it is known as a Confounding Variable .","title":"#2: Exogenity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#residual-analysis","text":"Since the errors are unobservable, the residuals are used to estimate the errors. If the fitted model is adequate - all relavent IVs are included in the right form, then the residuals should closely resemble the errors and therefore be structureless (random). However, if there are patterns in the residuals , it indicates that there is additional information that can be used to improve the model and thus should be included. Due to the OLS, the correlation between residuals and existing IVs will always be 0 indicating no linear relationship . To check for unmodelled non-linear relationships , a Residual Plot of the IVs against the Residuals can be used. For instance, if the residual plot shows a quadractic pattern (curve), then a quadractic IV should be added into the model. Mathematically, it can be expressed as a function of the existing estimates: \\[ \\begin{aligned} \\hat{y_i} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\varepsilon}_i \\\\ \\hat{\\varepsilon_i} &= \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\text{ (From residual plot)} \\\\ \\hat{y_i} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &= (\\hat{\\beta}_0 + \\hat{\\gamma}_0) + (\\hat{\\beta}_1 + \\hat{\\gamma}_1)x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &= \\hat{\\beta}_0' + \\hat{\\beta}_1'x + \\hat{\\beta}_2' x^2 \\end{aligned} \\]","title":"Residual Analysis"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#3-no-perfect-collinearity","text":"Collinearity refers to when an IV can be expressed as a linear combination of one or more other IVs . Perfect collinearity is an extreme case where an IV can be perfectly expressed as a combination of another. Technically speaking, Collinearity refers to one to one variable relationship, while Multicollinearity refers to one to many variable relationship, hence \"Multi\". Variable is a multiple of another: \\(x_1 = cx_2\\) Variable differs by a constant from another: \\(x_1 = x_2 \\pm c\\) Variable is an affine transformation of another: Sum of several variables is fixed: Dummy Variable Trap The issue with perfect collinearity is that it affects the linear algebra used to solve for the coefficients (EG. Two equations to solve for three unknowns). There will be no unique solutions - many different values for the coefficients could work equally well.","title":"#3: No Perfect Collinearity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#imperfect-collinearity","text":"Imperfect Collinearity is a less extreme case where an IV is highly (but not perfectly) correlated with one or more other IVs. Recall that correlation refers to the extent of a Linear relationship. Non-linear relationships between variables are fine (EG. Polynomial Regression). This means that including the IV does not bring much additional predctive power into the model as its effects are already captured through related predictors and thus can be removed from the model. Unlike in the perfect case, high collinearity does not prevent OLS from finding a solution. However, the intepretation of the variables become complicated: The original intepretation of coefficients \"holding other variables constant\" is no longer true as highly correlated variables tend to move together. Thus, it is hard to seperate the effects of an individual variable . Consequently, OLS has difficulty estimating these coefficients, which could result in weird meaningless estimates ; EG. Large positive coefficient but large negative for its correlated counterpart. This also results in higher standard errors for the coefficients of correlated variables. This reduces the magnitude of t-statistic , which results in more false negatives , failing to reject the null when it should. This results in important variables being omitted from the regression. Technically speaking, there is nothing wrong with collinearity if the purpose of the model is solely for prediction. However, if the purpose of the model was to establish causality, then collinearity poses a problem as it interferes with statistical inference.","title":"Imperfect Collinearity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#detecting-collinearity","text":"The simplest way to detect collinearity is through a Scatterplot or Correlation Matrix , which shows the correlations between pairs of variables . A correlation of 0.8 and higher is typically considered high enough where the collinearity becomes problematic. However, the issue is that this method can only detect collinearity between pairs of variable at a time. In order to detect collinearity among three or more variables ( multicollinearity ), then the Variance Inflation Factor (VIF) should be used instead. \\[ VIF = \\frac{1}{1-R^2_j} \\] The VIF is derived from the variance of the regression coefficient. As mentioned previously, under the presence of collinearity, the standard error and hence variance of the coefficient increases (\"inflated\"). The extent of the increase is known as the VIF. \\[ \\hat{Var}(\\hat{\\beta_1}) = \\frac{MS_{RSS}}{(n-1) s^2} * \\frac{1}{1-R^2_j} \\] The \\(R^2_j\\) in the VIF is the coefficient of determination of a model where the jth IV is regressed against all other IVs . A high \\(R^2_j\\) means that the IV is well explained by the other IVs (high correlation), which indicates the presence of collinearity. Generally, a \\(VIF > 10\\) is deemed to have severe collinearity .","title":"Detecting Collinearity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#4-no-extreme-outliers","text":"Outliers are observations with unusual values of the DV relative to fitted regression model. The last OLS assumption is that there are no extreme outliers in the dataset used to create the regression model. Generally, as long as the DV and IVs have a positive and finite Kurtosis , then the probability of such observations occuring are low. Outliers are problematic as OLS is sensitive to outliers . Extreme outliers have large residuals which receive more weight in the optimization process, which causes the resulting model to accomodate it (when it should not), causing the resulting coefficients to be biased.","title":"#4: No Extreme Outliers"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#identifying-outliers","text":"By definition, Outliers have unusually large residuals . In order to gauge what is considered a \"large residual\", the residuals are standardized for comparison. Standardization requires knowledge of the sampling distribution of the residuals . Given that the errors have a constant variance of \\(\\sigma^2\\) , the variance of the residuals can be shown to be: \\[ var(\\hat{\\varepsilon}_i) = \\sigma^2 (1-h_{ii}) \\] \\(h_ii\\) is known as the Leverage of the observation, which will be covered in the following section. The sampling distribution can then be determined: \\[ \\hat{\\varepsilon}_i ~ N(0, \\sigma^2 (1-h_{ii})) \\] Thus, the standardized residuals are the raw residuals scaled by their standard error: \\[ \\hat{\\varepsilon}_i^{\\text{standardized}} = \\frac{\\hat{\\varepsilon}_i}{\\sqrt{\\sigma^2 (1-h_{ii})}} \\] In practice, the variance of the errors are unknown, thus it is estimated using the Sample Variance of the residuals instead. Residuals with standardized values of larger than 2 or 3 are considered large and thus can be considered as an outlier.","title":"Identifying Outliers"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#high-leverage-points","text":"While outliers are unusual points of the DV, High Leverage observations have unusual values of the IV relative to the majority of the values. It is easy to identify high leverage points when there is only one IV through a scatterplot - simply find the observation that is away from the rest. It becomes much more complicated when there are multiple IVs. The observation's values for each of the IVs could be in the common range of each IV, but unusual when taken collectively : The leverage of the observation can be determined from the MLR: \\[ \\begin{aligned} \\hat{\\boldsymbol{y}} &= \\boldsymbol{X}\\boldsymbol{\\beta} \\\\ &= \\boldsymbol{X}[(X'X)^{-1}y] \\\\ &= \\boldsymbol{H}\\boldsymbol{y} \\end{aligned} \\] \\(\\boldsymbol{H}\\) is known as the Hat Matrix as it puts a hat on y in the notation. The leverage of the \\(ith\\) observation is the \\(ith\\) diagonal element of the matrix, \\(h_{ii}\\) . An observation is considered to have high leverage if its leverage is greater than three times the average leverage: \\[ h_{ii} \\gt 3 \\left(\\frac{p+1}{n}\\right) \\]","title":"High Leverage Points"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#influential-points","text":"The effect of Outliers and High leverage points can both be summarized into a concept known as Influence . An observation is influential if the exclusion of the observation from the regression leads to significantly differently results. In general the process involves three steps: Fit the original model with all \\(n\\) observations; determine the \\(j-th\\) fitted value \\(\\hat{y}_j\\) Fit an adjusted model with omitting the \\(i-th\\) observation, determine the \\(j-th\\) fitted value \\(\\hat{y}_{j(i)}\\) Calculate the change in the \\(j-th\\) fitted value This process has to be repeated for all fitted values for all observations . Cook's Distance summarizes the effect of the \\(i-th\\) observation on the whole model: \\[ D_i = \\frac{\\sum^n_{j=1} (\\hat{y}_j - \\hat{y}_{j(i)})^2}{(p+1) \\cdot MS_{Residuals}} \\] This method of computation requires \\(n+1\\) datasets - 1 dataset with all the observation and \\(n\\) datasets with the \\(i-th\\) observation omitted. It is also extremely time consuming to have to fit a model to each dataset. An alternative method of determining Cook's Distance is to make use of both the Outliers and Leverage: \\[ D_i = \\frac{1}{p+1} \\cdot (e^{\\text{standardized}})^2 \\cdot \\frac{h_{ii}}{1-h_{ii}} \\] Thus, an observation must be unusual in BOTH the DV and IV in order to be considered influential. Outliers and High leverage points are necessary but not sufficient conditions to be influential.","title":"Influential Points"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#gauss-markov-assumptions","text":"","title":"Gauss Markov Assumptions"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#1-conditional-homoscedasticity","text":"Homoscedasticity refers to the error terms having constant variance while Heteroscedasticity refers to having non-constant variance. \\[ var(\\varepsilon_i | x_i) = \\sigma^2 \\] Under homoscedasticity, the sampling distribution of the estimates are easily derived and thus it can be shown that they are the most efficient estimators. The same cannot be proven under heteroscedasticity. Note that that the OLS estimates are still unbiased; they are just not the most efficient.","title":"#1: Conditional Homoscedasticity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#identifying-heteroscedasticity","text":"Similar to before, if the model is adequate, then the residuals should resemble the errors and have constant variance . Thus, this can be easily determined through a residual plot of the Residuals against the fitted values. If the points are equally spread out about the mean (0) and show no pattern , then homoscedasticity is present. However, if the points show an increasing or decreasing variance (typically in the shape of a funnel ), then heteroscedasticity is present. Alternatively, a hypothesis test can be conducted to determine if heteroscedasticity is present, known as the Bresuch Pagan Test . \\[ \\begin{aligned} H_0 &: \\sigma^2 \\\\ H_1 &: \\sigma^2 + \\boldsymbol{Z\\gamma} \\end{aligned} \\] The test-statistic is computed as follows: Compute the squared standardized residuals from the original model Regress them onto the variables in Z (LOL need to change this part) Compute the RegSS of the new regression \\[ T = \\frac{RegSS}{2} \\] The test-statistic follows a chi-square distribution with \\(q\\) degrees of freedom, where \\(q\\) is the number of variables in \\(\\boldsymbol{Z}\\) . \\[ T \\sim \\chi^2_q \\]","title":"Identifying Heteroscedasticity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#dealing-with-heteroscedasticity","text":"If prior information is known about the structure of the data, then the most intuitive method would be to incorporate that information into the data. \\[ Var(\\varepsilon_i) = \\frac{\\sigma^2}{w_i} \\] If no prior information is known, then the Heteroscedasticity can be reduced by using a Variance Stabilizing Transformation , such as the Log or Squareroot . Note that since they require positive data, a constant can be added to each term before the transformation to ensure that the values are positive. It is out of the scope for this set of notes to show why these help to stabilize variance. Alternatively, if there is only mild heteroscedasticity in the data, then OLS can be used but with an adjustment to the standard errors of the coefficients, known as heteroscedastic-robust standard errors . Due to complexity of the computations, it will not be covered in this set of notes. However, the general idea is that an weighted estimate of the variance covariance matrix is computed and the standard errors are computed from there.","title":"Dealing with Heteroscedasticity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#2-no-serial-correlation","text":"If errors are correlated with one another, it is known as Serial Correlation or Autocorrelation . It implies that there are other unmodelled factors that can be used for prediction, which would imply the current model specification to be wrong. Thus, for the SLR model to be true, the errors must be independent of one another . \\[ Cov(\\varepsilon_i,\\varepsilon_j) = 0 \\] Confidence Intervals and PI are narrower than it should be > 95% PI is actually < 95%> P values lower > Appear statisticlaly significant when they shld not be Time series tends to have errors that are positively correlated, which is why it has its own dedicated section No Serial Correlation > Outcome of zero conditional mean, but most likely in time series data","title":"#2: No Serial Correlation"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#error-distribution","text":"Although not needed for OLS estimation or Guass Markov, the errors of the regression are usually assumed to be normally distributed . \\[ \\varepsilon \\sim N(0, \\sigma^2) \\] If the errors are normally distributed, then it follows that \\(\\beta\\) is normally distributed as well since they are linear and additive. This greatly eases the computation needed to determine the sampling distribution for statistical inference.","title":"Error Distribution"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#q-q-plots","text":"Since the errors are normally distributed, the residuals should be normally distributed as well. This can be verified using a Quantile-Quantile Plot (QQ Plot) , which compares the quantiles of two distributions. The first distribution is plotted on the x-axis while the second on the y-axis. If the quantiles are the same (same distribution), then the points should lie on \\(y = x\\) , the 45 degree line.","title":"Q-Q Plots"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/","text":"Statistical Learning \u00b6 The regression concepts covered in the previous sections are widely considered to be traditional applied statistics . In a contemporary context, regression is just one of the many methods that fall under Statistical Learning . It is a framework of harnessing data to gain an understanding of how the data is related to one another and/or how a group of variables can be used to accurately predict another. Similar to regression, the relationship between variables can be expressed as a combination of a Signal Function and a Noise term: \\[ y = f(x) + \\varepsilon \\] The two terms originate from Engineering, where Signal refers to the meaningful component of the data while Noise refers to the random variation that inteferes with the signal. Statistical learning models are distinguished based on their signal function. There are two main kinds of models: Parametric Models Non-Parametric Assumes DV follows a specific functional form Does not assume any functional form Function parameters determined from the data - Does not require a large amount of data Requires a large amount of data to work well EG. Linear Regression EG. Clustering | May not fit the data well | Fits the data well | | Described by parameters | No parameters used | | Simple to implement | Requires large amount of data to implement | | Risk that assumed function is wrong | Does not make any assumption about data | They can also be further split according to: Supervised Learning Unsupervised Learning Specified DV to supervise the learning No specific variable chosen Inference/Prediction with respect to the DV Inference/Prediction for all variables EG. Linear Regression EG. Clustering Regression vs classification classification - classifying the observations to a certian level Model Accuracy \u00b6 Since the end goal of the model is to make predictions on new unobserved data, the quality of the model should be evaluated against its performance on unobserved data as well. The observed data used to create the model is known as the Training Data as it helps train the signal function to identify relationships between variables, while the unobserved data used to evaluate the model is known as the Test Data . The quality of the model can then be quantified by the extent to which the model predictions match the data . Similar to regression, this quantity is known as the Error of the model and is summarized through the Mean Square Error (MSE) statistic. The MSE is the average of the sum of squared errors and can be calculated for both the training and test data: \\[ \\begin{aligned} \\text{Training MSE} &= \\frac{[y_i - \\hat{f}(x_0)]^2}{n_{training}} \\\\ \\text{Test MSE} &= \\frac{[y_0 - \\hat{f}(x_0)]^2}{n_{test}} \\end{aligned} \\] Note that this is different from the MSE defined in ANOVA, where the MS is divded by its degrees of freedom rather than the number of observations. The MSE defined in this section is a general concept while the ANOVA MSE is a purely regression concept. The Training MSE reflects the goodness of fit of the model while the Test MSE reflects its prediction accuracy. As alluded to earlier, the goal of statistical learning is to choose the model with the lowest Test MSE . In general, the training MSE should always be smaller than the testing MSE, which is why they are not interchangeable. This is because all models are trained to match the training data to various extents, thus they should naturally have relatively smaller errors. On the flipside, test MSE should always be higher because the model is likely to have mistakenly captured some of the noise in the training data that do not generalize to the test data, resulting in a higher testing error. The extent of the difference is dependent on how well the model fits the training data; the extent to which it learns from it: High Flexibility/Complexity - Tends to overfit the training data; matches data too much; learns too much Low Flexibility/Complexity - Tends to underfit the training data; matches data too little; learns too little This is not to say that low flexibility models are better. In fact, some level of flexibility is needed for the model to pick up most of the signals in the training data, but not too much such that the noise is captured as well. Thus, the test MSE generally decreases with flexibility up till a certain point, following which it increases, forming a U shaped curve : Bias Variance Tradeoff \u00b6 The test MSE can be better understood by decomposing it into its consistuent commponents: \\[ \\text{MSE} = \\text{Bias}[\\hat{f}(x_0)]^2 + \\text{Variance}[\\hat{f}(x_0)] \\] The Bias of the model (also known as the Accuracy ) is the difference in expected value of the estimated signal function and the true signal function. More complex models are better able to capture the signal in the data, thus tends to have a lower bias. The Variance of the model (also known as Precision ) is the change in the estimated signal function across different datasets. Ideally, the model should have low variance such that it would be relatively stable across different training data. While more complex models are better able to capture signals, this makes them prone to overfitting and hence more sensitive to differences in the training data , leading to higher variance. Note that although Precision & Accuracy are both synonyms in English, they have distinct meanings in statistics. Ideally, a model should have both low bias and low variance. However, as explained above, there is an inherent tension between Bias and Variance due to the complexity of the model, known as the Bias-Variance Tradeoff . A relatively simple model (underfitted) tends to have a high bias but low variance . As the complexity increases, the bias initially decreases more than the variance increases , causing the test MSE to fall. At some point, the model becomes too complex (overfitted), where the increase in variance outweighs the fall in bias , resulting in the U-shaped curve as seen previously. Thus, the goal is to find an optimal balance in between Bias and Variance where the test MSE is minimized. Resampling Methods \u00b6 Validation Set \u00b6 LOO Cross Validation \u00b6 K fold Cross Validation \u00b6 Model Selection \u00b6 Feature Selection \u00b6 Forward Stepwise Selection \u00b6 Backward Stepwise Selection \u00b6 Stepwise Selection \u00b6 Shrinkage Methods \u00b6","title":"Statistical Learning"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#statistical-learning","text":"The regression concepts covered in the previous sections are widely considered to be traditional applied statistics . In a contemporary context, regression is just one of the many methods that fall under Statistical Learning . It is a framework of harnessing data to gain an understanding of how the data is related to one another and/or how a group of variables can be used to accurately predict another. Similar to regression, the relationship between variables can be expressed as a combination of a Signal Function and a Noise term: \\[ y = f(x) + \\varepsilon \\] The two terms originate from Engineering, where Signal refers to the meaningful component of the data while Noise refers to the random variation that inteferes with the signal. Statistical learning models are distinguished based on their signal function. There are two main kinds of models: Parametric Models Non-Parametric Assumes DV follows a specific functional form Does not assume any functional form Function parameters determined from the data - Does not require a large amount of data Requires a large amount of data to work well EG. Linear Regression EG. Clustering | May not fit the data well | Fits the data well | | Described by parameters | No parameters used | | Simple to implement | Requires large amount of data to implement | | Risk that assumed function is wrong | Does not make any assumption about data | They can also be further split according to: Supervised Learning Unsupervised Learning Specified DV to supervise the learning No specific variable chosen Inference/Prediction with respect to the DV Inference/Prediction for all variables EG. Linear Regression EG. Clustering Regression vs classification classification - classifying the observations to a certian level","title":"Statistical Learning"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#model-accuracy","text":"Since the end goal of the model is to make predictions on new unobserved data, the quality of the model should be evaluated against its performance on unobserved data as well. The observed data used to create the model is known as the Training Data as it helps train the signal function to identify relationships between variables, while the unobserved data used to evaluate the model is known as the Test Data . The quality of the model can then be quantified by the extent to which the model predictions match the data . Similar to regression, this quantity is known as the Error of the model and is summarized through the Mean Square Error (MSE) statistic. The MSE is the average of the sum of squared errors and can be calculated for both the training and test data: \\[ \\begin{aligned} \\text{Training MSE} &= \\frac{[y_i - \\hat{f}(x_0)]^2}{n_{training}} \\\\ \\text{Test MSE} &= \\frac{[y_0 - \\hat{f}(x_0)]^2}{n_{test}} \\end{aligned} \\] Note that this is different from the MSE defined in ANOVA, where the MS is divded by its degrees of freedom rather than the number of observations. The MSE defined in this section is a general concept while the ANOVA MSE is a purely regression concept. The Training MSE reflects the goodness of fit of the model while the Test MSE reflects its prediction accuracy. As alluded to earlier, the goal of statistical learning is to choose the model with the lowest Test MSE . In general, the training MSE should always be smaller than the testing MSE, which is why they are not interchangeable. This is because all models are trained to match the training data to various extents, thus they should naturally have relatively smaller errors. On the flipside, test MSE should always be higher because the model is likely to have mistakenly captured some of the noise in the training data that do not generalize to the test data, resulting in a higher testing error. The extent of the difference is dependent on how well the model fits the training data; the extent to which it learns from it: High Flexibility/Complexity - Tends to overfit the training data; matches data too much; learns too much Low Flexibility/Complexity - Tends to underfit the training data; matches data too little; learns too little This is not to say that low flexibility models are better. In fact, some level of flexibility is needed for the model to pick up most of the signals in the training data, but not too much such that the noise is captured as well. Thus, the test MSE generally decreases with flexibility up till a certain point, following which it increases, forming a U shaped curve :","title":"Model Accuracy"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#bias-variance-tradeoff","text":"The test MSE can be better understood by decomposing it into its consistuent commponents: \\[ \\text{MSE} = \\text{Bias}[\\hat{f}(x_0)]^2 + \\text{Variance}[\\hat{f}(x_0)] \\] The Bias of the model (also known as the Accuracy ) is the difference in expected value of the estimated signal function and the true signal function. More complex models are better able to capture the signal in the data, thus tends to have a lower bias. The Variance of the model (also known as Precision ) is the change in the estimated signal function across different datasets. Ideally, the model should have low variance such that it would be relatively stable across different training data. While more complex models are better able to capture signals, this makes them prone to overfitting and hence more sensitive to differences in the training data , leading to higher variance. Note that although Precision & Accuracy are both synonyms in English, they have distinct meanings in statistics. Ideally, a model should have both low bias and low variance. However, as explained above, there is an inherent tension between Bias and Variance due to the complexity of the model, known as the Bias-Variance Tradeoff . A relatively simple model (underfitted) tends to have a high bias but low variance . As the complexity increases, the bias initially decreases more than the variance increases , causing the test MSE to fall. At some point, the model becomes too complex (overfitted), where the increase in variance outweighs the fall in bias , resulting in the U-shaped curve as seen previously. Thus, the goal is to find an optimal balance in between Bias and Variance where the test MSE is minimized.","title":"Bias Variance Tradeoff"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#resampling-methods","text":"","title":"Resampling Methods"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#validation-set","text":"","title":"Validation Set"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#loo-cross-validation","text":"","title":"LOO Cross Validation"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#k-fold-cross-validation","text":"","title":"K fold Cross Validation"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#model-selection","text":"","title":"Model Selection"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#feature-selection","text":"","title":"Feature Selection"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#forward-stepwise-selection","text":"","title":"Forward Stepwise Selection"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#backward-stepwise-selection","text":"","title":"Backward Stepwise Selection"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#stepwise-selection","text":"","title":"Stepwise Selection"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#shrinkage-methods","text":"","title":"Shrinkage Methods"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/6.%20Generalized%20Linear%20Models/","text":"","title":"6. Generalized Linear Models"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/7.%20Time%20Series%20Models/","text":"","title":"7. Time Series Models"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/8.%20Tree%20Models/","text":"","title":"8. Tree Models"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/9.%20Principal%20Component%20Analysis/","text":"","title":"9. Principal Component Analysis"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/","text":"Review of Statistical Theory \u00b6 This is meant to be a quick review of the basic statistical concepts of VEE: Mathematical Statistics and other undergrad statitics courses that will be relevant for this exam. Overview of Statistics \u00b6 Statistics is a discipline revolving around data. A Population refers to the theoretical set of all possible data of the event of interest. The goal of statistics is to determine certain attributes that summarizes or describes the population, known as Parameters . However, it is impossible to study the entire population at once, thus a subset of the population is studied instead, known as the Sample . Attributes that summarize or describe the sample are known as Statistics . Ideally, the sample is representative of the population, which means that findings from the sample can be applied to the population as a whole. This means that the sample statistics can be used estimate population parameters. We distinguish between the two (when they have the same notation) through the Hat accent (^) - Population Parameters are their written without the hat ( \\(x\\) ) while their corresponding sample statistics are written with the hat ( \\(\\hat{x}\\) ). Common Sample Statistics \u00b6 The Mean is the average of the population. Population Mean Sample Mean \\(\\mu = \\sum\\limits_{i=i}^n x_i * p(x_i)\\) \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=i}^n x_i\\) The Variance measures the spread of values about the mean. However, the units of Variance are unintuitive, thus we consider the Standard Deviation for more practical purposes, which is the square root of the variance . Population Variance Sample Variance \\(\\sigma^2 = \\sum\\limits_{i=i}^n (x_i - \\mu)^2 * p(x_i)\\) \\(s^2 = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x})^2\\) Covariance is a measure of the linear relationship between two variables: Positive Covariance - Variables move in the same direction Negative Covariance - Variables move in opposite directions Population Covariance Sample Covariance \\(\\sigma_{x, y} = \\mu_{xy} - \\mu_x \\mu_y\\) \\(\\hat{\\sigma}_{x, y} = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x}) * (y_i - \\bar{y})\\) However, there are two issues with Covariance - the units are unintuitive (similar to variance) and there is no benchmark as to what constitutes a strong/weak relationship. Thus, the Correlation is an adjusted measure of the relationship between -1 and 1 . Population Correlation Sample Correlation \\(\\rho = \\frac{\\sigma_{x,y}}{\\sigma_x * \\sigma_y}\\) \\(r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x} \\sum(y_i - \\bar{y})}\\) Properties of Estimators Biased Consistent Efficient Degrees of freedom Sampling Distribution \u00b6 Whenever a sample is drawn from a population and a statistic is calculated, it is known as a Point Estimate . Due to measurement error, a different sample would be drawn each time, thus leading to a different point estimate . If this process were to be repeated a large number times, the probability distribution of the resulting point estimates is known as the Sampling Distribution of the statistic. There is no rule surrounding the sampling distribution - it depends on the distribution of the population, statistic being measured, sampling method etc. The standard deviation of this sampling distribution is known as the Standard Error of the statistic: \\[ \\sigma_{\\theta} = \\sqrt {\\sigma^2_{\\theta}} \\] A special case is the Sample Mean . If the population is normally distributed, then it is normally distributed as well. Regardless of the population distribution, it is also approximately normally distributed through the Central Limit Theorem or Law of Large Numbers . \\[ \\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\] Following that, we can compute the standard error: \\[ \\sigma_{\\bar{x}} = \\sqrt \\frac{\\sigma^2_{\\bar{x}}}{n} = \\frac{\\sigma}{\\sqrt{n}} \\] However, the population variance is usually unknown. Thus, it can be approximated using the Sample Variance, which is an unbiased estimator for it. The result is known as an Estimate for the Standard Error : \\[ \\hat{\\sigma}_{\\bar{x}} = \\sqrt \\frac{s^2}{n} = \\frac{s}{\\sqrt{n}} \\] Note that only the sample variance is an unbiased estimator for the population variance. Although it may look like it, the sample SD is NOT an unbiased estimator for the population SD. Confidence Interval \u00b6 Given that there is only one true value for the population parameter and a whole distribution of estimators, it is unlikely that a point estimate will be equal to the population parameter. Thus, instead of a point estimate, a range of estimates is used, known as a Confidence Interval . The interval is made a chosen Confidence Level which represents the proportion of confidence intervals that will contain the true value . In other words, if a large number of confidence intervals constructed in the same manner were to be made, \\((1-\\alpha)%\\) of them would contain the true value. \\[ P(- \\text{Margin of Error} < \\theta < \\text{Margin of Error}) = 1 - \\alpha \\] The Margin of Error represents the range of values on either side of the point estimate that the true value could lie. For instance, for a 95% confidence interval, the confidence lies within the 0.025 and 0.975 percentile of the sampling distribution. The margin of error can be calculated by finding the corresponding values of the sampling distribution at these percentiles. Consider the 95% confidence interval for the Sample Mean , which is normally distributed. For convenience, it is usually normalized such that it will become a Standard Normal Distribution : \\[ \\begin{aligned} P(-1.96 < \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt n}} < 1.96) &= 0.95 \\\\ P(\\bar{x} - 1.96 \\frac{\\sigma}{\\sqrt n} < \\mu < \\bar{x} + 1.96 \\frac{\\sigma}{\\sqrt n}) &= 0.95 \\\\ \\end{aligned} \\] \\[ \\therefore \\text{Margin of Error} = \\bar{x} + Z_{\\frac{\\alpha}{2}} * \\frac{\\sigma}{\\sqrt n} \\] Hypothesis Testing \u00b6 Hypothesis Testing is a formal method of making inferences about the population parameters based on the sample statistic. It starts with a Hypothesis which is a conjecture about the population parameters: Null Hypothesis - What is currently believed to be true Alternative Hypothesis - What is to be proven A Test Statistic is then calculated, which would quantify the behaviour of population such that it would distinguish the null and alternative hypothesis. Assuming the Null Hypothesis is true , the sampling distribution of the test statistic is determined. From the sampling distribution, the p-value is calculated, which is the probability of observing the calculated sample statistic or more extreme . If the p-value is smaller than a pre-determined level of Statistical Significance ( \\(\\alpha\\) ), then the test is \"successful\" and the null hypothesis is rejected. If not, then the test \"fails\" and the null is not rejected. Note that it rejecting the null hypothesis does NOT mean that the alternative hypothesis is accepted - EG. Rejected then hypothesis of a large paw print being from a bear does not mean that it is from Bigfoot. Thus, the hypotheses are usually constructed such that the two hypothesis are complementary , such that rejecting the null allows acceptance of the alternative, leading to a definitive insight. In layman terms, a hypothesis test is a test of extremeness . Assuming the null hypothesis is true, how extreme (low probability) is the observed sample? If the p-value is sufficiently low, it means that the sample is rare - it could be due to pure chance or that it is actually not rare because the null is not true. We distinguish between the two mathematically through \\(\\alpha\\) . It is the probability of a False Positive - that the result obtained was by pure chance. It is typically set at 5%, which means that 5% of all statistics calculated are expected to be extreme under the null. Thus, if the p-value of the sample is smaller than \\(\\alpha\\) , then it is likely that the observation was not due to chance but instead because the null was false . Alternatively, instead of comparing p-value to \\(alpha\\) , the test-statistic and the corresponding value of \\(\\alpha\\) on the sampling distribution can be used. It is known as the Critical Value , which represents the boundary of Reject Null Do not Reject Null p-value smaller than \\(\\alpha\\) p-value smaller than \\(\\alpha\\) test-statistic larger than critical value test-statistic smaller than critical value Let the random variable \\(T\\) denote the test statistics. There are many different kinds of test statistics depending on the distribution and what is being investigated. Z-statistic \u00b6 The most simple test statistic involve the Sample Mean , which is normally distributed: \\[ T = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] If the population variance is known , then the test statistic has a Standard Normal Distribution and thus the test-statistic is known as a Z-Statistic . \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\sqrt n \\\\ &= Z * \\sqrt n \\\\ \\therefore T &\\sim N(0,1) \\end{aligned} \\] t-statistic \u00b6 If the population variance is unknown, then it will be approximated by the Sample Variance . Through algebraic manipulation, the test statistic can still be expressed in the form of a Z variable, but with an additional term: \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}} \\\\ &= \\frac{\\bar{x} - \\mu}{s} * \\sqrt n \\\\ &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\frac{\\sigma}{s} * \\sqrt n \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{s^2}{\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{(n-1)s^2}{(n-1)\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt {\\frac{(n-1)s^2}{\\sigma^2} * \\frac{1}{n-1}} \\\\ \\end{aligned} \\] The additional term can be shown to have a Chi-Squared Distribution of \\(n-1\\) degrees of freedom, which by definition is the sum of \\(n-1\\) independent standard normal variables: \\[ \\begin{aligned} \\chi^2_n &= Z^2 \\\\ &= \\sum \\left(\\frac {\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &= \\sum \\left(\\frac {(x_i - \\bar{x}) + (\\bar{x} - \\mu)}{\\sigma}\\right)^2 \\\\ &= \\sum \\left(\\frac{x_i - \\bar{x}}{\\sigma}\\right)^2 + \\sum \\left(\\frac{\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &= \\frac{1}{\\sigma^2} \\sum (x_i - \\bar{x})^2 + \\sum Z^2 + 0 \\\\ &= \\frac{(n-1)}{\\sigma^2} \\frac{\\sum (x_i - \\bar{x})^2}{n-1} + \\chi^2_1 \\\\ &= \\frac{(n-1)s^2}{\\sigma^2} + \\chi^2_1 \\\\ \\end{aligned} \\] \\[\\therefore \\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\] Thus, the test statistic is the ratio of a Standard Normal Variable to the squareroot of a Chi Squared Variable (divided by its degrees of freedom). By definition, this test statistic has a t-distribution with the same degrees of freedom and is known as the t-statistic : \\[ \\begin{align*} T &= \\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\\\ &= t_{n-1} * \\sqrt{n} \\\\ \\end{align*} \\] \\[\\therefore T \\sim t_{n-1}\\] Despite the slightly convoluted proof, the t-distribution is simply a standard normal distribution with heavier tails . This means that extreme values are slightly more likely, which is meant to account for the increase in variability due to the use of the sample variance rather than the population variance. F-statistic \u00b6 The square of t-statisic has an F-Distribution , which is defined as the ratio of two independent chi-square variables (divided by their respective degrees of freedom). It has two dimensions for its degree of freedom, reflecting the two chi-square variables. \\[ F_{m,n} = \\frac{\\frac{\\chi_m}{m}}{\\frac{\\chi_n}{n}} \\] The square of a t-statistic follows an F-distribution because: The square of the standard normal variable in the numerator becomes a \\(\\chi_1\\) variable The squareroot is removed in the denominator, becoming a \\(\\chi_{n-1}\\) over its degree of freedom \\[ \\begin{aligned} t_{n-1}^2 &= \\left(\\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\right)^2 \\\\ &= \\frac{\\chi_1}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &= \\frac{\\frac{\\chi_1}{1}}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &= F_{1,n-1} * n \\end{aligned} \\] \\[ \\therefore t_{n-1}^2 \\sim F_{1, n-1} \\] Thus, the square of the t-statistic is known as the F-statistic , which is usually used to test for the equality of variance . Maximum Likelihood Estimation \u00b6 If the population distribution is known, there is an alternative method of estimating the parameters apart from calculating the corresponding sample statistics. We model a set of observations as a random sample from an unknown joint probability distribution which is expressed in terms of a set of parameters. The goal of maximum likelihood estimation is to determine the parameters for which the observed data have the highest joint probability. The goal of maximum likelihood estimation is to find the values of the model parameters that maximize the likelihood function over the parameter space","title":"**Review of Statistical Theory**"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#review-of-statistical-theory","text":"This is meant to be a quick review of the basic statistical concepts of VEE: Mathematical Statistics and other undergrad statitics courses that will be relevant for this exam.","title":"Review of Statistical Theory"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#overview-of-statistics","text":"Statistics is a discipline revolving around data. A Population refers to the theoretical set of all possible data of the event of interest. The goal of statistics is to determine certain attributes that summarizes or describes the population, known as Parameters . However, it is impossible to study the entire population at once, thus a subset of the population is studied instead, known as the Sample . Attributes that summarize or describe the sample are known as Statistics . Ideally, the sample is representative of the population, which means that findings from the sample can be applied to the population as a whole. This means that the sample statistics can be used estimate population parameters. We distinguish between the two (when they have the same notation) through the Hat accent (^) - Population Parameters are their written without the hat ( \\(x\\) ) while their corresponding sample statistics are written with the hat ( \\(\\hat{x}\\) ).","title":"Overview of Statistics"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#common-sample-statistics","text":"The Mean is the average of the population. Population Mean Sample Mean \\(\\mu = \\sum\\limits_{i=i}^n x_i * p(x_i)\\) \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=i}^n x_i\\) The Variance measures the spread of values about the mean. However, the units of Variance are unintuitive, thus we consider the Standard Deviation for more practical purposes, which is the square root of the variance . Population Variance Sample Variance \\(\\sigma^2 = \\sum\\limits_{i=i}^n (x_i - \\mu)^2 * p(x_i)\\) \\(s^2 = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x})^2\\) Covariance is a measure of the linear relationship between two variables: Positive Covariance - Variables move in the same direction Negative Covariance - Variables move in opposite directions Population Covariance Sample Covariance \\(\\sigma_{x, y} = \\mu_{xy} - \\mu_x \\mu_y\\) \\(\\hat{\\sigma}_{x, y} = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x}) * (y_i - \\bar{y})\\) However, there are two issues with Covariance - the units are unintuitive (similar to variance) and there is no benchmark as to what constitutes a strong/weak relationship. Thus, the Correlation is an adjusted measure of the relationship between -1 and 1 . Population Correlation Sample Correlation \\(\\rho = \\frac{\\sigma_{x,y}}{\\sigma_x * \\sigma_y}\\) \\(r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x} \\sum(y_i - \\bar{y})}\\) Properties of Estimators Biased Consistent Efficient Degrees of freedom","title":"Common Sample Statistics"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#sampling-distribution","text":"Whenever a sample is drawn from a population and a statistic is calculated, it is known as a Point Estimate . Due to measurement error, a different sample would be drawn each time, thus leading to a different point estimate . If this process were to be repeated a large number times, the probability distribution of the resulting point estimates is known as the Sampling Distribution of the statistic. There is no rule surrounding the sampling distribution - it depends on the distribution of the population, statistic being measured, sampling method etc. The standard deviation of this sampling distribution is known as the Standard Error of the statistic: \\[ \\sigma_{\\theta} = \\sqrt {\\sigma^2_{\\theta}} \\] A special case is the Sample Mean . If the population is normally distributed, then it is normally distributed as well. Regardless of the population distribution, it is also approximately normally distributed through the Central Limit Theorem or Law of Large Numbers . \\[ \\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\] Following that, we can compute the standard error: \\[ \\sigma_{\\bar{x}} = \\sqrt \\frac{\\sigma^2_{\\bar{x}}}{n} = \\frac{\\sigma}{\\sqrt{n}} \\] However, the population variance is usually unknown. Thus, it can be approximated using the Sample Variance, which is an unbiased estimator for it. The result is known as an Estimate for the Standard Error : \\[ \\hat{\\sigma}_{\\bar{x}} = \\sqrt \\frac{s^2}{n} = \\frac{s}{\\sqrt{n}} \\] Note that only the sample variance is an unbiased estimator for the population variance. Although it may look like it, the sample SD is NOT an unbiased estimator for the population SD.","title":"Sampling Distribution"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#confidence-interval","text":"Given that there is only one true value for the population parameter and a whole distribution of estimators, it is unlikely that a point estimate will be equal to the population parameter. Thus, instead of a point estimate, a range of estimates is used, known as a Confidence Interval . The interval is made a chosen Confidence Level which represents the proportion of confidence intervals that will contain the true value . In other words, if a large number of confidence intervals constructed in the same manner were to be made, \\((1-\\alpha)%\\) of them would contain the true value. \\[ P(- \\text{Margin of Error} < \\theta < \\text{Margin of Error}) = 1 - \\alpha \\] The Margin of Error represents the range of values on either side of the point estimate that the true value could lie. For instance, for a 95% confidence interval, the confidence lies within the 0.025 and 0.975 percentile of the sampling distribution. The margin of error can be calculated by finding the corresponding values of the sampling distribution at these percentiles. Consider the 95% confidence interval for the Sample Mean , which is normally distributed. For convenience, it is usually normalized such that it will become a Standard Normal Distribution : \\[ \\begin{aligned} P(-1.96 < \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt n}} < 1.96) &= 0.95 \\\\ P(\\bar{x} - 1.96 \\frac{\\sigma}{\\sqrt n} < \\mu < \\bar{x} + 1.96 \\frac{\\sigma}{\\sqrt n}) &= 0.95 \\\\ \\end{aligned} \\] \\[ \\therefore \\text{Margin of Error} = \\bar{x} + Z_{\\frac{\\alpha}{2}} * \\frac{\\sigma}{\\sqrt n} \\]","title":"Confidence Interval"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#hypothesis-testing","text":"Hypothesis Testing is a formal method of making inferences about the population parameters based on the sample statistic. It starts with a Hypothesis which is a conjecture about the population parameters: Null Hypothesis - What is currently believed to be true Alternative Hypothesis - What is to be proven A Test Statistic is then calculated, which would quantify the behaviour of population such that it would distinguish the null and alternative hypothesis. Assuming the Null Hypothesis is true , the sampling distribution of the test statistic is determined. From the sampling distribution, the p-value is calculated, which is the probability of observing the calculated sample statistic or more extreme . If the p-value is smaller than a pre-determined level of Statistical Significance ( \\(\\alpha\\) ), then the test is \"successful\" and the null hypothesis is rejected. If not, then the test \"fails\" and the null is not rejected. Note that it rejecting the null hypothesis does NOT mean that the alternative hypothesis is accepted - EG. Rejected then hypothesis of a large paw print being from a bear does not mean that it is from Bigfoot. Thus, the hypotheses are usually constructed such that the two hypothesis are complementary , such that rejecting the null allows acceptance of the alternative, leading to a definitive insight. In layman terms, a hypothesis test is a test of extremeness . Assuming the null hypothesis is true, how extreme (low probability) is the observed sample? If the p-value is sufficiently low, it means that the sample is rare - it could be due to pure chance or that it is actually not rare because the null is not true. We distinguish between the two mathematically through \\(\\alpha\\) . It is the probability of a False Positive - that the result obtained was by pure chance. It is typically set at 5%, which means that 5% of all statistics calculated are expected to be extreme under the null. Thus, if the p-value of the sample is smaller than \\(\\alpha\\) , then it is likely that the observation was not due to chance but instead because the null was false . Alternatively, instead of comparing p-value to \\(alpha\\) , the test-statistic and the corresponding value of \\(\\alpha\\) on the sampling distribution can be used. It is known as the Critical Value , which represents the boundary of Reject Null Do not Reject Null p-value smaller than \\(\\alpha\\) p-value smaller than \\(\\alpha\\) test-statistic larger than critical value test-statistic smaller than critical value Let the random variable \\(T\\) denote the test statistics. There are many different kinds of test statistics depending on the distribution and what is being investigated.","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#z-statistic","text":"The most simple test statistic involve the Sample Mean , which is normally distributed: \\[ T = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] If the population variance is known , then the test statistic has a Standard Normal Distribution and thus the test-statistic is known as a Z-Statistic . \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\sqrt n \\\\ &= Z * \\sqrt n \\\\ \\therefore T &\\sim N(0,1) \\end{aligned} \\]","title":"Z-statistic"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#t-statistic","text":"If the population variance is unknown, then it will be approximated by the Sample Variance . Through algebraic manipulation, the test statistic can still be expressed in the form of a Z variable, but with an additional term: \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}} \\\\ &= \\frac{\\bar{x} - \\mu}{s} * \\sqrt n \\\\ &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\frac{\\sigma}{s} * \\sqrt n \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{s^2}{\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{(n-1)s^2}{(n-1)\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt {\\frac{(n-1)s^2}{\\sigma^2} * \\frac{1}{n-1}} \\\\ \\end{aligned} \\] The additional term can be shown to have a Chi-Squared Distribution of \\(n-1\\) degrees of freedom, which by definition is the sum of \\(n-1\\) independent standard normal variables: \\[ \\begin{aligned} \\chi^2_n &= Z^2 \\\\ &= \\sum \\left(\\frac {\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &= \\sum \\left(\\frac {(x_i - \\bar{x}) + (\\bar{x} - \\mu)}{\\sigma}\\right)^2 \\\\ &= \\sum \\left(\\frac{x_i - \\bar{x}}{\\sigma}\\right)^2 + \\sum \\left(\\frac{\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &= \\frac{1}{\\sigma^2} \\sum (x_i - \\bar{x})^2 + \\sum Z^2 + 0 \\\\ &= \\frac{(n-1)}{\\sigma^2} \\frac{\\sum (x_i - \\bar{x})^2}{n-1} + \\chi^2_1 \\\\ &= \\frac{(n-1)s^2}{\\sigma^2} + \\chi^2_1 \\\\ \\end{aligned} \\] \\[\\therefore \\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\] Thus, the test statistic is the ratio of a Standard Normal Variable to the squareroot of a Chi Squared Variable (divided by its degrees of freedom). By definition, this test statistic has a t-distribution with the same degrees of freedom and is known as the t-statistic : \\[ \\begin{align*} T &= \\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\\\ &= t_{n-1} * \\sqrt{n} \\\\ \\end{align*} \\] \\[\\therefore T \\sim t_{n-1}\\] Despite the slightly convoluted proof, the t-distribution is simply a standard normal distribution with heavier tails . This means that extreme values are slightly more likely, which is meant to account for the increase in variability due to the use of the sample variance rather than the population variance.","title":"t-statistic"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#f-statistic","text":"The square of t-statisic has an F-Distribution , which is defined as the ratio of two independent chi-square variables (divided by their respective degrees of freedom). It has two dimensions for its degree of freedom, reflecting the two chi-square variables. \\[ F_{m,n} = \\frac{\\frac{\\chi_m}{m}}{\\frac{\\chi_n}{n}} \\] The square of a t-statistic follows an F-distribution because: The square of the standard normal variable in the numerator becomes a \\(\\chi_1\\) variable The squareroot is removed in the denominator, becoming a \\(\\chi_{n-1}\\) over its degree of freedom \\[ \\begin{aligned} t_{n-1}^2 &= \\left(\\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\right)^2 \\\\ &= \\frac{\\chi_1}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &= \\frac{\\frac{\\chi_1}{1}}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &= F_{1,n-1} * n \\end{aligned} \\] \\[ \\therefore t_{n-1}^2 \\sim F_{1, n-1} \\] Thus, the square of the t-statistic is known as the F-statistic , which is usually used to test for the equality of variance .","title":"F-statistic"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#maximum-likelihood-estimation","text":"If the population distribution is known, there is an alternative method of estimating the parameters apart from calculating the corresponding sample statistics. We model a set of observations as a random sample from an unknown joint probability distribution which is expressed in terms of a set of parameters. The goal of maximum likelihood estimation is to determine the parameters for which the observed data have the highest joint probability. The goal of maximum likelihood estimation is to find the values of the model parameters that maximize the likelihood function over the parameter space","title":"Maximum Likelihood Estimation"}]}