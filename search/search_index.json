{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Actuarial Exam Notes \u00b6 Work in Progress!","title":"**Actuarial Exam Notes**"},{"location":"#actuarial-exam-notes","text":"Work in Progress!","title":"Actuarial Exam Notes"},{"location":"1.%20Introductory/Review%20of%20Mathematics/","text":"Review of Mathematics \u00b6 Algebra \u00b6 Simultaneous Equations Quadractic Equations Logarithm and Exponents Logarithm Rules \u00b6 A quick review of the rules of manipulating logarithms can be found below: Warning Note that rule 1 and 2 are often misunderstood , leading people to believe that the following is true when they are NOT: \\[ \\begin{aligned} \\ln (A + B) &= \\ln A \\cdot \\ln B \\\\ \\ln (A - B) &= \\frac{\\ln A}{\\ln B} \\end{aligned} \\] Linear Algebra \u00b6 Vectors Matrices Differentiation \u00b6 Integration \u00b6 Definite Integrals By parts","title":"**Review of Mathematics**"},{"location":"1.%20Introductory/Review%20of%20Mathematics/#review-of-mathematics","text":"","title":"Review of Mathematics"},{"location":"1.%20Introductory/Review%20of%20Mathematics/#algebra","text":"Simultaneous Equations Quadractic Equations Logarithm and Exponents","title":"Algebra"},{"location":"1.%20Introductory/Review%20of%20Mathematics/#logarithm-rules","text":"A quick review of the rules of manipulating logarithms can be found below: Warning Note that rule 1 and 2 are often misunderstood , leading people to believe that the following is true when they are NOT: \\[ \\begin{aligned} \\ln (A + B) &= \\ln A \\cdot \\ln B \\\\ \\ln (A - B) &= \\frac{\\ln A}{\\ln B} \\end{aligned} \\]","title":"Logarithm Rules"},{"location":"1.%20Introductory/Review%20of%20Mathematics/#linear-algebra","text":"Vectors Matrices","title":"Linear Algebra"},{"location":"1.%20Introductory/Review%20of%20Mathematics/#differentiation","text":"","title":"Differentiation"},{"location":"1.%20Introductory/Review%20of%20Mathematics/#integration","text":"Definite Integrals By parts","title":"Integration"},{"location":"1.%20Introductory/ASA-FM/Test/","text":"Contents \u00b6 Chapter 1: Interest Measurement [5](#chapter-1-interest-measurement) Overview of Interest Theory [5](#overview-of-interest-theory) Mathematical Notation [5](#mathematical-notation) Amount function, \\(\\mathbf{At}\\) 5 Accumulation Function, \\(\\mathbf{at}\\) 5 Interest Function, \\(\\mathbf{It}\\) 5 Visualizing via Timelines [6](#visualizing-via-timelines) Effective Interest [6](#effective-interest) Deriving Amount Function: [6](#deriving-amount-function) Expressing Interest Rates [6](#expressing-interest-rates) Time Value of Money [7](#time-value-of-money) Effective Rate of Discount [7](#effective-rate-of-discount) Compounding Frequency & Nominal Rates [8](#compounding-frequency-nominal-rates) Definition of \\(\\mathbf{m}\\) 8 Visual Representation [9](#visual-representation) Force of Interest [10](#force-of-interest) Deriving Force of Interest [10](#deriving-force-of-interest) Dealing with Variable Force of Interest [10](#dealing-with-variable-force-of-interest) Equivalent Rates [10](#equivalent-rates) Chapter 2: Level Annuities [11](#chapter-2-level-annuities) Types of Annuities [11](#types-of-annuities) Deriving Annuity Formulas: [12](#deriving-annuity-formulas) Linking the two kinds of Annuities [13](#linking-the-two-kinds-of-annuities) Via the payment amount [13](#via-the-payment-amount) Via the number of payments [13](#via-the-number-of-payments) Special Annuity Cases [14](#special-annuity-cases) Deferred Annuities [14](#deferred-annuities) Balloon & Drop Payments [15](#balloon-drop-payments) Varying Interest Rates [15](#varying-interest-rates) Continuous Annuities [16](#continuous-annuities) Deriving Continuous Annuity Formulas [16](#deriving-continuous-annuity-formulas) Comparing all three annuity types [16](#comparing-all-three-annuity-types) Chapter 2.5: Non-level Annuities [17](#chapter-2.5-non-level-annuities) Geometric Annuities [17](#geometric-annuities) Deriving Geometric Formula (Immediate) [17](#deriving-geometric-formula-immediate) Converting non-level to Level [17](#converting-non-level-to-level) Arithmetic Annuities [18](#arithmetic-annuities) Deriving Arithmetic Formula (Immediate) [18](#deriving-arithmetic-formula-immediate) Special case: Unit Increasing [19](#special-case-unit-increasing) Special case: Unit Decreasing [19](#special-case-unit-decreasing) Continuously Increasing Annuities [19](#continuously-increasing-annuities) Deriving Continuously increasing Annuities [19](#deriving-continuously-increasing-annuities) Perpetuities [20](#perpetuities) Other Important Annuity Patterns [21](#other-important-annuity-patterns) Block Payments (For any increase/decrease -- very powerful concept) [21](#block-payments-for-any-increasedecrease-very-powerful-concept) Repeat Reverse Annuities (For increases/decreases of 1) [21](#repeat-reverse-annuities-for-increasesdecreases-of-1) Odd-Even Split (For alternating increases) [21](#odd-even-split-for-alternating-increases) Annuities into Perpetuities: [22](#annuities-into-perpetuities) Annuities with unknown number of payments [23](#annuities-with-unknown-number-of-payments) Matching Payment and Interest frequencies [24](#matching-payment-and-interest-frequencies) Lower to Higher payment frequency [24](#lower-to-higher-payment-frequency) Higher to Lower payment frequency [25](#higher-to-lower-payment-frequency) Chapter 3: Spot, Forward and Interest Swaps [26](#chapter-3-spot-forward-and-interest-swaps) Spot & Forward Rates [26](#spot-forward-rates) Linking the two rates [26](#linking-the-two-rates) Forward rates lasting more than one period [26](#_Toc89369556) Interest Rate Swaps [27](#interest-rate-swaps) Swap Mechanics [27](#swap-mechanics) Deferred Swaps [27](#deferred-swaps) Level Swap Shortcuts [28](#level-swap-shortcuts) Other Swap Calculations [29](#other-swap-calculations) Net Swap Payments [29](#net-swap-payments) Net Interest Payment [29](#net-interest-payment) Market Value [29](#market-value) Why interest rate swaps? [29](#why-interest-rate-swaps) Chapter 4: Rates of Return [30](#chapter-4-rates-of-return) One Period Return Rates [30](#one-period-return-rates) Time Weighted Rate of Return (TWRR) [30](#time-weighted-rate-of-return-twrr) Dollar Weighted Rate of Return (DWRR) [30](#dollar-weighted-rate-of-return-dwrr) Differences between each measure: [31](#differences-between-each-measure) Different Timespans [31](#different-timespans) Multiple periods return rates [32](#multiple-periods-return-rates) Arithmetic Mean [32](#arithmetic-mean) Geometric Mean (AKA Cumulative Annual Growth Rate -- CAGR) [32](#geometric-mean-aka-cumulative-annual-growth-rate-cagr) Discounted Cashflow Analysis [32](#discounted-cashflow-analysis) Net Present Value (NPV) [32](#net-present-value-npv) Internal Rate of Return (IRR) [32](#internal-rate-of-return-irr) Other Measures of Return (FNCE101) [32](#other-measures-of-return-fnce101) Dollar Returns [32](#dollar-returns) Percentage Return [32](#percentage-return) Chapter 5: Loans [33](#chapter-5-loans) What is a loan? [33](#what-is-a-loan) Loan Calculations [34](#loan-calculations) Loan Variables [34](#loan-variables) Three ways to Calculate Outstanding Loan Balance [34](#three-ways-to-calculate-outstanding-loan-balance) Loan Amortization [35](#loan-amortization) Other Payment Patterns [36](#other-payment-patterns) Level Principal Payments [36](#level-principal-payments) Non-Level Payments [36](#non-level-payments) Missing or Additional Payments [37](#missing-or-additional-payments) Payments that scale with Interest [37](#payments-that-scale-with-interest) Other situations: [37](#other-situations) Chapter 6: Bonds and Stocks [38](#chapter-6-bonds-and-stocks) What is a Bond? [38](#what-is-a-bond) Bond Valuation [38](#bond-valuation) Dirty Prices [38](#dirty-prices) Bond Premium & Discount [39](#bond-premium-discount) Bond Amortization [39](#bond-amortization) Special Type of Bonds [40](#special-type-of-bonds) Zero Coupon Bonds (Pure discount Bonds) [40](#zero-coupon-bonds-pure-discount-bonds) Floating Rate Bonds [40](#floating-rate-bonds) Callable Bonds [40](#callable-bonds) Stock Valuation: Dividend Discount Model [41](#stock-valuation-dividend-discount-model) Chapter 7: Bond Yield & Term Structure of Interest Rates [42](#chapter-7-bond-yield-term-structure-of-interest-rates) Measures of Bond Yield [42](#measures-of-bond-yield) Current Yield & Nominal Yield [42](#current-yield-nominal-yield) Yield to Maturity [42](#yield-to-maturity) Par Yield [42](#par-yield) Holding Period Yield [42](#holding-period-yield) Spot Rates and the Term Structure [43](#spot-rates-and-the-term-structure) Discretely Compounded Yield Curve [43](#discretely-compounded-yield-curve) Continuously Compounded Yield Curve [43](#continuously-compounded-yield-curve) Chapter 8: Bond Management [44](#chapter-8-bond-management) Macaulay Duration (MacD) [44](#macaulay-duration-macd) Deriving Macaulay Duration [44](#deriving-macaulay-duration) Other Time Frequencies [44](#other-time-frequencies) Macaulay Duration Shortcuts [44](#macaulay-duration-shortcuts) Modified Duration [45](#modified-duration) Deriving Modified Duration [45](#deriving-modified-duration) Portfolio Duration [45](#portfolio-duration) Different Comparison date [46](#different-comparison-date) Using Durations for Estimations [47](#using-durations-for-estimations) An alternative (Intuitive) way to think about Approximation [47](#an-alternative-intuitive-way-to-think-about-approximation) Convexity [48](#convexity) Derivation [48](#derivation) Additional Formulas [48](#additional-formulas) Immunization [49](#immunization) Redington Immunization (Duration Matching) [49](#redington-immunization-duration-matching) Full Immunization [50](#full-immunization) Immunization Shortcut [50](#immunization-shortcut) Alternative: Exact Cashflow Matching [51](#alternative-exact-cashflow-matching) Chapter 9: Determinants of Interest Rates [52](#chapter-9-determinants-of-interest-rates) How are Interest rates determined? [52](#how-are-interest-rates-determined) Central Banks and Interest Rates [53](#central-banks-and-interest-rates) What is the central bank? [53](#what-is-the-central-bank) What if banks fail to meet this requirement? [53](#what-if-banks-fail-to-meet-this-requirement) Impact on Interest rates [53](#impact-on-interest-rates) Components of Interest Rate [54](#components-of-interest-rate) Maturity Risk Premium [54](#maturity-risk-premium) Default Risk Premium [54](#default-risk-premium) Inflation Risk Premium [55](#inflation-risk-premium) Liquidity Risk Premium [56](#liquidity-risk-premium) Financial Calculator Tips [57](#financial-calculator-tips) Overview of using the BA II Plus [57](#overview-of-using-the-ba-ii-plus) Basic settings to toggle beforehand [57](#basic-settings-to-toggle-beforehand) Basic calculator features [57](#basic-calculator-features) Positive and Negative Signs [57](#positive-and-negative-signs) Time Value of Money [58](#time-value-of-money-1) Amortization Schedule [58](#amortization-schedule) Cashflow Function [58](#cashflow-function) Interest Rate Conversion (Nominal to Effective) [58](#interest-rate-conversion-nominal-to-effective) [Chapter 1: Interest Measurement]{.underline} \u00b6 Overview of Interest Theory \u00b6 Assume we want to grow our money with just a SINGLE deposit Our initial deposit is known as the principal amount and how much earn on that is known as the interest payments (As compensation for the loss of use of money) There are two types of growth we can experience: Linear Growth \u2192 Simple Interest Exponential Growth \u2192 Compound Interest Simple Interest initially earns more than Compound interest. But in the long run, because Interest will earn interest , Compound Interest will earn more Compound Interest powerful because the interest is essentially reinvested at the same rate**. If we were to withdraw the interest each period, we would have simple interest** Mathematical Notation \u00b6 Amount function, \\(\\mathbf{A}\\left( \\mathbf{t} \\right)\\) \u00b6 \\(A(t) = A(0)*a(t)\\) Represents the total amount of money in that fund at that point of time Accumulation Function, \\(\\mathbf{a}\\left( \\mathbf{t} \\right)\\) \u00b6 Represents the change in money from time 0 to time \\(\\mathbf{t}\\) Simple Interest (Linear) \u2192 \\(a(t) = 1 + it\\) Compound Interest (Polynomial) \u2192 \\(\\ a(t) = (1 + i)^{t}\\) Double/Triple growth (Integer) \u2192 \\(a(t) = 2\\ or\\ 3\\) It is a special case of the amount function where the initial amount is 1. Thus, by multiplying any initial amount to it, we obtain the total amount. Additionally, it also means that any formula that uses \\(\\mathbf{A}\\left( \\mathbf{t} \\right)\\) in both numerator and denominators can be substituted with \\(\\mathbf{a}\\left( \\mathbf{t} \\right)\\) If our starting time is not 0 , we use a more general expression of the amount function \\(a_{k}(t - k)\\) where it represents the change in value from time 0 to time \\(\\mathbf{k}\\) \\(a_{k}(t - k) = \\frac{a(t)}{a(k)} \\rightarrow \\mathbf{Intuitive\\ through\\ recursion\\ \\&\\ forward\\ rates}\\) Interest Function, \\(\\mathbf{I}\\left( \\mathbf{t} \\right)\\) \u00b6 \\(I(t) = A(t) - A(t - 1) = A(0)\\left\\lbrack a(t) - a(t - 1) \\right\\rbrack\\) Since the only change in the fund is interest, we can find it by taking the difference in amounts across two periods Visualizing via Timelines \u00b6 To save on memory work, it is important that we understand how formulas are derived so that we can apply it to any context The first thing we should do for any question is to draw a timeline of the cashflows There are some important points we should remember: Cash Inflows or Outflows should be strictly on one side of the timeline respectively The units of time can be in any units (Years, Months) & with any interpretation (End or start of the period). The important thing is to label it and be consistent It is good practice to mark out the comparison date we want to use so that we do not get confused along the way Subtracting time depends on the cashflows we want to consider: By default, it assumes that only one of the end points has a cashflow We need to add 1 period if both end points has a cashflow Effective Interest \u00b6 Unlike the simplified example earlier, most amount functions don't have smooth curves The fund grows at different rates at different times during the period. We can summarize the entire growth into an effective interest rate for that period This creates a standardized measure of growth during the period. Two funds with the same effective interest will lead to the same amounts, regardless of their volatility These interest rates can be Yearly, monthly etc. The key is to be consistent with the time periods you use throughout the same calculations: Convert time periods to be expressed in the same way as effective interest Convert effective interest to be expressed in the same way as time periods \\[i_{t} = \\frac{\\text{Fund\\ growth}\\text{\\ during\\ the\\ period}\\ (Interest)}{\\mathbf{Initial\\ fund\\ value}\\ at\\ the\\ start\\ of\\ period} = \\frac{A(t) - A(t - 1)}{A(t - 1)} = \\frac{a(t) - a(t - 1)}{a(t - 1)}\\] Deriving Amount Function: \u00b6 \\[i_{t} = \\frac{A(t) - A(t - 1)}{A(t - 1)}\\] \\[i_{t}*A(t - 1) = A(t) - A(t - 1)\\] \\[A(t) = A(t - 1) + i_{t}*A(t - 1)\\] \\[A(t) = A(t - 1)\\left\\lbrack 1 + i_{t} \\right\\rbrack\\] We can recursively add to this formula... \\[A(t) = A(t - 2)*\\left\\lbrack 1 + i_{t - 1} \\right\\rbrack*\\left\\lbrack 1 + i_{t} \\right\\rbrack\\] \\[A(t) = A(t - 3)*\\left\\lbrack 1 + i_{t - 2} \\right\\rbrack*\\left\\lbrack 1 + i_{t - 1} \\right\\rbrack*\\left\\lbrack 1 + i_{t} \\right\\rbrack\\] \\[\\vdots\\] \\[A(t) = A(0)*\\left\\lbrack 1 + i_{1} \\right\\rbrack*\\left\\lbrack 1 + i_{2} \\right\\rbrack*\\ldots\\left\\lbrack 1 + i_{t} \\right\\rbrack\\] If we have a constant effective interest across all periods, \\[A(t) = A(0)*\\lbrack 1 + i\\rbrack^{t}\\] More generally, if we start at time k and end at time t, \\[A(t) = A(k)*\\lbrack 1 + i\\rbrack^{t - k}\\] Expressing Interest Rates \u00b6 Typically expressed as a Percentage , where \\(1\\% = 0.01\\) For extremely small interest rates, it could be expressed as Basis Points instead, where \\(1\\ BP = 0.01\\% = 0.00001\\) Time Value of Money \u00b6 \"A dollar today is worth more than a dollar in the future\" This is based off the principal of interest. If you invested the dollar today, you should be getting \\(\\left( 1 + i_{t} \\right)^{t}\\) in the future. Conversely, a dollar in the future is worth \\(\\frac{1}{\\left( 1 + i_{t} \\right)^{t}}\\) today Bringing values into the future is known as Accumulating which results in the Future Value. Conversely, bringing future ones into the present is known as Discounting which results in the Present Value The key is that when working with cashflows across different time periods, we should bring them to the same point in time, be it present or future to compare them \\[\\mathbf{Present\\ Value,\\ A}\\left( \\mathbf{k} \\right) = \\frac{A(t)}{\\left\\lbrack 1 + i_{t} \\right\\rbrack^{t - k}} = A(t)*\\left\\lbrack 1 + i_{t} \\right\\rbrack^{- (t - k)}\\] \\[\\mathbf{Future\\ Value,\\ A}\\left( \\mathbf{t} \\right) = A(k)*\\left\\lbrack 1 + i_{t} \\right\\rbrack^{t - k}\\] Notice that the formulas are almost identical with the main difference being the starting value and the exponent The FV has a Positive Exponent , signifying that we are going forward in time while the PV has a Negative Exponent , signifying that we are going backwards in time Since we use discounting and compounding so often, we use the variable \\(\\mathbf{v =}\\frac{\\mathbf{1}}{\\mathbf{a}\\left( \\mathbf{t} \\right)}\\) to represent the Discount Factor so that we simplify our workings Effective Rate of Discount \u00b6 Instead of Effective Interest Rates which measure the Interest relative to the starting fund value, another standardized way of measuring growth is Effective rate of Discount , which measures Interest relative to the ending fund value instead They are simply an alternative measure to Interest Rates , and can be used in a similar fashion to accumulate or discount values DO NOT confuse it with the discount factor. The discount factor is simply a notation we use to simplify the equation while discount rate is a completely new concept +-----------------------------------+-----------------------------------+ | [Effective Interest | **[Effective Discount | | Rate]{.underline} | Rate]{.underline}** | +===================================+===================================+ | \\( (i_{t} = | ( \\(d | | frac{A(t) - A(t - 1)}{\\mathbf{A}\\ | _{t} = \\frac{A(t) - A(t - 1)}{\\ma | | left( \\mathbf{t - 1} \\right)} = \\ | thbf{A}\\left( \\mathbf{t} \\right)} | | frac{a(t) - a(t - 1)}{a(t - 1)}\\) \\) | = \\frac{a(t) - a(t - 1)}{a(t)}\\) \\) | +-----------------------------------+-----------------------------------+ | If we fairly assume that the fund | | | is growing, \\(A(t) > A(t - 1)\\) . | | | Thus, \\(i_{t} > d_{t}\\) . | | +-----------------------------------+-----------------------------------+ | $ | \\( (\\ma | | \\(\\mathbf{FV} = A(k)*\\left\\lbrack | thbf{FV} = A(k)*\\left\\lbrack 1 - | | 1 + i_{t} \\right\\rbrack^{t - k}\\) \\) | d_{t} \\right\\rbrack^{- (t - k)}\\) $ | | | | | \\( (\\math | ( \\(\\ | | bf{PV} = \\ A(t)*\\left\\lbrack 1 + | mathbf{PV} = \\ A(t)*\\left\\lbrack | | i_{t} \\right\\rbrack^{- (t - k)}\\) \\) | 1 - d_{t} \\right\\rbrack^{t - k}\\) \\) | +-----------------------------------+-----------------------------------+ | Interest rate naturally goes | | | forward in time (Positive | | | exponent when accumulating) while | | | Discount rate naturally goes | | | backward in time (Positive | | | Exponent when Discounting) | | +-----------------------------------+-----------------------------------+ Another way to think about it is that Interest rates are usually paid at the end of the period, while discount rates is interest that is paid at the start of the period instead. But both are equivalent: \\[d = \\frac{i}{1 + i} = i*\\frac{1}{1 + i} = iv\\] Compounding Frequency & Nominal Rates \u00b6 Compounding annually is just one of the ways we can compound. We could compound interest Monthly, Quarterly, semi-annually etc However, quoting effective rates with different timespans can become confusing. Thus, we multiply to make them reflect an annual nominal rate instead: Monthly rates multiply by 12; Quarterly rate multiply by 4 The word nominal means \"in name only\" , which implies that these rates are just placeholders. If we actually compound 12 months using a monthly rate VS compounding once using the nominal rate, we get two different values This is because the nominal rate is in name only and not to be used . Unfortunately, most interest rates are quoted in nominal terms. We always need to remember that to convert nominal rates to effective rates. +----------------------+----------------------+-----------------------+ | | [Interest | **[Discount | | | Rates]{.underline} | Rate]{.underline}** | +======================+======================+=======================+ | Annual Effective | ( \\(i\\) \\) | ( \\(d\\) \\) | | Rate | | | +----------------------+----------------------+-----------------------+ | Annual Nominal | ( \\(i^{(m)}\\) \\) | ( \\(d^{(m)}\\) \\) | | Rate | | | | | | | | (Compounded | | | | \\(\\mathbf{m}\\) times | | | | a year, every | | | | \\(\\frac{\\math | | | | bf{12}}{\\mathbf{m}}\\) | | | | months) | | | +----------------------+----------------------+-----------------------+ | Monthly Effective | ( \\(j\\) \\) | ( \\(k\\) \\) | | Rate | | | | | | | | (Any time other | | | | than Annual) | | | +----------------------+----------------------+-----------------------+ | Convert Nominal to | ( (j = | ( \\(k | | m-Effective** | \\frac{i^{(m)}}{m}\\) \\) | = \\frac{d^{(m)}}{m}\\) \\) | +----------------------+----------------------+-----------------------+ | **Convert m to | ( ((1 | ( \\((1 | | Annual Effective** | + j)^{m} = (1 + i)\\) \\) | - k)^{m} = (1 - d)\\) \\) | | | | | | | $$i | $$ | | | = (1 + j)^{m} - 1$$ | d = 1 - (1 - k)^{m}$$ | +----------------------+----------------------+-----------------------+ | **Convert Annual to | ( (\\left( 1 + | ( \\(\\left( 1 - | | Nominal (Sub in | frac{i^{(m)}}{m} \\ri | \\frac{d^{(m)}}{m} \\r | | any** | ght)^{m} = (1 + i)\\) \\) | ight)^{m} = (1 - d)\\) \\) | | \\(\\mathbf{m}\\) )** | | | | | \\( (i^{(m)} = m\\l | ( \\(d^{(m)} = m | | | eft( (1 + i)^{\\frac{ | \\left( 1 - (1 - d)^{\\ | | | 1}{m}} - 1 \\right)\\) \\) | frac{1}{m}} \\right)\\) \\) | +----------------------+----------------------+-----------------------+ | Interesting | ( \\(\\frac{1}{m} = | | | Result** | \\frac{1}{d^{(m)}} - | | | | \\frac{1}{i^{(m)}}\\) \\) | | | **(Nominal = | | | | Nominal) | | | +----------------------+----------------------+-----------------------+ Definition of \\(\\mathbf{m}\\) \u00b6 \\(m\\) refers to the number of times interest is compounded within a year Compound once a year \u2192 \\(m = 1\\) EG. One year annual effective rate Compound multiple times a year \u2192 \\(m > 1\\) EG. 1-year nominal rate, compounded monthly Compound once in a few years \u2192 \\(m < 1\\) EG. 4-year nominal rate, compounded yearly Visual Representation \u00b6 {width=\"6.268055555555556in\" height=\"3.421527777777778in\"} Assuming the SAME effective rate of interest , the above diagram provides a visual representation of the values of the various rates Effective rates are always at the tail ends -- They move toward the centre as the compounding frequency for each type of rate, with infinite compounding (Force of interest) as the midpoint between both Nominal Interest Rate is always lower than the equivalent effective rate . This allows companies to give the false perception of lower interest rates. This is why nominal rates are often used in the industry (Vice versa for Discount) <!-- --> - Use this as a sense check on the values calculated -- whether or not it should be higher or lower Force of Interest \u00b6 As the compounding frequency increases, we can see that both the interest and discount rates move towards the centre of the graph If the compounding frequency \\(\\mathbf{m}\\) , tends to infinity , both the discount and interest rates tend to a central rate as well, known as the Force of Interest, \\(\\mathbf{\\delta}\\) . It represents how much the fund accumulates at that instantaneous time t Deriving Force of Interest \u00b6 +-----------------------------------+-----------------------------------+ | Constant Force of Interest | **Variable Force of Interest | | (Constants) | (Polynomials)** | +===================================+===================================+ | By definition, if \\(m\\) tends to | By definition, it is | | infinity: | instantaneous: | | | | | $ | \\( (\\delta = \\frac{A^{'}(t) | | \\(i^{(m)} = m\\left\\lbrack (1 + i)^ | }{A(t)} = \\frac{a^{'}(t)}{a(t)}\\) \\) | | {\\frac{1}{m}} - 1 \\right\\rbrack\\) $ | | | | \\( (\\int_{0}^{t}\\delta | | ( \\(\\lim_{m \\rightar | _{r}dr = \\ln\\left( a(t) \\right)\\) \\) | | row \\infty}i^{(m)} = \\ln(1 + i)\\) \\) | | | | \\(\\therefore a(t) = (1 + i)^{t} = | | ( \\(\\de | e^{\\int_{0}^{t}{\\delta_{r}\\ dr}}\\) | | lta = i^{(\\infty)} = \\ln(1 + i)\\) \\) | | | | If simple interest instead, | | \\( \\(e^{\\delta} = (1 + i)\\) \\) | | | | \\( (\\delta = \\frac{a^{ | | ( \\(\\therefore a( | '}(t)}{a(t)} = \\frac{i}{1 + it}\\) \\) | | t) = (1 + i)^{t} = e^{\\delta t}\\) \\) | | | | | | This only holds for | | | \\(\\left( \\mathbf{0,t} \\right)\\) . If | | | we are calculating the forward | | | rate, this formula is not true. | | +-----------------------------------+-----------------------------------+ Dealing with Variable Force of Interest \u00b6 Always find the accumulation function FIRST Take note that the limits of integration \\((0,t)\\) is for a standard accumulation function \\(a_{0}(t)\\) . For a more general accumulation function \\(a_{k}(t - k)\\) the limits would be \\((k,t)\\) Always consider logic when dealing with Variable Force -- the interest function should always be different for different years and periods When your start time is fixed, integrating from \\((0,t)\\) makes sense But when even the start time is not fixed (Yearly interest payments) then \\((t,t + 1)\\) would be better If you ever see a constant output while having a variable force, something is wrong Equivalent Rates \u00b6 As alluded to earlier, if two funds have the same starting and ending values, they will have the same Accumulation Function regardless which types of rates were used We get an all-in-one formula that we convert from one to another: \\[a(t) = (1 + i)^{t} = (1 - d)^{- t} = \\left( 1 + \\frac{i^{(m)}}{m} \\right)^{m} = \\left( 1 - \\frac{d(m)}{m} \\right)^{m} = e^{\\delta t} = e^{\\int_{0}^{t}{\\delta_{r}\\ dr}} = v^{- t}\\] There is no equivalent Compound -- Simple Interest rate. We can convert between simple interest rates across time periods using the same principle -- equating accumulation functions The conversion will simplify to become just multiplying by a factor to convert the rate [Chapter 2: Level Annuities]{.underline} \u00b6 Types of Annuities \u00b6 An annuity is a series of level payments at equal intervals Our goal is determining the Present or Future Value of the Annuity. It is the SUM of the Present or Future value of EVERY payment There are two main types of annuities: Immediate \u2192 Payments made at the end of the period (Arrears) We denote the PV as \\(a_{n\u23cbi}\\) and FV as \\(s_{n\u23cbi}\\) Due \u2192 Payments are made at the start of the period (Advance) We denote the PV as \\({\\ddot{a}}_{n\u23cbi}\\) and FV as \\({\\ddot{s}}_{n\u23cbi}\\) We calculate the PV at the start of the first period and the FV at the end of the last period , regardless of the type of Annuity \\(a_{n\u23cbi}\\) is calculated one period before the first the payment while \\(s_{n\u23cbi}\\) is calculated on the period of the last payment Conversely, \\({\\ddot{a}}_{n\u23cbi}\\) is calculated on the period of the first payment while \\({\\ddot{s}}_{n\u23cbi}\\) is calculated one period after the last payment Notice how both are symmetrical on a timeline: Important: Remember that the start of the period is also the end of the previous period . This means that given any set of Cashflows, we can treat it as either an Immediate or Due , depending on the comparison date we want to use The cashflows above are one cycle above for illustration purposes to drive home the idea that they have different comparison dates Deriving Annuity Formulas: \u00b6 Since payments are level, we can factorise out the common payments P to make the expressions simpler. However, we have to multiply it back for our final answers Effectively, this allows us to derive the formulas assuming payments of 1 +------+-------------------------------+-------------------------------+ | | [Annuity | **[Annuity Due]{.underline} | | | Immediate]{.underline}** | | +======+===============================+===============================+ | ** | Since we discount to one | Since we discount to the | | PV** | period before the first | period of the first payment, | | | payment, we discount the | we **don't discount the first | | | first payment (Start from | payment (Start from 0) | | | 1)** | | | | | \\( ({\\d | | | ( \\(a_{n\u23cbi} = v + v | dot{a}}_{n\u23cbi} = v^{0} + v^{1} | | | ^{1} + v^{2} + \\ldots v^{n}\\) \\) | + v^{2} + \\ldots v^{n - 1}\\) \\) | | | | | | | $$ | \\( \\({\\ddot{a}}_{n | | | a_{n\u23cbi} = \\sum_{1}^{n}v^{n}\\) \\) | \u23cbi} = \\sum_{0}^{n - 1}v^{n}$$ | | | | | | | \\( (a_{n\u23cbi} = | ( \\({\\ddot{a}}_{n\u23cbi | | | \\frac{v - v^{n + 1}}{1 - v}\\) \\) | } = \\frac{1 - v^{n}}{1 - v}\\) \\) | | | | | | | \\( (a_{n\u23cbi} = \\frac{v\\left | ( \\({\\ddot{a}}_ | | | ( 1 - v^{n} \\right)}{1 - v}\\) \\) | {n\u23cbi} = \\frac{1 - v^{n}}{d}\\) \\) | | | | | | | \\( \\(a_ | | | | {n\u23cbi} = \\frac{1 - v^{n}}{i}\\) \\) | | +------+-------------------------------+-------------------------------+ | ** | Since we accumulate to the | Since we accumulate to the | | FV** | period of the last payment, | one period after the last | | | we don't accumulate the | payment, we **accumulate the | | | last payment (End with 0) | last payment (End with 1)** | | | | | | | \\( (s_{n\u23cbi} = (1 + i)^ | ( \\({\\ddot{s}}_{n\u23cbi} = (1 + | | | {n - 1} + \\ldots(1 + i)^{0}\\) \\) | i)^{n} + \\ldots(1 + i)^{1}\\) \\) | | | | | | | \\( (s_{n\u23cbi} = | ( \\({\\ddot{s}}_{n\u23cbi | | | \\sum_{0}^{n - 1}(1 + i)^{n}\\) \\) | } = \\sum_{1}^{n}(1 + i)^{n}\\) \\) | | | | | | | \\( (s_{n\u23cbi} = \\frac{1 | ( \\({\\ddot{s | | | - (1 + i)^{n}}{1 - (1 + i)}\\) \\) | }}_{n\u23cbi} = \\frac{(1 + i) - (1 | | | | + i)^{n + 1}}{1 - (1 + i)}\\) \\) | | | \\( \\(s_{n\u23cbi} = | | | | \\frac{1 - (1 + i)^{n}}{- i}\\) \\) | \\( ({\\ddot{s}}_{n\u23cbi} = \\frac | | | | {(1 + i)\\left\\lbrack 1 - (1 + | | | ( \\(s_{n\u23cbi} | i)^{n} \\right\\rbrack}{- i}\\) \\) | | | = \\frac{(1 + i)^{n} - 1}{i}\\) \\) | | | | | \\( \\({\\ddot{s}}_{n\u23cbi} | | | | = \\frac{(1 + i)^{n} - 1}{d}\\) \\) | +------+-------------------------------+-------------------------------+ | Bo | Alternatively, if we know | | | th | either the Present or Future | | | | value, we can simply | | | | Accumulate or Discount that | | | | by the number of periods to | | | | amount to obtain the other: | | | | | | | | \\( \\(s | | | | _{n\u23cbi} = a_{n\u23cbi}(1 + i)^{n}\\) \\) | | | | | | | | \\( \\({\\ddot{s}}_{n\u23cbi} = | | | | {\\ddot{a}}_{n\u23cbi}(1 + i)^{n}\\) \\) | | +------+-------------------------------+-------------------------------+ | * | \\( \\(Sum\\ of\\ Geom | | | *Oth | etric\\ Series = \\frac{First\\ | | | er** | term - First\\ Omitted\\ Term\\ | | | | (\"Next\"\\ term)}{1 - Factor}\\) \\) | | | | | | | | - First omitted term is | | | | zero for an infinite | | | | series (No omission) | | | | | | | | \\( \\(\\fr | | | | ac{v}{1 - v} = \\frac{\\frac{1} | | | | {1 + i}}{1 - \\frac{1}{1 + i}} | | | | = \\frac{\\frac{1}{1 + i}}{\\fr | | | | ac{i}{1 + i}} = \\frac{1}{i}\\) \\) | | | | | | | | \\( \\(1 - v = 1 - \\frac{1}{1 | | | | + i} = \\frac{i}{1 + i} = d\\) \\) | | | | | | | | We can easily remember the | | | | denominator for each by | | | | \\(\\mathbf{i}\\) for Immediate | | | | and \\(\\mathbf{d}\\) for | | | | Due | | +------+-------------------------------+-------------------------------+ [Linking the two kinds of Annuities]{.underline} \u00b6 The financial calculator naturally assumes that the Annuities we calculate are Annuity Immediate (Need to convert for Due) If we know how to convert Immediate to Due, we can simply use the calculator and apply the conversion which will save a lot of time Both methods are viable, but usually the first method is easier and more direct. But the magic of the second approach is that it does not require you to know the interest rate Via the payment amount \u00b6 If we compare the payments for Immediate and Due, we see the regardless FV or PV, the payments for Annuity Due are always greater by a factor of \\(\\left( \\mathbf{1 + i} \\right)\\) Thus, we can go from Immediate to Due by multiplying \\((1 + i)\\) \\({\\ddot{a}}_{n\u23cbi} = (1 + i)a_{n\u23cbi}\\) \\({\\ddot{s}}_{n\u23cbi} = (1 + i)s_{n\u23cbi}\\) Alternatively, we can also use this by changing the payment amount by \\(\\left( \\mathbf{1 + i} \\right)\\) in the calculator to directly calculate Annuity Due Via the number of payments \u00b6 The difference between the two is that an Annuity Due has additional payment at time 0 and one less payment at time \\(\\mathbf{n}\\) Opposite effects on both because they are comparing in different time periods Given any series of cashflows, we can convert between the two by: <!-- --> - Present Value - **Additional payment at time 0** \u2192 Add a one additional payment $( + 1)$ - **One less payment at time** $\\mathbf{n}$ \u2192 Consider one less payment $\\left( a_{n - 1\u23cbi} \\right)$ - $\\mathbf{\\therefore}{\\ddot{a}}_{n\u23cbi} = 1 + a_{n - 1\u23cbi}$ {width=\"4.220833333333333in\" height=\"1.2145833333333333in\"} Future Value Additional payment at time 0 \u2192 Consider additional payment \\(\\left( s_{n + 1\u23cbi} \\right)\\) One less payment at time \\(\\mathbf{n}\\) \u2192 Subtract one less payment \\(( - 1)\\) \\(\\therefore\\) \\({\\ddot{s}}_{n\u23cbi} = s_{n + 1\u23cbi} - 1\\) Easy way to remember Annuity Due is always the subject the equation; Annuity Immediate is not If the RHS time is PLUS/MINUS then the it must be MINUS/PLUS 1 (Opposite) [Special Annuity Cases]{.underline} \u00b6 Deferred Annuities \u00b6 Variant where the annuity is deferred and starts at some later date (not time 0) We denote the present value of an annuity deferred by m periods as \\(_{m|}^{\\ }a_{n\u23cbi}\\) Calculating Values Deferred Annuities \u00b6 Standard Method If we already know the PV/FV of the normal annuity (Any of the 4 times) we can simply discount that value to the time we want \\(_{m|}^{\\ }a_{n\u23cbi} = v^{m}*{\\ddot{a}}_{n\u23cbi}\\) (Or any of the other values) Differences in Annuities Consider the cashflows involved in terms of their raw equation We can take a larger annuity minus a smaller annuity to obtain the deferred annuity that we want Calculating values in the middle of the annuity payments \u00b6 All our previous formulas helped us to find the value of the annuity at the start or end We have no formula to help us to find the value of the annuity in the middle of it Similarly, we can use either: Simple method \u2192 Accumulate discount a value from the start/end respectively Sum of annuities Split the annuity into two at the period we want to compare This artificially creates two annuities with end points at the time we want to compare, allowing us to sum together their values to obtain the value of the overall annuity Balloon & Drop Payments \u00b6 Sometimes instead of finding PV/FV, we are tasked to find the number of payments Since this is a calculated value, we often got non-integer values (EG. 15.42). This means that there are 15 standard payments and a fractional payment Balloon Payment \u2192 Combine the last standard and fractional payment together Drop Payment \u2192 Treat the fractional payment as its own payment Let \\(X\\) be the fractional payment: \\[\\mathbf{Balloon\\ Payment} = P + X\\] \\[PV_{Balloon} = \\ a_{n\u23cbi} + X*v^{n}\\] \\[\\mathbf{Drop\\ Payment} = X(1 + i)\\ \\] \\[PV_{Drop} = \\ a_{n\u23cbi} + X(1 + i)*v^{n + 1}\\] Since the fractional payment was treated as its own payment in the next period, we have to account for the time value of money and accumulate it accordingly Varying Interest Rates \u00b6 So far, we have used just one fixed effective rate to discount or accumulate annuities More realistically, interest rates are bound to change across periods Assume that we have [two periods]{.underline} with rates \\(i_{1}\\ \\&\\ i_{2},\\ i_{1} \\neq i_{2}\\) . There are two methods to use: Market Portfolio Method: End of Period 1 \u2192 Accumulate first payment by \\(\\left( 1 + i_{1} \\right)\\) End of Period 2 \u2192 Accumulate both payments by \\(\\left( 1 + i_{2} \\right)\\) Result \u2192 \\(FV = \\ P\\left( 1 + i_{1} \\right)\\left( 1 + i_{2} \\right) + P\\left( 1 + i_{2} \\right)\\) Yield-Curve Method: End of Period 1 \u2192 Accumulate first payment by \\(\\left( 1 + i_{1} \\right)\\) End of Period 2 \u2192 Accumulate payments by \\(\\left( 1 + i_{1} \\right)\\) and \\(\\left( 1 + i_{2} \\right)\\) respectively Result \u2192 \\(FV = P\\left( 1 + i_{1} \\right)^{2} + P\\left( 1 + i_{2} \\right)\\) Market Portfolio is more intuitive. The key difference is how the interest of the first payment (More generally, all previous periods) is treated Market Portfolio accumulates according to rate of the current period Yield Curve accumulates according to the rate when that payment was made We can know which method to use based on how the question defines interest rates, be it according to the time of payment or prevailing period Either way, we will need to manually map out each payment and discount/accumulate them according to their corresponding interest rates Continuous Annuities \u00b6 Instead of making payments at fixed time intervals like the start or end of the period, payments are made continuously throughout the period , totalling up to the same amount Similarly, we denote the PV/FV of such payments with \\({\\overline{a}}_{n\u23cbi}\\) and \\({\\overline{s}}_{n\u23cbi}\\) Deriving Continuous Annuity Formulas \u00b6 +-----------------------------------+-----------------------------------+ | [Present Value]{.underline} | [Future Value]{.underline} | +===================================+===================================+ | \\( ({\\overline{a}}_{n\u23cbi} = | ( \\({\\overline{s}}_{n\u23cbi} | | int_{0}^{n}{e^{- \\delta t}\\ }dt\\) \\) | = \\int_{0}^{n}e^{\\delta t}\\ dt\\) \\) | | | | | \\( ({\\overline{a}}_{n\u23cbi} = \\l | ( \\({\\overline{s}}_{n\u23cbi} | | eft\\lbrack \\frac{e^{- \\delta t}}{ | = \\left\\lbrack \\frac{e^{\\delta t} | | - \\delta} \\right\\rbrack_{0}^{n}\\) \\) | }{\\delta} \\right\\rbrack_{0}^{n}\\) \\) | | | | | \\( ({\\overlin | ( \\({\\ove | | e{a}}_{n\u23cbi} = \\frac{e^{- \\delta n | rline{s}}_{n\u23cbi} = \\frac{e^{\\delta | | }}{- \\delta} + \\frac{1}{\\delta}\\) \\) | n}}{\\delta} - \\frac{1}{\\delta}\\) \\) | | | | | \\( ({\\overline{a}}_{n\u23cbi} = \\f | ( \\({\\overline{s}}_{n\u23cbi} = | | rac{1 - e^{- \\delta n}}{\\delta}\\) \\) | \\frac{e^{n\\delta} - 1}{\\delta}\\) \\) | | | | | \\( ({\\overline{a}}_{ | ( \\({\\overline{s}}_{n\u23cbi} = | | n\u23cbi} = \\frac{1 - v^{n}}{\\delta}\\) \\) | \\frac{(1 + i)^{n} - 1}{\\delta}\\) \\) | +-----------------------------------+-----------------------------------+ | Similarly, we factorise out | | | constant CONTINOUS payments of | | | P which assumes a continuous | | | rate of cashflows of 1 for the | | | proof | | | | | | The accumulation function for | | | the force of interest is | | | \\(a(t) = e^{\\delta t}\\) , which | | | represents the accumulated or | | | discounted cashflows at each time | | | \\(t.\\) If \\(\\delta\\) is variable, we | | | change the accumulation factor to | | | \\(e^{\\int_{k}^{t}\\delta_{r}\\ dr}\\) | | | and perform the same steps. | | | | | | Since we want the sum of the | | | discounted or accumulated | | | cashflows from \\((0,n)\\) , we | | | integrate our general | | | accumulation function from that | | | same limit. It is the same idea | | | as our discrete annuities, but we | | | use Integrals instead of | | | Sigma to perform the sum. | | | | | | The limit of integration should | | | follow the accumulation function | | | used: | | | | | | - If we use the standard | | | accumulation function | | | \\(\\mat | | | hbf{a}\\left( \\mathbf{t} \\right)\\) , | | | then use \\((0,n)\\) | | | | | | - If we use the general | | | accumulation function | | | \\(\\mathbf{a}_{\\mathbf{k | | | }}\\left( \\mathbf{t - k} \\right)\\) , | | | then use \\((0,t - k)\\) | | +-----------------------------------+-----------------------------------+ Comparing all three annuity types \u00b6 [Annuity Due]{.underline} [Annuity Immediate]{.underline} [Continuous Annuity]{.underline} \\( \\({\\ddot{a}}_{n\u23cbi} = \\frac{1 - v^{n}}{d}\\) \\) \\( \\(a_{n\u23cbi} = \\frac{1 - v^{n}}{i}\\) \\) \\( \\({\\overline{a}}_{n\u23cbi} = \\frac{1 - v^{n}}{\\delta}\\) \\) Since \\(d\\) is the smallest rate, this produces the largest PV Since \\(i\\) is the largest rate, this produces the Since \\(\\delta\\) is the midpoint rate, it produces the smallest PV midpoint PV $${\\overline{a}}_{n\u23cbi} = \\frac{1}{\\delta}a_{n\u23cbi}$$ \\( \\({\\ddot{a}}_{n\u23cbi} = \\frac{i}{d}a_{n\u23cbi} = (1 + i)a_{n\u23cbi}\\) \\) [Chapter 2.5: Non-level Annuities]{.underline} \u00b6 Geometric Annuities \u00b6 Payments change according to a Geometric Series (By a common factor \\(k\\) ) Deriving Geometric Formula (Immediate) \u00b6 Since the original annuity proof already made use of the Geometric series , increasing payments by a geometrically simply just changes the factor we use in the proof \\[PV_{Geometric} = \\ v + (1 + k)v^{2} + \\ldots(1 + k)^{n - 1}v^{n}\\] \\[PV_{Geometric} = v\\left\\lbrack 1 + (1 + k)v + \\ldots + \\left( (1 + k)v \\right)^{n - 1} \\right\\rbrack\\] \\[PV_{Geometric} = v\\left\\lbrack \\frac{1 - (1 + k)^{n}v^{n}}{1 - (1 + k)v} \\right\\rbrack\\] \\[PV_{Geometric} = v\\left\\lbrack \\frac{1 - \\left( \\frac{1 + k}{1 + i} \\right)^{n}}{1 - \\frac{1 + k}{1 + i}} \\right\\rbrack\\] \\[PV_{Geometric} = v\\left\\lbrack \\frac{1 - \\left( \\frac{1 + k}{1 + i} \\right)^{n}}{\\frac{i - k}{1 + i}} \\right\\rbrack\\] \\[PV_{Geometric} = \\frac{1 - \\left( \\frac{1 + k}{1 + i} \\right)^{n}}{i - k}\\] This only holds for \\(i \\neq k\\) , derive the formula manually again to see the interaction. The terms will cancel each other, becoming a standard summation problem If k is negative, remember that the denominator nets out to become a plus instead Converting non-level to Level \u00b6 Looking at the proof, we notice that it is VERY similar to the level version Since the Financial Calculator only allows level payments, converting a non-level annuity to a level one would save us a lot of time The key is noticing that we can substitute the common factors for each other: +-----------------------------------+-----------------------------------+ | \\( \\(\\mathbf{i > k}\\) \\) | \\( \\(\\mathbf{i < k}\\) \\) | +===================================+===================================+ | \\( ((1 | ( \\((1 | | + k)v = \\frac{1 + k}{1 + i} < 1\\) \\) | + k)v = \\frac{1 + k}{1 + i} > 1\\) \\) | | | | | \\( \\(v^{'} = \\frac{1 + k}{1 + i}\\) \\) | $$ | | | 1 + i^{'} = \\frac{1 + k}{1 + i}$$ | | $$ | | | 1 + i^{'} = \\frac{1 + i}{1 + k}$$ | $$ | | | i^{'} = \\frac{1 + k}{1 + i} - 1$$ | | $$ | | | i^{'} = \\frac{1 + i}{1 + k} - 1$$ | | +-----------------------------------+-----------------------------------+ Depending on the relative magnitude of \\(i\\) and \\(k\\) , we can determine if the factor is closer to the Present Value, or the Future Value formula & choose the appropriate conversion <!-- --> - When converting from FV to PV (vice versa), remember to use \\(\\left( \\mathbf{1 +}\\mathbf{i}^{\\mathbf{'}} \\right)\\) not \\((1 + i)\\) This method is preferred when we have to solve for \\(\\mathbf{i}\\) or \\(\\mathbf{k}\\) as it simplifies the algebra. When solving for \\(i'\\) or \\(v'\\) , do not be alarmed by weird results Arithmetic Annuities \u00b6 Payments change according to an Arithmetic Progression (By fixed constant Q) Be careful when using AP Formula -- The formula assumes the first payment occurs at time is 1 , but sometimes your payments start at time 0 so an adjustment is needed Deriving Arithmetic Formula (Immediate) \u00b6 ALWAYS split up the payments into their individual components to better visualize it We notice that we can form several different level annuities: One level annuity with \\(n\\) payments of \\(P\\) \\(j - 1\\) deferred level annuities with \\(n - j\\) payments of \\(Q\\) We consider the PV at time 0: \\[PV_{Arithmetic} = P*a_{n\u23cbi} + Q*\\sum_{j = 1}^{n - 1}{v^{j}*\\frac{1 - v^{n - j}}{i}}\\] \\[PV_{Arithmetic} = \\ P*a_{n\u23cbi} + Q*\\sum_{j = 1}^{n - 1}\\frac{v^{j} - v^{n}}{i}\\] \\[PV_{Arithmetic} = \\ P*a_{n\u23cbi} + Q\\left( \\sum_{j = 1}^{n - 1}\\frac{v^{j}}{i} - \\frac{nv^{n}}{i} \\right)\\] \\[PV_{Arithmetic} = \\ P*a_{n\u23cbi} + Q\\left( \\frac{a_{n\u23cbi}}{i} - \\frac{nv^{n}}{i} \\right)\\] \\[PV_{Arithmetic} = a_{n\u23cbi}\\left( P + \\frac{Q}{i} \\right) - \\frac{Qnv^{n}}{i}\\] Using the Financial Calculator, we can quickly calculate this value by setting \\(\\left( P + \\frac{Q}{i} \\right)\\) as the Payment and \\(\\left( \\frac{Qnv^{n}}{i} \\right)\\) as the Future Value Understanding this proof is important as this concept of splitting annuities is extremely important when dealing with Arithmetic Annuities Special case: Unit Increasing \u00b6 When \\(P = Q = 1\\) . The condition can be met for any \\(\\mathbf{P = Q = x,}\\) as we can simply factor out \\(x\\) to obtain \\(P = Q = 1\\) , but remember to multiply back \\(\\mathbf{x}\\) First payment starts out at 1 and increases by 1 each period until \\(\\mathbf{n}\\) in the last period When \\(P = Q = 1,\\) \\[(Ia)_{n\u23cbi} = \\ a_{n\u23cbi} + \\left( \\frac{a_{n\u23cbi}}{i} - \\frac{nv^{n}}{i} \\right)\\] \\[(Ia)_{n\u23cbi} = \\frac{i*a_{n\u23cbi} + a_{n\u23cbi} - nv^{n}}{i}\\ \\] \\[(Ia)_{n\u23cbi} = \\frac{a_{n\u23cbi}(1 + i) - nv^{n}}{i}\\] \\[(Ia)_{n\u23cbi} = \\frac{{\\ddot{a}}_{n\u23cbi} - nv^{n}}{i}\\] Special case: Unit Decreasing \u00b6 When \\(P = n\\) and \\(Q = - 1\\) First payment starts out at \\(n\\) and decreases by 1 each period until 1 in the last period When \\(P = n,\\ Q = - 1,\\) \\[(Da)_{n\u23cbi} = n*a_{n\u23cbi} - \\left( \\frac{a_{n\u23cbi}}{i} - \\frac{nv^{n}}{i} \\right)\\] \\[(Da)_{n\u23cbi} = n*\\left( \\frac{1 - v^{n}}{i} \\right) - \\left( \\frac{a_{n\u23cbi}}{i} - \\frac{nv^{n}}{i} \\right)\\] \\[(Da)_{n\u23cbi} = \\frac{\\begin{array}{r} n \\\\ \\left( 1 - v^{n} \\right) - \\left( a_{n\u23cbi} - nv^{n} \\right) \\\\ \\end{array}}{i}\\] \\[(Da)_{n\u23cbi} = \\frac{n - nv^{n} - a_{n\u23cbi} + nv^{n}}{i}\\] \\[(Da)_{n\u23cbi} = \\frac{n - a_{n\u23cbi}}{i}\\] Continuously Increasing Annuities \u00b6 Annuities that increase continuously rather than at discrete times Rate of payment can be described as a function of \\(\\mathbf{t}\\) (Since \\(t\\) is always increasing as well) Deriving Continuously increasing Annuities \u00b6 Similar to the level version, we integrate the payments and the accumulation function: \\[\\left( \\overline{Ia} \\right)_{n\u23cbi} = \\ \\int_{0}^{n}{f(t)*e^{\\delta t}\\ dt}\\] \\[PV = \\ \\int_{0}^{n}{f(t)*e^{\\int_{0}^{t}{\\delta_{r}\\ dr}}\\ dt}\\] Tabular integration : Differentiate Integrate \\( \\(f(t)\\) \\) \\( \\(\\int_{}^{}e^{\\delta t}\\) \\) \\( \\(f^{'}(t)\\) \\) \\( \\(\\int_{}^{}{\\int_{}^{}e^{\\delta t}}\\) \\) \\( \\(\\vdots\\) \\) \\( \\(\\vdots\\) \\) \\( \\(1\\) \\) \\( \\(\\int_{}^{}{\\ldots\\int_{}^{}e^{\\delta t}}\\) \\) Perpetuities \u00b6 Annuities whose payments go on forever (No end date) All formulas can be derived by letting \\(\\mathbf{n \\rightarrow \\infty}\\) Level Annuities \u00b6 [Annuity Due]{.underline} [Annuity Immediate]{.underline} [Continuous Annuity]{.underline} \\( \\({\\ddot{a}}_{n\u23cbi} = \\frac{1 - v^{n}}{d}\\) \\) \\( \\(a_{n\u23cbi} = \\frac{1 - v^{n}}{i}\\) \\) \\( \\({\\overline{a}}_{n\u23cbi} = \\frac{1 - v^{n}}{\\delta}\\) \\) \\( \\({\\ddot{a}}_{\\infty \u23cbi} = \\frac{1}{d}\\) \\) \\( \\(a_{\\infty \u23cbi} = \\frac{1}{i}\\) \\) \\( \\({\\overline{a}}_{\\infty \u23cbi} = \\frac{1}{\\delta}\\) \\) Non-level Annuities \u00b6 +-----------------------------------+-----------------------------------+ | [Geometric | **[Arithmetic | | Annuities]{.underline} | Annuities]{.underline}** | +===================================+===================================+ | \\( (PV_{Geome | ( \\(PV_{Arithmetic} = \\ | | tric} = \\frac{1 - \\left( \\frac{1 | P*a_{n\u23cbi} + Q\\left( \\frac{a_{n\u23cbi} | | + k}{1 + i} \\right)^{n}}{i - k}\\) \\) | }{i} - \\frac{nv^{n}}{i} \\right)\\) \\) | | | | | \\( (PV_{Geomet | ( \\(PV_{Arithmetic,\\in | | ric,\\ \\infty} = \\frac{1}{i - k}\\) \\) | fty} = P*a_{\\infty \u23cbi} + Q\\left( | | | \\frac{a_{\\infty \u23cbi}}{i} \\right)\\) \\) | | If \\(\\mathbf{k}\\) is | | | negative , the denominator nets | \\( \\(PV_{Arithmetic | | out to **become a plus instead** | ,\\infty} = P*\\frac{1}{i} + Q\\left | | | ( \\frac{\\frac{1}{i}}{i} \\right)\\) \\) | | | | | | \\( \\(PV_{Arithmetic,\\infty} | | | = \\frac{P}{i} + \\frac{Q}{i^{2}}\\) \\) | +-----------------------------------+-----------------------------------+ Important concepts for Perpetuities \u00b6 The difference between two perpetuities is always an Annuity . Since both payments go on forever, the difference is simply when they start The present value of a Perpetuity at any time is always the same. This is because the payments go forever, so it doesn't matter when it starts We can approximate Perpetuities in the TVM by setting the number of years to be 9999, which is sufficiently large to approximate such cashflows [Other Important Annuity Patterns]{.underline} \u00b6 Block Payments (For any increase/decrease -- very powerful concept) \u00b6 Annuities that pay out in blocks. Payments within blocks are the same but each block has a different set of payments We can treat each block as a deferred annuity, but a faster way is to add/subtract them If we start from the date furthest from the comparison date and add the change in payment levels, the final result is the PV/FV of the entire annuity {width=\"6.268055555555556in\" height=\"2.222916666666667in\"} Repeat Reverse Annuities (For increases/decreases of 1) \u00b6 Annuities that start of increasing, reach a peak, and decrease back to the start We could split into an increasing and decreasing annuity, but another faster method would be to recognise that they are all the same deferred annuity : {width=\"6.268055555555556in\" height=\"3.08125in\"} Odd-Even Split (For alternating increases) \u00b6 Annuities that follow a cyclical pattern (Increase in odd and decrease in even years) We can split them into a level annuity and an increasing annuity with different intervals: {width=\"6.268055555555556in\" height=\"0.9333333333333333in\"} Annuities into Perpetuities: \u00b6 Decreasing Annuity into a Perpetuity: \u00b6 Annuities that decrease till a certain level then becomes a level perpetuity We could treat each block as a decreasing annuity and deferred perpetuity But a faster way would be to treat them as a perpetuity and unit decreasing annuity : \\[PV = \\frac{k}{i} + (Da)_{\\overline{n - k}\u2142i}\\] \\[PV = \\frac{k}{i} + \\frac{n - k + a_{\\overline{n}\u2142i}}{i}\\] \\[PV = \\frac{n + a_{\\overline{n}\u2142i}}{i}\\] {width=\"6.268055555555556in\" height=\"1.8979166666666667in\"} Increasing Annuity into perpetuity \u00b6 Annuities that increase till a certain level then become a level perpetuity We could treat each block as an increasing annuity then deferred perpetuity But a faster way would be to treat them as the difference of Unit Increasing Perpetuities : \\[PV = (Ia)_{\\overline{\\infty}\u2142i} - v^{n}(Ia)_{\\overline{\\infty}\u2142i}\\] \\[PV = (Ia)_{\\overline{\\infty}\u2142i}\\left( 1 - v^{n} \\right)\\] \\[PV = \\left( \\frac{1}{i} + \\frac{1}{i^{2}} \\right)\\left( 1 - v^{n} \\right)\\] \\[PV = \\left( \\frac{1 + i}{i^{2}} \\right)\\left( 1 - v^{n} \\right)\\] \\[PV = \\frac{1}{i}*\\frac{1 - v^{n}}{d}\\] \\[PV = \\frac{{\\ddot{a}}_{\\overline{n}\u2142i}}{i}\\] {width=\"6.268055555555556in\" height=\"2.3256944444444443in\"} Annuities with unknown number of payments \u00b6 Annuities that can be split into different smaller annuities We could treat it as one large annuity, but we may not have all the information needed (Typically, this is used for questions where \\(n\\) is not stated) But a faster method would be to treat each section as its own Annuity (Based on what information we know) and then discount them accordingly: \\[a_{3n\u23cbi} = a_{n\u23cbi} + v^{n}*a_{n\u23cbi} + v^{2n}*a_{n\u23cbi}\\] \\[a_{Xn\u23cbi} = a_{n\u23cbi} + v^{n}*a_{n\u23cbi} + v^{2n}*a_{n\u23cbi} + \\ldots v^{Xn}*a_{n\u23cbi}\\] {width=\"6.268055555555556in\" height=\"2.775in\"} Matching Payment and Interest frequencies \u00b6 For us to use ALL of the above annuity formulas, the rates we use MUST MATCH the frequency of the payments (EG. Monthly interest with Monthly Payments) If they are not in sync, we have two methods to make them sync Converting Interest Rates Using the principle of Equivalent Rates (Chapter 1) Generally preferred, but not applicable for symbolic questions Converting Payments Lower to higher payment frequency \u2192 Expand payments Higher to lower payment frequency \u2192 Collapse payments Lower to Higher payment frequency \u00b6 Payments occur less frequently than the effective rate given EG. Quarterly Payments but Monthly Interest \u2192 Convert to monthly payment We can split the payments into \\(k\\) smaller payments of \\(X\\) to be aligned with the given rate We must keep the PV/FV of these payments to be equal to the original payment. We can use the level annuity formulas to calculate the smaller payment \\(X\\) Since \\(X\\) matches the given rates, we can use both in our annuity formulas Deriving Formulas \u00b6 +-----------------------------------+-----------------------------------+ | Payment at the start of | **Payment at the end of Period | | period** | | +===================================+===================================+ | \\( \\(P = X*a_{k\u23cbi}\\) \\) | \\( \\(P = X*s_{k\u23cbi}\\) \\) | | | | | \\( \\(X = \\frac{P}{a_{k\u23cbi}}\\) \\) | \\( \\(X = \\frac{P}{s_{k\u23cbi}}\\) \\) | +-----------------------------------+-----------------------------------+ | \\( \\(PV = X*a_{n\u23cbi}\\) \\) | \\( \\(PV = X*a_{n\u23cbi}\\) \\) | | | | | $ | $ | | \\(PV = \\frac{P}{a_{k\u23cbi}}*a_{n\u23cbi}\\) $ | \\(PV = \\frac{P}{s_{k\u23cbi}}*a_{n\u23cbi}\\) $ | | | | | $ | $ | | \\(PV = P*\\frac{a_{n\u23cbi}}{a_{k\u23cbi}}\\) $ | \\(PV = P*\\frac{a_{n\u23cbi}}{s_{k\u23cbi}}\\) $ | +-----------------------------------+-----------------------------------+ | \\( \\(FV = X*s_{n\u23cbi}\\) \\) | \\( \\(FV = X*s_{n\u23cbi}\\) \\) | | | | | $ | $ | | \\(FV = \\frac{P}{a_{k\u23cbi}}*s_{n\u23cbi}\\) $ | \\(FV = \\frac{P}{s_{k\u23cbi}}*s_{n\u23cbi}\\) $ | | | | | $ | $ | | \\(FV = P*\\frac{a_{n\u23cbi}}{a_{k\u23cbi}}\\) $ | \\(FV = P*\\frac{s_{n\u23cbi}}{s_{k\u23cbi}}\\) $ | +-----------------------------------+-----------------------------------+ Higher to Lower payment frequency \u00b6 Payments occur more frequently than the effective rate given EG. Monthly payments but yearly interest given \u2192 Convert to Yearly payment We can combine \\(k\\) smaller payments into one payment of \\(X\\) be aligned with the given rate We must keep the PV/FV of these payments to be equal to the original payment. We can use the level annuity formulas to calculate the larger payment \\(X\\) <!-- --> - Since \\(X\\) matches the given rates, we can use both in our annuity formulas Deriving Formulas \u00b6 We first show the equivalent rates: \\[\\left( 1 + i^{(m)} \\right)^{k} = (1 + i)\\] \\[i = \\left( 1 + i^{(m)} \\right)^{k} - 1\\] We then accumulate to the end of the period: \\[X = P*s_{n\u23cbi}\\] \\[X = \\ P*\\frac{\\left( 1 + i^{(m)} \\right)^{k} - 1}{i^{(m)}}\\] \\[X = P*\\frac{i}{i^{(m)}}\\] \\[PV = X*a_{n\u23cbi}\\] \\[PV = \\ P*\\frac{i}{i^{(m)}}*a_{n\u23cbi}\\] [Chapter 3: Spot, Forward and Interest Swaps]{.underline} \u00b6 Spot & Forward Rates \u00b6 Rates offered are likely to change in accordance with the investment horizon. Different periods of investment will result in different rates There are two main kinds of rates: Spot \\(\\left( \\mathbf{s}_{\\mathbf{t}} \\right)\\) \u2192 From today to some future date \\(\\left( \\mathbf{0\\ to\\ t} \\right)\\) <!-- --> - Forward \\(\\left( \\mathbf{f}_{\\mathbf{t - 1,t}} \\right)\\) \u2192 From a future date to 1 year ahead \\(\\left( \\mathbf{t - 1\\ to\\ t} \\right)\\) Linking the two rates \u00b6 Both rates are quoted Annual Effective . This is important for Spot rates, as it spans across more than one period: \\(a(t) \\neq \\left( 1 + s_{t} \\right)\\) \\(a(t) = \\left( 1 + s_{t} \\right)^{t}\\) <!-- --> - Using the concept of Equivalent Rates, we can convert between the two by: - $\\left( 1 + s_{t} \\right)^{t} = \\left( 1 + s_{t - 1} \\right)^{t - 1}\\left( 1 + f_{t} \\right)$ - **OR** $\\left( 1 + s_{t} \\right)^{t} = \\left( 1 + f_{1} \\right)\\left( 1 + f_{2} \\right)\\ldots\\left( 1 + f_{t} \\right)$ - By definition, $s_{1} \\equiv f_{1}$ Using these formulas, we can form the accumulation function and solve cashflow problems using first principles by manually accumulating/discounting every cashflow Forward Rates calculated this way are known as the Implied Rates. They are an estimate of what will be the actual 1-year spot rate during that time , known as the Quoted Rates. Where possible, we should use the Quoted Rate over the implied forward rates. Forward rates lasting more than one period \u00b6 They are like spot rates in the sense that they last more than one period. Thus, we can use the same principle of equivalent rates to calculate them: Using spot rates: \\(\\left( 1 + s_{t + m} \\right)^{t + m} = \\left( 1 + s_{t} \\right)^{t}*\\left( 1 + f_{t,t + m} \\right)^{m}\\) Using forward rates: \\(\\left( 1 + f_{t,t + m} \\right)^{m} = \\left( 1 + f_{t,t + 1} \\right)*\\ldots\\left( 1 + f_{t + m - 1,t + m} \\right)\\) Sometimes, we are given forward rates for cashflows that occur before the forward rate period. This means that we should assume the cashflow is reinvested at that forward rate and we should find the value after reinvestment Interest Rate Swaps \u00b6 Two counterparties currently have two kinds of liabilities with different style of interests: Party A \u2192 Variable Interest Party B \u2192 Fixed Interest Each party would now like the other style of payment for various reasons: Hedge against interest rates Predictability of payments One solution would be to Swap their interest payments. However, since they can't directly swap their contract, they can achieve a similar outcome by paying for the other party: Party A will pay the fixed rate to party B ( Payer ) Party B will pay the variable rate to party A ( Receiver ) Swap Mechanics \u00b6 Fixed payments will be made at a constant Swap Rate \\(\\left( \\mathbf{R} \\right)\\) at periodic intervals known as the Settlement Period Variable payments will be made at the prevailing rates at the time, modelled by using the current Forward Rates \\(\\left( \\mathbf{f}_{\\mathbf{n}} \\right)\\) Since both parties may not have identical principals, the payments are made according to some common Notional Amount \\(\\left( \\mathbf{X}_{\\mathbf{n}} \\right)\\) Constant Notional \u2192 Level Swap Increasing Notional \u2192 Accreting Swap Decreasing Notional \u2192 Amortizing Swap To ensure that the deal is fair, the PV of both Cashflows must be the same: \\[PV(Fixed\\ Payments) = PV(Variable\\ Payments)\\] \\[\\frac{X_{1}R}{\\left( 1 + s_{1} \\right)} + \\ldots\\frac{X_{n}R}{\\left( 1 + s_{n} \\right)^{n}} = \\frac{X_{1}f_{1}}{\\left( 1 + s_{1} \\right)} + \\ldots\\frac{X_{n}f_{n}}{\\left( 1 + s_{n} \\right)^{n}}\\] Deferred Swaps \u00b6 Similar concept to a Deferred Annuity. However, when an SOA says \\(\\mathbf{n}\\) -year deferred , \\(\\mathbf{m}\\) -year interest rate swaps, the actual swap occurs for \\(\\mathbf{n - m}\\) years after \\(\\mathbf{n}\\) years In other words, the swap period defined by SOA includes the deferral as well Level Swap Shortcuts \u00b6 There are two shortcuts available for Level Swaps. If the swap is Accreting or Amortizing, we will have to perform the calculations manually The first shortcut is factoring out X : If the swap is level, \\(X_{1} = X_{2} = \\ldots = X_{n}\\) , which allows it to be factored out from both sides of the equation and cancelled This simplifies the calculation to just using the interest rates The second is to use the Bond pricing formula: This method uses an entirely different approach to calculating the swap rate We consider the net cashflows for Party A , including their current obligation. We notice that the resulting cashflows looks very similar to that of a bond, thus we can make use of that formula to calculate R as well! {width=\"6.268055555555556in\" height=\"4.055555555555555in\"} Bond Pricing Formula: \\[Price\\ of\\ Bond = PV(Coupon\\ Payments) + PV(Principal)\\] \\[X = \\frac{XR}{\\left( 1 + s_{1} \\right)} + \\ldots + \\frac{XR}{\\left( 1 + s_{n} \\right)^{n}} + \\frac{X}{\\left( 1 + s_{n} \\right)^{n}}\\] Since level swap, we can factorise & cancel out X: \\[1 = R\\left( \\frac{1}{\\left( 1 + s_{1} \\right)} + \\ldots + \\frac{1}{\\left( 1 + s_{n} \\right)^{n}} \\right) + \\frac{1}{\\left( 1 + s_{n} \\right)^{n}}\\] If it is a deferred swap , we all we need to do is to discount the LHS to the first period as well. [Other Swap Calculations]{.underline} \u00b6 Net Swap Payments \u00b6 Since both parties pay each other on the same day, it is redundant to make payments Instead, it is more efficient for one party to just make a Net Swap Payment instead It is important to consider whose perspective we are taking : An outflow for one party is an Inflow for another Net Interest Payment \u00b6 We want to consider how much one party pays in total on the settlement date There are two cashflows from their perspective: Net Swap Payment Original Interest Payment The Net Interest Payment is simply the sum of these two cashflows, which is how much they end up paying in total Similarly, it is also important to consider whose perspective when computing the net interest payment Market Value \u00b6 One may choose to leave the position before the end of the Swap Term In order to sell it to another person, we must know the market value of the contract We can calculate it by computing the Net Present Value of all future Net Swap Payments By definition, the Market Value of a contract at time 0 will be 0 because \\(\\mathbf{NPV = 0}\\) However, due to interest rates changing (Our projected forward rate not being the actual rate come the appropriate time), the market value of the contract at some later time is not likely to be 0 We should always use the prevailing Spot Rates at the time we are calculating the market value instead of the forward rates we predicted in the past It is important to consider whose perspective we are looking at, as one person's gain is another person's loss Why interest rate swaps? \u00b6 Typically, the payer of the swap rate currently has an existing loan with interest payments based on some floating rate The payer believes that interest rates will rise, hence wants to pay a fixed swap rate to hedge against these rising interest rates The receiver believes that interest rates will fall, hence is willing to pay the lower variable rate [Chapter 4: Rates of Return]{.underline} \u00b6 [One Period Return Rates]{.underline} \u00b6 Time Weighted Rate of Return (TWRR) \u00b6 Let R be sub-period return rate: \\[R_{n} = \\frac{Balance\\ before\\ new\\ transaction}{Balance\\ after\\ previous\\ transaction} = \\frac{B_{n}^{Before}}{B_{n - 1}^{After}}\\] Using the concept of equivalent rates, we can solve for the TWRR \\[\\left( R_{1} \\right)\\left( R_{2} \\right)\\ldots\\left( R_{n} \\right) = \\left( 1 + i_{TWRR} \\right)\\] Balance before 0 \\( \\(B_{1}^{Before}\\) \\) \\( \\(\\ldots\\) \\) \\( \\(B_{n - 1}^{Before}\\) \\) \\( \\(B_{n}^{After}\\) \\) transaction Transaction Initial Deposit First Transaction \\( \\(\\ldots\\) \\) Final Transaction (Injection/Withdrawal) Balance after \\( \\(B_{0}^{After}\\) \\) \\( \\(B_{1}^{After}\\) \\) \\( \\(\\ldots\\) \\) \\( \\(B_{n - 1}^{After}\\) \\) transaction Sub-period return rate \\( \\(R_{1}\\) \\) \\( \\(R_{2}\\) \\) \\( \\(R_{n - 1}\\) \\) \\( \\(R_{n}\\) \\) Dollar Weighted Rate of Return (DWRR) \u00b6 DWRR is essentially the IRR of the fund : \\[B_{Start}\\left( 1 + i_{DWRR} \\right)^{1} + CF_{1}\\left( 1 + i_{DWRR} \\right)^{\\frac{11}{12}} + \\ldots + CF_{n}\\left( 1 + i_{DWRR} \\right)^{\\frac{(1 - n)}{n}} = B_{End}\\] However, as seen earlier, IRR is numerically complicated to compute Thus, we consider a Simple Interest Approximation instead: \\[B_{Start}\\left( 1 + i_{DWRR} \\right) + CF_{1}\\left( 1 + \\frac{11}{12}i_{DWRR} \\right) + \\ldots + CF_{n}\\left( 1 + \\frac{(1 - n)}{n}i_{DWRR} \\right) = B_{End}\\] \\[i_{DWRR} = \\frac{B_{End} - B_{Start} - \\sum_{}^{}{CF_{t}}}{B_{Start} + \\sum_{}^{}{CF_{t}(1 - t)}}\\] For time spans of exactly one-year , this simple interest approximation will be equivalent to the one-year IRR of the fund Can be verified using Desmos by plotting \\(y = \\left( 1 + i_{IRR} \\right)^{x}\\) and \\(y = \\left( 1 + i_{DWRR}x \\right)\\) For any \\(i_{IRR}\\ = i_{DWRR}\\) , the intersection point will always occur at \\(x = 1\\) Phrased another way, as long as \\(x = t = 1,\\) \\(i_{DWRR} = i_{IRR}\\) <!-- --> - Notice that this formula is extremely similar to the regular interest formula . This means that if the cashflows are sufficiently small , the simple interest approximation method of DWRR can be used to estimate the true annual effective rate Note: Investment Income refers to the interest earned on all the cashflows during the period. Differences between each measure: \u00b6 TWRR makes use of Compound Interest while DWRR uses simple interest approximation TWRR is a measure of the Fund Managers Performance , while DWRR is a measure of the Overall Performance of the Fund TWRR removes the effect of the timing and additional transactions. This is similar to how a fund manager does not control over when they receive funds and how much , thus is used to approximate the fund manager performance If DWRR > TWRR, it means that the timing of deposits/withdrawals was very good -- deposited right before a high growth period after or withdrawn before a crash Different Timespans \u00b6 The above formulas use timespans of one year to compute the rates Thus, the calculated TWRR and DWRR are naturally assumed to be annual 1-year rate However, some questions may specify a different timespan -- 6 months or 2-year The most important thing is to MAINTAIN the formula For TWRR, maintain the LHS as just \\((1 + i)\\) and for DWRR, keep the coefficients of \\(i\\) to be under 1 (Proportion till the end of the given time) This way, the rates calculated will be the 6-month or 2-year rate Using the concept of equivalent rates , we will need to convert these 6-month or 2-year rates into annual effective 1-year rates ( True TWRR/DWRR) [Multiple periods return rates]{.underline} \u00b6 Arithmetic Mean \u00b6 Given multiple one period returns \\(\\left( R_{1}\\ldots R_{n} \\right)\\) , the arithmetic mean rate of return is the average of all of them It is a measure of the average fund performance if you were to hold it for one year. Some say that this measure is overly optimistic for long time horizons \\[R_{Arithmetic} = \\frac{R_{1} + \\ldots R_{n}}{n}\\] Geometric Mean (AKA Cumulative Annual Growth Rate -- CAGR) \u00b6 Given multiple one period returns \\(\\left( R_{1}\\ldots R_{n} \\right)\\) , the geometric mean rate of return is the annualized effective rate of all of them It is a measure of the expected annualized rate if you were to hold it for \\(\\mathbf{n}\\) years. Some say that this is overly pessimistic for short time horizons \\[\\left( 1 + R_{1} \\right)\\ldots\\left( 1 + R_{n} \\right) = \\left( 1 + R_{Geometic} \\right)^{n}\\] Important: To find the variance of these return rates, we should use the Sample Variance formula : \\[s^{2} = \\frac{1}{n - 1}\\sum_{}^{}\\left( r_{i} - \\overline{r} \\right)\\] [Discounted Cashflow Analysis]{.underline} \u00b6 Net Present Value (NPV) \u00b6 It is the difference between the PV of Cash Inflows and Outflows . It thus measures the raw value created for the firm \\(NPV = PV(Inflows) - PV(Outflows)\\) Internal Rate of Return (IRR) \u00b6 The required interest rate that makes the NPV of a project zero . It represents the annual rate of return that will be earned \\(NPV = 0 \\rightarrow Solve\\ for\\ i\\ using\\ Financial\\ Calc\\) Can also be used to solve quadratic equations involving \\(v\\) [Other Measures of Return (FNCE101)]{.underline} \u00b6 Dollar Returns \u00b6 A measure of the absolute return on an investment over a period of time Poor measure of return since there is no indication if it is high or low \\[Dollar\\ Return = Income\\ from\\ Investment + Capital\\ Gain\\ or\\ Loss\\] Percentage Return \u00b6 Measure of the percentage return on an investment over a period of time Better measure as it considers the beginning values of the assets \\[Percentage\\ Return = Dividend\\ Yield + Capital\\ Gain\\ Yield\\] \\[Percentage\\ Return = \\frac{Dividend}{Beginning\\ Price} + \\frac{Capital\\ Gain\\ or\\ Loss}{Beginning\\ Price}\\] [Chapter 5: Loans]{.underline} \u00b6 What is a loan? \u00b6 When we borrow money from the bank or another company, we are taking a Loan The outstanding loan balance earns interest , increasing the amount we have to pay for each period we do not fully repay the loan We consider the standard scenario where loans are repaid at level amounts at regular intervals (Forming a Level Annuity) Like all other problems we have dealt with till now, the key principle is that the*\\ * \\( \\(\\mathbf{PV}\\left( \\mathbf{Loan\\ Amount} \\right)\\mathbf{= PV}\\left( \\mathbf{Loan\\ Payments} \\right)\\) \\) There are two components to each loan repayment: Interest repaid Principal repaid Each repayment made will first pay off any interest earned so far , and the remaining amount will pay off the principal We are interested in calculating the not only the repayment amount but the various components of the loan at each time to better understand the status of Debt {width=\"6.268055555555556in\" height=\"3.520138888888889in\"} {width=\"6.268055555555556in\" height=\"2.0243055555555554in\"} [Loan Calculations]{.underline} \u00b6 Loan Variables \u00b6 \\(B_{t}\\) to denote the Outstanding Balance, where \\(B_{0}\\) is the principal \\(R_{t}\\) to denote the Repayment Amount (Assumed to be Constant) \\(I_{t}\\) to denote the amount that was used to pay off Interest \\(P_{t}\\) to denote the amount that was used to pay off the principal [Three ways to Calculate Outstanding Loan Balance]{.underline} \u00b6 Recursive Method \u00b6 Using first principals and manually calculating the change in each period: \\[B_{t} = B_{t - 1} - P_{t}\\] \\[B_{t} = B_{t - 1} + I_{t} - R\\] \\[B_{t} = B_{t - 1} + B_{t - 1}*i - R\\] \\[B_{t} = B_{t - 1}(1 + i) - R\\] Applying recursion, \\[B_{t} = \\left( B_{t - 2}(1 + i) - R \\right)(1 + i) - R\\] \\[\\vdots\\] \\[B_{t} = B_{0}(1 + i)^{t} - R - \\ldots R(1 + i)^{t - 1}\\] \\[B_{t} = B_{0}(1 + i)^{t} - R*s_{n\u23cbi}\\] Retrospective Method \u00b6 The outstanding balance is the Time adjusted difference of the principal & what was already paid : \\[B_{t} = B_{0}(1 + i)^{t} - R*s_{n\u23cbi}\\] Notice that this is the same result as the Recursive Method ; they are the same Prospective Method \u00b6 The outstanding balance is the PV of all future loan payments : \\[B_{t} = R*a_{n - t\\ \u23cbi}\\] Note that this method can also be used if we have a future loan balance as well By discounting the Future Loan Balance and any payments between then and the current time , we obtain the Current Loan Balance {width=\"6.268055555555556in\" height=\"2.5555555555555554in\"} All three methods are equivalent and will lead to the same results. We need to know all three because it depends on what information the question gives us . It is faster to use a particular method if the question has already given us the relevant information. Loan Amortization \u00b6 In finance, Amortization means to spread payments (of a Loan, in this case) <!-- --> - The goal is to create an Amortization Schedule that shows how each Repayment is used to pay off both the Interest and Principal in each period , slowly reducing our Outstanding Balance to 0 at the end of the Loan For simplicity, we factor out all payments to 1 (Similar to our Annuity Proofs). Remember to multiply by the payment amount for actual questions Recursive/Retrospective Schedule \u00b6 Using the Recursive Method, we can determine each component in any given period However, this method is best done in Excel when the calculations can be performed quickly. For the exam, this may not be the best method +------+---------------+--------------+--------------+----------------+ | ** | Repayment | Interest | **Principal | **Loan | | Peri | | Repaid | Repaid** | Balance** | | od** | | | | | | | | $$I_{t} = B | $ | $ | | | | _{t - 1} i$$ | (P_{t} = R_{ | \\(B_{t} = B_{t | | | | | t} - I_{t}\\) \\) | - 1} - P_{t}$$ | +======+===============+==============+==============+================+ | 0 | | | | ( \\(B_{0}\\) \\) | +------+---------------+--------------+--------------+----------------+ | 1 | 1 | ( \\(B_{0}*i\\) \\) | ( (1 | ( \\(B_{0 | | | | | - B_{0}*i\\) \\) | }(1 + i) - 1\\) \\) | +------+---------------+--------------+--------------+----------------+ | $$ | ( \\(\\vdots\\) \\) | ( \\(\\vdots\\) \\) | ( \\(\\vdots\\) \\) | ( \\(\\vdots\\) \\) | | \\vdo | | | | | | ts$$ | | | | | +------+---------------+--------------+--------------+----------------+ | $ | 1 | $$B | $$1 - B | $ | | \\(n\\) $ | | _{n - 1}*i$$ | _{n - 1}*i$$ | \\(B_{0}(1 + i)^ | | | | | | {t} - s_{n\u23cb}\\) $ | +------+---------------+--------------+--------------+----------------+ | * | ( \\(n\\) \\) | ( (\\S | ( \\(n - \\S | | | *Tot | | igma I_{t}\\) \\) | igma I_{t}\\) \\) | | | al* | | | | | +------+---------------+--------------+--------------+----------------+ Prospective Schedule \u00b6 Using the prospective method instead, we notice that the Loan Balance forms an Annuity Using this Annuity formula, we can see how the schedule becomes simplified: +-------+-----------+---------------+---------------+-----------------+ | Per | **Re | **Interest | **Principal | **Loan | | iod | payment** | Repaid** | Repaid** | Balance** | | | | | | | | | Factor to | \\( (I_{t} = | ( (P_{t} | ( \\(B_{t} | | | 1 | B_{t - 1}*i\\) \\) | = 1 - I_{t}\\) \\) | = a_{n - t\u23cbi}\\) \\) | | | | | | | | | | \\( (I | ( \\(P_{t} = v^ | | | | | _{t} = 1 - v^ | {n - t + 1}\\) \\) | | | | | {n - t + 1}\\) \\) | | | +=======+===========+===============+===============+=================+ | 0 | | | | \\( \\(a_{n\u23cbi}\\) \\) | +-------+-----------+---------------+---------------+-----------------+ | 1 | 1 | \\( \\(1 - v^{n}\\) \\) | \\( \\(v^{n}\\) \\) | \\( \\(a_{n - 1\u23cbi}\\) \\) | +-------+-----------+---------------+---------------+-----------------+ | $$\\vd | $ | \\( \\(\\vdots\\) \\) | \\( \\(\\vdots\\) \\) | \\( \\(\\vdots\\) \\) | | ots$$ | \\(\\vdots\\) $ | | | | +-------+-----------+---------------+---------------+-----------------+ | \\( \\(n\\) \\) | 1 | \\( \\(1 - v\\) \\) | \\( \\(v\\) \\) | \\( \\(a_{0\u23cbi}\\) \\) | +-------+-----------+---------------+---------------+-----------------+ | To | ( \\(n\\) \\) | $$ | ( \\(a_{n\u23cbi}\\) \\) | | | tal | | n - a_{n\u23cbi}$$ | | | +-------+-----------+---------------+---------------+-----------------+ From this insight, we can see that all we need is the interest rate and repayment amount to identify any component If we are given the principal repaid in one period , we can use the accumulate or discount it accordingly to find the principal repaid in another Principal Payments are proportional -- \\(\\frac{Principal_{t}}{Principal_{t + 2}} = \\frac{Principal_{t + n}}{Principal_{t + n + 2}} = v^{2}\\) The principal payments can also be viewed in a timeline: Varying Interest Rates \u00b6 If interest rates change midway, the remaining schedule has to be changed as well In particular, we have to recalculate the remaining number of payments needed. Using this, we can calculate the remaining loan balance after the interest rate change However, the shortcut method still works -- just use the appropriate interest rates at the time when they change [Other Payment Patterns]{.underline} \u00b6 Level Principal Payments \u00b6 Instead of having level overall payments, this method of payment ensures that the principle decreases by a flat amount in each period We can determine the amount at which it decreases by taking the loan balance divided by the number of remaining payments Given this information, we are able to easily calculate the outstanding balance , and thus the Interest payment and subsequently the overall repayment amount each period \\[Principal\\ Repayment\\ each\\ period = \\frac{B_{0}}{n}\\] \\[Outstanding\\ Balance\\ each\\ period = \\frac{n - t + 1}{n}B_{0}\\] \\[Total\\ Repayment\\ each\\ period = i\\left( \\frac{n - t}{n} \\right)B_{0} + \\frac{B_{0}}{n}\\] It is important to note that the outstanding loan balance forms a decreasing arithmetic progression . We can make use of this to easily calculate total interest and hence total amount paid: \\(Total\\ Interest = i*\\left( B_{0} + B_{1} + B_{2} + \\ldots 0 \\right)\\) \\(Sum\\ of\\ Arithmetic\\ progression = \\frac{N}{2}(First\\ Term + Last\\ Term)\\) \\(Total\\ Amount\\ Paid = Total\\ Interest + Loan\\ Amount\\) A few key insights: Since the outstanding balance falls each period , the interest falls each period as well. This means that the repayments fall each period The repayment amount will be initially higher than the flat repayment amount, but will fall and become smaller than it Since the total repayment amount falls over time, this means that you will pay lesser compared to the typical method Non-Level Payments \u00b6 Payments that follow an arithmetic or geometric annuity Use their respective PV/FV formulas to find the loan balance at every time With the various loan balances, we should focus on using the recursive method : \\(Balance_{t + 1} = Balance_{t} - P_{t + 1}\\) \\(P_{t + 1} = \\ Balance_{t + 1} - \\ Balance_{t}\\) Questions may look the same, but the approach is completely different If the question has a balloon/drop payment involved , always calculate the remaining balance in the period before the balloon/drop . Then ROLL FORWARD the balance to obtain the Ballooned or Dropped Payment Missing or Additional Payments \u00b6 The general schedule above assumes that payments are made every period. However, if payments are missed or additional payments are made , then the Loan will take longer/shorter to amortize If payments are missed , then the Loan balance at the end of the usual loan period will still be positive \u2192 How much? If additional payments were made, then the Loan Balance will hit 0 before the usual loan period \u2192 How Much/When? In either case, we can easily visualize the problem by drawing out the timeline to better visualize the problem +-----------------------------------+-----------------------------------+ | [Missing | **[Additional | | Payments]{.underline} | Payments]{.underline}** | +===================================+===================================+ | The key idea is that the | The key idea is that the | | present/future value will | Present/future value MUST | | **NOT tally with the original | tally** with the original payment | | payment schedule. | schedule. | | | | | Using this idea, the difference | Using this idea, we can form two | | in the two values is the | equations and set them equal | | remaining amount of the loan. | to each other to solve for the | | | missing value we are interested | | A key shortcut understanding | in. | | is that if payments are missed, | | | the outstanding Loan Balance will | Given the additional payment | | be the sum of the accumulated | amount, we can solve for the new | | value of any missing payments | ending time. | | | | | Using the same concept, if we are | Given the new ending time | | given the Outstanding Balance at | instead, we can solve for the | | various points instead, we can | additional payment amount. | | roll forward the loan balance | | | assuming no payments were made. | | | We then use Retrospective | | | Method to calculate the | | | remaining Loan Balance instead. | | +-----------------------------------+-----------------------------------+ Payments that scale with Interest \u00b6 Some loan payments will scale based on the interest amount EG. Equal to 100% of interest payments. 150% of interest payments etc. There are some special insights to take note of: 100% of interest \u2192 No principal is repaid; loan balance stays the same 150% of interest \u2192 Balance falls by \\((150 - 100)\\%*Interest\\) every period \\(\\mathbf{90\\%}\\) of interest \u2192 Balance increases by \\((100 - 90)\\%*Interest\\) every period Other situations: \u00b6 Different Interest Rates for Interest payments and discount \u2192 Draw timeline Cost of loan \u2192 Reduces the principal amount to control the yield rate [Chapter 6: Bonds and Stocks]{.underline} \u00b6 What is a Bond? \u00b6 A bond is a special type of Loan: The party who issues the bond (Bond Issuer) borrows money from the party who purchases the bought the bond (Bond Investor) However, Bonds have a different payment structure : Every period \u2192 Coupon payments These coupon payments are based off interest on the Face Amount/Par Value of the bond, which is simply a notional amount for calculations Last period \u2192 Redemption Value If not specified otherwise, the Redemption value is equal to the Face Amount/Par Value. It is paid on-top of the coupon payment Thus, unlike a Loan who has Interest and Principal payments every period, Bonds repay the interest every period and only repays the principal at the end in a lump sum {width=\"6.268055555555556in\" height=\"1.6166666666666667in\"} Bond Valuation \u00b6 Let us use the following notations: \\(\\mathbf{P}\\) is the Price of the Bond \\(\\mathbf{r}\\) is the Coupon Rate (Quoted Annual Nominal Rate) \\(\\mathbf{F}\\) is the Face Amount \\(\\mathbf{C}\\) is the Call Value / Redemption Value \\(\\mathbf{i}\\) is the interest rate equivalent to Yield to Maturity (Quoted Annual Nominal Rate) \\[Bond\\ Price = PV(Coupons) + PV(Redemption\\ Value)\\] \\[\\mathbf{\\therefore P = Fr*}\\mathbf{a}_{\\mathbf{n\u23cbi}}\\mathbf{+ C*}\\mathbf{v}^{\\mathbf{n}}\\] Dirty Prices \u00b6 When valuing a Bond, we typically assume that the bond is being valued 6-months or 1-year intervals from the issue date. Prices valued this way are known as the Quoted Price However, if we purchase a Bond, it is likely that we will buying it in between valuation dates . We will thus have to adjust the price, known as the Dirty Price The idea is that we must compensate the previous bondholder for the additional time after the previous coupon payment by paying him a corresponding portion of the Coupon \\[\\mathbf{Dirty\\ Price} = Quoted\\ Price + Accrued\\ Interest\\] \\[\\mathbf{Accrued\\ Interest} = \\frac{Number\\ of\\ Days\\ since\\ Coupon}{Number\\ of\\ days\\ between\\ Coupons}*Coupon\\] \\[\\mathbf{Alternative\\ Dirty\\ Price} = Quoted\\ Price*(1 + i)^{\\frac{Number\\ of\\ Days\\ since\\ Coupon}{Number\\ of\\ days\\ between\\ Coupons}}\\] Bond Premium & Discount \u00b6 We recall the Annuity Immediate formula: \\[a_{n\u23cbi} = \\frac{1 - v^{n}}{i}\\] \\[v^{n} = 1 - i*a_{n\u23cbi}\\] Using this formula, we can rewrite our Bond Price formula: \\[P = Fr*a_{n\u23cbi} + C*v^{n}\\] \\[P = Fr*a_{n\u23cbi} + C*\\left( 1 - i*a_{n\u23cbi} \\right)\\] \\[P = Fr*a_{n\u23cbi} + C - Ci*a_{n\u23cbi}\\] \\[P = C + (Fr - Ci)*a_{n\u23cbi}\\] Where \\((Fr - Ci)*a_{n\u23cbi}\\) is the Premium/Discount of the Bond We consider the difference in the Price of the Bond & the Redemption Value \\(P - C,\\) +----------------------+----------------------+-----------------------+ | [Premium | **[Discount | **[Par | | Bond]{.underline} | Bond]{.underline}** | Bond]{.underline}** | +======================+======================+=======================+ | \\( \\(P - C > 0\\) \\) | \\( \\(P - C < 0\\) \\) | \\( \\(P - C = 0\\) \\) | | | | | | \\( ((Fr | ( ((Fr | ( \\((F | | - Ci)*a_{n\u23cbi} > 0\\) \\) | - Ci)*a_{n\u23cbi} < 0\\) \\) | r - Ci)*a_{n\u23cbi} = 0\\) \\) | | | | | | \\( \\((Fr - Ci) > 0\\) \\) | \\( \\((Fr - Ci) < 0\\) \\) | \\( \\((Fr - Ci) = 0\\) \\) | | | | | | \\( \\(Fr > Ci\\) \\) | \\( \\(Fr < Ci\\) \\) | \\( \\(Fr = Ci\\) \\) | | | | | | \\( \\(\\mathbf{r > i}\\) \\) | \\( \\(\\mathbf{r < i}\\) \\) | \\( \\(r = i\\) \\) | +----------------------+----------------------+-----------------------+ | Coupon payments | Coupon payments | Coupon payments | | received are | received are | received are equal | | **larger than the | **smaller than the | to the amount | | expected amount they | expected amount they | expected, thus the | | would receive from a | would receive from a | Bond has no premium | | similar par bond.** | similar par bond.** | or discount. | | | | | | This is additional | This is | \\( \\(\\mathbf{P | | **periodic income**, | **insufficient | rice = Face = Call}\\) \\) | | which is why the | income**, which is | | | bond costs more than | why the bond costs | | | par. | less than par. | | +----------------------+----------------------+-----------------------+ Bond Amortization \u00b6 There are some terminology differences between Loan and Bonds: Coupon Payment \u2192 Repayment Book Value \u2192 Outstanding Balance Write-up/down or Discount/Premium Amortization \u2192 Principal Repaid Also known as the Accumulation of Discoumt/Premium The goal is to create an Amortization Schedule that shows how each Coupon Payment is used to offset the Premium/Discount in each period, slowly bringing the Book Value to the Redemption Value (Usually equal to Par Value) at the end of the Loan Similarly, each coupon first pays off interest , then the remaining amount is used to offset the premium/discount on the bond, which decreases/increases the book value to Par All types of questions that apply to loans ALSO apply to Bond Amortization! +-----+------+---------------------+---------------+-----------------+ | * | ** | Interest Repaid | Wr | **Book Value | | Pe | Coup | | ite-up/down* | | | rio | on** | \\( \\(B_{t - 1}*i\\) \\) | | \\( (C + (Fr - C | | d** | | | ( \\(\\ | i)a_{n - t\u23cbi}\\) \\) | | | ( (F | ( \\(Fr + (Fr - | left| Fr\\math | | | | *r\\) \\) | Ci)*v^{n - t + 1}\\) \\) | bf{-}B_{t - 1 | | | | | | }*i \\right|\\) \\) | | | | | | | | | | | | $$ | | | | | | \\left| (Fr - | | | | | | Ci)v^{n - t + | | | | | | 1} \\right|$$ | | +=====+======+=====================+===============+=================+ | 0 | | | | \\( \\(C + (Fr | | | | | | - Ci)a_{n\u23cbi}\\) \\) | +-----+------+---------------------+---------------+-----------------+ | 1 | $$ | \\( (Fr | ( (\\left | ( \\(C + (Fr - C | | | Fr\\) \\) | + (Fr - Ci)*v^{n}\\) \\) | | (Fr - Ci)v^ | i)a_{n - 1\u23cbi}\\) \\) | | | | | {n} \\right|$$ | | +-----+------+---------------------+---------------+-----------------+ | $ | $$ | \\( \\(\\vdots\\) \\) | \\( \\(\\vdots\\) \\) | \\( \\(\\vdots\\) \\) | | \\(\\v | \\vdo | | | | | dot | ts\\) $ | | | | | s$$ | | | | | +-----+------+---------------------+---------------+-----------------+ | $$ | $$ | $ | \\( ( | ( \\(C + (Fr | | n\\) \\) | Fr\\) \\) | \\(Fr + (Fr - Ci)*v\\) $ | left| (Fr - C | - Ci)a_{0\u23cbi}$$ | | | | | i)v \\right|$$ | | +-----+------+---------------------+---------------+-----------------+ | T | | | | | | ota | | | | | | l | | | | | +-----+------+---------------------+---------------+-----------------+ [Special Type of Bonds]{.underline} \u00b6 Zero Coupon Bonds (Pure discount Bonds) \u00b6 Bonds that do not pay coupons, only the redemption value at the back Due to only having one cashflow at the end of \\(t\\) periods, the yield of the zero-coupon bond is essentially the Spot Rate for that length of time. Thus, given the price of any zero-coupon bond, we can calculate the spot rate However, we may not be given a zero-coupon bond directly . Like the interest rate swap shortcut, by buying and selling regular bonds in a portfolio , the net cashflows per period may be zero , essentially creating a zero-coupon bond. Floating Rate Bonds \u00b6 Bonds whose coupon rates vary with some external index. Their values rise when interest rises as well, thus they suffer less interest rate risk Callable Bonds \u00b6 Bonds where there is a choice to fully repay the bond on an earlier date instead Choosing to repay the Bond earlier is known as Calling the Bond. The redemption value at the time is known as the Call Price To ensure that the Investor is properly compensated for lost coupon payments if the Bond is called, the Call Price is higher the earlier the Bond is called Callable bonds are good for Issuers, as their flexible nature allows them to take advantage of falling interest rates (Issue new one at lower cost) and allows them to eliminate obligations at will with the main disadvantage being the higher Coupon Rates Put Provisions are good for Investors, but not issuer, as they may have to buy back the bond at an unattractive price , but they allow the issuer to use lower coupon rates Scenario Testing \u00b6 Callable Bond cashflows are uncertain as they are at the discretion of the Bond issuer Thus, the main approach to work with Callable bonds is to Scenario test -- Consider every possible outcome at form an analysis from that There are two main types: Given Price, determine Yield Use the Fin Calc to quickly reverse engineer the Yields for each scenario We focus on the Lowest Yield -- Yield to Worst This represents the worst-case scenario for the investor Given Yield, determine Price Use the Fin Calc to quickly reverse engineer to price for each scenario We focus on the LOWEST Price -- Maximum amount they should pay This represents the highest price an investor will have to pay to guarantee at least Yield to Worst , regardless which scenario ends up happening Price and Yield have an inverse relationship Thus, by choosing the lowest price, it means that even if another scenario were to occur, we would earn even higher than YTW It is also known as the maximum price , because if we increase price beyond this point, other scenarios may be fine, but this scenario will lead to a situation worse than YTW, that means that overall YTW is no longer guaranteed We can eliminate scenarios to test by using the shortcuts: YTW always occurs at the end points -- First & Last year that the call price changes (Including maturity) YTW is the First end date for Premium & Last for Discount Bonds for each group of call dates with the same call price Stock Valuation: Dividend Discount Model \u00b6 Like all other financial instruments, we have seen so far, we can value a stock by taking the Present Value of its future cashflows (Dividends) If we expect to sell the stock in the future, we treat that as a cashflow as well Since corporate lifespans are infinite, these dividend payments can occur infinitely, forming a perpetuity. Depending on the type of stock, Dividends may remain constant or continue to grow every period, forming a growing perpetuity \\[PV = \\frac{D}{i - k} = \\frac{D_{0}(1 + k)}{i - k}\\] The math behind the calculation is a simple application of the growing perpetuity concept There are a few things to look out for: Questions may give you the current dividends paid. Since the dividends grows, we can find the time 1 dividend by multiplying by the growth factor Given the dividend or stock value at any one point of time, we can simply multiply it by the growth factor to determine the value at another time If the growth rate changes midway, we consider a two-stage model , which simply means we now have an Annuity and Deferred Perpetuity [Chapter 7: Bond Yield & Term Structure of Interest Rates]{.underline} \u00b6 [Measures of Bond Yield]{.underline} \u00b6 Current Yield & Nominal Yield \u00b6 Simple measure of the potential return of the bond -- How much an investor would expect to make if they held the bond for a year However, it is not an accurate reflection of the potential gain of the bond: It does not take into the time value of money It does not consider capital gain or loss (Discount/Premiums) It is based on Quoted Prices while the actual investment is Dirty Prices \\[Current\\ Yield = \\frac{\\mathbf{Annual}\\ Coupon\\ Payments}{Quoted\\ Price}\\] \\[Nominal\\ Yield = \\frac{\\mathbf{Annual}\\ Coupon\\ Payments}{Face\\ value} = Coupon\\ rate\\] Yield to Maturity \u00b6 When we price bonds, we are supposed to use prevailing spot rates at the time (Obtained from yield curve) for more accurate representation of the time value of money However, in practice the term structure is not observable , but the transaction prices are . Thus, if we solve for the discount rate that sets the discounted cashflows to the transaction price, we obtain the Yield to Maturity It is the IRR for a bond -- which measures the average annual rate we would receive on the bond should we hold it to maturity. It is a sort of \"weighted average\" of the spot rates For this course, we assume the yield curve to be flat. Thus, the interest rate used to discount is constant, thus is also the YTM of the bond However, there are some issues with using YTM to evaluate Bonds: There is no closed form solution for YTM. It can only be calculated using numerical methods Although YTM is an \"average\" of the spot rates, it also changes with the coupon rate, which makes it an \"impure\" measure It also assumes that the bond will be held to maturity (Most are traded) as well as that the coupons are reinvested (Interest on Interest) at the same rate (Hard) Par Yield \u00b6 An alternative measure is par yield, which is the coupon rate that makes the bond trade at par under the current term structure ( \\(\\mathbf{Price = Face = Call}\\) ) However, it is an inaccurate measure -- It is more of a summary of the existing term structure rather than measures of a potential return of a bond Holding Period Yield \u00b6 Measure of the return we get by holding a bond for a period of time (Assuming it is redeemed at the end of the period) Based on the idea of interest -- It takes the difference between the ending value and starting value, accounting for any additional income (Coupons) as well \\[P_{0}(1 + i)^{t} = P_{t} + FV(Coupons)\\] Solve for \\(i\\) as the t-period holding period yield Spot Rates and the Term Structure \u00b6 The plot of the various spot rates and against time is known as the Yield Curve & the mathematical relationship between the two is known as the Term Structure of Interest Rates Empirically, the yield curve has been shown to take many shapes : Upward, Downward, Flat etc Upward Yield Curves are the most common. However, for this course, we assume a flat yield curve unless stated otherwise (Spot rates are constant throughout) If the yield curve is not flat, we do have a few options to determine the yield curve: Discretely Compounded Yield Curve \u00b6 It is called Discrete because it only calculates points of the yield curve at discrete points (Typically 6-month intervals) It uses the Prices of Bonds of various lengths to determine to reverse engineer the prevailing spot rates at the time The first period bond must be a zero-coupon bond . All other bonds can be regular annual/semi-annual coupon bonds This method of calculating is also known as a bootstrap method : Calculate the first spot rate using the zero-coupon bond If the second bond has a coupon, substitute in the spot rate calculated before to discount it and then solve for the second spot rate Repeatedly and recursively do this until all spot rates have been found If all Bonds were zero-coupon bonds, then this method would be a lot easier Continuously Compounded Yield Curve \u00b6 We now consider continuous interest and time points. One key assumption we make is that the force of interest is constant between two successive valuation dates It is based on the concept of spot rates and forward rates. By combining an initial spot rate and multiple forward rates, we are able to obtain spot rates for any period We then form a piecewise function based on the time period to form the yield curve Linking Spot Rates and Force of Interest: \\[e^{s_{t}*t} = e^{\\int_{0}^{t}\\delta_{t}}\\] \\[s_{t} = \\frac{1}{t}\\int_{0}^{t}\\delta_{t}\\] Recursive nature (Splitting integration) \\[e^{s_{t_{2}}*t_{2}} = e^{- \\int_{0}^{t_{2}}\\delta_{t_{2}}}\\] \\[e^{s_{t_{2}}*t_{2}} = e^{- \\int_{0}^{t_{1}}\\delta_{t_{1}} + \\int_{t_{1}}^{t_{2}}\\delta_{t_{2} - t_{1}}}\\] \\[s_{t_{2}} = \\frac{1}{t_{2}}\\left( \\int_{0}^{t_{1}}\\delta_{t_{1}} + \\int_{t_{1}}^{t_{2}}\\delta_{t_{2} - t_{1}} \\right)\\] In practice, we solve for the various \\(\\mathbf{\\delta}\\) using bond prices: The same key concept of splitting the force of interest intervals applies [Zero Coupon bonds]{.underline} [Regular Coupon Bonds]{.underline} \\( \\(P_{t} = C*e^{- \\int_{0}^{t}\\delta_{t}}\\) \\) \\( \\(P_{t} = (C + Fr)*e^{- \\int_{0}^{t}\\delta_{t}} + Fr*e^{- \\int_{0}^{t - 1}\\delta_{t - 1}}\\) \\) [Chapter 8: Bond Management]{.underline} \u00b6 Macaulay Duration (MacD) \u00b6 It is the Weighted Average of the time taken in YEARS for cashflows to occur, weighted based on the relative size of the PV of future cashflows. It calculates average amount of time needed to get breakeven on your investment Based on this, Cashflows that are Larger and Occur Earlier (Don't need to discount as much) will influence the calculation more . We can use this intuition to check our calculations to see if they make sense Deriving Macaulay Duration \u00b6 \\[\\therefore MacD = \\frac{1*PV + 2*PV_{2} + \\ldots + n*PV_{n}}{PV_{1} + PV_{2} + \\ldots PV_{N}} = \\frac{\\sum_{}^{}{t*v^{t}*CF_{t}}}{{\\sum_{}^{}{v^{t}*CF}}_{t}} = - \\frac{P^{'}(\\delta)}{P(\\delta)}\\] If all cashflows are level \\(\\left( CF_{1} = \\ldots CF_{n} \\right)\\) , they can be factorized out from both the numerator and denominator, effectively cancelling out Building on this, consider two sets of Cashflows: Set A: 1, 2, 3, 4... Set B: 100, 200, 300, 400... We can factorise out 100 from Set B, which will cancel out and result in the same cashflows as Set A. Thus, MacD is dependent not on the size but the pattern of cashflows Other Time Frequencies \u00b6 The typical duration formulas use full years (1,2,3...) However, some questions (especially semi-annual bonds) have cashflows semi-annually We can treat the semi-annual timings as a full year, then convert it back to half years at the end! Alternatively, we can treat them as \\(\\mathbf{0.5}\\) right from the get-go Macaulay Duration Shortcuts \u00b6 Zero-Coupon Bond (Single ( \\(n\\) \\) Cashflow) Level Annuity (Loans) \\( \\(\\frac{(Ia)_{n\u23cbi}}{a_{n\u23cbi}}\\) \\) Standard Bond* \\( \\(\\frac{{Fr*(Ia)}_{n\u23cbi} + nCv^{n}}{Fr*a_{n\u23cbi} + Cv^{n}}\\) \\) *Note that a deferred Bond cannot use this formula directly; an adjustment will have to be made +-----------------------------------+-----------------------------------+ | [Par Bond]{.underline} | **[Common Stock (Increasing | | \\(\\left( \\m | Perpetuity)]{.underline}** | | athbf{P = F = C,\\ i = r} \\right)\\) | | +===================================+===================================+ | \\( (MacD = \\frac{{Fr*(Ia)}_{n\u23cbi} | ( \\(P = (i - k)^{- 1}\\) \\) | | + nCv^{n}}{Fr*a_{n\u23cbi} + Cv^{n}}\\) \\) | | | | \\( \\(P' = - (1 - k)^{- 2}\\) \\) | | \\( \\(MacD = \\frac{{i*(Ia)}_{n\u23cb | | | i} + nv^{n}}{i*a_{n\u23cbi} + v^{n}}\\) \\) | \\( (ModD = - \\frac{P^{ | | | '}}{P} = - \\frac{(i - k)^{- 2}}{- | | ( \\(Mac | (1 - k)^{- 1}} = (i - k)^{- 1}\\) \\) | | D = \\frac{{\\ddot{a}}_{n\u23cbi} - nv^{ | | | n} + nv^{n}}{1 - v^{n} + v^{n}}\\) \\) | \\( \\(MacD = Mod | | | D*(1 + i) = \\frac{1 + i}{i - k}\\) \\) | | \\( \\(MacD = {\\ddot{a}}_{n\u23cbi}\\) \\) | | +-----------------------------------+-----------------------------------+ Modified Duration \u00b6 As its name suggests, ModD is simply a modification of MacD <!-- --> - The longer we must wait to get back our money, the larger the window for those cashflows to be affected by interest rates hence higher sensitivity to them <!-- --> - Thus, we can modify MacD to directly approximate the Sensitivity of the change in price of a Bond to changes in Interest Rates. Thus, ModD measures the percentage decrease in the value of the security per unit increase in interest rates (Price Risk) Deriving Modified Duration \u00b6 \\[Price\\ of\\ a\\ Security,\\ P(i) = \\sum_{}^{}{v^{t}*CF_{t}} = \\sum_{}^{}{(1 + i)^{- t}*CF_{t}}\\] \\[Changes\\ in\\ Price,\\ P^{'}(i) = \\sum_{}^{}{- t*(1 + i)^{- t - 1}*CF_{t}} = \\sum_{}^{}{- t*v^{t + 1}*CF_{t}}\\] \\[\\therefore ModD = - \\frac{P^{'}(i)}{P(i)} = - \\frac{\\sum_{}^{}{- t*v^{t + 1}*CF_{t}}}{\\sum_{}^{}{v^{t}*CF_{t}}} = \\frac{\\sum_{}^{}{t*v^{t + 1}*CF_{t}}}{\\sum_{}^{}{v^{t}*CF_{t}}} = v*\\frac{\\sum_{}^{}{t*v^{t}*CF_{t}}}{{\\sum_{}^{}{v^{t}*CF}}_{t}} = \\frac{MacD}{1 + i}\\] If interest compounds continuously, we can show a similar result for MacD as well Note that if no specification was made, Duration refers to Macaulay Duration Portfolio Duration \u00b6 Consider a portfolio of Bonds. We can aggregate the cashflows for the entire portfolio and then calculate the Duration using the same method However, this can be time consuming. If we already know the individual Durations for each component, we can quickly calculate the Duration of the entire portfolio by using their relative weights (Similar to Modern Portfolio Theory) Important: The weights MUST be time-value adjusted \\[MacD_{Portfolio} = w_{X}*MacD_{X} + w_{Y}*MacD_{Y} + w_{Z}*MacD_{Z} + \\ldots\\] This same portfolio method applies for Convexity as well Different Comparison date \u00b6 The base definition of Duration assumes that it is calculated at time 0 We can generalize the formula and calculate them on any date by understanding that t represents the time till the cashflow occurs from the comparison date However, we can link the durations across time which may help us save time <!-- --> - If we were to move the comparison one period later, there are two different scenarios we must consider: - **Calculate Duration BEFORE the payment** $\\left( \\mathbf{Mac}\\mathbf{D}_{\\mathbf{t}}^{\\mathbf{'}} \\right)$ - Under this scenario, the **cashflows remain the same** as compared to the original. The only thing that changed was time - Thus, the average time taken for cashflows just moved forward a period: - $MacD_{t}^{'} = MacD_{t - 1} - 1$ - **Calculate Duration AFTER the payment** $\\left( \\mathbf{Mac}\\mathbf{D}_{\\mathbf{t}} \\right)$ - Since the payment has occurred, both the **cashflows and timing are different** as compared to the original - Thus, we cannot form a relationship & must be manually recalculated - For any given period**,** $\\mathbf{Mac}\\mathbf{D}_{\\mathbf{t}}\\mathbf{> Mac}\\mathbf{D}_{\\mathbf{t}}^{\\mathbf{'}}$ - This is because $MacD_{t}$ removes the earliest cashflow, effectively removing the smallest value from the calculation - Since MacD is an average, **removing the smallest value will force the average to increase in value as compared to** $\\mathbf{Mac}\\mathbf{D}_{\\mathbf{t}}^{\\mathbf{'}}$ {width=\"6.268055555555556in\" height=\"4.204861111111111in\"} Using Durations for Estimations \u00b6 Given the only current price of a Bond, we can estimate the new price of the bond for a specified change in interest rates using Durations We consider a graph of the price of the Bond. Without knowing the equation for Price, we can approximate the new price using the tangent at the current price : {width=\"3.4534339457567804in\" height=\"2.4800240594925635in\"} Firstly, we consider the first principles interpretation of Tangent/Slope , \\[P^{'}\\left( i_{0} \\right) = \\frac{P^{*}\\left( i_{n} \\right) - P\\left( i_{0} \\right)}{i_{n} - i_{0}}\\] Next, we use the ModD formula , \\[ModD = - \\frac{P^{'}\\left( i_{0} \\right)}{P\\left( i_{0} \\right)}\\] \\[P^{'}\\left( i_{0} \\right) = - P\\left( i_{0} \\right)*ModD\\] Lastly, we combine the two and rearrange, \\[\\frac{P^{*}\\left( i_{n} \\right) - P\\left( i_{0} \\right)}{i_{n} - i_{0}} = - P\\left( i_{0} \\right)*ModD\\] \\(P^{*}\\left( i_{n} \\right) - P\\left( i_{0} \\right) = \\left( - P\\left( i_{0} \\right)*ModD \\right)*\\left( i_{n} - i_{0} \\right)\\) \\[\\mathbf{P}^{\\mathbf{*}}\\left( \\mathbf{i}_{\\mathbf{n}} \\right)\\mathbf{= \\ P}\\left( \\mathbf{i}_{\\mathbf{0}} \\right)\\mathbf{-}\\mathbf{P}\\left( \\mathbf{i}_{\\mathbf{0}} \\right)\\left( \\mathbf{i}_{\\mathbf{n}}\\mathbf{-}\\mathbf{i}_{\\mathbf{0}} \\right)\\left( \\mathbf{ModD} \\right)\\] First Order Modified Approximation First Order Macaulay Approximation \\( \\(P^{*}\\left( i_{n} \\right) = \\ P\\left( i_{0} \\right) - P\\left( i_{0} \\right)\\left( i_{n} - i_{0} \\right)(ModD)\\) \\) \\( \\(P\\left( i_{n} \\right) \\approx P\\left( i_{0} \\right)\\left( \\frac{1 + i_{0}}{1 + i_{n}} \\right)^{MacD}\\) \\) Proof for Macaulay variant is algebraically challenging and is not shown Generally, Macaulay Approximation is slightly more accurate than the Modified one An alternative (Intuitive) way to think about Approximation \u00b6 Premise \u2192 Macaulay and Modified Duration represents time Looking at the formulas for the approximation, we notice that we have seen them before: Macaulay \u2192 \\(Interest^{Time}\\) \u2192 Compound Interest Modified \u2192 \\(Interest*Time\\) \u2192 Simple Interest If interest rates were constant, we can simply use the original accumulation function. However, since interest rates changed, we need to make a correction to the original Macaulay approximation is a Compound Interest Correction, while Modified Approximation is a Simple Interest Correction Following the nature of each type of interest, Macaulay (Compound Interest) uses Multiplication/Division while Modified (Simple Interest) used Addition/Subtraction Convexity \u00b6 Notice that we are using a Line to approximate a Curve . Thus, the first order approximations are good for small changes in price where the curve is relatively flat over that length Since the curve is Convex, the line will always under-approximate the price of the bond. This under-approximation becomes greater the larger the change in interest rates as the curve begins to bend more We can thus use the Curvature to approximate this inherent error (Convexity) and account for it to obtain a more accurate approximation Derivation \u00b6 +-----------------------------------+-----------------------------------+ | [Modified | **[Macaulay | | Convexity]{.underline} | Convexity]{.underline}** | +===================================+===================================+ | \\( \\(ModC = \\frac{P^{''}(i)}{P(i)}\\) \\) | \\( \\(MacC = \\ | | | frac{P^{''}(\\delta)}{P(\\delta)}\\) \\) | | \\( \\(P(i) = \\ | | | sum_{}^{}{(1 + i)^{- t}*CF_{t}}\\) \\) | \\( \\(P(i) = \\s | | | um_{}^{}{e^{- \\delta t}*CF_{t}}\\) \\) | | \\( \\(P^{'}(i) = \\sum_{}^{ | | | }{- t*(1 + i)^{- t - 1}*CF_{t}}\\) \\) | \\( \\(P^{'}(i) = \\sum_{ | | | }^{}{- t*e^{- \\delta t}*CF_{t}}\\) \\) | | $ | | | \\(P^{''}(i) = \\sum_{}^{}{- t*( - t | ( \\(P^{''}(i) = \\sum_{}^{}{ | | - 1)*(1 + i)^{- t - 2}*CF_{t}}\\) \\) | - t* - t*e^{- \\delta t}*CF_{t}}\\) $ | | | | | \\( (P^{''}(i) = \\sum_{}^{}{t*(t + | ( \\(P^{''}(i) = \\sum_{}^ | | 1)*(1 + i)^{- (t + 2)}*CF_{t}}\\) \\) | {}{t^{2}*e^{- \\delta t}*CF_{t}}\\) \\) | | | | | $$ | \\( \\(\\therefore\\mathbf{MacC} = | | \\therefore\\mathbf{ModC} = \\frac{\\ | \\frac{\\sum_{}^{}{t^{2}*v^{t}*CF_ | | sum_{}^{}{t*(t + 1)*v^{t + 2}*CF_ | {t}}}{\\sum_{}^{}{v^{t}*CF_{t}}}\\) \\) | | {t}}}{\\sum_{} {}{v CF_{t}}}$$ | | | | **MacC for a single cashflow is* | | $ | \\(\\mathbf{n}^{\\mathbf{2}}\\) | | \\(\\mathbf{\\equiv ModC} = \\frac{\\su | | | m_{}^{}{t*(t + 1)*v^{t}*CF_{t}}}{ | | | \\sum_{}^{}{v^{t}*CF_{t}}}*v^{2}\\) $ | | +-----------------------------------+-----------------------------------+ | [Portfolio Convexity (Same as | | | before)]{.underline} | | | | | | \\( \\(MacC_{Portfo | | | lio} = w_{X}*MacC_{X} + w_{Y}*Mac | | | C_{Y} + w_{Z}*MacC_{Z} + \\ldots\\) \\) | | | | | | Note: If not specified , | | | Convexity always refers to | | | Modified Convexity | | +-----------------------------------+-----------------------------------+ [Additional Formulas]{.underline} \u00b6 Linking Duration and Convexity: \\[ModC = v^{2}*(MacC + MacD)\\] Second Order Modified Approximation: \\[P\\left( i_{n} \\right) \\approx P\\left( i_{0} \\right) - P\\left( i_{0} \\right)*\\left( i_{n} - i_{0} \\right)(ModD) + P\\left( i_{0} \\right)*\\frac{\\left( i_{n} - i_{0} \\right)^{2}}{2}(ModC)\\] Second Order Macaulay Approximation: \\[P\\left( i_{n} \\right) \\approx P\\left( i_{0} \\right)\\left( \\frac{1 + i_{0}}{1 + i_{n}} \\right)^{MacD}*\\left\\lbrack 1 + \\left( \\frac{i - i_{0}}{1 + i_{0}} \\right)^{2}\\left( \\frac{MacC - MacD^{2}}{2} \\right) \\right\\rbrack\\] Immunization \u00b6 Companies often make payments (Liability Cashflows) and receive payments from their investments (Asset Cashflows) The goal of any company is to ensure that they have enough Asset Cashflows to cover their Liability Cashflows . In other words, the Present Value of Asset Cashflows should be more than Liability Cashflows. We call the excess a Surplus. \\(Surplus = PV\\left( CF_{Assets} \\right) - PV\\left( CF_{Liabilities} \\right)\\) However, this Surplus is prone to interest rate risk : When interest rates fall or rise, the present value of assets might fall, present value of liabilities might rise or both This causes the surplus to become negative which may put the company in a state of financial distress The goal of any company is to thus find the combination of securities such that no matter how interest rates change, the surplus will always increase or stay the same . Essentially, they are immunizing their surplus from the effects of interest rates Redington Immunization (Duration Matching) \u00b6 Three assumptions needed : Interest rates for all maturities are identical (Flat Yield Curve) A change in interest rates affects all maturities (Parallel shift) A change in interest rates do not affect cashflows (Fixed cashflows) Under these three assumptions, a portfolio is immunized from small changes in interest rates if the following three conditions are met: \\(\\mathbf{PV}\\left( \\mathbf{Assets} \\right)\\mathbf{= PV}\\left( \\mathbf{Liabilities} \\right)\\mathbf{\\Longleftrightarrow}\\mathbf{P}_{\\mathbf{A}}\\mathbf{=}\\mathbf{P}_{\\mathbf{L}}\\) They are the same because PV is equivalent to price \\(\\mathbf{Duratio}\\mathbf{n}_{\\mathbf{Asset}}\\mathbf{= Duratio}\\mathbf{n}_{\\mathbf{Liability}}\\mathbf{\\Longleftrightarrow}\\mathbf{P}_{\\mathbf{A}}^{\\mathbf{'}}\\mathbf{=}\\mathbf{P}_{\\mathbf{L}}^{\\mathbf{'}}\\) Both Macaulay and Modified duration can be used here Since prices are the same, both their denominators cancel out , thus we only need to consider the first derivative \\(\\mathbf{Convexit}\\mathbf{y}_{\\mathbf{Asset}}\\mathbf{> Convexit}\\mathbf{y}_{\\mathbf{Liability}}\\mathbf{\\Longleftrightarrow}\\mathbf{P}_{\\mathbf{A}}^{\\mathbf{''}}\\mathbf{>}\\mathbf{P}_{\\mathbf{L}}^{\\mathbf{''}}\\) Similarly, both Macaulay and Modified convexity applies. Since prices are the same, we only consider their second derivative We can view this relationship graphically: Since prices are the same, both Asset and Liability price curves intersect at the current interest rate Since duration is the same, they have the same tangent at the intersection point Since Asset Convexity is greater, it curves higher than Liability Given the same start point but asset curves more, the Asset will always increase more and decrease less than a similar change in Liabilities, ensuring surplus is protected Thus, we can use the first two conditions to solve for the optimal portfolio, and then use the third condition to check if the solution is valid, if necessary Another way to express the conditions: \u00b6 We can also express the conditions in terms of Net Present Value (NPV) First condition states that the present values must match, \\(\\mathbf{NPV = 0}\\) Second condition states that the first derivatives must match, \\(\\mathbf{NP}\\mathbf{V}^{\\mathbf{'}}\\mathbf{= 0}\\) Third condition states that the second derivative of inflows must be higher, \\(\\mathbf{NPV'' > 0}\\) Full Immunization \u00b6 Similarly, it has the same assumptions as Redington : Flat Yield Curve, Parallel Shifts & Fixed Cashflows It also has similar conditions as Redington, with only the last being different: \\(PV(Assets) = PV(Liabilities) \\Longleftrightarrow P_{A} = P_{L}\\) \\(Duration_{Asset} = Duration_{Liability} \\Longleftrightarrow P_{A}^{'} = P_{L}^{'}\\) There must be one Asset Cashflow BEFORE and AFTER a Liability Cashflow There can be more than one Liability, but each must be sandwiched between two Asset cashflows They do not have to be unique; there can be infinitely many Liabilities between two Assets Cashflows also include small ones such as Bond Coupons The intuition behind it is more qualitative: Another way to think about Immunization is for the firm to is for cashflows to be worth the amount they expected, regardless of market conditions Imagine there was a rise in interest after the first asset cashflow. The value of liabilities would rise, but so would the Asset Cashflow that comes after that, mitigating the risk of interest rate changes. Vice-versa applies as well. Comparing both types of Immunization: Full immunization covers ALL changes in interest rates, while Redington covers only small interest rate changes Full immunization is a stricter version of Redington Immunization -- Full is a subset of Redington immunization. If Full Immunization conditions are met, it can also be used for a Redington problem Immunization Shortcut \u00b6 Find the duration of EACH asset and the overall duration of the liabilities . Using the portfolio duration method , equate both liabilities together: \\(w_{1}*MacD_{Asset\\ 1} + w_{2}*MacD_{Asset\\ 2} + \\ldots = MacD_{Liabilities}\\) Solve for the weights of the portfolio, where \\(w_{2} = 1 - w_{1} + \\ldots\\) Find the overall PV of the liabilities . Multiply the calculated weights by the PV of the liabilities, to obtain the PV of each of the assets used. Multiply by interest accordingly to find the value of the assets when their cashflows occur This process can be done as all liabilities at once , or for individual liabilities If we are given the Asset Cashflows, we can find its PV and calculate the weight using by comparing it to the PV of the liabilities (Reverse order) Alternative: Exact Cashflow Matching \u00b6 Redington and Full Immunization require very hard assumptions that are unrealistic in the real world This provides a simpler, more intuitive method by matching the timing and amount of Asset and Liability Cashflows This method STILL immunizes against ALL changes in interest rates Assume we have three bonds: \\(Bond_{1}\\) matures in 1 year, with Face \\(F_{1}\\) , Coupon Rate \\(R_{1}\\) and \\(YTM_{1}\\) \\(Bond_{n}\\) matures in n years, with Face \\(F_{n}\\) , Coupon Rate \\(R_{n}\\) and \\(YTM_{n}\\) \\(Bond_{N}\\) matures in \\(N\\) years, with Face \\(F_{N}\\) , Coupon Rate \\(R_{N}\\) and \\(YTM_{N}\\) **Period 1** **Period n** **Period N** \\( \\(Bond_{1}\\) \\) \\( \\(F_{1}R_{1}\\) \\) - - \\( \\(Bond_{n}\\) \\) \\( \\(F_{n}R_{n}\\) \\) \\( \\(F_{n} + F_{n}R_{n}\\) \\) - \\( \\(Bond_{N}\\) \\) \\( \\(F_{N}R_{N}\\) \\) \\( \\(F_{N}R_{N}\\) \\) \\( \\(F_{N} + F_{N}R_{N}\\) \\) $${Liability}_{1}$$ $${Liability}_{n}$$ $${Liability}_{N}$$ Methodology: \u00b6 We use the Longest Bond to match the Longest Liability : \\[F_{N} + F_{N}R_{N} = Liability_{N}\\ \\] If the liability is larger than the Face and Redemption of a single bond, then we need to buy multiple bonds . Let \\(x\\) be the number of units to buy , where \\(x\\) can be a non-integer: \\[x_{N}\\left( F_{N} + F_{N}R_{N} \\right) = Liability_{N}\\ \\] Fill in the coupons for previous period, then repeat for the next longest liability: \\[x_{n}\\left( F_{n} + F_{n}R_{n} \\right) + x_{N}\\left( F_{N}R_{N} \\right) = Liability_{n}\\] \\[\\vdots\\] The total cost of exact matching is the number of units multiplied by the price of each bond: \\[Total\\ Cost = x_{1}*P_{1} + x_{n}*P_{n} + x_{N}*P_{N}\\] Note that each individual price is calculated using their respective \\(YTM\\) If the coupons are all zero-coupon bonds, then the total cost of matching is simply the present value of the liabilities (no need to do the above steps) Generally, there are two kinds of problems given: Face Value of the bonds are given . In this case, you NEED to consider the UNITS of bonds of to buy since the Par Value is predetermined Face Value of the Bonds are NOT given . In this case, you can directly solve for the \"par\" value of the bond needed Both of these are the SAME. Both are calculating the true par value invested. Method 2 directly calculates this while method 1 has to consider the units of the bond because there is a fixed par value [Chapter 9: Determinants of Interest Rates]{.underline} \u00b6 How are Interest rates determined? \u00b6 Interest rates can be viewed as the cost of borrowing capital . Like all things in economics, we can determine the equilibrium by considering the supply and demand of capital There are two perspectives we must consider: People with money (Lenders) They can either choose to spend it now, or lend the money out & receive interest payments The higher the interest, the more likely these people are to lend the money. Thus, the money supply curve is upward sloping People without money (Borrowers) They can either choose to refrain from spending, or borrow the money & pay interest The lower the interest, the more likely these people are to borrow money . Thus, the money demand curve is downward sloping By plotting out these two curves, the point at where they intersect is the equilibrium interest rate for the economy This is an over-simplified explanation, as there are other factors that affect it as well: Demand for capital \u2192 Higher when economy is expanding or when new technology is discovered; people want more capital in these cases to take advantage of opportunities Supply of capital \u2192 Affected by Time Preference, Risk Appetite, Inflation & individual characteristics of securities {width=\"6.208333333333333in\" height=\"4.666666666666667in\"} [Central Banks and Interest Rates]{.underline} \u00b6 What is the central bank? \u00b6 It functions as a bank to other commercial banks in the country The central bank requires every commercial bank to deposit and maintain a specific amount with it, known as the bank reserve requirement This reserve is to ensure that these banks have enough capital so that they do not fall into financial distress The amount required varies from bank to bank as well as other situational factors What if banks fail to meet this requirement? \u00b6 Banks may fail to hit the requirements for a variety of reasons, the most common being that they were too aggressive in lending money which results in more outflows than inflows during that period <!-- --> - They have two options to meet the shortfall in capital: - Borrow from the **Central Bank** at the **discount rate** - Borrow from other **Commercial Banks** at the **federal funds rate** The central bank will always say yes to lending, which is why it is known as a lender of last resort . However, borrowing money directly from them reflects poorly on the bank , as it indicates difficulty in borrowing from other institutions due to credibility issues This may result in more scrutiny over the bank , which is why they prefer to borrow from another commercial bank instead Impact on Interest rates \u00b6 The part of the central bank that oversees this process is the Federal Open Market Committee (FOMC) They set the target interest rates that they would like for the federal funds rate , and then they will buy and sell T-bills in the secondary market to influence the federal funds rate This affects the money demand and supply for T-bills , which changes the discount rate, which makes borrowing from T-bills vs other banks more or less desirable It is in the banks best interest to lend money out , thus they will adjust the fed funds rate to ensure that they are more competitive (LOWER) than the discount rate to ensure that other banks will borrow from them rather than the CB If the discount rate increases , federal fund rate increases , it becomes more expensive for Banks to borrow money hence more costly for them to fall below the reserve requirement They will become less aggressive and not lend out as much money , reducing money supply and hence increasing interest rates Treasury Bills \u00b6 +-----------------------------------+-----------------------------------+ | [US Treasury | **[Canadian Treasury | | Bills]{.underline} | Bills]{.underline}** | +===================================+===================================+ | \\( (Quoted rat | ( \\(Quoted\\ Rat | | e = \\ \\frac{360}{N}*\\frac{I}{C}\\) \\) | e = \\ \\frac{365}{N}*\\frac{I}{P}\\) \\) | +-----------------------------------+-----------------------------------+ | Based on the state of the US | Based on the level of economic | | economy | activity in Canada, including | | | Supply/Demand of CND | +-----------------------------------+-----------------------------------+ | They use different | | | approximations for the number of | | | days in the year -- US assumes | | | each month has 30 days | | | \\((30*12 = 360)\\) while Canada uses | | | the conventional 365 days. Also, | | | the US uses the redemption | | | value while Canada uses the | | | price. | | | | | | US bonds (Liquid) are traded | | | more than Canadian Bonds | | | (Ill-liquid). This lower | | | liquidity risk means that US | | | bonds yield a lower rate . All | | | else equal, this also means US | | | has a lower price. | | +-----------------------------------+-----------------------------------+ Components of Interest Rate \u00b6 We can decompose interest rates into several smaller components: Real-Risk Free Rate \u2192 Compensation for deferred consumption Maturity Risk Premium \u2192 Compensation for the risk of longer-term investments Default Risk Premium \u2192 Compensation for the risk of money not being paid Inflation Risk Premium \u2192 Compensation for loss of purchasing power due Liquidity Risk Premium \u2192 Compensation for added cost of converting to cash This is consistent with what we learnt in FNCE101 -- The interest rates we observe is the combination of the Risk-Free Rate + Various Risk Premiums (Reward for risk) +-----------------------------------+-----------------------------------+ | [Discrete Rates]{.underline} | [Continuous | | | Rates]{.underline} | +===================================+===================================+ | \\( ((1 + R)^{t} = \\left\\lbrack | ( \\(e^{Rt} = e^{r_{1} | | \\left( 1 + r_{1} \\right)\\left( 1 | t}*e^{r_{2}t}*\\ldots e^{r_{n}t}\\) \\) | | + r_{2} \\right)\\ldots\\left( 1 + | | | r_{n} \\right) \\right\\rbrack^{t}\\) \\) | \\( \\(R | | | = r_{1} + r_{2} + \\ldots r_{n}\\) \\) | | \\( \\(R = \\left( 1 + r_{1} \\ | | | right)\\left( 1 + r_{2} \\right)\\ld | | | ots\\left( 1 + r_{n} \\right) - 1\\) \\) | | +-----------------------------------+-----------------------------------+ Both ways of determining the various components are correct. However, for simplicity , we prefer to use the Continuous Rates Some parts of this section are purely theoretical while others involve some calculation to actually determine the value of the component \\(r\\) Maturity Risk Premium \u00b6 All else equal, longer-term investments will have a higher interest rate There are several explanations: Market Segmentation Theory \u2192 Different investors have clearly defined and different investment horizons . This results in different supply & demand factors for different time horizons, resulting in different rates Preferred Habitat Theory \u2192 Builds off the above theory but says that the segments are not written in stone . These segments are just the preferred segment for investors. Given better rates in another segment, these investors will move Liquidity Preference Theory \u2192 Lenders typically prefer to lend for a shorter time ; thus, they require higher rates to lend for longer times Expectation Theory \u2192 The longer the period of time, the more risk there is for interest rates to vary, which is why longer investments require a higher rate It also means that the current forward rates are an unbiased estimator of the future spot rates \\(\\left( EG.\\ f_{1}\\ now\\ is\\ s_{1}\\ one\\ year\\ from\\ now \\right)\\) Default Risk Premium \u00b6 All else equal, investments with a higher chance of not paying have higher interest rates There are two scenarios when it comes to defaulting: Defaults with no recovery (Pay some smaller amount instead) Defaults with recovery (Pay nothing) Consider the lenders perspective . In either case, they want to get the same amount , no matter what the scenario. Thus, they charge an appropriate default risk premium: \\(Total\\ \\#*Loan\\ Received = \\#\\ Non - Default*X + \\#\\ Default*Recovery\\) \\(X\\) represents the amount that needs to be RECEIVED \\(\\left( \\mathbf{Loan*Interest} \\right)\\) \\(X = Loan\\ amount*e^{rt}\\) Solve for \\(\\mathbf{r}\\) as our interest needed \u2192 Difference between this and the original interest rate is the default risk premium Inflation Risk Premium \u00b6 Inflation is the rise in the prices of goods and services over time Inflation is approximated via tracking one of the following indices: Consumer Price Index (CPI) \u2192 Basket of typical consumer items Producer Price Index (PPI) \u2192 Basket of typical producer items Inflation decreasing the purchasing power of money. Thus, Lenders of money want to be compensated via higher interest payments that scale with inflation. However, Borrowers are unwilling to pay this extra amount We see this pan out in two different scenarios: Lenders issue Loan with Inflation Protection for themselves The amount of interest that the borrowers pay scales with inflation. To incentivise borrowers, they lower they base interest by the cost of protection \u00a9 \\(R = r - c + i_{Actual}\\) In other words, Lenders are willing to receive a lower amount of base interest to pass on the risk of inflation to the borrower . In finance terms, since they experience less risk, they require a lower reward \\((r - c)\\) Lenders issue Loan without Inflation Protection for themselves Not all borrowers are willing to bear the inflation risk themselves. They would rather pay an extra but certain amount to the lender to compensate them for bearing the inflation risk instead \\(R = r + i_{Expected} + i_{Unexpected}\\) In other words, Lenders are willing to pay a higher base interest to retain the risk of inflation with the lender . In finance terms, since the lender experiences more risk, they have a higher reward \\(\\left( r + i_{e} + i_{ue} \\right)\\) The problem is that the there is no guarantee that the amount loaded is equivalent to the actual inflation rate experienced . It could be better, or it could be worse In finance/economics terms, we can view them in two different ways: With inflation protection \u2192 Real Interest Rate \\(\\left( \\mathbf{r - c} \\right)\\) Without inflation protection \u2192 Nominal Interest Rate \\(\\left( \\mathbf{r +}\\mathbf{i}_{\\mathbf{e}}\\mathbf{+}\\mathbf{i}_{\\mathbf{ue}} \\right)\\) In a perfect world , where inflation can be predicted perfectly \\(\\left( i_{e} + i_{ue} = i_{a} \\right)\\) , then the following equation holds true (But the idea is there): {width=\"3.7416666666666667in\" height=\"2.057791994750656in\"} Other Points \u00b6 Bonds in the US are usually issued at nominal rates rather than real rates US tax is linked to the inflation rates Liquidity Risk Premium \u00b6 All else equal, an investment that is less-liquid (Harder to convert to cash) have higher interest rates Liquidity is defined as the speed and ease at which an asset can be converted to cash, without significant loss in value The risk posed by an ill-liquid investment is that if it has to be quickly converted to cash, it will lose a significant amount of its value Thus, in order to compensate for this loss of value, ill-liquid investments demand higher interest rates [Financial Calculator Tips]{.underline} \u00b6 Overview of using the BA II Plus \u00b6 It is a calculator with pre-programmed financial formulas . While this saves you the time of remembering and writing out formulas, you must still understand how the formulas work to know which to use and what to input Its main usage should be for these financial calculations only. While it can perform more general calculations, it is recommended to stick to a scientific calculator for those Basic settings to toggle beforehand \u00b6 By default, the calculator only shows values up to 2 decimal places. Due to rounding errors in intermediate steps, this is not ideal for a math paper. We can change the number of decimal places displayed: [2 nd .]{.underline} \u2192 DEC \u2192 [9]{.underline} This sets the calculator to FLOAT , which means that it displays up to 8 decimal places if available, otherwise up to the number of non-zero places The calculator only allows for one step of calculations at a time and evaluates them immediately. It can be annoying to calculate a value using an equation this way, as one would have to use brackets (which can't be easily seen) to input successive steps. We can change the way the calculator evaluates a series of instructions: [2 nd .]{.underline} \u2192 CHN \u2192 [2 nd ENTER]{.underline} This sets the calculator to Algebraic Operating System which follows the BODMAS format of evaluating equations Basic calculator features \u00b6 Financial Functions -- Since the calculator already has formulas built in, a large part of the process is about assigning values to these variables. There are two different ways: Value to Variable \u2192 Key in the [VALUES]{.underline} then press the [VARIABLE]{.underline} [CPT]{.underline} \u2192 [TARGET VARIABLE]{.underline} Variable to Value \u2192 Press the [VARIABLE]{.underline} then key in the [VALUES]{.underline} & press [ENTER]{.underline} [TARGET VARIABLE]{.underline} \u2192 [CPT]{.underline} Always ensure that the [TARGET VARIABLE]{.underline} is set to [0]{.underline} In all cases, we can confirm that the value has been properly assigned to the variable when we see a full stop after the value Once we are done with the function, we should use [2 ND CE|C]{.underline} or [2 nd FV]{.underline} to reset all other variables Storing Variables -- It is not practical or memorize intermediate values. We can store them in one of the 9 memory slots and recall them at any time: [VALUE]{.underline} \u2192 [STO]{.underline} \u2192 [1-9]{.underline} (Stores a value in the chosen memory slot) [RCL]{.underline} \u2192 [1-9]{.underline} (Recalls a value from the chosen memory slot) [2 nd 0]{.underline} \u2192 [M1-9]{.underline} \u2192 [VALUE]{.underline} \u2192 [ENTER]{.underline} (View current stored data and manually edit Positive and Negative Signs \u00b6 We use Positive signs for Cash Inflows and Negative signs for Cash Outflows. We can toggle between positive and negative using \\(+ | -\\) We can determine whether they are Inflows and Outflows by choosing whose perspective we want to take - Investor or the Fund Manager As an Investor , you pay an initial amount along with periodic payments. At the end, we receive our returns. [PV]{.underline} and [PMT]{.underline} are thus negative , while [FV]{.underline} is positive. As the Fund Manager , you receive an initial amount along with periodic payments. At the end, you pay back the total returns. [PV]{.underline} and [PMT]{.underline} are thus positive , while [FV]{.underline} is negative Time Value of Money \u00b6 The function can be used to determine the present value of any annuities and Bonds [N]{.underline} \u2192 Number of periods As the name suggests, this can represent any time period (Months/Years etc) For perpetuities, we can input any number sufficiently large number (~999) [I/Y]{.underline} \u2192 Effective Interest per period (Integer) [PV]{.underline} \u2192 Present Value [PMT]{.underline} \u2192 Payment for Level Annuity Immediate (Arrears) By default, the option is payments at the end of the period . We can change this by going to [2 nd PMT]{.underline} \u2192 [2 nd ENTER]{.underline} which sets the mode to Beginning of the period instead [FV]{.underline} \u2192 Future Value Amortization Schedule \u00b6 This mode can only be accessed after filling in the relevant TVM variables for a bond [2 nd PV]{.underline} \u2192 Amortization Mode P1 \u2192 Starting Period P2 \u2192 Ending Period Both should be the same value to find amortization in a specific period BAL \u2192 Remaining Balance of the Loan after P2 PRN \u2192 Sum of the Principal paid from P1 to P2 INT \u2192 Sum of the interest paid from P1 to P2 Cashflow Function \u00b6 [CF]{.underline} \u2192 Cashflow Mode CF0 \u2192 Cashflow at time 0 This is typically the initial investment amount & hence should be a negative value C01 \u2192 Cashflow at time 1 F01 \u2192 Frequency of C01 (How many periods it repeats -- For perpetuities, put 999) \\(\\vdots\\) [NPV]{.underline} \u2192 Net Present Value Mode I \u2192 Interest Rate (Integer, in %) [IRR]{.underline} \u2192 Internal Rate of Return Interest Rate Conversion (Nominal to Effective) \u00b6 [2 nd 2]{.underline} \u2192 Enter Interest rate conversion mode EFF \u2192 Effective Interest ( EAR - Effective Annual Rate) NOM \u2192 Nominal Interest Rate ( APR -- Annual Percentage Rate) It is always assumed to be compounded 12 times a year unless stated otherwise C/Y \u2192 Compounding Frequency over a year \\((m)\\)","title":"STAT203: Financial Mathematics"},{"location":"1.%20Introductory/ASA-FM/Test/#contents","text":"Chapter 1: Interest Measurement [5](#chapter-1-interest-measurement) Overview of Interest Theory [5](#overview-of-interest-theory) Mathematical Notation [5](#mathematical-notation) Amount function, \\(\\mathbf{At}\\) 5 Accumulation Function, \\(\\mathbf{at}\\) 5 Interest Function, \\(\\mathbf{It}\\) 5 Visualizing via Timelines [6](#visualizing-via-timelines) Effective Interest [6](#effective-interest) Deriving Amount Function: [6](#deriving-amount-function) Expressing Interest Rates [6](#expressing-interest-rates) Time Value of Money [7](#time-value-of-money) Effective Rate of Discount [7](#effective-rate-of-discount) Compounding Frequency & Nominal Rates [8](#compounding-frequency-nominal-rates) Definition of \\(\\mathbf{m}\\) 8 Visual Representation [9](#visual-representation) Force of Interest [10](#force-of-interest) Deriving Force of Interest [10](#deriving-force-of-interest) Dealing with Variable Force of Interest [10](#dealing-with-variable-force-of-interest) Equivalent Rates [10](#equivalent-rates) Chapter 2: Level Annuities [11](#chapter-2-level-annuities) Types of Annuities [11](#types-of-annuities) Deriving Annuity Formulas: [12](#deriving-annuity-formulas) Linking the two kinds of Annuities [13](#linking-the-two-kinds-of-annuities) Via the payment amount [13](#via-the-payment-amount) Via the number of payments [13](#via-the-number-of-payments) Special Annuity Cases [14](#special-annuity-cases) Deferred Annuities [14](#deferred-annuities) Balloon & Drop Payments [15](#balloon-drop-payments) Varying Interest Rates [15](#varying-interest-rates) Continuous Annuities [16](#continuous-annuities) Deriving Continuous Annuity Formulas [16](#deriving-continuous-annuity-formulas) Comparing all three annuity types [16](#comparing-all-three-annuity-types) Chapter 2.5: Non-level Annuities [17](#chapter-2.5-non-level-annuities) Geometric Annuities [17](#geometric-annuities) Deriving Geometric Formula (Immediate) [17](#deriving-geometric-formula-immediate) Converting non-level to Level [17](#converting-non-level-to-level) Arithmetic Annuities [18](#arithmetic-annuities) Deriving Arithmetic Formula (Immediate) [18](#deriving-arithmetic-formula-immediate) Special case: Unit Increasing [19](#special-case-unit-increasing) Special case: Unit Decreasing [19](#special-case-unit-decreasing) Continuously Increasing Annuities [19](#continuously-increasing-annuities) Deriving Continuously increasing Annuities [19](#deriving-continuously-increasing-annuities) Perpetuities [20](#perpetuities) Other Important Annuity Patterns [21](#other-important-annuity-patterns) Block Payments (For any increase/decrease -- very powerful concept) [21](#block-payments-for-any-increasedecrease-very-powerful-concept) Repeat Reverse Annuities (For increases/decreases of 1) [21](#repeat-reverse-annuities-for-increasesdecreases-of-1) Odd-Even Split (For alternating increases) [21](#odd-even-split-for-alternating-increases) Annuities into Perpetuities: [22](#annuities-into-perpetuities) Annuities with unknown number of payments [23](#annuities-with-unknown-number-of-payments) Matching Payment and Interest frequencies [24](#matching-payment-and-interest-frequencies) Lower to Higher payment frequency [24](#lower-to-higher-payment-frequency) Higher to Lower payment frequency [25](#higher-to-lower-payment-frequency) Chapter 3: Spot, Forward and Interest Swaps [26](#chapter-3-spot-forward-and-interest-swaps) Spot & Forward Rates [26](#spot-forward-rates) Linking the two rates [26](#linking-the-two-rates) Forward rates lasting more than one period [26](#_Toc89369556) Interest Rate Swaps [27](#interest-rate-swaps) Swap Mechanics [27](#swap-mechanics) Deferred Swaps [27](#deferred-swaps) Level Swap Shortcuts [28](#level-swap-shortcuts) Other Swap Calculations [29](#other-swap-calculations) Net Swap Payments [29](#net-swap-payments) Net Interest Payment [29](#net-interest-payment) Market Value [29](#market-value) Why interest rate swaps? [29](#why-interest-rate-swaps) Chapter 4: Rates of Return [30](#chapter-4-rates-of-return) One Period Return Rates [30](#one-period-return-rates) Time Weighted Rate of Return (TWRR) [30](#time-weighted-rate-of-return-twrr) Dollar Weighted Rate of Return (DWRR) [30](#dollar-weighted-rate-of-return-dwrr) Differences between each measure: [31](#differences-between-each-measure) Different Timespans [31](#different-timespans) Multiple periods return rates [32](#multiple-periods-return-rates) Arithmetic Mean [32](#arithmetic-mean) Geometric Mean (AKA Cumulative Annual Growth Rate -- CAGR) [32](#geometric-mean-aka-cumulative-annual-growth-rate-cagr) Discounted Cashflow Analysis [32](#discounted-cashflow-analysis) Net Present Value (NPV) [32](#net-present-value-npv) Internal Rate of Return (IRR) [32](#internal-rate-of-return-irr) Other Measures of Return (FNCE101) [32](#other-measures-of-return-fnce101) Dollar Returns [32](#dollar-returns) Percentage Return [32](#percentage-return) Chapter 5: Loans [33](#chapter-5-loans) What is a loan? [33](#what-is-a-loan) Loan Calculations [34](#loan-calculations) Loan Variables [34](#loan-variables) Three ways to Calculate Outstanding Loan Balance [34](#three-ways-to-calculate-outstanding-loan-balance) Loan Amortization [35](#loan-amortization) Other Payment Patterns [36](#other-payment-patterns) Level Principal Payments [36](#level-principal-payments) Non-Level Payments [36](#non-level-payments) Missing or Additional Payments [37](#missing-or-additional-payments) Payments that scale with Interest [37](#payments-that-scale-with-interest) Other situations: [37](#other-situations) Chapter 6: Bonds and Stocks [38](#chapter-6-bonds-and-stocks) What is a Bond? [38](#what-is-a-bond) Bond Valuation [38](#bond-valuation) Dirty Prices [38](#dirty-prices) Bond Premium & Discount [39](#bond-premium-discount) Bond Amortization [39](#bond-amortization) Special Type of Bonds [40](#special-type-of-bonds) Zero Coupon Bonds (Pure discount Bonds) [40](#zero-coupon-bonds-pure-discount-bonds) Floating Rate Bonds [40](#floating-rate-bonds) Callable Bonds [40](#callable-bonds) Stock Valuation: Dividend Discount Model [41](#stock-valuation-dividend-discount-model) Chapter 7: Bond Yield & Term Structure of Interest Rates [42](#chapter-7-bond-yield-term-structure-of-interest-rates) Measures of Bond Yield [42](#measures-of-bond-yield) Current Yield & Nominal Yield [42](#current-yield-nominal-yield) Yield to Maturity [42](#yield-to-maturity) Par Yield [42](#par-yield) Holding Period Yield [42](#holding-period-yield) Spot Rates and the Term Structure [43](#spot-rates-and-the-term-structure) Discretely Compounded Yield Curve [43](#discretely-compounded-yield-curve) Continuously Compounded Yield Curve [43](#continuously-compounded-yield-curve) Chapter 8: Bond Management [44](#chapter-8-bond-management) Macaulay Duration (MacD) [44](#macaulay-duration-macd) Deriving Macaulay Duration [44](#deriving-macaulay-duration) Other Time Frequencies [44](#other-time-frequencies) Macaulay Duration Shortcuts [44](#macaulay-duration-shortcuts) Modified Duration [45](#modified-duration) Deriving Modified Duration [45](#deriving-modified-duration) Portfolio Duration [45](#portfolio-duration) Different Comparison date [46](#different-comparison-date) Using Durations for Estimations [47](#using-durations-for-estimations) An alternative (Intuitive) way to think about Approximation [47](#an-alternative-intuitive-way-to-think-about-approximation) Convexity [48](#convexity) Derivation [48](#derivation) Additional Formulas [48](#additional-formulas) Immunization [49](#immunization) Redington Immunization (Duration Matching) [49](#redington-immunization-duration-matching) Full Immunization [50](#full-immunization) Immunization Shortcut [50](#immunization-shortcut) Alternative: Exact Cashflow Matching [51](#alternative-exact-cashflow-matching) Chapter 9: Determinants of Interest Rates [52](#chapter-9-determinants-of-interest-rates) How are Interest rates determined? [52](#how-are-interest-rates-determined) Central Banks and Interest Rates [53](#central-banks-and-interest-rates) What is the central bank? [53](#what-is-the-central-bank) What if banks fail to meet this requirement? [53](#what-if-banks-fail-to-meet-this-requirement) Impact on Interest rates [53](#impact-on-interest-rates) Components of Interest Rate [54](#components-of-interest-rate) Maturity Risk Premium [54](#maturity-risk-premium) Default Risk Premium [54](#default-risk-premium) Inflation Risk Premium [55](#inflation-risk-premium) Liquidity Risk Premium [56](#liquidity-risk-premium) Financial Calculator Tips [57](#financial-calculator-tips) Overview of using the BA II Plus [57](#overview-of-using-the-ba-ii-plus) Basic settings to toggle beforehand [57](#basic-settings-to-toggle-beforehand) Basic calculator features [57](#basic-calculator-features) Positive and Negative Signs [57](#positive-and-negative-signs) Time Value of Money [58](#time-value-of-money-1) Amortization Schedule [58](#amortization-schedule) Cashflow Function [58](#cashflow-function) Interest Rate Conversion (Nominal to Effective) [58](#interest-rate-conversion-nominal-to-effective)","title":"Contents"},{"location":"1.%20Introductory/ASA-FM/Test/#chapter-1-interest-measurementunderline","text":"","title":"[Chapter 1: Interest Measurement]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#overview-of-interest-theory","text":"Assume we want to grow our money with just a SINGLE deposit Our initial deposit is known as the principal amount and how much earn on that is known as the interest payments (As compensation for the loss of use of money) There are two types of growth we can experience: Linear Growth \u2192 Simple Interest Exponential Growth \u2192 Compound Interest Simple Interest initially earns more than Compound interest. But in the long run, because Interest will earn interest , Compound Interest will earn more Compound Interest powerful because the interest is essentially reinvested at the same rate**. If we were to withdraw the interest each period, we would have simple interest**","title":"Overview of Interest Theory"},{"location":"1.%20Introductory/ASA-FM/Test/#mathematical-notation","text":"","title":"Mathematical Notation"},{"location":"1.%20Introductory/ASA-FM/Test/#amount-function-mathbfaleft-mathbft-right","text":"\\(A(t) = A(0)*a(t)\\) Represents the total amount of money in that fund at that point of time","title":"Amount function, \\(\\mathbf{A}\\left( \\mathbf{t} \\right)\\)"},{"location":"1.%20Introductory/ASA-FM/Test/#accumulation-function-mathbfaleft-mathbft-right","text":"Represents the change in money from time 0 to time \\(\\mathbf{t}\\) Simple Interest (Linear) \u2192 \\(a(t) = 1 + it\\) Compound Interest (Polynomial) \u2192 \\(\\ a(t) = (1 + i)^{t}\\) Double/Triple growth (Integer) \u2192 \\(a(t) = 2\\ or\\ 3\\) It is a special case of the amount function where the initial amount is 1. Thus, by multiplying any initial amount to it, we obtain the total amount. Additionally, it also means that any formula that uses \\(\\mathbf{A}\\left( \\mathbf{t} \\right)\\) in both numerator and denominators can be substituted with \\(\\mathbf{a}\\left( \\mathbf{t} \\right)\\) If our starting time is not 0 , we use a more general expression of the amount function \\(a_{k}(t - k)\\) where it represents the change in value from time 0 to time \\(\\mathbf{k}\\) \\(a_{k}(t - k) = \\frac{a(t)}{a(k)} \\rightarrow \\mathbf{Intuitive\\ through\\ recursion\\ \\&\\ forward\\ rates}\\)","title":"Accumulation Function, \\(\\mathbf{a}\\left( \\mathbf{t} \\right)\\)"},{"location":"1.%20Introductory/ASA-FM/Test/#interest-function-mathbfileft-mathbft-right","text":"\\(I(t) = A(t) - A(t - 1) = A(0)\\left\\lbrack a(t) - a(t - 1) \\right\\rbrack\\) Since the only change in the fund is interest, we can find it by taking the difference in amounts across two periods","title":"Interest Function, \\(\\mathbf{I}\\left( \\mathbf{t} \\right)\\)"},{"location":"1.%20Introductory/ASA-FM/Test/#visualizing-via-timelines","text":"To save on memory work, it is important that we understand how formulas are derived so that we can apply it to any context The first thing we should do for any question is to draw a timeline of the cashflows There are some important points we should remember: Cash Inflows or Outflows should be strictly on one side of the timeline respectively The units of time can be in any units (Years, Months) & with any interpretation (End or start of the period). The important thing is to label it and be consistent It is good practice to mark out the comparison date we want to use so that we do not get confused along the way Subtracting time depends on the cashflows we want to consider: By default, it assumes that only one of the end points has a cashflow We need to add 1 period if both end points has a cashflow","title":"Visualizing via Timelines"},{"location":"1.%20Introductory/ASA-FM/Test/#effective-interest","text":"Unlike the simplified example earlier, most amount functions don't have smooth curves The fund grows at different rates at different times during the period. We can summarize the entire growth into an effective interest rate for that period This creates a standardized measure of growth during the period. Two funds with the same effective interest will lead to the same amounts, regardless of their volatility These interest rates can be Yearly, monthly etc. The key is to be consistent with the time periods you use throughout the same calculations: Convert time periods to be expressed in the same way as effective interest Convert effective interest to be expressed in the same way as time periods \\[i_{t} = \\frac{\\text{Fund\\ growth}\\text{\\ during\\ the\\ period}\\ (Interest)}{\\mathbf{Initial\\ fund\\ value}\\ at\\ the\\ start\\ of\\ period} = \\frac{A(t) - A(t - 1)}{A(t - 1)} = \\frac{a(t) - a(t - 1)}{a(t - 1)}\\]","title":"Effective Interest"},{"location":"1.%20Introductory/ASA-FM/Test/#deriving-amount-function","text":"\\[i_{t} = \\frac{A(t) - A(t - 1)}{A(t - 1)}\\] \\[i_{t}*A(t - 1) = A(t) - A(t - 1)\\] \\[A(t) = A(t - 1) + i_{t}*A(t - 1)\\] \\[A(t) = A(t - 1)\\left\\lbrack 1 + i_{t} \\right\\rbrack\\] We can recursively add to this formula... \\[A(t) = A(t - 2)*\\left\\lbrack 1 + i_{t - 1} \\right\\rbrack*\\left\\lbrack 1 + i_{t} \\right\\rbrack\\] \\[A(t) = A(t - 3)*\\left\\lbrack 1 + i_{t - 2} \\right\\rbrack*\\left\\lbrack 1 + i_{t - 1} \\right\\rbrack*\\left\\lbrack 1 + i_{t} \\right\\rbrack\\] \\[\\vdots\\] \\[A(t) = A(0)*\\left\\lbrack 1 + i_{1} \\right\\rbrack*\\left\\lbrack 1 + i_{2} \\right\\rbrack*\\ldots\\left\\lbrack 1 + i_{t} \\right\\rbrack\\] If we have a constant effective interest across all periods, \\[A(t) = A(0)*\\lbrack 1 + i\\rbrack^{t}\\] More generally, if we start at time k and end at time t, \\[A(t) = A(k)*\\lbrack 1 + i\\rbrack^{t - k}\\]","title":"Deriving Amount Function:"},{"location":"1.%20Introductory/ASA-FM/Test/#expressing-interest-rates","text":"Typically expressed as a Percentage , where \\(1\\% = 0.01\\) For extremely small interest rates, it could be expressed as Basis Points instead, where \\(1\\ BP = 0.01\\% = 0.00001\\)","title":"Expressing Interest Rates"},{"location":"1.%20Introductory/ASA-FM/Test/#time-value-of-money","text":"\"A dollar today is worth more than a dollar in the future\" This is based off the principal of interest. If you invested the dollar today, you should be getting \\(\\left( 1 + i_{t} \\right)^{t}\\) in the future. Conversely, a dollar in the future is worth \\(\\frac{1}{\\left( 1 + i_{t} \\right)^{t}}\\) today Bringing values into the future is known as Accumulating which results in the Future Value. Conversely, bringing future ones into the present is known as Discounting which results in the Present Value The key is that when working with cashflows across different time periods, we should bring them to the same point in time, be it present or future to compare them \\[\\mathbf{Present\\ Value,\\ A}\\left( \\mathbf{k} \\right) = \\frac{A(t)}{\\left\\lbrack 1 + i_{t} \\right\\rbrack^{t - k}} = A(t)*\\left\\lbrack 1 + i_{t} \\right\\rbrack^{- (t - k)}\\] \\[\\mathbf{Future\\ Value,\\ A}\\left( \\mathbf{t} \\right) = A(k)*\\left\\lbrack 1 + i_{t} \\right\\rbrack^{t - k}\\] Notice that the formulas are almost identical with the main difference being the starting value and the exponent The FV has a Positive Exponent , signifying that we are going forward in time while the PV has a Negative Exponent , signifying that we are going backwards in time Since we use discounting and compounding so often, we use the variable \\(\\mathbf{v =}\\frac{\\mathbf{1}}{\\mathbf{a}\\left( \\mathbf{t} \\right)}\\) to represent the Discount Factor so that we simplify our workings","title":"Time Value of Money"},{"location":"1.%20Introductory/ASA-FM/Test/#effective-rate-of-discount","text":"Instead of Effective Interest Rates which measure the Interest relative to the starting fund value, another standardized way of measuring growth is Effective rate of Discount , which measures Interest relative to the ending fund value instead They are simply an alternative measure to Interest Rates , and can be used in a similar fashion to accumulate or discount values DO NOT confuse it with the discount factor. The discount factor is simply a notation we use to simplify the equation while discount rate is a completely new concept +-----------------------------------+-----------------------------------+ | [Effective Interest | **[Effective Discount | | Rate]{.underline} | Rate]{.underline}** | +===================================+===================================+ | \\( (i_{t} = | ( \\(d | | frac{A(t) - A(t - 1)}{\\mathbf{A}\\ | _{t} = \\frac{A(t) - A(t - 1)}{\\ma | | left( \\mathbf{t - 1} \\right)} = \\ | thbf{A}\\left( \\mathbf{t} \\right)} | | frac{a(t) - a(t - 1)}{a(t - 1)}\\) \\) | = \\frac{a(t) - a(t - 1)}{a(t)}\\) \\) | +-----------------------------------+-----------------------------------+ | If we fairly assume that the fund | | | is growing, \\(A(t) > A(t - 1)\\) . | | | Thus, \\(i_{t} > d_{t}\\) . | | +-----------------------------------+-----------------------------------+ | $ | \\( (\\ma | | \\(\\mathbf{FV} = A(k)*\\left\\lbrack | thbf{FV} = A(k)*\\left\\lbrack 1 - | | 1 + i_{t} \\right\\rbrack^{t - k}\\) \\) | d_{t} \\right\\rbrack^{- (t - k)}\\) $ | | | | | \\( (\\math | ( \\(\\ | | bf{PV} = \\ A(t)*\\left\\lbrack 1 + | mathbf{PV} = \\ A(t)*\\left\\lbrack | | i_{t} \\right\\rbrack^{- (t - k)}\\) \\) | 1 - d_{t} \\right\\rbrack^{t - k}\\) \\) | +-----------------------------------+-----------------------------------+ | Interest rate naturally goes | | | forward in time (Positive | | | exponent when accumulating) while | | | Discount rate naturally goes | | | backward in time (Positive | | | Exponent when Discounting) | | +-----------------------------------+-----------------------------------+ Another way to think about it is that Interest rates are usually paid at the end of the period, while discount rates is interest that is paid at the start of the period instead. But both are equivalent: \\[d = \\frac{i}{1 + i} = i*\\frac{1}{1 + i} = iv\\]","title":"Effective Rate of Discount"},{"location":"1.%20Introductory/ASA-FM/Test/#compounding-frequency-nominal-rates","text":"Compounding annually is just one of the ways we can compound. We could compound interest Monthly, Quarterly, semi-annually etc However, quoting effective rates with different timespans can become confusing. Thus, we multiply to make them reflect an annual nominal rate instead: Monthly rates multiply by 12; Quarterly rate multiply by 4 The word nominal means \"in name only\" , which implies that these rates are just placeholders. If we actually compound 12 months using a monthly rate VS compounding once using the nominal rate, we get two different values This is because the nominal rate is in name only and not to be used . Unfortunately, most interest rates are quoted in nominal terms. We always need to remember that to convert nominal rates to effective rates. +----------------------+----------------------+-----------------------+ | | [Interest | **[Discount | | | Rates]{.underline} | Rate]{.underline}** | +======================+======================+=======================+ | Annual Effective | ( \\(i\\) \\) | ( \\(d\\) \\) | | Rate | | | +----------------------+----------------------+-----------------------+ | Annual Nominal | ( \\(i^{(m)}\\) \\) | ( \\(d^{(m)}\\) \\) | | Rate | | | | | | | | (Compounded | | | | \\(\\mathbf{m}\\) times | | | | a year, every | | | | \\(\\frac{\\math | | | | bf{12}}{\\mathbf{m}}\\) | | | | months) | | | +----------------------+----------------------+-----------------------+ | Monthly Effective | ( \\(j\\) \\) | ( \\(k\\) \\) | | Rate | | | | | | | | (Any time other | | | | than Annual) | | | +----------------------+----------------------+-----------------------+ | Convert Nominal to | ( (j = | ( \\(k | | m-Effective** | \\frac{i^{(m)}}{m}\\) \\) | = \\frac{d^{(m)}}{m}\\) \\) | +----------------------+----------------------+-----------------------+ | **Convert m to | ( ((1 | ( \\((1 | | Annual Effective** | + j)^{m} = (1 + i)\\) \\) | - k)^{m} = (1 - d)\\) \\) | | | | | | | $$i | $$ | | | = (1 + j)^{m} - 1$$ | d = 1 - (1 - k)^{m}$$ | +----------------------+----------------------+-----------------------+ | **Convert Annual to | ( (\\left( 1 + | ( \\(\\left( 1 - | | Nominal (Sub in | frac{i^{(m)}}{m} \\ri | \\frac{d^{(m)}}{m} \\r | | any** | ght)^{m} = (1 + i)\\) \\) | ight)^{m} = (1 - d)\\) \\) | | \\(\\mathbf{m}\\) )** | | | | | \\( (i^{(m)} = m\\l | ( \\(d^{(m)} = m | | | eft( (1 + i)^{\\frac{ | \\left( 1 - (1 - d)^{\\ | | | 1}{m}} - 1 \\right)\\) \\) | frac{1}{m}} \\right)\\) \\) | +----------------------+----------------------+-----------------------+ | Interesting | ( \\(\\frac{1}{m} = | | | Result** | \\frac{1}{d^{(m)}} - | | | | \\frac{1}{i^{(m)}}\\) \\) | | | **(Nominal = | | | | Nominal) | | | +----------------------+----------------------+-----------------------+","title":"Compounding Frequency &amp; Nominal Rates"},{"location":"1.%20Introductory/ASA-FM/Test/#definition-of-mathbfm","text":"\\(m\\) refers to the number of times interest is compounded within a year Compound once a year \u2192 \\(m = 1\\) EG. One year annual effective rate Compound multiple times a year \u2192 \\(m > 1\\) EG. 1-year nominal rate, compounded monthly Compound once in a few years \u2192 \\(m < 1\\) EG. 4-year nominal rate, compounded yearly","title":"Definition of \\(\\mathbf{m}\\)"},{"location":"1.%20Introductory/ASA-FM/Test/#visual-representation","text":"{width=\"6.268055555555556in\" height=\"3.421527777777778in\"} Assuming the SAME effective rate of interest , the above diagram provides a visual representation of the values of the various rates Effective rates are always at the tail ends -- They move toward the centre as the compounding frequency for each type of rate, with infinite compounding (Force of interest) as the midpoint between both Nominal Interest Rate is always lower than the equivalent effective rate . This allows companies to give the false perception of lower interest rates. This is why nominal rates are often used in the industry (Vice versa for Discount) <!-- --> - Use this as a sense check on the values calculated -- whether or not it should be higher or lower","title":"Visual Representation"},{"location":"1.%20Introductory/ASA-FM/Test/#force-of-interest","text":"As the compounding frequency increases, we can see that both the interest and discount rates move towards the centre of the graph If the compounding frequency \\(\\mathbf{m}\\) , tends to infinity , both the discount and interest rates tend to a central rate as well, known as the Force of Interest, \\(\\mathbf{\\delta}\\) . It represents how much the fund accumulates at that instantaneous time t","title":"Force of Interest"},{"location":"1.%20Introductory/ASA-FM/Test/#deriving-force-of-interest","text":"+-----------------------------------+-----------------------------------+ | Constant Force of Interest | **Variable Force of Interest | | (Constants) | (Polynomials)** | +===================================+===================================+ | By definition, if \\(m\\) tends to | By definition, it is | | infinity: | instantaneous: | | | | | $ | \\( (\\delta = \\frac{A^{'}(t) | | \\(i^{(m)} = m\\left\\lbrack (1 + i)^ | }{A(t)} = \\frac{a^{'}(t)}{a(t)}\\) \\) | | {\\frac{1}{m}} - 1 \\right\\rbrack\\) $ | | | | \\( (\\int_{0}^{t}\\delta | | ( \\(\\lim_{m \\rightar | _{r}dr = \\ln\\left( a(t) \\right)\\) \\) | | row \\infty}i^{(m)} = \\ln(1 + i)\\) \\) | | | | \\(\\therefore a(t) = (1 + i)^{t} = | | ( \\(\\de | e^{\\int_{0}^{t}{\\delta_{r}\\ dr}}\\) | | lta = i^{(\\infty)} = \\ln(1 + i)\\) \\) | | | | If simple interest instead, | | \\( \\(e^{\\delta} = (1 + i)\\) \\) | | | | \\( (\\delta = \\frac{a^{ | | ( \\(\\therefore a( | '}(t)}{a(t)} = \\frac{i}{1 + it}\\) \\) | | t) = (1 + i)^{t} = e^{\\delta t}\\) \\) | | | | | | This only holds for | | | \\(\\left( \\mathbf{0,t} \\right)\\) . If | | | we are calculating the forward | | | rate, this formula is not true. | | +-----------------------------------+-----------------------------------+","title":"Deriving Force of Interest"},{"location":"1.%20Introductory/ASA-FM/Test/#dealing-with-variable-force-of-interest","text":"Always find the accumulation function FIRST Take note that the limits of integration \\((0,t)\\) is for a standard accumulation function \\(a_{0}(t)\\) . For a more general accumulation function \\(a_{k}(t - k)\\) the limits would be \\((k,t)\\) Always consider logic when dealing with Variable Force -- the interest function should always be different for different years and periods When your start time is fixed, integrating from \\((0,t)\\) makes sense But when even the start time is not fixed (Yearly interest payments) then \\((t,t + 1)\\) would be better If you ever see a constant output while having a variable force, something is wrong","title":"Dealing with Variable Force of Interest"},{"location":"1.%20Introductory/ASA-FM/Test/#equivalent-rates","text":"As alluded to earlier, if two funds have the same starting and ending values, they will have the same Accumulation Function regardless which types of rates were used We get an all-in-one formula that we convert from one to another: \\[a(t) = (1 + i)^{t} = (1 - d)^{- t} = \\left( 1 + \\frac{i^{(m)}}{m} \\right)^{m} = \\left( 1 - \\frac{d(m)}{m} \\right)^{m} = e^{\\delta t} = e^{\\int_{0}^{t}{\\delta_{r}\\ dr}} = v^{- t}\\] There is no equivalent Compound -- Simple Interest rate. We can convert between simple interest rates across time periods using the same principle -- equating accumulation functions The conversion will simplify to become just multiplying by a factor to convert the rate","title":"Equivalent Rates"},{"location":"1.%20Introductory/ASA-FM/Test/#chapter-2-level-annuitiesunderline","text":"","title":"[Chapter 2: Level Annuities]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#types-of-annuities","text":"An annuity is a series of level payments at equal intervals Our goal is determining the Present or Future Value of the Annuity. It is the SUM of the Present or Future value of EVERY payment There are two main types of annuities: Immediate \u2192 Payments made at the end of the period (Arrears) We denote the PV as \\(a_{n\u23cbi}\\) and FV as \\(s_{n\u23cbi}\\) Due \u2192 Payments are made at the start of the period (Advance) We denote the PV as \\({\\ddot{a}}_{n\u23cbi}\\) and FV as \\({\\ddot{s}}_{n\u23cbi}\\) We calculate the PV at the start of the first period and the FV at the end of the last period , regardless of the type of Annuity \\(a_{n\u23cbi}\\) is calculated one period before the first the payment while \\(s_{n\u23cbi}\\) is calculated on the period of the last payment Conversely, \\({\\ddot{a}}_{n\u23cbi}\\) is calculated on the period of the first payment while \\({\\ddot{s}}_{n\u23cbi}\\) is calculated one period after the last payment Notice how both are symmetrical on a timeline: Important: Remember that the start of the period is also the end of the previous period . This means that given any set of Cashflows, we can treat it as either an Immediate or Due , depending on the comparison date we want to use The cashflows above are one cycle above for illustration purposes to drive home the idea that they have different comparison dates","title":"Types of Annuities"},{"location":"1.%20Introductory/ASA-FM/Test/#deriving-annuity-formulas","text":"Since payments are level, we can factorise out the common payments P to make the expressions simpler. However, we have to multiply it back for our final answers Effectively, this allows us to derive the formulas assuming payments of 1 +------+-------------------------------+-------------------------------+ | | [Annuity | **[Annuity Due]{.underline} | | | Immediate]{.underline}** | | +======+===============================+===============================+ | ** | Since we discount to one | Since we discount to the | | PV** | period before the first | period of the first payment, | | | payment, we discount the | we **don't discount the first | | | first payment (Start from | payment (Start from 0) | | | 1)** | | | | | \\( ({\\d | | | ( \\(a_{n\u23cbi} = v + v | dot{a}}_{n\u23cbi} = v^{0} + v^{1} | | | ^{1} + v^{2} + \\ldots v^{n}\\) \\) | + v^{2} + \\ldots v^{n - 1}\\) \\) | | | | | | | $$ | \\( \\({\\ddot{a}}_{n | | | a_{n\u23cbi} = \\sum_{1}^{n}v^{n}\\) \\) | \u23cbi} = \\sum_{0}^{n - 1}v^{n}$$ | | | | | | | \\( (a_{n\u23cbi} = | ( \\({\\ddot{a}}_{n\u23cbi | | | \\frac{v - v^{n + 1}}{1 - v}\\) \\) | } = \\frac{1 - v^{n}}{1 - v}\\) \\) | | | | | | | \\( (a_{n\u23cbi} = \\frac{v\\left | ( \\({\\ddot{a}}_ | | | ( 1 - v^{n} \\right)}{1 - v}\\) \\) | {n\u23cbi} = \\frac{1 - v^{n}}{d}\\) \\) | | | | | | | \\( \\(a_ | | | | {n\u23cbi} = \\frac{1 - v^{n}}{i}\\) \\) | | +------+-------------------------------+-------------------------------+ | ** | Since we accumulate to the | Since we accumulate to the | | FV** | period of the last payment, | one period after the last | | | we don't accumulate the | payment, we **accumulate the | | | last payment (End with 0) | last payment (End with 1)** | | | | | | | \\( (s_{n\u23cbi} = (1 + i)^ | ( \\({\\ddot{s}}_{n\u23cbi} = (1 + | | | {n - 1} + \\ldots(1 + i)^{0}\\) \\) | i)^{n} + \\ldots(1 + i)^{1}\\) \\) | | | | | | | \\( (s_{n\u23cbi} = | ( \\({\\ddot{s}}_{n\u23cbi | | | \\sum_{0}^{n - 1}(1 + i)^{n}\\) \\) | } = \\sum_{1}^{n}(1 + i)^{n}\\) \\) | | | | | | | \\( (s_{n\u23cbi} = \\frac{1 | ( \\({\\ddot{s | | | - (1 + i)^{n}}{1 - (1 + i)}\\) \\) | }}_{n\u23cbi} = \\frac{(1 + i) - (1 | | | | + i)^{n + 1}}{1 - (1 + i)}\\) \\) | | | \\( \\(s_{n\u23cbi} = | | | | \\frac{1 - (1 + i)^{n}}{- i}\\) \\) | \\( ({\\ddot{s}}_{n\u23cbi} = \\frac | | | | {(1 + i)\\left\\lbrack 1 - (1 + | | | ( \\(s_{n\u23cbi} | i)^{n} \\right\\rbrack}{- i}\\) \\) | | | = \\frac{(1 + i)^{n} - 1}{i}\\) \\) | | | | | \\( \\({\\ddot{s}}_{n\u23cbi} | | | | = \\frac{(1 + i)^{n} - 1}{d}\\) \\) | +------+-------------------------------+-------------------------------+ | Bo | Alternatively, if we know | | | th | either the Present or Future | | | | value, we can simply | | | | Accumulate or Discount that | | | | by the number of periods to | | | | amount to obtain the other: | | | | | | | | \\( \\(s | | | | _{n\u23cbi} = a_{n\u23cbi}(1 + i)^{n}\\) \\) | | | | | | | | \\( \\({\\ddot{s}}_{n\u23cbi} = | | | | {\\ddot{a}}_{n\u23cbi}(1 + i)^{n}\\) \\) | | +------+-------------------------------+-------------------------------+ | * | \\( \\(Sum\\ of\\ Geom | | | *Oth | etric\\ Series = \\frac{First\\ | | | er** | term - First\\ Omitted\\ Term\\ | | | | (\"Next\"\\ term)}{1 - Factor}\\) \\) | | | | | | | | - First omitted term is | | | | zero for an infinite | | | | series (No omission) | | | | | | | | \\( \\(\\fr | | | | ac{v}{1 - v} = \\frac{\\frac{1} | | | | {1 + i}}{1 - \\frac{1}{1 + i}} | | | | = \\frac{\\frac{1}{1 + i}}{\\fr | | | | ac{i}{1 + i}} = \\frac{1}{i}\\) \\) | | | | | | | | \\( \\(1 - v = 1 - \\frac{1}{1 | | | | + i} = \\frac{i}{1 + i} = d\\) \\) | | | | | | | | We can easily remember the | | | | denominator for each by | | | | \\(\\mathbf{i}\\) for Immediate | | | | and \\(\\mathbf{d}\\) for | | | | Due | | +------+-------------------------------+-------------------------------+","title":"Deriving Annuity Formulas:"},{"location":"1.%20Introductory/ASA-FM/Test/#linking-the-two-kinds-of-annuitiesunderline","text":"The financial calculator naturally assumes that the Annuities we calculate are Annuity Immediate (Need to convert for Due) If we know how to convert Immediate to Due, we can simply use the calculator and apply the conversion which will save a lot of time Both methods are viable, but usually the first method is easier and more direct. But the magic of the second approach is that it does not require you to know the interest rate","title":"[Linking the two kinds of Annuities]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#via-the-payment-amount","text":"If we compare the payments for Immediate and Due, we see the regardless FV or PV, the payments for Annuity Due are always greater by a factor of \\(\\left( \\mathbf{1 + i} \\right)\\) Thus, we can go from Immediate to Due by multiplying \\((1 + i)\\) \\({\\ddot{a}}_{n\u23cbi} = (1 + i)a_{n\u23cbi}\\) \\({\\ddot{s}}_{n\u23cbi} = (1 + i)s_{n\u23cbi}\\) Alternatively, we can also use this by changing the payment amount by \\(\\left( \\mathbf{1 + i} \\right)\\) in the calculator to directly calculate Annuity Due","title":"Via the payment amount"},{"location":"1.%20Introductory/ASA-FM/Test/#via-the-number-of-payments","text":"The difference between the two is that an Annuity Due has additional payment at time 0 and one less payment at time \\(\\mathbf{n}\\) Opposite effects on both because they are comparing in different time periods Given any series of cashflows, we can convert between the two by: <!-- --> - Present Value - **Additional payment at time 0** \u2192 Add a one additional payment $( + 1)$ - **One less payment at time** $\\mathbf{n}$ \u2192 Consider one less payment $\\left( a_{n - 1\u23cbi} \\right)$ - $\\mathbf{\\therefore}{\\ddot{a}}_{n\u23cbi} = 1 + a_{n - 1\u23cbi}$ {width=\"4.220833333333333in\" height=\"1.2145833333333333in\"} Future Value Additional payment at time 0 \u2192 Consider additional payment \\(\\left( s_{n + 1\u23cbi} \\right)\\) One less payment at time \\(\\mathbf{n}\\) \u2192 Subtract one less payment \\(( - 1)\\) \\(\\therefore\\) \\({\\ddot{s}}_{n\u23cbi} = s_{n + 1\u23cbi} - 1\\) Easy way to remember Annuity Due is always the subject the equation; Annuity Immediate is not If the RHS time is PLUS/MINUS then the it must be MINUS/PLUS 1 (Opposite)","title":"Via the number of payments"},{"location":"1.%20Introductory/ASA-FM/Test/#special-annuity-casesunderline","text":"","title":"[Special Annuity Cases]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#deferred-annuities","text":"Variant where the annuity is deferred and starts at some later date (not time 0) We denote the present value of an annuity deferred by m periods as \\(_{m|}^{\\ }a_{n\u23cbi}\\)","title":"Deferred Annuities"},{"location":"1.%20Introductory/ASA-FM/Test/#calculating-values-deferred-annuities","text":"Standard Method If we already know the PV/FV of the normal annuity (Any of the 4 times) we can simply discount that value to the time we want \\(_{m|}^{\\ }a_{n\u23cbi} = v^{m}*{\\ddot{a}}_{n\u23cbi}\\) (Or any of the other values) Differences in Annuities Consider the cashflows involved in terms of their raw equation We can take a larger annuity minus a smaller annuity to obtain the deferred annuity that we want","title":"Calculating Values Deferred Annuities"},{"location":"1.%20Introductory/ASA-FM/Test/#calculating-values-in-the-middle-of-the-annuity-payments","text":"All our previous formulas helped us to find the value of the annuity at the start or end We have no formula to help us to find the value of the annuity in the middle of it Similarly, we can use either: Simple method \u2192 Accumulate discount a value from the start/end respectively Sum of annuities Split the annuity into two at the period we want to compare This artificially creates two annuities with end points at the time we want to compare, allowing us to sum together their values to obtain the value of the overall annuity","title":"Calculating values in the middle of the annuity payments"},{"location":"1.%20Introductory/ASA-FM/Test/#balloon-drop-payments","text":"Sometimes instead of finding PV/FV, we are tasked to find the number of payments Since this is a calculated value, we often got non-integer values (EG. 15.42). This means that there are 15 standard payments and a fractional payment Balloon Payment \u2192 Combine the last standard and fractional payment together Drop Payment \u2192 Treat the fractional payment as its own payment Let \\(X\\) be the fractional payment: \\[\\mathbf{Balloon\\ Payment} = P + X\\] \\[PV_{Balloon} = \\ a_{n\u23cbi} + X*v^{n}\\] \\[\\mathbf{Drop\\ Payment} = X(1 + i)\\ \\] \\[PV_{Drop} = \\ a_{n\u23cbi} + X(1 + i)*v^{n + 1}\\] Since the fractional payment was treated as its own payment in the next period, we have to account for the time value of money and accumulate it accordingly","title":"Balloon &amp; Drop Payments"},{"location":"1.%20Introductory/ASA-FM/Test/#varying-interest-rates","text":"So far, we have used just one fixed effective rate to discount or accumulate annuities More realistically, interest rates are bound to change across periods Assume that we have [two periods]{.underline} with rates \\(i_{1}\\ \\&\\ i_{2},\\ i_{1} \\neq i_{2}\\) . There are two methods to use: Market Portfolio Method: End of Period 1 \u2192 Accumulate first payment by \\(\\left( 1 + i_{1} \\right)\\) End of Period 2 \u2192 Accumulate both payments by \\(\\left( 1 + i_{2} \\right)\\) Result \u2192 \\(FV = \\ P\\left( 1 + i_{1} \\right)\\left( 1 + i_{2} \\right) + P\\left( 1 + i_{2} \\right)\\) Yield-Curve Method: End of Period 1 \u2192 Accumulate first payment by \\(\\left( 1 + i_{1} \\right)\\) End of Period 2 \u2192 Accumulate payments by \\(\\left( 1 + i_{1} \\right)\\) and \\(\\left( 1 + i_{2} \\right)\\) respectively Result \u2192 \\(FV = P\\left( 1 + i_{1} \\right)^{2} + P\\left( 1 + i_{2} \\right)\\) Market Portfolio is more intuitive. The key difference is how the interest of the first payment (More generally, all previous periods) is treated Market Portfolio accumulates according to rate of the current period Yield Curve accumulates according to the rate when that payment was made We can know which method to use based on how the question defines interest rates, be it according to the time of payment or prevailing period Either way, we will need to manually map out each payment and discount/accumulate them according to their corresponding interest rates","title":"Varying Interest Rates"},{"location":"1.%20Introductory/ASA-FM/Test/#continuous-annuities","text":"Instead of making payments at fixed time intervals like the start or end of the period, payments are made continuously throughout the period , totalling up to the same amount Similarly, we denote the PV/FV of such payments with \\({\\overline{a}}_{n\u23cbi}\\) and \\({\\overline{s}}_{n\u23cbi}\\)","title":"Continuous Annuities"},{"location":"1.%20Introductory/ASA-FM/Test/#deriving-continuous-annuity-formulas","text":"+-----------------------------------+-----------------------------------+ | [Present Value]{.underline} | [Future Value]{.underline} | +===================================+===================================+ | \\( ({\\overline{a}}_{n\u23cbi} = | ( \\({\\overline{s}}_{n\u23cbi} | | int_{0}^{n}{e^{- \\delta t}\\ }dt\\) \\) | = \\int_{0}^{n}e^{\\delta t}\\ dt\\) \\) | | | | | \\( ({\\overline{a}}_{n\u23cbi} = \\l | ( \\({\\overline{s}}_{n\u23cbi} | | eft\\lbrack \\frac{e^{- \\delta t}}{ | = \\left\\lbrack \\frac{e^{\\delta t} | | - \\delta} \\right\\rbrack_{0}^{n}\\) \\) | }{\\delta} \\right\\rbrack_{0}^{n}\\) \\) | | | | | \\( ({\\overlin | ( \\({\\ove | | e{a}}_{n\u23cbi} = \\frac{e^{- \\delta n | rline{s}}_{n\u23cbi} = \\frac{e^{\\delta | | }}{- \\delta} + \\frac{1}{\\delta}\\) \\) | n}}{\\delta} - \\frac{1}{\\delta}\\) \\) | | | | | \\( ({\\overline{a}}_{n\u23cbi} = \\f | ( \\({\\overline{s}}_{n\u23cbi} = | | rac{1 - e^{- \\delta n}}{\\delta}\\) \\) | \\frac{e^{n\\delta} - 1}{\\delta}\\) \\) | | | | | \\( ({\\overline{a}}_{ | ( \\({\\overline{s}}_{n\u23cbi} = | | n\u23cbi} = \\frac{1 - v^{n}}{\\delta}\\) \\) | \\frac{(1 + i)^{n} - 1}{\\delta}\\) \\) | +-----------------------------------+-----------------------------------+ | Similarly, we factorise out | | | constant CONTINOUS payments of | | | P which assumes a continuous | | | rate of cashflows of 1 for the | | | proof | | | | | | The accumulation function for | | | the force of interest is | | | \\(a(t) = e^{\\delta t}\\) , which | | | represents the accumulated or | | | discounted cashflows at each time | | | \\(t.\\) If \\(\\delta\\) is variable, we | | | change the accumulation factor to | | | \\(e^{\\int_{k}^{t}\\delta_{r}\\ dr}\\) | | | and perform the same steps. | | | | | | Since we want the sum of the | | | discounted or accumulated | | | cashflows from \\((0,n)\\) , we | | | integrate our general | | | accumulation function from that | | | same limit. It is the same idea | | | as our discrete annuities, but we | | | use Integrals instead of | | | Sigma to perform the sum. | | | | | | The limit of integration should | | | follow the accumulation function | | | used: | | | | | | - If we use the standard | | | accumulation function | | | \\(\\mat | | | hbf{a}\\left( \\mathbf{t} \\right)\\) , | | | then use \\((0,n)\\) | | | | | | - If we use the general | | | accumulation function | | | \\(\\mathbf{a}_{\\mathbf{k | | | }}\\left( \\mathbf{t - k} \\right)\\) , | | | then use \\((0,t - k)\\) | | +-----------------------------------+-----------------------------------+","title":"Deriving Continuous Annuity Formulas"},{"location":"1.%20Introductory/ASA-FM/Test/#comparing-all-three-annuity-types","text":"[Annuity Due]{.underline} [Annuity Immediate]{.underline} [Continuous Annuity]{.underline} \\( \\({\\ddot{a}}_{n\u23cbi} = \\frac{1 - v^{n}}{d}\\) \\) \\( \\(a_{n\u23cbi} = \\frac{1 - v^{n}}{i}\\) \\) \\( \\({\\overline{a}}_{n\u23cbi} = \\frac{1 - v^{n}}{\\delta}\\) \\) Since \\(d\\) is the smallest rate, this produces the largest PV Since \\(i\\) is the largest rate, this produces the Since \\(\\delta\\) is the midpoint rate, it produces the smallest PV midpoint PV $${\\overline{a}}_{n\u23cbi} = \\frac{1}{\\delta}a_{n\u23cbi}$$ \\( \\({\\ddot{a}}_{n\u23cbi} = \\frac{i}{d}a_{n\u23cbi} = (1 + i)a_{n\u23cbi}\\) \\)","title":"Comparing all three annuity types"},{"location":"1.%20Introductory/ASA-FM/Test/#chapter-25-non-level-annuitiesunderline","text":"","title":"[Chapter 2.5: Non-level Annuities]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#geometric-annuities","text":"Payments change according to a Geometric Series (By a common factor \\(k\\) )","title":"Geometric Annuities"},{"location":"1.%20Introductory/ASA-FM/Test/#deriving-geometric-formula-immediate","text":"Since the original annuity proof already made use of the Geometric series , increasing payments by a geometrically simply just changes the factor we use in the proof \\[PV_{Geometric} = \\ v + (1 + k)v^{2} + \\ldots(1 + k)^{n - 1}v^{n}\\] \\[PV_{Geometric} = v\\left\\lbrack 1 + (1 + k)v + \\ldots + \\left( (1 + k)v \\right)^{n - 1} \\right\\rbrack\\] \\[PV_{Geometric} = v\\left\\lbrack \\frac{1 - (1 + k)^{n}v^{n}}{1 - (1 + k)v} \\right\\rbrack\\] \\[PV_{Geometric} = v\\left\\lbrack \\frac{1 - \\left( \\frac{1 + k}{1 + i} \\right)^{n}}{1 - \\frac{1 + k}{1 + i}} \\right\\rbrack\\] \\[PV_{Geometric} = v\\left\\lbrack \\frac{1 - \\left( \\frac{1 + k}{1 + i} \\right)^{n}}{\\frac{i - k}{1 + i}} \\right\\rbrack\\] \\[PV_{Geometric} = \\frac{1 - \\left( \\frac{1 + k}{1 + i} \\right)^{n}}{i - k}\\] This only holds for \\(i \\neq k\\) , derive the formula manually again to see the interaction. The terms will cancel each other, becoming a standard summation problem If k is negative, remember that the denominator nets out to become a plus instead","title":"Deriving Geometric Formula (Immediate)"},{"location":"1.%20Introductory/ASA-FM/Test/#converting-non-level-to-level","text":"Looking at the proof, we notice that it is VERY similar to the level version Since the Financial Calculator only allows level payments, converting a non-level annuity to a level one would save us a lot of time The key is noticing that we can substitute the common factors for each other: +-----------------------------------+-----------------------------------+ | \\( \\(\\mathbf{i > k}\\) \\) | \\( \\(\\mathbf{i < k}\\) \\) | +===================================+===================================+ | \\( ((1 | ( \\((1 | | + k)v = \\frac{1 + k}{1 + i} < 1\\) \\) | + k)v = \\frac{1 + k}{1 + i} > 1\\) \\) | | | | | \\( \\(v^{'} = \\frac{1 + k}{1 + i}\\) \\) | $$ | | | 1 + i^{'} = \\frac{1 + k}{1 + i}$$ | | $$ | | | 1 + i^{'} = \\frac{1 + i}{1 + k}$$ | $$ | | | i^{'} = \\frac{1 + k}{1 + i} - 1$$ | | $$ | | | i^{'} = \\frac{1 + i}{1 + k} - 1$$ | | +-----------------------------------+-----------------------------------+ Depending on the relative magnitude of \\(i\\) and \\(k\\) , we can determine if the factor is closer to the Present Value, or the Future Value formula & choose the appropriate conversion <!-- --> - When converting from FV to PV (vice versa), remember to use \\(\\left( \\mathbf{1 +}\\mathbf{i}^{\\mathbf{'}} \\right)\\) not \\((1 + i)\\) This method is preferred when we have to solve for \\(\\mathbf{i}\\) or \\(\\mathbf{k}\\) as it simplifies the algebra. When solving for \\(i'\\) or \\(v'\\) , do not be alarmed by weird results","title":"Converting non-level to Level"},{"location":"1.%20Introductory/ASA-FM/Test/#arithmetic-annuities","text":"Payments change according to an Arithmetic Progression (By fixed constant Q) Be careful when using AP Formula -- The formula assumes the first payment occurs at time is 1 , but sometimes your payments start at time 0 so an adjustment is needed","title":"Arithmetic Annuities"},{"location":"1.%20Introductory/ASA-FM/Test/#deriving-arithmetic-formula-immediate","text":"ALWAYS split up the payments into their individual components to better visualize it We notice that we can form several different level annuities: One level annuity with \\(n\\) payments of \\(P\\) \\(j - 1\\) deferred level annuities with \\(n - j\\) payments of \\(Q\\) We consider the PV at time 0: \\[PV_{Arithmetic} = P*a_{n\u23cbi} + Q*\\sum_{j = 1}^{n - 1}{v^{j}*\\frac{1 - v^{n - j}}{i}}\\] \\[PV_{Arithmetic} = \\ P*a_{n\u23cbi} + Q*\\sum_{j = 1}^{n - 1}\\frac{v^{j} - v^{n}}{i}\\] \\[PV_{Arithmetic} = \\ P*a_{n\u23cbi} + Q\\left( \\sum_{j = 1}^{n - 1}\\frac{v^{j}}{i} - \\frac{nv^{n}}{i} \\right)\\] \\[PV_{Arithmetic} = \\ P*a_{n\u23cbi} + Q\\left( \\frac{a_{n\u23cbi}}{i} - \\frac{nv^{n}}{i} \\right)\\] \\[PV_{Arithmetic} = a_{n\u23cbi}\\left( P + \\frac{Q}{i} \\right) - \\frac{Qnv^{n}}{i}\\] Using the Financial Calculator, we can quickly calculate this value by setting \\(\\left( P + \\frac{Q}{i} \\right)\\) as the Payment and \\(\\left( \\frac{Qnv^{n}}{i} \\right)\\) as the Future Value Understanding this proof is important as this concept of splitting annuities is extremely important when dealing with Arithmetic Annuities","title":"Deriving Arithmetic Formula (Immediate)"},{"location":"1.%20Introductory/ASA-FM/Test/#special-case-unit-increasing","text":"When \\(P = Q = 1\\) . The condition can be met for any \\(\\mathbf{P = Q = x,}\\) as we can simply factor out \\(x\\) to obtain \\(P = Q = 1\\) , but remember to multiply back \\(\\mathbf{x}\\) First payment starts out at 1 and increases by 1 each period until \\(\\mathbf{n}\\) in the last period When \\(P = Q = 1,\\) \\[(Ia)_{n\u23cbi} = \\ a_{n\u23cbi} + \\left( \\frac{a_{n\u23cbi}}{i} - \\frac{nv^{n}}{i} \\right)\\] \\[(Ia)_{n\u23cbi} = \\frac{i*a_{n\u23cbi} + a_{n\u23cbi} - nv^{n}}{i}\\ \\] \\[(Ia)_{n\u23cbi} = \\frac{a_{n\u23cbi}(1 + i) - nv^{n}}{i}\\] \\[(Ia)_{n\u23cbi} = \\frac{{\\ddot{a}}_{n\u23cbi} - nv^{n}}{i}\\]","title":"Special case: Unit Increasing"},{"location":"1.%20Introductory/ASA-FM/Test/#special-case-unit-decreasing","text":"When \\(P = n\\) and \\(Q = - 1\\) First payment starts out at \\(n\\) and decreases by 1 each period until 1 in the last period When \\(P = n,\\ Q = - 1,\\) \\[(Da)_{n\u23cbi} = n*a_{n\u23cbi} - \\left( \\frac{a_{n\u23cbi}}{i} - \\frac{nv^{n}}{i} \\right)\\] \\[(Da)_{n\u23cbi} = n*\\left( \\frac{1 - v^{n}}{i} \\right) - \\left( \\frac{a_{n\u23cbi}}{i} - \\frac{nv^{n}}{i} \\right)\\] \\[(Da)_{n\u23cbi} = \\frac{\\begin{array}{r} n \\\\ \\left( 1 - v^{n} \\right) - \\left( a_{n\u23cbi} - nv^{n} \\right) \\\\ \\end{array}}{i}\\] \\[(Da)_{n\u23cbi} = \\frac{n - nv^{n} - a_{n\u23cbi} + nv^{n}}{i}\\] \\[(Da)_{n\u23cbi} = \\frac{n - a_{n\u23cbi}}{i}\\]","title":"Special case: Unit Decreasing"},{"location":"1.%20Introductory/ASA-FM/Test/#continuously-increasing-annuities","text":"Annuities that increase continuously rather than at discrete times Rate of payment can be described as a function of \\(\\mathbf{t}\\) (Since \\(t\\) is always increasing as well)","title":"Continuously Increasing Annuities"},{"location":"1.%20Introductory/ASA-FM/Test/#deriving-continuously-increasing-annuities","text":"Similar to the level version, we integrate the payments and the accumulation function: \\[\\left( \\overline{Ia} \\right)_{n\u23cbi} = \\ \\int_{0}^{n}{f(t)*e^{\\delta t}\\ dt}\\] \\[PV = \\ \\int_{0}^{n}{f(t)*e^{\\int_{0}^{t}{\\delta_{r}\\ dr}}\\ dt}\\] Tabular integration : Differentiate Integrate \\( \\(f(t)\\) \\) \\( \\(\\int_{}^{}e^{\\delta t}\\) \\) \\( \\(f^{'}(t)\\) \\) \\( \\(\\int_{}^{}{\\int_{}^{}e^{\\delta t}}\\) \\) \\( \\(\\vdots\\) \\) \\( \\(\\vdots\\) \\) \\( \\(1\\) \\) \\( \\(\\int_{}^{}{\\ldots\\int_{}^{}e^{\\delta t}}\\) \\)","title":"Deriving Continuously increasing Annuities"},{"location":"1.%20Introductory/ASA-FM/Test/#perpetuities","text":"Annuities whose payments go on forever (No end date) All formulas can be derived by letting \\(\\mathbf{n \\rightarrow \\infty}\\)","title":"Perpetuities"},{"location":"1.%20Introductory/ASA-FM/Test/#level-annuities","text":"[Annuity Due]{.underline} [Annuity Immediate]{.underline} [Continuous Annuity]{.underline} \\( \\({\\ddot{a}}_{n\u23cbi} = \\frac{1 - v^{n}}{d}\\) \\) \\( \\(a_{n\u23cbi} = \\frac{1 - v^{n}}{i}\\) \\) \\( \\({\\overline{a}}_{n\u23cbi} = \\frac{1 - v^{n}}{\\delta}\\) \\) \\( \\({\\ddot{a}}_{\\infty \u23cbi} = \\frac{1}{d}\\) \\) \\( \\(a_{\\infty \u23cbi} = \\frac{1}{i}\\) \\) \\( \\({\\overline{a}}_{\\infty \u23cbi} = \\frac{1}{\\delta}\\) \\)","title":"Level Annuities"},{"location":"1.%20Introductory/ASA-FM/Test/#non-level-annuities","text":"+-----------------------------------+-----------------------------------+ | [Geometric | **[Arithmetic | | Annuities]{.underline} | Annuities]{.underline}** | +===================================+===================================+ | \\( (PV_{Geome | ( \\(PV_{Arithmetic} = \\ | | tric} = \\frac{1 - \\left( \\frac{1 | P*a_{n\u23cbi} + Q\\left( \\frac{a_{n\u23cbi} | | + k}{1 + i} \\right)^{n}}{i - k}\\) \\) | }{i} - \\frac{nv^{n}}{i} \\right)\\) \\) | | | | | \\( (PV_{Geomet | ( \\(PV_{Arithmetic,\\in | | ric,\\ \\infty} = \\frac{1}{i - k}\\) \\) | fty} = P*a_{\\infty \u23cbi} + Q\\left( | | | \\frac{a_{\\infty \u23cbi}}{i} \\right)\\) \\) | | If \\(\\mathbf{k}\\) is | | | negative , the denominator nets | \\( \\(PV_{Arithmetic | | out to **become a plus instead** | ,\\infty} = P*\\frac{1}{i} + Q\\left | | | ( \\frac{\\frac{1}{i}}{i} \\right)\\) \\) | | | | | | \\( \\(PV_{Arithmetic,\\infty} | | | = \\frac{P}{i} + \\frac{Q}{i^{2}}\\) \\) | +-----------------------------------+-----------------------------------+","title":"Non-level Annuities"},{"location":"1.%20Introductory/ASA-FM/Test/#important-concepts-for-perpetuities","text":"The difference between two perpetuities is always an Annuity . Since both payments go on forever, the difference is simply when they start The present value of a Perpetuity at any time is always the same. This is because the payments go forever, so it doesn't matter when it starts We can approximate Perpetuities in the TVM by setting the number of years to be 9999, which is sufficiently large to approximate such cashflows","title":"Important concepts for Perpetuities"},{"location":"1.%20Introductory/ASA-FM/Test/#other-important-annuity-patternsunderline","text":"","title":"[Other Important Annuity Patterns]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#block-payments-for-any-increasedecrease-very-powerful-concept","text":"Annuities that pay out in blocks. Payments within blocks are the same but each block has a different set of payments We can treat each block as a deferred annuity, but a faster way is to add/subtract them If we start from the date furthest from the comparison date and add the change in payment levels, the final result is the PV/FV of the entire annuity {width=\"6.268055555555556in\" height=\"2.222916666666667in\"}","title":"Block Payments (For any increase/decrease -- very powerful concept)"},{"location":"1.%20Introductory/ASA-FM/Test/#repeat-reverse-annuities-for-increasesdecreases-of-1","text":"Annuities that start of increasing, reach a peak, and decrease back to the start We could split into an increasing and decreasing annuity, but another faster method would be to recognise that they are all the same deferred annuity : {width=\"6.268055555555556in\" height=\"3.08125in\"}","title":"Repeat Reverse Annuities (For increases/decreases of 1)"},{"location":"1.%20Introductory/ASA-FM/Test/#odd-even-split-for-alternating-increases","text":"Annuities that follow a cyclical pattern (Increase in odd and decrease in even years) We can split them into a level annuity and an increasing annuity with different intervals: {width=\"6.268055555555556in\" height=\"0.9333333333333333in\"}","title":"Odd-Even Split (For alternating increases)"},{"location":"1.%20Introductory/ASA-FM/Test/#annuities-into-perpetuities","text":"","title":"Annuities into Perpetuities:"},{"location":"1.%20Introductory/ASA-FM/Test/#decreasing-annuity-into-a-perpetuity","text":"Annuities that decrease till a certain level then becomes a level perpetuity We could treat each block as a decreasing annuity and deferred perpetuity But a faster way would be to treat them as a perpetuity and unit decreasing annuity : \\[PV = \\frac{k}{i} + (Da)_{\\overline{n - k}\u2142i}\\] \\[PV = \\frac{k}{i} + \\frac{n - k + a_{\\overline{n}\u2142i}}{i}\\] \\[PV = \\frac{n + a_{\\overline{n}\u2142i}}{i}\\] {width=\"6.268055555555556in\" height=\"1.8979166666666667in\"}","title":"Decreasing Annuity into a Perpetuity:"},{"location":"1.%20Introductory/ASA-FM/Test/#increasing-annuity-into-perpetuity","text":"Annuities that increase till a certain level then become a level perpetuity We could treat each block as an increasing annuity then deferred perpetuity But a faster way would be to treat them as the difference of Unit Increasing Perpetuities : \\[PV = (Ia)_{\\overline{\\infty}\u2142i} - v^{n}(Ia)_{\\overline{\\infty}\u2142i}\\] \\[PV = (Ia)_{\\overline{\\infty}\u2142i}\\left( 1 - v^{n} \\right)\\] \\[PV = \\left( \\frac{1}{i} + \\frac{1}{i^{2}} \\right)\\left( 1 - v^{n} \\right)\\] \\[PV = \\left( \\frac{1 + i}{i^{2}} \\right)\\left( 1 - v^{n} \\right)\\] \\[PV = \\frac{1}{i}*\\frac{1 - v^{n}}{d}\\] \\[PV = \\frac{{\\ddot{a}}_{\\overline{n}\u2142i}}{i}\\] {width=\"6.268055555555556in\" height=\"2.3256944444444443in\"}","title":"Increasing Annuity into perpetuity"},{"location":"1.%20Introductory/ASA-FM/Test/#annuities-with-unknown-number-of-payments","text":"Annuities that can be split into different smaller annuities We could treat it as one large annuity, but we may not have all the information needed (Typically, this is used for questions where \\(n\\) is not stated) But a faster method would be to treat each section as its own Annuity (Based on what information we know) and then discount them accordingly: \\[a_{3n\u23cbi} = a_{n\u23cbi} + v^{n}*a_{n\u23cbi} + v^{2n}*a_{n\u23cbi}\\] \\[a_{Xn\u23cbi} = a_{n\u23cbi} + v^{n}*a_{n\u23cbi} + v^{2n}*a_{n\u23cbi} + \\ldots v^{Xn}*a_{n\u23cbi}\\] {width=\"6.268055555555556in\" height=\"2.775in\"}","title":"Annuities with unknown number of payments"},{"location":"1.%20Introductory/ASA-FM/Test/#matching-payment-and-interest-frequencies","text":"For us to use ALL of the above annuity formulas, the rates we use MUST MATCH the frequency of the payments (EG. Monthly interest with Monthly Payments) If they are not in sync, we have two methods to make them sync Converting Interest Rates Using the principle of Equivalent Rates (Chapter 1) Generally preferred, but not applicable for symbolic questions Converting Payments Lower to higher payment frequency \u2192 Expand payments Higher to lower payment frequency \u2192 Collapse payments","title":"Matching Payment and Interest frequencies"},{"location":"1.%20Introductory/ASA-FM/Test/#lower-to-higher-payment-frequency","text":"Payments occur less frequently than the effective rate given EG. Quarterly Payments but Monthly Interest \u2192 Convert to monthly payment We can split the payments into \\(k\\) smaller payments of \\(X\\) to be aligned with the given rate We must keep the PV/FV of these payments to be equal to the original payment. We can use the level annuity formulas to calculate the smaller payment \\(X\\) Since \\(X\\) matches the given rates, we can use both in our annuity formulas","title":"Lower to Higher payment frequency"},{"location":"1.%20Introductory/ASA-FM/Test/#deriving-formulas","text":"+-----------------------------------+-----------------------------------+ | Payment at the start of | **Payment at the end of Period | | period** | | +===================================+===================================+ | \\( \\(P = X*a_{k\u23cbi}\\) \\) | \\( \\(P = X*s_{k\u23cbi}\\) \\) | | | | | \\( \\(X = \\frac{P}{a_{k\u23cbi}}\\) \\) | \\( \\(X = \\frac{P}{s_{k\u23cbi}}\\) \\) | +-----------------------------------+-----------------------------------+ | \\( \\(PV = X*a_{n\u23cbi}\\) \\) | \\( \\(PV = X*a_{n\u23cbi}\\) \\) | | | | | $ | $ | | \\(PV = \\frac{P}{a_{k\u23cbi}}*a_{n\u23cbi}\\) $ | \\(PV = \\frac{P}{s_{k\u23cbi}}*a_{n\u23cbi}\\) $ | | | | | $ | $ | | \\(PV = P*\\frac{a_{n\u23cbi}}{a_{k\u23cbi}}\\) $ | \\(PV = P*\\frac{a_{n\u23cbi}}{s_{k\u23cbi}}\\) $ | +-----------------------------------+-----------------------------------+ | \\( \\(FV = X*s_{n\u23cbi}\\) \\) | \\( \\(FV = X*s_{n\u23cbi}\\) \\) | | | | | $ | $ | | \\(FV = \\frac{P}{a_{k\u23cbi}}*s_{n\u23cbi}\\) $ | \\(FV = \\frac{P}{s_{k\u23cbi}}*s_{n\u23cbi}\\) $ | | | | | $ | $ | | \\(FV = P*\\frac{a_{n\u23cbi}}{a_{k\u23cbi}}\\) $ | \\(FV = P*\\frac{s_{n\u23cbi}}{s_{k\u23cbi}}\\) $ | +-----------------------------------+-----------------------------------+","title":"Deriving Formulas"},{"location":"1.%20Introductory/ASA-FM/Test/#higher-to-lower-payment-frequency","text":"Payments occur more frequently than the effective rate given EG. Monthly payments but yearly interest given \u2192 Convert to Yearly payment We can combine \\(k\\) smaller payments into one payment of \\(X\\) be aligned with the given rate We must keep the PV/FV of these payments to be equal to the original payment. We can use the level annuity formulas to calculate the larger payment \\(X\\) <!-- --> - Since \\(X\\) matches the given rates, we can use both in our annuity formulas","title":"Higher to Lower payment frequency"},{"location":"1.%20Introductory/ASA-FM/Test/#deriving-formulas_1","text":"We first show the equivalent rates: \\[\\left( 1 + i^{(m)} \\right)^{k} = (1 + i)\\] \\[i = \\left( 1 + i^{(m)} \\right)^{k} - 1\\] We then accumulate to the end of the period: \\[X = P*s_{n\u23cbi}\\] \\[X = \\ P*\\frac{\\left( 1 + i^{(m)} \\right)^{k} - 1}{i^{(m)}}\\] \\[X = P*\\frac{i}{i^{(m)}}\\] \\[PV = X*a_{n\u23cbi}\\] \\[PV = \\ P*\\frac{i}{i^{(m)}}*a_{n\u23cbi}\\]","title":"Deriving Formulas"},{"location":"1.%20Introductory/ASA-FM/Test/#chapter-3-spot-forward-and-interest-swapsunderline","text":"","title":"[Chapter 3: Spot, Forward and Interest Swaps]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#spot-forward-rates","text":"Rates offered are likely to change in accordance with the investment horizon. Different periods of investment will result in different rates There are two main kinds of rates: Spot \\(\\left( \\mathbf{s}_{\\mathbf{t}} \\right)\\) \u2192 From today to some future date \\(\\left( \\mathbf{0\\ to\\ t} \\right)\\) <!-- --> - Forward \\(\\left( \\mathbf{f}_{\\mathbf{t - 1,t}} \\right)\\) \u2192 From a future date to 1 year ahead \\(\\left( \\mathbf{t - 1\\ to\\ t} \\right)\\)","title":"Spot &amp; Forward Rates"},{"location":"1.%20Introductory/ASA-FM/Test/#linking-the-two-rates","text":"Both rates are quoted Annual Effective . This is important for Spot rates, as it spans across more than one period: \\(a(t) \\neq \\left( 1 + s_{t} \\right)\\) \\(a(t) = \\left( 1 + s_{t} \\right)^{t}\\) <!-- --> - Using the concept of Equivalent Rates, we can convert between the two by: - $\\left( 1 + s_{t} \\right)^{t} = \\left( 1 + s_{t - 1} \\right)^{t - 1}\\left( 1 + f_{t} \\right)$ - **OR** $\\left( 1 + s_{t} \\right)^{t} = \\left( 1 + f_{1} \\right)\\left( 1 + f_{2} \\right)\\ldots\\left( 1 + f_{t} \\right)$ - By definition, $s_{1} \\equiv f_{1}$ Using these formulas, we can form the accumulation function and solve cashflow problems using first principles by manually accumulating/discounting every cashflow Forward Rates calculated this way are known as the Implied Rates. They are an estimate of what will be the actual 1-year spot rate during that time , known as the Quoted Rates. Where possible, we should use the Quoted Rate over the implied forward rates.","title":"Linking the two rates"},{"location":"1.%20Introductory/ASA-FM/Test/#forward-rates-lasting-more-than-one-period","text":"They are like spot rates in the sense that they last more than one period. Thus, we can use the same principle of equivalent rates to calculate them: Using spot rates: \\(\\left( 1 + s_{t + m} \\right)^{t + m} = \\left( 1 + s_{t} \\right)^{t}*\\left( 1 + f_{t,t + m} \\right)^{m}\\) Using forward rates: \\(\\left( 1 + f_{t,t + m} \\right)^{m} = \\left( 1 + f_{t,t + 1} \\right)*\\ldots\\left( 1 + f_{t + m - 1,t + m} \\right)\\) Sometimes, we are given forward rates for cashflows that occur before the forward rate period. This means that we should assume the cashflow is reinvested at that forward rate and we should find the value after reinvestment","title":"Forward rates lasting more than one period"},{"location":"1.%20Introductory/ASA-FM/Test/#interest-rate-swaps","text":"Two counterparties currently have two kinds of liabilities with different style of interests: Party A \u2192 Variable Interest Party B \u2192 Fixed Interest Each party would now like the other style of payment for various reasons: Hedge against interest rates Predictability of payments One solution would be to Swap their interest payments. However, since they can't directly swap their contract, they can achieve a similar outcome by paying for the other party: Party A will pay the fixed rate to party B ( Payer ) Party B will pay the variable rate to party A ( Receiver )","title":"Interest Rate Swaps"},{"location":"1.%20Introductory/ASA-FM/Test/#swap-mechanics","text":"Fixed payments will be made at a constant Swap Rate \\(\\left( \\mathbf{R} \\right)\\) at periodic intervals known as the Settlement Period Variable payments will be made at the prevailing rates at the time, modelled by using the current Forward Rates \\(\\left( \\mathbf{f}_{\\mathbf{n}} \\right)\\) Since both parties may not have identical principals, the payments are made according to some common Notional Amount \\(\\left( \\mathbf{X}_{\\mathbf{n}} \\right)\\) Constant Notional \u2192 Level Swap Increasing Notional \u2192 Accreting Swap Decreasing Notional \u2192 Amortizing Swap To ensure that the deal is fair, the PV of both Cashflows must be the same: \\[PV(Fixed\\ Payments) = PV(Variable\\ Payments)\\] \\[\\frac{X_{1}R}{\\left( 1 + s_{1} \\right)} + \\ldots\\frac{X_{n}R}{\\left( 1 + s_{n} \\right)^{n}} = \\frac{X_{1}f_{1}}{\\left( 1 + s_{1} \\right)} + \\ldots\\frac{X_{n}f_{n}}{\\left( 1 + s_{n} \\right)^{n}}\\]","title":"Swap Mechanics"},{"location":"1.%20Introductory/ASA-FM/Test/#deferred-swaps","text":"Similar concept to a Deferred Annuity. However, when an SOA says \\(\\mathbf{n}\\) -year deferred , \\(\\mathbf{m}\\) -year interest rate swaps, the actual swap occurs for \\(\\mathbf{n - m}\\) years after \\(\\mathbf{n}\\) years In other words, the swap period defined by SOA includes the deferral as well","title":"Deferred Swaps"},{"location":"1.%20Introductory/ASA-FM/Test/#level-swap-shortcuts","text":"There are two shortcuts available for Level Swaps. If the swap is Accreting or Amortizing, we will have to perform the calculations manually The first shortcut is factoring out X : If the swap is level, \\(X_{1} = X_{2} = \\ldots = X_{n}\\) , which allows it to be factored out from both sides of the equation and cancelled This simplifies the calculation to just using the interest rates The second is to use the Bond pricing formula: This method uses an entirely different approach to calculating the swap rate We consider the net cashflows for Party A , including their current obligation. We notice that the resulting cashflows looks very similar to that of a bond, thus we can make use of that formula to calculate R as well! {width=\"6.268055555555556in\" height=\"4.055555555555555in\"} Bond Pricing Formula: \\[Price\\ of\\ Bond = PV(Coupon\\ Payments) + PV(Principal)\\] \\[X = \\frac{XR}{\\left( 1 + s_{1} \\right)} + \\ldots + \\frac{XR}{\\left( 1 + s_{n} \\right)^{n}} + \\frac{X}{\\left( 1 + s_{n} \\right)^{n}}\\] Since level swap, we can factorise & cancel out X: \\[1 = R\\left( \\frac{1}{\\left( 1 + s_{1} \\right)} + \\ldots + \\frac{1}{\\left( 1 + s_{n} \\right)^{n}} \\right) + \\frac{1}{\\left( 1 + s_{n} \\right)^{n}}\\] If it is a deferred swap , we all we need to do is to discount the LHS to the first period as well.","title":"Level Swap Shortcuts"},{"location":"1.%20Introductory/ASA-FM/Test/#other-swap-calculationsunderline","text":"","title":"[Other Swap Calculations]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#net-swap-payments","text":"Since both parties pay each other on the same day, it is redundant to make payments Instead, it is more efficient for one party to just make a Net Swap Payment instead It is important to consider whose perspective we are taking : An outflow for one party is an Inflow for another","title":"Net Swap Payments"},{"location":"1.%20Introductory/ASA-FM/Test/#net-interest-payment","text":"We want to consider how much one party pays in total on the settlement date There are two cashflows from their perspective: Net Swap Payment Original Interest Payment The Net Interest Payment is simply the sum of these two cashflows, which is how much they end up paying in total Similarly, it is also important to consider whose perspective when computing the net interest payment","title":"Net Interest Payment"},{"location":"1.%20Introductory/ASA-FM/Test/#market-value","text":"One may choose to leave the position before the end of the Swap Term In order to sell it to another person, we must know the market value of the contract We can calculate it by computing the Net Present Value of all future Net Swap Payments By definition, the Market Value of a contract at time 0 will be 0 because \\(\\mathbf{NPV = 0}\\) However, due to interest rates changing (Our projected forward rate not being the actual rate come the appropriate time), the market value of the contract at some later time is not likely to be 0 We should always use the prevailing Spot Rates at the time we are calculating the market value instead of the forward rates we predicted in the past It is important to consider whose perspective we are looking at, as one person's gain is another person's loss","title":"Market Value"},{"location":"1.%20Introductory/ASA-FM/Test/#why-interest-rate-swaps","text":"Typically, the payer of the swap rate currently has an existing loan with interest payments based on some floating rate The payer believes that interest rates will rise, hence wants to pay a fixed swap rate to hedge against these rising interest rates The receiver believes that interest rates will fall, hence is willing to pay the lower variable rate","title":"Why interest rate swaps?"},{"location":"1.%20Introductory/ASA-FM/Test/#chapter-4-rates-of-returnunderline","text":"","title":"[Chapter 4: Rates of Return]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#one-period-return-ratesunderline","text":"","title":"[One Period Return Rates]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#time-weighted-rate-of-return-twrr","text":"Let R be sub-period return rate: \\[R_{n} = \\frac{Balance\\ before\\ new\\ transaction}{Balance\\ after\\ previous\\ transaction} = \\frac{B_{n}^{Before}}{B_{n - 1}^{After}}\\] Using the concept of equivalent rates, we can solve for the TWRR \\[\\left( R_{1} \\right)\\left( R_{2} \\right)\\ldots\\left( R_{n} \\right) = \\left( 1 + i_{TWRR} \\right)\\] Balance before 0 \\( \\(B_{1}^{Before}\\) \\) \\( \\(\\ldots\\) \\) \\( \\(B_{n - 1}^{Before}\\) \\) \\( \\(B_{n}^{After}\\) \\) transaction Transaction Initial Deposit First Transaction \\( \\(\\ldots\\) \\) Final Transaction (Injection/Withdrawal) Balance after \\( \\(B_{0}^{After}\\) \\) \\( \\(B_{1}^{After}\\) \\) \\( \\(\\ldots\\) \\) \\( \\(B_{n - 1}^{After}\\) \\) transaction Sub-period return rate \\( \\(R_{1}\\) \\) \\( \\(R_{2}\\) \\) \\( \\(R_{n - 1}\\) \\) \\( \\(R_{n}\\) \\)","title":"Time Weighted Rate of Return (TWRR)"},{"location":"1.%20Introductory/ASA-FM/Test/#dollar-weighted-rate-of-return-dwrr","text":"DWRR is essentially the IRR of the fund : \\[B_{Start}\\left( 1 + i_{DWRR} \\right)^{1} + CF_{1}\\left( 1 + i_{DWRR} \\right)^{\\frac{11}{12}} + \\ldots + CF_{n}\\left( 1 + i_{DWRR} \\right)^{\\frac{(1 - n)}{n}} = B_{End}\\] However, as seen earlier, IRR is numerically complicated to compute Thus, we consider a Simple Interest Approximation instead: \\[B_{Start}\\left( 1 + i_{DWRR} \\right) + CF_{1}\\left( 1 + \\frac{11}{12}i_{DWRR} \\right) + \\ldots + CF_{n}\\left( 1 + \\frac{(1 - n)}{n}i_{DWRR} \\right) = B_{End}\\] \\[i_{DWRR} = \\frac{B_{End} - B_{Start} - \\sum_{}^{}{CF_{t}}}{B_{Start} + \\sum_{}^{}{CF_{t}(1 - t)}}\\] For time spans of exactly one-year , this simple interest approximation will be equivalent to the one-year IRR of the fund Can be verified using Desmos by plotting \\(y = \\left( 1 + i_{IRR} \\right)^{x}\\) and \\(y = \\left( 1 + i_{DWRR}x \\right)\\) For any \\(i_{IRR}\\ = i_{DWRR}\\) , the intersection point will always occur at \\(x = 1\\) Phrased another way, as long as \\(x = t = 1,\\) \\(i_{DWRR} = i_{IRR}\\) <!-- --> - Notice that this formula is extremely similar to the regular interest formula . This means that if the cashflows are sufficiently small , the simple interest approximation method of DWRR can be used to estimate the true annual effective rate Note: Investment Income refers to the interest earned on all the cashflows during the period.","title":"Dollar Weighted Rate of Return (DWRR)"},{"location":"1.%20Introductory/ASA-FM/Test/#differences-between-each-measure","text":"TWRR makes use of Compound Interest while DWRR uses simple interest approximation TWRR is a measure of the Fund Managers Performance , while DWRR is a measure of the Overall Performance of the Fund TWRR removes the effect of the timing and additional transactions. This is similar to how a fund manager does not control over when they receive funds and how much , thus is used to approximate the fund manager performance If DWRR > TWRR, it means that the timing of deposits/withdrawals was very good -- deposited right before a high growth period after or withdrawn before a crash","title":"Differences between each measure:"},{"location":"1.%20Introductory/ASA-FM/Test/#different-timespans","text":"The above formulas use timespans of one year to compute the rates Thus, the calculated TWRR and DWRR are naturally assumed to be annual 1-year rate However, some questions may specify a different timespan -- 6 months or 2-year The most important thing is to MAINTAIN the formula For TWRR, maintain the LHS as just \\((1 + i)\\) and for DWRR, keep the coefficients of \\(i\\) to be under 1 (Proportion till the end of the given time) This way, the rates calculated will be the 6-month or 2-year rate Using the concept of equivalent rates , we will need to convert these 6-month or 2-year rates into annual effective 1-year rates ( True TWRR/DWRR)","title":"Different Timespans"},{"location":"1.%20Introductory/ASA-FM/Test/#multiple-periods-return-ratesunderline","text":"","title":"[Multiple periods return rates]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#arithmetic-mean","text":"Given multiple one period returns \\(\\left( R_{1}\\ldots R_{n} \\right)\\) , the arithmetic mean rate of return is the average of all of them It is a measure of the average fund performance if you were to hold it for one year. Some say that this measure is overly optimistic for long time horizons \\[R_{Arithmetic} = \\frac{R_{1} + \\ldots R_{n}}{n}\\]","title":"Arithmetic Mean"},{"location":"1.%20Introductory/ASA-FM/Test/#geometric-mean-aka-cumulative-annual-growth-rate-cagr","text":"Given multiple one period returns \\(\\left( R_{1}\\ldots R_{n} \\right)\\) , the geometric mean rate of return is the annualized effective rate of all of them It is a measure of the expected annualized rate if you were to hold it for \\(\\mathbf{n}\\) years. Some say that this is overly pessimistic for short time horizons \\[\\left( 1 + R_{1} \\right)\\ldots\\left( 1 + R_{n} \\right) = \\left( 1 + R_{Geometic} \\right)^{n}\\] Important: To find the variance of these return rates, we should use the Sample Variance formula : \\[s^{2} = \\frac{1}{n - 1}\\sum_{}^{}\\left( r_{i} - \\overline{r} \\right)\\]","title":"Geometric Mean (AKA Cumulative Annual Growth Rate -- CAGR)"},{"location":"1.%20Introductory/ASA-FM/Test/#discounted-cashflow-analysisunderline","text":"","title":"[Discounted Cashflow Analysis]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#net-present-value-npv","text":"It is the difference between the PV of Cash Inflows and Outflows . It thus measures the raw value created for the firm \\(NPV = PV(Inflows) - PV(Outflows)\\)","title":"Net Present Value (NPV)"},{"location":"1.%20Introductory/ASA-FM/Test/#internal-rate-of-return-irr","text":"The required interest rate that makes the NPV of a project zero . It represents the annual rate of return that will be earned \\(NPV = 0 \\rightarrow Solve\\ for\\ i\\ using\\ Financial\\ Calc\\) Can also be used to solve quadratic equations involving \\(v\\)","title":"Internal Rate of Return (IRR)"},{"location":"1.%20Introductory/ASA-FM/Test/#other-measures-of-return-fnce101underline","text":"","title":"[Other Measures of Return (FNCE101)]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#dollar-returns","text":"A measure of the absolute return on an investment over a period of time Poor measure of return since there is no indication if it is high or low \\[Dollar\\ Return = Income\\ from\\ Investment + Capital\\ Gain\\ or\\ Loss\\]","title":"Dollar Returns"},{"location":"1.%20Introductory/ASA-FM/Test/#percentage-return","text":"Measure of the percentage return on an investment over a period of time Better measure as it considers the beginning values of the assets \\[Percentage\\ Return = Dividend\\ Yield + Capital\\ Gain\\ Yield\\] \\[Percentage\\ Return = \\frac{Dividend}{Beginning\\ Price} + \\frac{Capital\\ Gain\\ or\\ Loss}{Beginning\\ Price}\\]","title":"Percentage Return"},{"location":"1.%20Introductory/ASA-FM/Test/#chapter-5-loansunderline","text":"","title":"[Chapter 5: Loans]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#what-is-a-loan","text":"When we borrow money from the bank or another company, we are taking a Loan The outstanding loan balance earns interest , increasing the amount we have to pay for each period we do not fully repay the loan We consider the standard scenario where loans are repaid at level amounts at regular intervals (Forming a Level Annuity) Like all other problems we have dealt with till now, the key principle is that the*\\ * \\( \\(\\mathbf{PV}\\left( \\mathbf{Loan\\ Amount} \\right)\\mathbf{= PV}\\left( \\mathbf{Loan\\ Payments} \\right)\\) \\) There are two components to each loan repayment: Interest repaid Principal repaid Each repayment made will first pay off any interest earned so far , and the remaining amount will pay off the principal We are interested in calculating the not only the repayment amount but the various components of the loan at each time to better understand the status of Debt {width=\"6.268055555555556in\" height=\"3.520138888888889in\"} {width=\"6.268055555555556in\" height=\"2.0243055555555554in\"}","title":"What is a loan?"},{"location":"1.%20Introductory/ASA-FM/Test/#loan-calculationsunderline","text":"","title":"[Loan Calculations]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#loan-variables","text":"\\(B_{t}\\) to denote the Outstanding Balance, where \\(B_{0}\\) is the principal \\(R_{t}\\) to denote the Repayment Amount (Assumed to be Constant) \\(I_{t}\\) to denote the amount that was used to pay off Interest \\(P_{t}\\) to denote the amount that was used to pay off the principal","title":"Loan Variables"},{"location":"1.%20Introductory/ASA-FM/Test/#three-ways-to-calculate-outstanding-loan-balanceunderline","text":"","title":"[Three ways to Calculate Outstanding Loan Balance]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#recursive-method","text":"Using first principals and manually calculating the change in each period: \\[B_{t} = B_{t - 1} - P_{t}\\] \\[B_{t} = B_{t - 1} + I_{t} - R\\] \\[B_{t} = B_{t - 1} + B_{t - 1}*i - R\\] \\[B_{t} = B_{t - 1}(1 + i) - R\\] Applying recursion, \\[B_{t} = \\left( B_{t - 2}(1 + i) - R \\right)(1 + i) - R\\] \\[\\vdots\\] \\[B_{t} = B_{0}(1 + i)^{t} - R - \\ldots R(1 + i)^{t - 1}\\] \\[B_{t} = B_{0}(1 + i)^{t} - R*s_{n\u23cbi}\\]","title":"Recursive Method"},{"location":"1.%20Introductory/ASA-FM/Test/#retrospective-method","text":"The outstanding balance is the Time adjusted difference of the principal & what was already paid : \\[B_{t} = B_{0}(1 + i)^{t} - R*s_{n\u23cbi}\\] Notice that this is the same result as the Recursive Method ; they are the same","title":"Retrospective Method"},{"location":"1.%20Introductory/ASA-FM/Test/#prospective-method","text":"The outstanding balance is the PV of all future loan payments : \\[B_{t} = R*a_{n - t\\ \u23cbi}\\] Note that this method can also be used if we have a future loan balance as well By discounting the Future Loan Balance and any payments between then and the current time , we obtain the Current Loan Balance {width=\"6.268055555555556in\" height=\"2.5555555555555554in\"} All three methods are equivalent and will lead to the same results. We need to know all three because it depends on what information the question gives us . It is faster to use a particular method if the question has already given us the relevant information.","title":"Prospective Method"},{"location":"1.%20Introductory/ASA-FM/Test/#loan-amortization","text":"In finance, Amortization means to spread payments (of a Loan, in this case) <!-- --> - The goal is to create an Amortization Schedule that shows how each Repayment is used to pay off both the Interest and Principal in each period , slowly reducing our Outstanding Balance to 0 at the end of the Loan For simplicity, we factor out all payments to 1 (Similar to our Annuity Proofs). Remember to multiply by the payment amount for actual questions","title":"Loan Amortization"},{"location":"1.%20Introductory/ASA-FM/Test/#recursiveretrospective-schedule","text":"Using the Recursive Method, we can determine each component in any given period However, this method is best done in Excel when the calculations can be performed quickly. For the exam, this may not be the best method +------+---------------+--------------+--------------+----------------+ | ** | Repayment | Interest | **Principal | **Loan | | Peri | | Repaid | Repaid** | Balance** | | od** | | | | | | | | $$I_{t} = B | $ | $ | | | | _{t - 1} i$$ | (P_{t} = R_{ | \\(B_{t} = B_{t | | | | | t} - I_{t}\\) \\) | - 1} - P_{t}$$ | +======+===============+==============+==============+================+ | 0 | | | | ( \\(B_{0}\\) \\) | +------+---------------+--------------+--------------+----------------+ | 1 | 1 | ( \\(B_{0}*i\\) \\) | ( (1 | ( \\(B_{0 | | | | | - B_{0}*i\\) \\) | }(1 + i) - 1\\) \\) | +------+---------------+--------------+--------------+----------------+ | $$ | ( \\(\\vdots\\) \\) | ( \\(\\vdots\\) \\) | ( \\(\\vdots\\) \\) | ( \\(\\vdots\\) \\) | | \\vdo | | | | | | ts$$ | | | | | +------+---------------+--------------+--------------+----------------+ | $ | 1 | $$B | $$1 - B | $ | | \\(n\\) $ | | _{n - 1}*i$$ | _{n - 1}*i$$ | \\(B_{0}(1 + i)^ | | | | | | {t} - s_{n\u23cb}\\) $ | +------+---------------+--------------+--------------+----------------+ | * | ( \\(n\\) \\) | ( (\\S | ( \\(n - \\S | | | *Tot | | igma I_{t}\\) \\) | igma I_{t}\\) \\) | | | al* | | | | | +------+---------------+--------------+--------------+----------------+","title":"Recursive/Retrospective Schedule"},{"location":"1.%20Introductory/ASA-FM/Test/#prospective-schedule","text":"Using the prospective method instead, we notice that the Loan Balance forms an Annuity Using this Annuity formula, we can see how the schedule becomes simplified: +-------+-----------+---------------+---------------+-----------------+ | Per | **Re | **Interest | **Principal | **Loan | | iod | payment** | Repaid** | Repaid** | Balance** | | | | | | | | | Factor to | \\( (I_{t} = | ( (P_{t} | ( \\(B_{t} | | | 1 | B_{t - 1}*i\\) \\) | = 1 - I_{t}\\) \\) | = a_{n - t\u23cbi}\\) \\) | | | | | | | | | | \\( (I | ( \\(P_{t} = v^ | | | | | _{t} = 1 - v^ | {n - t + 1}\\) \\) | | | | | {n - t + 1}\\) \\) | | | +=======+===========+===============+===============+=================+ | 0 | | | | \\( \\(a_{n\u23cbi}\\) \\) | +-------+-----------+---------------+---------------+-----------------+ | 1 | 1 | \\( \\(1 - v^{n}\\) \\) | \\( \\(v^{n}\\) \\) | \\( \\(a_{n - 1\u23cbi}\\) \\) | +-------+-----------+---------------+---------------+-----------------+ | $$\\vd | $ | \\( \\(\\vdots\\) \\) | \\( \\(\\vdots\\) \\) | \\( \\(\\vdots\\) \\) | | ots$$ | \\(\\vdots\\) $ | | | | +-------+-----------+---------------+---------------+-----------------+ | \\( \\(n\\) \\) | 1 | \\( \\(1 - v\\) \\) | \\( \\(v\\) \\) | \\( \\(a_{0\u23cbi}\\) \\) | +-------+-----------+---------------+---------------+-----------------+ | To | ( \\(n\\) \\) | $$ | ( \\(a_{n\u23cbi}\\) \\) | | | tal | | n - a_{n\u23cbi}$$ | | | +-------+-----------+---------------+---------------+-----------------+ From this insight, we can see that all we need is the interest rate and repayment amount to identify any component If we are given the principal repaid in one period , we can use the accumulate or discount it accordingly to find the principal repaid in another Principal Payments are proportional -- \\(\\frac{Principal_{t}}{Principal_{t + 2}} = \\frac{Principal_{t + n}}{Principal_{t + n + 2}} = v^{2}\\) The principal payments can also be viewed in a timeline:","title":"Prospective Schedule"},{"location":"1.%20Introductory/ASA-FM/Test/#varying-interest-rates_1","text":"If interest rates change midway, the remaining schedule has to be changed as well In particular, we have to recalculate the remaining number of payments needed. Using this, we can calculate the remaining loan balance after the interest rate change However, the shortcut method still works -- just use the appropriate interest rates at the time when they change","title":"Varying Interest Rates"},{"location":"1.%20Introductory/ASA-FM/Test/#other-payment-patternsunderline","text":"","title":"[Other Payment Patterns]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#level-principal-payments","text":"Instead of having level overall payments, this method of payment ensures that the principle decreases by a flat amount in each period We can determine the amount at which it decreases by taking the loan balance divided by the number of remaining payments Given this information, we are able to easily calculate the outstanding balance , and thus the Interest payment and subsequently the overall repayment amount each period \\[Principal\\ Repayment\\ each\\ period = \\frac{B_{0}}{n}\\] \\[Outstanding\\ Balance\\ each\\ period = \\frac{n - t + 1}{n}B_{0}\\] \\[Total\\ Repayment\\ each\\ period = i\\left( \\frac{n - t}{n} \\right)B_{0} + \\frac{B_{0}}{n}\\] It is important to note that the outstanding loan balance forms a decreasing arithmetic progression . We can make use of this to easily calculate total interest and hence total amount paid: \\(Total\\ Interest = i*\\left( B_{0} + B_{1} + B_{2} + \\ldots 0 \\right)\\) \\(Sum\\ of\\ Arithmetic\\ progression = \\frac{N}{2}(First\\ Term + Last\\ Term)\\) \\(Total\\ Amount\\ Paid = Total\\ Interest + Loan\\ Amount\\) A few key insights: Since the outstanding balance falls each period , the interest falls each period as well. This means that the repayments fall each period The repayment amount will be initially higher than the flat repayment amount, but will fall and become smaller than it Since the total repayment amount falls over time, this means that you will pay lesser compared to the typical method","title":"Level Principal Payments"},{"location":"1.%20Introductory/ASA-FM/Test/#non-level-payments","text":"Payments that follow an arithmetic or geometric annuity Use their respective PV/FV formulas to find the loan balance at every time With the various loan balances, we should focus on using the recursive method : \\(Balance_{t + 1} = Balance_{t} - P_{t + 1}\\) \\(P_{t + 1} = \\ Balance_{t + 1} - \\ Balance_{t}\\) Questions may look the same, but the approach is completely different If the question has a balloon/drop payment involved , always calculate the remaining balance in the period before the balloon/drop . Then ROLL FORWARD the balance to obtain the Ballooned or Dropped Payment","title":"Non-Level Payments"},{"location":"1.%20Introductory/ASA-FM/Test/#missing-or-additional-payments","text":"The general schedule above assumes that payments are made every period. However, if payments are missed or additional payments are made , then the Loan will take longer/shorter to amortize If payments are missed , then the Loan balance at the end of the usual loan period will still be positive \u2192 How much? If additional payments were made, then the Loan Balance will hit 0 before the usual loan period \u2192 How Much/When? In either case, we can easily visualize the problem by drawing out the timeline to better visualize the problem +-----------------------------------+-----------------------------------+ | [Missing | **[Additional | | Payments]{.underline} | Payments]{.underline}** | +===================================+===================================+ | The key idea is that the | The key idea is that the | | present/future value will | Present/future value MUST | | **NOT tally with the original | tally** with the original payment | | payment schedule. | schedule. | | | | | Using this idea, the difference | Using this idea, we can form two | | in the two values is the | equations and set them equal | | remaining amount of the loan. | to each other to solve for the | | | missing value we are interested | | A key shortcut understanding | in. | | is that if payments are missed, | | | the outstanding Loan Balance will | Given the additional payment | | be the sum of the accumulated | amount, we can solve for the new | | value of any missing payments | ending time. | | | | | Using the same concept, if we are | Given the new ending time | | given the Outstanding Balance at | instead, we can solve for the | | various points instead, we can | additional payment amount. | | roll forward the loan balance | | | assuming no payments were made. | | | We then use Retrospective | | | Method to calculate the | | | remaining Loan Balance instead. | | +-----------------------------------+-----------------------------------+","title":"Missing or Additional Payments"},{"location":"1.%20Introductory/ASA-FM/Test/#payments-that-scale-with-interest","text":"Some loan payments will scale based on the interest amount EG. Equal to 100% of interest payments. 150% of interest payments etc. There are some special insights to take note of: 100% of interest \u2192 No principal is repaid; loan balance stays the same 150% of interest \u2192 Balance falls by \\((150 - 100)\\%*Interest\\) every period \\(\\mathbf{90\\%}\\) of interest \u2192 Balance increases by \\((100 - 90)\\%*Interest\\) every period","title":"Payments that scale with Interest"},{"location":"1.%20Introductory/ASA-FM/Test/#other-situations","text":"Different Interest Rates for Interest payments and discount \u2192 Draw timeline Cost of loan \u2192 Reduces the principal amount to control the yield rate","title":"Other situations:"},{"location":"1.%20Introductory/ASA-FM/Test/#chapter-6-bonds-and-stocksunderline","text":"","title":"[Chapter 6: Bonds and Stocks]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#what-is-a-bond","text":"A bond is a special type of Loan: The party who issues the bond (Bond Issuer) borrows money from the party who purchases the bought the bond (Bond Investor) However, Bonds have a different payment structure : Every period \u2192 Coupon payments These coupon payments are based off interest on the Face Amount/Par Value of the bond, which is simply a notional amount for calculations Last period \u2192 Redemption Value If not specified otherwise, the Redemption value is equal to the Face Amount/Par Value. It is paid on-top of the coupon payment Thus, unlike a Loan who has Interest and Principal payments every period, Bonds repay the interest every period and only repays the principal at the end in a lump sum {width=\"6.268055555555556in\" height=\"1.6166666666666667in\"}","title":"What is a Bond?"},{"location":"1.%20Introductory/ASA-FM/Test/#bond-valuation","text":"Let us use the following notations: \\(\\mathbf{P}\\) is the Price of the Bond \\(\\mathbf{r}\\) is the Coupon Rate (Quoted Annual Nominal Rate) \\(\\mathbf{F}\\) is the Face Amount \\(\\mathbf{C}\\) is the Call Value / Redemption Value \\(\\mathbf{i}\\) is the interest rate equivalent to Yield to Maturity (Quoted Annual Nominal Rate) \\[Bond\\ Price = PV(Coupons) + PV(Redemption\\ Value)\\] \\[\\mathbf{\\therefore P = Fr*}\\mathbf{a}_{\\mathbf{n\u23cbi}}\\mathbf{+ C*}\\mathbf{v}^{\\mathbf{n}}\\]","title":"Bond Valuation"},{"location":"1.%20Introductory/ASA-FM/Test/#dirty-prices","text":"When valuing a Bond, we typically assume that the bond is being valued 6-months or 1-year intervals from the issue date. Prices valued this way are known as the Quoted Price However, if we purchase a Bond, it is likely that we will buying it in between valuation dates . We will thus have to adjust the price, known as the Dirty Price The idea is that we must compensate the previous bondholder for the additional time after the previous coupon payment by paying him a corresponding portion of the Coupon \\[\\mathbf{Dirty\\ Price} = Quoted\\ Price + Accrued\\ Interest\\] \\[\\mathbf{Accrued\\ Interest} = \\frac{Number\\ of\\ Days\\ since\\ Coupon}{Number\\ of\\ days\\ between\\ Coupons}*Coupon\\] \\[\\mathbf{Alternative\\ Dirty\\ Price} = Quoted\\ Price*(1 + i)^{\\frac{Number\\ of\\ Days\\ since\\ Coupon}{Number\\ of\\ days\\ between\\ Coupons}}\\]","title":"Dirty Prices"},{"location":"1.%20Introductory/ASA-FM/Test/#bond-premium-discount","text":"We recall the Annuity Immediate formula: \\[a_{n\u23cbi} = \\frac{1 - v^{n}}{i}\\] \\[v^{n} = 1 - i*a_{n\u23cbi}\\] Using this formula, we can rewrite our Bond Price formula: \\[P = Fr*a_{n\u23cbi} + C*v^{n}\\] \\[P = Fr*a_{n\u23cbi} + C*\\left( 1 - i*a_{n\u23cbi} \\right)\\] \\[P = Fr*a_{n\u23cbi} + C - Ci*a_{n\u23cbi}\\] \\[P = C + (Fr - Ci)*a_{n\u23cbi}\\] Where \\((Fr - Ci)*a_{n\u23cbi}\\) is the Premium/Discount of the Bond We consider the difference in the Price of the Bond & the Redemption Value \\(P - C,\\) +----------------------+----------------------+-----------------------+ | [Premium | **[Discount | **[Par | | Bond]{.underline} | Bond]{.underline}** | Bond]{.underline}** | +======================+======================+=======================+ | \\( \\(P - C > 0\\) \\) | \\( \\(P - C < 0\\) \\) | \\( \\(P - C = 0\\) \\) | | | | | | \\( ((Fr | ( ((Fr | ( \\((F | | - Ci)*a_{n\u23cbi} > 0\\) \\) | - Ci)*a_{n\u23cbi} < 0\\) \\) | r - Ci)*a_{n\u23cbi} = 0\\) \\) | | | | | | \\( \\((Fr - Ci) > 0\\) \\) | \\( \\((Fr - Ci) < 0\\) \\) | \\( \\((Fr - Ci) = 0\\) \\) | | | | | | \\( \\(Fr > Ci\\) \\) | \\( \\(Fr < Ci\\) \\) | \\( \\(Fr = Ci\\) \\) | | | | | | \\( \\(\\mathbf{r > i}\\) \\) | \\( \\(\\mathbf{r < i}\\) \\) | \\( \\(r = i\\) \\) | +----------------------+----------------------+-----------------------+ | Coupon payments | Coupon payments | Coupon payments | | received are | received are | received are equal | | **larger than the | **smaller than the | to the amount | | expected amount they | expected amount they | expected, thus the | | would receive from a | would receive from a | Bond has no premium | | similar par bond.** | similar par bond.** | or discount. | | | | | | This is additional | This is | \\( \\(\\mathbf{P | | **periodic income**, | **insufficient | rice = Face = Call}\\) \\) | | which is why the | income**, which is | | | bond costs more than | why the bond costs | | | par. | less than par. | | +----------------------+----------------------+-----------------------+","title":"Bond Premium &amp; Discount"},{"location":"1.%20Introductory/ASA-FM/Test/#bond-amortization","text":"There are some terminology differences between Loan and Bonds: Coupon Payment \u2192 Repayment Book Value \u2192 Outstanding Balance Write-up/down or Discount/Premium Amortization \u2192 Principal Repaid Also known as the Accumulation of Discoumt/Premium The goal is to create an Amortization Schedule that shows how each Coupon Payment is used to offset the Premium/Discount in each period, slowly bringing the Book Value to the Redemption Value (Usually equal to Par Value) at the end of the Loan Similarly, each coupon first pays off interest , then the remaining amount is used to offset the premium/discount on the bond, which decreases/increases the book value to Par All types of questions that apply to loans ALSO apply to Bond Amortization! +-----+------+---------------------+---------------+-----------------+ | * | ** | Interest Repaid | Wr | **Book Value | | Pe | Coup | | ite-up/down* | | | rio | on** | \\( \\(B_{t - 1}*i\\) \\) | | \\( (C + (Fr - C | | d** | | | ( \\(\\ | i)a_{n - t\u23cbi}\\) \\) | | | ( (F | ( \\(Fr + (Fr - | left| Fr\\math | | | | *r\\) \\) | Ci)*v^{n - t + 1}\\) \\) | bf{-}B_{t - 1 | | | | | | }*i \\right|\\) \\) | | | | | | | | | | | | $$ | | | | | | \\left| (Fr - | | | | | | Ci)v^{n - t + | | | | | | 1} \\right|$$ | | +=====+======+=====================+===============+=================+ | 0 | | | | \\( \\(C + (Fr | | | | | | - Ci)a_{n\u23cbi}\\) \\) | +-----+------+---------------------+---------------+-----------------+ | 1 | $$ | \\( (Fr | ( (\\left | ( \\(C + (Fr - C | | | Fr\\) \\) | + (Fr - Ci)*v^{n}\\) \\) | | (Fr - Ci)v^ | i)a_{n - 1\u23cbi}\\) \\) | | | | | {n} \\right|$$ | | +-----+------+---------------------+---------------+-----------------+ | $ | $$ | \\( \\(\\vdots\\) \\) | \\( \\(\\vdots\\) \\) | \\( \\(\\vdots\\) \\) | | \\(\\v | \\vdo | | | | | dot | ts\\) $ | | | | | s$$ | | | | | +-----+------+---------------------+---------------+-----------------+ | $$ | $$ | $ | \\( ( | ( \\(C + (Fr | | n\\) \\) | Fr\\) \\) | \\(Fr + (Fr - Ci)*v\\) $ | left| (Fr - C | - Ci)a_{0\u23cbi}$$ | | | | | i)v \\right|$$ | | +-----+------+---------------------+---------------+-----------------+ | T | | | | | | ota | | | | | | l | | | | | +-----+------+---------------------+---------------+-----------------+","title":"Bond Amortization"},{"location":"1.%20Introductory/ASA-FM/Test/#special-type-of-bondsunderline","text":"","title":"[Special Type of Bonds]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#zero-coupon-bonds-pure-discount-bonds","text":"Bonds that do not pay coupons, only the redemption value at the back Due to only having one cashflow at the end of \\(t\\) periods, the yield of the zero-coupon bond is essentially the Spot Rate for that length of time. Thus, given the price of any zero-coupon bond, we can calculate the spot rate However, we may not be given a zero-coupon bond directly . Like the interest rate swap shortcut, by buying and selling regular bonds in a portfolio , the net cashflows per period may be zero , essentially creating a zero-coupon bond.","title":"Zero Coupon Bonds (Pure discount Bonds)"},{"location":"1.%20Introductory/ASA-FM/Test/#floating-rate-bonds","text":"Bonds whose coupon rates vary with some external index. Their values rise when interest rises as well, thus they suffer less interest rate risk","title":"Floating Rate Bonds"},{"location":"1.%20Introductory/ASA-FM/Test/#callable-bonds","text":"Bonds where there is a choice to fully repay the bond on an earlier date instead Choosing to repay the Bond earlier is known as Calling the Bond. The redemption value at the time is known as the Call Price To ensure that the Investor is properly compensated for lost coupon payments if the Bond is called, the Call Price is higher the earlier the Bond is called Callable bonds are good for Issuers, as their flexible nature allows them to take advantage of falling interest rates (Issue new one at lower cost) and allows them to eliminate obligations at will with the main disadvantage being the higher Coupon Rates Put Provisions are good for Investors, but not issuer, as they may have to buy back the bond at an unattractive price , but they allow the issuer to use lower coupon rates","title":"Callable Bonds"},{"location":"1.%20Introductory/ASA-FM/Test/#scenario-testing","text":"Callable Bond cashflows are uncertain as they are at the discretion of the Bond issuer Thus, the main approach to work with Callable bonds is to Scenario test -- Consider every possible outcome at form an analysis from that There are two main types: Given Price, determine Yield Use the Fin Calc to quickly reverse engineer the Yields for each scenario We focus on the Lowest Yield -- Yield to Worst This represents the worst-case scenario for the investor Given Yield, determine Price Use the Fin Calc to quickly reverse engineer to price for each scenario We focus on the LOWEST Price -- Maximum amount they should pay This represents the highest price an investor will have to pay to guarantee at least Yield to Worst , regardless which scenario ends up happening Price and Yield have an inverse relationship Thus, by choosing the lowest price, it means that even if another scenario were to occur, we would earn even higher than YTW It is also known as the maximum price , because if we increase price beyond this point, other scenarios may be fine, but this scenario will lead to a situation worse than YTW, that means that overall YTW is no longer guaranteed We can eliminate scenarios to test by using the shortcuts: YTW always occurs at the end points -- First & Last year that the call price changes (Including maturity) YTW is the First end date for Premium & Last for Discount Bonds for each group of call dates with the same call price","title":"Scenario Testing"},{"location":"1.%20Introductory/ASA-FM/Test/#stock-valuation-dividend-discount-model","text":"Like all other financial instruments, we have seen so far, we can value a stock by taking the Present Value of its future cashflows (Dividends) If we expect to sell the stock in the future, we treat that as a cashflow as well Since corporate lifespans are infinite, these dividend payments can occur infinitely, forming a perpetuity. Depending on the type of stock, Dividends may remain constant or continue to grow every period, forming a growing perpetuity \\[PV = \\frac{D}{i - k} = \\frac{D_{0}(1 + k)}{i - k}\\] The math behind the calculation is a simple application of the growing perpetuity concept There are a few things to look out for: Questions may give you the current dividends paid. Since the dividends grows, we can find the time 1 dividend by multiplying by the growth factor Given the dividend or stock value at any one point of time, we can simply multiply it by the growth factor to determine the value at another time If the growth rate changes midway, we consider a two-stage model , which simply means we now have an Annuity and Deferred Perpetuity","title":"Stock Valuation: Dividend Discount Model"},{"location":"1.%20Introductory/ASA-FM/Test/#chapter-7-bond-yield-term-structure-of-interest-ratesunderline","text":"","title":"[Chapter 7: Bond Yield &amp; Term Structure of Interest Rates]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#measures-of-bond-yieldunderline","text":"","title":"[Measures of Bond Yield]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#current-yield-nominal-yield","text":"Simple measure of the potential return of the bond -- How much an investor would expect to make if they held the bond for a year However, it is not an accurate reflection of the potential gain of the bond: It does not take into the time value of money It does not consider capital gain or loss (Discount/Premiums) It is based on Quoted Prices while the actual investment is Dirty Prices \\[Current\\ Yield = \\frac{\\mathbf{Annual}\\ Coupon\\ Payments}{Quoted\\ Price}\\] \\[Nominal\\ Yield = \\frac{\\mathbf{Annual}\\ Coupon\\ Payments}{Face\\ value} = Coupon\\ rate\\]","title":"Current Yield &amp; Nominal Yield"},{"location":"1.%20Introductory/ASA-FM/Test/#yield-to-maturity","text":"When we price bonds, we are supposed to use prevailing spot rates at the time (Obtained from yield curve) for more accurate representation of the time value of money However, in practice the term structure is not observable , but the transaction prices are . Thus, if we solve for the discount rate that sets the discounted cashflows to the transaction price, we obtain the Yield to Maturity It is the IRR for a bond -- which measures the average annual rate we would receive on the bond should we hold it to maturity. It is a sort of \"weighted average\" of the spot rates For this course, we assume the yield curve to be flat. Thus, the interest rate used to discount is constant, thus is also the YTM of the bond However, there are some issues with using YTM to evaluate Bonds: There is no closed form solution for YTM. It can only be calculated using numerical methods Although YTM is an \"average\" of the spot rates, it also changes with the coupon rate, which makes it an \"impure\" measure It also assumes that the bond will be held to maturity (Most are traded) as well as that the coupons are reinvested (Interest on Interest) at the same rate (Hard)","title":"Yield to Maturity"},{"location":"1.%20Introductory/ASA-FM/Test/#par-yield","text":"An alternative measure is par yield, which is the coupon rate that makes the bond trade at par under the current term structure ( \\(\\mathbf{Price = Face = Call}\\) ) However, it is an inaccurate measure -- It is more of a summary of the existing term structure rather than measures of a potential return of a bond","title":"Par Yield"},{"location":"1.%20Introductory/ASA-FM/Test/#holding-period-yield","text":"Measure of the return we get by holding a bond for a period of time (Assuming it is redeemed at the end of the period) Based on the idea of interest -- It takes the difference between the ending value and starting value, accounting for any additional income (Coupons) as well \\[P_{0}(1 + i)^{t} = P_{t} + FV(Coupons)\\] Solve for \\(i\\) as the t-period holding period yield","title":"Holding Period Yield"},{"location":"1.%20Introductory/ASA-FM/Test/#spot-rates-and-the-term-structure","text":"The plot of the various spot rates and against time is known as the Yield Curve & the mathematical relationship between the two is known as the Term Structure of Interest Rates Empirically, the yield curve has been shown to take many shapes : Upward, Downward, Flat etc Upward Yield Curves are the most common. However, for this course, we assume a flat yield curve unless stated otherwise (Spot rates are constant throughout) If the yield curve is not flat, we do have a few options to determine the yield curve:","title":"Spot Rates and the Term Structure"},{"location":"1.%20Introductory/ASA-FM/Test/#discretely-compounded-yield-curve","text":"It is called Discrete because it only calculates points of the yield curve at discrete points (Typically 6-month intervals) It uses the Prices of Bonds of various lengths to determine to reverse engineer the prevailing spot rates at the time The first period bond must be a zero-coupon bond . All other bonds can be regular annual/semi-annual coupon bonds This method of calculating is also known as a bootstrap method : Calculate the first spot rate using the zero-coupon bond If the second bond has a coupon, substitute in the spot rate calculated before to discount it and then solve for the second spot rate Repeatedly and recursively do this until all spot rates have been found If all Bonds were zero-coupon bonds, then this method would be a lot easier","title":"Discretely Compounded Yield Curve"},{"location":"1.%20Introductory/ASA-FM/Test/#continuously-compounded-yield-curve","text":"We now consider continuous interest and time points. One key assumption we make is that the force of interest is constant between two successive valuation dates It is based on the concept of spot rates and forward rates. By combining an initial spot rate and multiple forward rates, we are able to obtain spot rates for any period We then form a piecewise function based on the time period to form the yield curve Linking Spot Rates and Force of Interest: \\[e^{s_{t}*t} = e^{\\int_{0}^{t}\\delta_{t}}\\] \\[s_{t} = \\frac{1}{t}\\int_{0}^{t}\\delta_{t}\\] Recursive nature (Splitting integration) \\[e^{s_{t_{2}}*t_{2}} = e^{- \\int_{0}^{t_{2}}\\delta_{t_{2}}}\\] \\[e^{s_{t_{2}}*t_{2}} = e^{- \\int_{0}^{t_{1}}\\delta_{t_{1}} + \\int_{t_{1}}^{t_{2}}\\delta_{t_{2} - t_{1}}}\\] \\[s_{t_{2}} = \\frac{1}{t_{2}}\\left( \\int_{0}^{t_{1}}\\delta_{t_{1}} + \\int_{t_{1}}^{t_{2}}\\delta_{t_{2} - t_{1}} \\right)\\] In practice, we solve for the various \\(\\mathbf{\\delta}\\) using bond prices: The same key concept of splitting the force of interest intervals applies [Zero Coupon bonds]{.underline} [Regular Coupon Bonds]{.underline} \\( \\(P_{t} = C*e^{- \\int_{0}^{t}\\delta_{t}}\\) \\) \\( \\(P_{t} = (C + Fr)*e^{- \\int_{0}^{t}\\delta_{t}} + Fr*e^{- \\int_{0}^{t - 1}\\delta_{t - 1}}\\) \\)","title":"Continuously Compounded Yield Curve"},{"location":"1.%20Introductory/ASA-FM/Test/#chapter-8-bond-managementunderline","text":"","title":"[Chapter 8: Bond Management]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#macaulay-duration-macd","text":"It is the Weighted Average of the time taken in YEARS for cashflows to occur, weighted based on the relative size of the PV of future cashflows. It calculates average amount of time needed to get breakeven on your investment Based on this, Cashflows that are Larger and Occur Earlier (Don't need to discount as much) will influence the calculation more . We can use this intuition to check our calculations to see if they make sense","title":"Macaulay Duration (MacD)"},{"location":"1.%20Introductory/ASA-FM/Test/#deriving-macaulay-duration","text":"\\[\\therefore MacD = \\frac{1*PV + 2*PV_{2} + \\ldots + n*PV_{n}}{PV_{1} + PV_{2} + \\ldots PV_{N}} = \\frac{\\sum_{}^{}{t*v^{t}*CF_{t}}}{{\\sum_{}^{}{v^{t}*CF}}_{t}} = - \\frac{P^{'}(\\delta)}{P(\\delta)}\\] If all cashflows are level \\(\\left( CF_{1} = \\ldots CF_{n} \\right)\\) , they can be factorized out from both the numerator and denominator, effectively cancelling out Building on this, consider two sets of Cashflows: Set A: 1, 2, 3, 4... Set B: 100, 200, 300, 400... We can factorise out 100 from Set B, which will cancel out and result in the same cashflows as Set A. Thus, MacD is dependent not on the size but the pattern of cashflows","title":"Deriving Macaulay Duration"},{"location":"1.%20Introductory/ASA-FM/Test/#other-time-frequencies","text":"The typical duration formulas use full years (1,2,3...) However, some questions (especially semi-annual bonds) have cashflows semi-annually We can treat the semi-annual timings as a full year, then convert it back to half years at the end! Alternatively, we can treat them as \\(\\mathbf{0.5}\\) right from the get-go","title":"Other Time Frequencies"},{"location":"1.%20Introductory/ASA-FM/Test/#macaulay-duration-shortcuts","text":"Zero-Coupon Bond (Single ( \\(n\\) \\) Cashflow) Level Annuity (Loans) \\( \\(\\frac{(Ia)_{n\u23cbi}}{a_{n\u23cbi}}\\) \\) Standard Bond* \\( \\(\\frac{{Fr*(Ia)}_{n\u23cbi} + nCv^{n}}{Fr*a_{n\u23cbi} + Cv^{n}}\\) \\) *Note that a deferred Bond cannot use this formula directly; an adjustment will have to be made +-----------------------------------+-----------------------------------+ | [Par Bond]{.underline} | **[Common Stock (Increasing | | \\(\\left( \\m | Perpetuity)]{.underline}** | | athbf{P = F = C,\\ i = r} \\right)\\) | | +===================================+===================================+ | \\( (MacD = \\frac{{Fr*(Ia)}_{n\u23cbi} | ( \\(P = (i - k)^{- 1}\\) \\) | | + nCv^{n}}{Fr*a_{n\u23cbi} + Cv^{n}}\\) \\) | | | | \\( \\(P' = - (1 - k)^{- 2}\\) \\) | | \\( \\(MacD = \\frac{{i*(Ia)}_{n\u23cb | | | i} + nv^{n}}{i*a_{n\u23cbi} + v^{n}}\\) \\) | \\( (ModD = - \\frac{P^{ | | | '}}{P} = - \\frac{(i - k)^{- 2}}{- | | ( \\(Mac | (1 - k)^{- 1}} = (i - k)^{- 1}\\) \\) | | D = \\frac{{\\ddot{a}}_{n\u23cbi} - nv^{ | | | n} + nv^{n}}{1 - v^{n} + v^{n}}\\) \\) | \\( \\(MacD = Mod | | | D*(1 + i) = \\frac{1 + i}{i - k}\\) \\) | | \\( \\(MacD = {\\ddot{a}}_{n\u23cbi}\\) \\) | | +-----------------------------------+-----------------------------------+","title":"Macaulay Duration Shortcuts"},{"location":"1.%20Introductory/ASA-FM/Test/#modified-duration","text":"As its name suggests, ModD is simply a modification of MacD <!-- --> - The longer we must wait to get back our money, the larger the window for those cashflows to be affected by interest rates hence higher sensitivity to them <!-- --> - Thus, we can modify MacD to directly approximate the Sensitivity of the change in price of a Bond to changes in Interest Rates. Thus, ModD measures the percentage decrease in the value of the security per unit increase in interest rates (Price Risk)","title":"Modified Duration"},{"location":"1.%20Introductory/ASA-FM/Test/#deriving-modified-duration","text":"\\[Price\\ of\\ a\\ Security,\\ P(i) = \\sum_{}^{}{v^{t}*CF_{t}} = \\sum_{}^{}{(1 + i)^{- t}*CF_{t}}\\] \\[Changes\\ in\\ Price,\\ P^{'}(i) = \\sum_{}^{}{- t*(1 + i)^{- t - 1}*CF_{t}} = \\sum_{}^{}{- t*v^{t + 1}*CF_{t}}\\] \\[\\therefore ModD = - \\frac{P^{'}(i)}{P(i)} = - \\frac{\\sum_{}^{}{- t*v^{t + 1}*CF_{t}}}{\\sum_{}^{}{v^{t}*CF_{t}}} = \\frac{\\sum_{}^{}{t*v^{t + 1}*CF_{t}}}{\\sum_{}^{}{v^{t}*CF_{t}}} = v*\\frac{\\sum_{}^{}{t*v^{t}*CF_{t}}}{{\\sum_{}^{}{v^{t}*CF}}_{t}} = \\frac{MacD}{1 + i}\\] If interest compounds continuously, we can show a similar result for MacD as well Note that if no specification was made, Duration refers to Macaulay Duration","title":"Deriving Modified Duration"},{"location":"1.%20Introductory/ASA-FM/Test/#portfolio-duration","text":"Consider a portfolio of Bonds. We can aggregate the cashflows for the entire portfolio and then calculate the Duration using the same method However, this can be time consuming. If we already know the individual Durations for each component, we can quickly calculate the Duration of the entire portfolio by using their relative weights (Similar to Modern Portfolio Theory) Important: The weights MUST be time-value adjusted \\[MacD_{Portfolio} = w_{X}*MacD_{X} + w_{Y}*MacD_{Y} + w_{Z}*MacD_{Z} + \\ldots\\] This same portfolio method applies for Convexity as well","title":"Portfolio Duration"},{"location":"1.%20Introductory/ASA-FM/Test/#different-comparison-date","text":"The base definition of Duration assumes that it is calculated at time 0 We can generalize the formula and calculate them on any date by understanding that t represents the time till the cashflow occurs from the comparison date However, we can link the durations across time which may help us save time <!-- --> - If we were to move the comparison one period later, there are two different scenarios we must consider: - **Calculate Duration BEFORE the payment** $\\left( \\mathbf{Mac}\\mathbf{D}_{\\mathbf{t}}^{\\mathbf{'}} \\right)$ - Under this scenario, the **cashflows remain the same** as compared to the original. The only thing that changed was time - Thus, the average time taken for cashflows just moved forward a period: - $MacD_{t}^{'} = MacD_{t - 1} - 1$ - **Calculate Duration AFTER the payment** $\\left( \\mathbf{Mac}\\mathbf{D}_{\\mathbf{t}} \\right)$ - Since the payment has occurred, both the **cashflows and timing are different** as compared to the original - Thus, we cannot form a relationship & must be manually recalculated - For any given period**,** $\\mathbf{Mac}\\mathbf{D}_{\\mathbf{t}}\\mathbf{> Mac}\\mathbf{D}_{\\mathbf{t}}^{\\mathbf{'}}$ - This is because $MacD_{t}$ removes the earliest cashflow, effectively removing the smallest value from the calculation - Since MacD is an average, **removing the smallest value will force the average to increase in value as compared to** $\\mathbf{Mac}\\mathbf{D}_{\\mathbf{t}}^{\\mathbf{'}}$ {width=\"6.268055555555556in\" height=\"4.204861111111111in\"}","title":"Different Comparison date"},{"location":"1.%20Introductory/ASA-FM/Test/#using-durations-for-estimations","text":"Given the only current price of a Bond, we can estimate the new price of the bond for a specified change in interest rates using Durations We consider a graph of the price of the Bond. Without knowing the equation for Price, we can approximate the new price using the tangent at the current price : {width=\"3.4534339457567804in\" height=\"2.4800240594925635in\"} Firstly, we consider the first principles interpretation of Tangent/Slope , \\[P^{'}\\left( i_{0} \\right) = \\frac{P^{*}\\left( i_{n} \\right) - P\\left( i_{0} \\right)}{i_{n} - i_{0}}\\] Next, we use the ModD formula , \\[ModD = - \\frac{P^{'}\\left( i_{0} \\right)}{P\\left( i_{0} \\right)}\\] \\[P^{'}\\left( i_{0} \\right) = - P\\left( i_{0} \\right)*ModD\\] Lastly, we combine the two and rearrange, \\[\\frac{P^{*}\\left( i_{n} \\right) - P\\left( i_{0} \\right)}{i_{n} - i_{0}} = - P\\left( i_{0} \\right)*ModD\\] \\(P^{*}\\left( i_{n} \\right) - P\\left( i_{0} \\right) = \\left( - P\\left( i_{0} \\right)*ModD \\right)*\\left( i_{n} - i_{0} \\right)\\) \\[\\mathbf{P}^{\\mathbf{*}}\\left( \\mathbf{i}_{\\mathbf{n}} \\right)\\mathbf{= \\ P}\\left( \\mathbf{i}_{\\mathbf{0}} \\right)\\mathbf{-}\\mathbf{P}\\left( \\mathbf{i}_{\\mathbf{0}} \\right)\\left( \\mathbf{i}_{\\mathbf{n}}\\mathbf{-}\\mathbf{i}_{\\mathbf{0}} \\right)\\left( \\mathbf{ModD} \\right)\\] First Order Modified Approximation First Order Macaulay Approximation \\( \\(P^{*}\\left( i_{n} \\right) = \\ P\\left( i_{0} \\right) - P\\left( i_{0} \\right)\\left( i_{n} - i_{0} \\right)(ModD)\\) \\) \\( \\(P\\left( i_{n} \\right) \\approx P\\left( i_{0} \\right)\\left( \\frac{1 + i_{0}}{1 + i_{n}} \\right)^{MacD}\\) \\) Proof for Macaulay variant is algebraically challenging and is not shown Generally, Macaulay Approximation is slightly more accurate than the Modified one","title":"Using Durations for Estimations"},{"location":"1.%20Introductory/ASA-FM/Test/#an-alternative-intuitive-way-to-think-about-approximation","text":"Premise \u2192 Macaulay and Modified Duration represents time Looking at the formulas for the approximation, we notice that we have seen them before: Macaulay \u2192 \\(Interest^{Time}\\) \u2192 Compound Interest Modified \u2192 \\(Interest*Time\\) \u2192 Simple Interest If interest rates were constant, we can simply use the original accumulation function. However, since interest rates changed, we need to make a correction to the original Macaulay approximation is a Compound Interest Correction, while Modified Approximation is a Simple Interest Correction Following the nature of each type of interest, Macaulay (Compound Interest) uses Multiplication/Division while Modified (Simple Interest) used Addition/Subtraction","title":"An alternative (Intuitive) way to think about Approximation"},{"location":"1.%20Introductory/ASA-FM/Test/#convexity","text":"Notice that we are using a Line to approximate a Curve . Thus, the first order approximations are good for small changes in price where the curve is relatively flat over that length Since the curve is Convex, the line will always under-approximate the price of the bond. This under-approximation becomes greater the larger the change in interest rates as the curve begins to bend more We can thus use the Curvature to approximate this inherent error (Convexity) and account for it to obtain a more accurate approximation","title":"Convexity"},{"location":"1.%20Introductory/ASA-FM/Test/#derivation","text":"+-----------------------------------+-----------------------------------+ | [Modified | **[Macaulay | | Convexity]{.underline} | Convexity]{.underline}** | +===================================+===================================+ | \\( \\(ModC = \\frac{P^{''}(i)}{P(i)}\\) \\) | \\( \\(MacC = \\ | | | frac{P^{''}(\\delta)}{P(\\delta)}\\) \\) | | \\( \\(P(i) = \\ | | | sum_{}^{}{(1 + i)^{- t}*CF_{t}}\\) \\) | \\( \\(P(i) = \\s | | | um_{}^{}{e^{- \\delta t}*CF_{t}}\\) \\) | | \\( \\(P^{'}(i) = \\sum_{}^{ | | | }{- t*(1 + i)^{- t - 1}*CF_{t}}\\) \\) | \\( \\(P^{'}(i) = \\sum_{ | | | }^{}{- t*e^{- \\delta t}*CF_{t}}\\) \\) | | $ | | | \\(P^{''}(i) = \\sum_{}^{}{- t*( - t | ( \\(P^{''}(i) = \\sum_{}^{}{ | | - 1)*(1 + i)^{- t - 2}*CF_{t}}\\) \\) | - t* - t*e^{- \\delta t}*CF_{t}}\\) $ | | | | | \\( (P^{''}(i) = \\sum_{}^{}{t*(t + | ( \\(P^{''}(i) = \\sum_{}^ | | 1)*(1 + i)^{- (t + 2)}*CF_{t}}\\) \\) | {}{t^{2}*e^{- \\delta t}*CF_{t}}\\) \\) | | | | | $$ | \\( \\(\\therefore\\mathbf{MacC} = | | \\therefore\\mathbf{ModC} = \\frac{\\ | \\frac{\\sum_{}^{}{t^{2}*v^{t}*CF_ | | sum_{}^{}{t*(t + 1)*v^{t + 2}*CF_ | {t}}}{\\sum_{}^{}{v^{t}*CF_{t}}}\\) \\) | | {t}}}{\\sum_{} {}{v CF_{t}}}$$ | | | | **MacC for a single cashflow is* | | $ | \\(\\mathbf{n}^{\\mathbf{2}}\\) | | \\(\\mathbf{\\equiv ModC} = \\frac{\\su | | | m_{}^{}{t*(t + 1)*v^{t}*CF_{t}}}{ | | | \\sum_{}^{}{v^{t}*CF_{t}}}*v^{2}\\) $ | | +-----------------------------------+-----------------------------------+ | [Portfolio Convexity (Same as | | | before)]{.underline} | | | | | | \\( \\(MacC_{Portfo | | | lio} = w_{X}*MacC_{X} + w_{Y}*Mac | | | C_{Y} + w_{Z}*MacC_{Z} + \\ldots\\) \\) | | | | | | Note: If not specified , | | | Convexity always refers to | | | Modified Convexity | | +-----------------------------------+-----------------------------------+","title":"Derivation"},{"location":"1.%20Introductory/ASA-FM/Test/#additional-formulasunderline","text":"Linking Duration and Convexity: \\[ModC = v^{2}*(MacC + MacD)\\] Second Order Modified Approximation: \\[P\\left( i_{n} \\right) \\approx P\\left( i_{0} \\right) - P\\left( i_{0} \\right)*\\left( i_{n} - i_{0} \\right)(ModD) + P\\left( i_{0} \\right)*\\frac{\\left( i_{n} - i_{0} \\right)^{2}}{2}(ModC)\\] Second Order Macaulay Approximation: \\[P\\left( i_{n} \\right) \\approx P\\left( i_{0} \\right)\\left( \\frac{1 + i_{0}}{1 + i_{n}} \\right)^{MacD}*\\left\\lbrack 1 + \\left( \\frac{i - i_{0}}{1 + i_{0}} \\right)^{2}\\left( \\frac{MacC - MacD^{2}}{2} \\right) \\right\\rbrack\\]","title":"[Additional Formulas]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#immunization","text":"Companies often make payments (Liability Cashflows) and receive payments from their investments (Asset Cashflows) The goal of any company is to ensure that they have enough Asset Cashflows to cover their Liability Cashflows . In other words, the Present Value of Asset Cashflows should be more than Liability Cashflows. We call the excess a Surplus. \\(Surplus = PV\\left( CF_{Assets} \\right) - PV\\left( CF_{Liabilities} \\right)\\) However, this Surplus is prone to interest rate risk : When interest rates fall or rise, the present value of assets might fall, present value of liabilities might rise or both This causes the surplus to become negative which may put the company in a state of financial distress The goal of any company is to thus find the combination of securities such that no matter how interest rates change, the surplus will always increase or stay the same . Essentially, they are immunizing their surplus from the effects of interest rates","title":"Immunization"},{"location":"1.%20Introductory/ASA-FM/Test/#redington-immunization-duration-matching","text":"Three assumptions needed : Interest rates for all maturities are identical (Flat Yield Curve) A change in interest rates affects all maturities (Parallel shift) A change in interest rates do not affect cashflows (Fixed cashflows) Under these three assumptions, a portfolio is immunized from small changes in interest rates if the following three conditions are met: \\(\\mathbf{PV}\\left( \\mathbf{Assets} \\right)\\mathbf{= PV}\\left( \\mathbf{Liabilities} \\right)\\mathbf{\\Longleftrightarrow}\\mathbf{P}_{\\mathbf{A}}\\mathbf{=}\\mathbf{P}_{\\mathbf{L}}\\) They are the same because PV is equivalent to price \\(\\mathbf{Duratio}\\mathbf{n}_{\\mathbf{Asset}}\\mathbf{= Duratio}\\mathbf{n}_{\\mathbf{Liability}}\\mathbf{\\Longleftrightarrow}\\mathbf{P}_{\\mathbf{A}}^{\\mathbf{'}}\\mathbf{=}\\mathbf{P}_{\\mathbf{L}}^{\\mathbf{'}}\\) Both Macaulay and Modified duration can be used here Since prices are the same, both their denominators cancel out , thus we only need to consider the first derivative \\(\\mathbf{Convexit}\\mathbf{y}_{\\mathbf{Asset}}\\mathbf{> Convexit}\\mathbf{y}_{\\mathbf{Liability}}\\mathbf{\\Longleftrightarrow}\\mathbf{P}_{\\mathbf{A}}^{\\mathbf{''}}\\mathbf{>}\\mathbf{P}_{\\mathbf{L}}^{\\mathbf{''}}\\) Similarly, both Macaulay and Modified convexity applies. Since prices are the same, we only consider their second derivative We can view this relationship graphically: Since prices are the same, both Asset and Liability price curves intersect at the current interest rate Since duration is the same, they have the same tangent at the intersection point Since Asset Convexity is greater, it curves higher than Liability Given the same start point but asset curves more, the Asset will always increase more and decrease less than a similar change in Liabilities, ensuring surplus is protected Thus, we can use the first two conditions to solve for the optimal portfolio, and then use the third condition to check if the solution is valid, if necessary","title":"Redington Immunization (Duration Matching)"},{"location":"1.%20Introductory/ASA-FM/Test/#another-way-to-express-the-conditions","text":"We can also express the conditions in terms of Net Present Value (NPV) First condition states that the present values must match, \\(\\mathbf{NPV = 0}\\) Second condition states that the first derivatives must match, \\(\\mathbf{NP}\\mathbf{V}^{\\mathbf{'}}\\mathbf{= 0}\\) Third condition states that the second derivative of inflows must be higher, \\(\\mathbf{NPV'' > 0}\\)","title":"Another way to express the conditions:"},{"location":"1.%20Introductory/ASA-FM/Test/#full-immunization","text":"Similarly, it has the same assumptions as Redington : Flat Yield Curve, Parallel Shifts & Fixed Cashflows It also has similar conditions as Redington, with only the last being different: \\(PV(Assets) = PV(Liabilities) \\Longleftrightarrow P_{A} = P_{L}\\) \\(Duration_{Asset} = Duration_{Liability} \\Longleftrightarrow P_{A}^{'} = P_{L}^{'}\\) There must be one Asset Cashflow BEFORE and AFTER a Liability Cashflow There can be more than one Liability, but each must be sandwiched between two Asset cashflows They do not have to be unique; there can be infinitely many Liabilities between two Assets Cashflows also include small ones such as Bond Coupons The intuition behind it is more qualitative: Another way to think about Immunization is for the firm to is for cashflows to be worth the amount they expected, regardless of market conditions Imagine there was a rise in interest after the first asset cashflow. The value of liabilities would rise, but so would the Asset Cashflow that comes after that, mitigating the risk of interest rate changes. Vice-versa applies as well. Comparing both types of Immunization: Full immunization covers ALL changes in interest rates, while Redington covers only small interest rate changes Full immunization is a stricter version of Redington Immunization -- Full is a subset of Redington immunization. If Full Immunization conditions are met, it can also be used for a Redington problem","title":"Full Immunization"},{"location":"1.%20Introductory/ASA-FM/Test/#immunization-shortcut","text":"Find the duration of EACH asset and the overall duration of the liabilities . Using the portfolio duration method , equate both liabilities together: \\(w_{1}*MacD_{Asset\\ 1} + w_{2}*MacD_{Asset\\ 2} + \\ldots = MacD_{Liabilities}\\) Solve for the weights of the portfolio, where \\(w_{2} = 1 - w_{1} + \\ldots\\) Find the overall PV of the liabilities . Multiply the calculated weights by the PV of the liabilities, to obtain the PV of each of the assets used. Multiply by interest accordingly to find the value of the assets when their cashflows occur This process can be done as all liabilities at once , or for individual liabilities If we are given the Asset Cashflows, we can find its PV and calculate the weight using by comparing it to the PV of the liabilities (Reverse order)","title":"Immunization Shortcut"},{"location":"1.%20Introductory/ASA-FM/Test/#alternative-exact-cashflow-matching","text":"Redington and Full Immunization require very hard assumptions that are unrealistic in the real world This provides a simpler, more intuitive method by matching the timing and amount of Asset and Liability Cashflows This method STILL immunizes against ALL changes in interest rates Assume we have three bonds: \\(Bond_{1}\\) matures in 1 year, with Face \\(F_{1}\\) , Coupon Rate \\(R_{1}\\) and \\(YTM_{1}\\) \\(Bond_{n}\\) matures in n years, with Face \\(F_{n}\\) , Coupon Rate \\(R_{n}\\) and \\(YTM_{n}\\) \\(Bond_{N}\\) matures in \\(N\\) years, with Face \\(F_{N}\\) , Coupon Rate \\(R_{N}\\) and \\(YTM_{N}\\) **Period 1** **Period n** **Period N** \\( \\(Bond_{1}\\) \\) \\( \\(F_{1}R_{1}\\) \\) - - \\( \\(Bond_{n}\\) \\) \\( \\(F_{n}R_{n}\\) \\) \\( \\(F_{n} + F_{n}R_{n}\\) \\) - \\( \\(Bond_{N}\\) \\) \\( \\(F_{N}R_{N}\\) \\) \\( \\(F_{N}R_{N}\\) \\) \\( \\(F_{N} + F_{N}R_{N}\\) \\) $${Liability}_{1}$$ $${Liability}_{n}$$ $${Liability}_{N}$$","title":"Alternative: Exact Cashflow Matching"},{"location":"1.%20Introductory/ASA-FM/Test/#methodology","text":"We use the Longest Bond to match the Longest Liability : \\[F_{N} + F_{N}R_{N} = Liability_{N}\\ \\] If the liability is larger than the Face and Redemption of a single bond, then we need to buy multiple bonds . Let \\(x\\) be the number of units to buy , where \\(x\\) can be a non-integer: \\[x_{N}\\left( F_{N} + F_{N}R_{N} \\right) = Liability_{N}\\ \\] Fill in the coupons for previous period, then repeat for the next longest liability: \\[x_{n}\\left( F_{n} + F_{n}R_{n} \\right) + x_{N}\\left( F_{N}R_{N} \\right) = Liability_{n}\\] \\[\\vdots\\] The total cost of exact matching is the number of units multiplied by the price of each bond: \\[Total\\ Cost = x_{1}*P_{1} + x_{n}*P_{n} + x_{N}*P_{N}\\] Note that each individual price is calculated using their respective \\(YTM\\) If the coupons are all zero-coupon bonds, then the total cost of matching is simply the present value of the liabilities (no need to do the above steps) Generally, there are two kinds of problems given: Face Value of the bonds are given . In this case, you NEED to consider the UNITS of bonds of to buy since the Par Value is predetermined Face Value of the Bonds are NOT given . In this case, you can directly solve for the \"par\" value of the bond needed Both of these are the SAME. Both are calculating the true par value invested. Method 2 directly calculates this while method 1 has to consider the units of the bond because there is a fixed par value","title":"Methodology:"},{"location":"1.%20Introductory/ASA-FM/Test/#chapter-9-determinants-of-interest-ratesunderline","text":"","title":"[Chapter 9: Determinants of Interest Rates]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#how-are-interest-rates-determined","text":"Interest rates can be viewed as the cost of borrowing capital . Like all things in economics, we can determine the equilibrium by considering the supply and demand of capital There are two perspectives we must consider: People with money (Lenders) They can either choose to spend it now, or lend the money out & receive interest payments The higher the interest, the more likely these people are to lend the money. Thus, the money supply curve is upward sloping People without money (Borrowers) They can either choose to refrain from spending, or borrow the money & pay interest The lower the interest, the more likely these people are to borrow money . Thus, the money demand curve is downward sloping By plotting out these two curves, the point at where they intersect is the equilibrium interest rate for the economy This is an over-simplified explanation, as there are other factors that affect it as well: Demand for capital \u2192 Higher when economy is expanding or when new technology is discovered; people want more capital in these cases to take advantage of opportunities Supply of capital \u2192 Affected by Time Preference, Risk Appetite, Inflation & individual characteristics of securities {width=\"6.208333333333333in\" height=\"4.666666666666667in\"}","title":"How are Interest rates determined?"},{"location":"1.%20Introductory/ASA-FM/Test/#central-banks-and-interest-ratesunderline","text":"","title":"[Central Banks and Interest Rates]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#what-is-the-central-bank","text":"It functions as a bank to other commercial banks in the country The central bank requires every commercial bank to deposit and maintain a specific amount with it, known as the bank reserve requirement This reserve is to ensure that these banks have enough capital so that they do not fall into financial distress The amount required varies from bank to bank as well as other situational factors","title":"What is the central bank?"},{"location":"1.%20Introductory/ASA-FM/Test/#what-if-banks-fail-to-meet-this-requirement","text":"Banks may fail to hit the requirements for a variety of reasons, the most common being that they were too aggressive in lending money which results in more outflows than inflows during that period <!-- --> - They have two options to meet the shortfall in capital: - Borrow from the **Central Bank** at the **discount rate** - Borrow from other **Commercial Banks** at the **federal funds rate** The central bank will always say yes to lending, which is why it is known as a lender of last resort . However, borrowing money directly from them reflects poorly on the bank , as it indicates difficulty in borrowing from other institutions due to credibility issues This may result in more scrutiny over the bank , which is why they prefer to borrow from another commercial bank instead","title":"What if banks fail to meet this requirement?"},{"location":"1.%20Introductory/ASA-FM/Test/#impact-on-interest-rates","text":"The part of the central bank that oversees this process is the Federal Open Market Committee (FOMC) They set the target interest rates that they would like for the federal funds rate , and then they will buy and sell T-bills in the secondary market to influence the federal funds rate This affects the money demand and supply for T-bills , which changes the discount rate, which makes borrowing from T-bills vs other banks more or less desirable It is in the banks best interest to lend money out , thus they will adjust the fed funds rate to ensure that they are more competitive (LOWER) than the discount rate to ensure that other banks will borrow from them rather than the CB If the discount rate increases , federal fund rate increases , it becomes more expensive for Banks to borrow money hence more costly for them to fall below the reserve requirement They will become less aggressive and not lend out as much money , reducing money supply and hence increasing interest rates","title":"Impact on Interest rates"},{"location":"1.%20Introductory/ASA-FM/Test/#treasury-bills","text":"+-----------------------------------+-----------------------------------+ | [US Treasury | **[Canadian Treasury | | Bills]{.underline} | Bills]{.underline}** | +===================================+===================================+ | \\( (Quoted rat | ( \\(Quoted\\ Rat | | e = \\ \\frac{360}{N}*\\frac{I}{C}\\) \\) | e = \\ \\frac{365}{N}*\\frac{I}{P}\\) \\) | +-----------------------------------+-----------------------------------+ | Based on the state of the US | Based on the level of economic | | economy | activity in Canada, including | | | Supply/Demand of CND | +-----------------------------------+-----------------------------------+ | They use different | | | approximations for the number of | | | days in the year -- US assumes | | | each month has 30 days | | | \\((30*12 = 360)\\) while Canada uses | | | the conventional 365 days. Also, | | | the US uses the redemption | | | value while Canada uses the | | | price. | | | | | | US bonds (Liquid) are traded | | | more than Canadian Bonds | | | (Ill-liquid). This lower | | | liquidity risk means that US | | | bonds yield a lower rate . All | | | else equal, this also means US | | | has a lower price. | | +-----------------------------------+-----------------------------------+","title":"Treasury Bills"},{"location":"1.%20Introductory/ASA-FM/Test/#components-of-interest-rate","text":"We can decompose interest rates into several smaller components: Real-Risk Free Rate \u2192 Compensation for deferred consumption Maturity Risk Premium \u2192 Compensation for the risk of longer-term investments Default Risk Premium \u2192 Compensation for the risk of money not being paid Inflation Risk Premium \u2192 Compensation for loss of purchasing power due Liquidity Risk Premium \u2192 Compensation for added cost of converting to cash This is consistent with what we learnt in FNCE101 -- The interest rates we observe is the combination of the Risk-Free Rate + Various Risk Premiums (Reward for risk) +-----------------------------------+-----------------------------------+ | [Discrete Rates]{.underline} | [Continuous | | | Rates]{.underline} | +===================================+===================================+ | \\( ((1 + R)^{t} = \\left\\lbrack | ( \\(e^{Rt} = e^{r_{1} | | \\left( 1 + r_{1} \\right)\\left( 1 | t}*e^{r_{2}t}*\\ldots e^{r_{n}t}\\) \\) | | + r_{2} \\right)\\ldots\\left( 1 + | | | r_{n} \\right) \\right\\rbrack^{t}\\) \\) | \\( \\(R | | | = r_{1} + r_{2} + \\ldots r_{n}\\) \\) | | \\( \\(R = \\left( 1 + r_{1} \\ | | | right)\\left( 1 + r_{2} \\right)\\ld | | | ots\\left( 1 + r_{n} \\right) - 1\\) \\) | | +-----------------------------------+-----------------------------------+ Both ways of determining the various components are correct. However, for simplicity , we prefer to use the Continuous Rates Some parts of this section are purely theoretical while others involve some calculation to actually determine the value of the component \\(r\\)","title":"Components of Interest Rate"},{"location":"1.%20Introductory/ASA-FM/Test/#maturity-risk-premium","text":"All else equal, longer-term investments will have a higher interest rate There are several explanations: Market Segmentation Theory \u2192 Different investors have clearly defined and different investment horizons . This results in different supply & demand factors for different time horizons, resulting in different rates Preferred Habitat Theory \u2192 Builds off the above theory but says that the segments are not written in stone . These segments are just the preferred segment for investors. Given better rates in another segment, these investors will move Liquidity Preference Theory \u2192 Lenders typically prefer to lend for a shorter time ; thus, they require higher rates to lend for longer times Expectation Theory \u2192 The longer the period of time, the more risk there is for interest rates to vary, which is why longer investments require a higher rate It also means that the current forward rates are an unbiased estimator of the future spot rates \\(\\left( EG.\\ f_{1}\\ now\\ is\\ s_{1}\\ one\\ year\\ from\\ now \\right)\\)","title":"Maturity Risk Premium"},{"location":"1.%20Introductory/ASA-FM/Test/#default-risk-premium","text":"All else equal, investments with a higher chance of not paying have higher interest rates There are two scenarios when it comes to defaulting: Defaults with no recovery (Pay some smaller amount instead) Defaults with recovery (Pay nothing) Consider the lenders perspective . In either case, they want to get the same amount , no matter what the scenario. Thus, they charge an appropriate default risk premium: \\(Total\\ \\#*Loan\\ Received = \\#\\ Non - Default*X + \\#\\ Default*Recovery\\) \\(X\\) represents the amount that needs to be RECEIVED \\(\\left( \\mathbf{Loan*Interest} \\right)\\) \\(X = Loan\\ amount*e^{rt}\\) Solve for \\(\\mathbf{r}\\) as our interest needed \u2192 Difference between this and the original interest rate is the default risk premium","title":"Default Risk Premium"},{"location":"1.%20Introductory/ASA-FM/Test/#inflation-risk-premium","text":"Inflation is the rise in the prices of goods and services over time Inflation is approximated via tracking one of the following indices: Consumer Price Index (CPI) \u2192 Basket of typical consumer items Producer Price Index (PPI) \u2192 Basket of typical producer items Inflation decreasing the purchasing power of money. Thus, Lenders of money want to be compensated via higher interest payments that scale with inflation. However, Borrowers are unwilling to pay this extra amount We see this pan out in two different scenarios: Lenders issue Loan with Inflation Protection for themselves The amount of interest that the borrowers pay scales with inflation. To incentivise borrowers, they lower they base interest by the cost of protection \u00a9 \\(R = r - c + i_{Actual}\\) In other words, Lenders are willing to receive a lower amount of base interest to pass on the risk of inflation to the borrower . In finance terms, since they experience less risk, they require a lower reward \\((r - c)\\) Lenders issue Loan without Inflation Protection for themselves Not all borrowers are willing to bear the inflation risk themselves. They would rather pay an extra but certain amount to the lender to compensate them for bearing the inflation risk instead \\(R = r + i_{Expected} + i_{Unexpected}\\) In other words, Lenders are willing to pay a higher base interest to retain the risk of inflation with the lender . In finance terms, since the lender experiences more risk, they have a higher reward \\(\\left( r + i_{e} + i_{ue} \\right)\\) The problem is that the there is no guarantee that the amount loaded is equivalent to the actual inflation rate experienced . It could be better, or it could be worse In finance/economics terms, we can view them in two different ways: With inflation protection \u2192 Real Interest Rate \\(\\left( \\mathbf{r - c} \\right)\\) Without inflation protection \u2192 Nominal Interest Rate \\(\\left( \\mathbf{r +}\\mathbf{i}_{\\mathbf{e}}\\mathbf{+}\\mathbf{i}_{\\mathbf{ue}} \\right)\\) In a perfect world , where inflation can be predicted perfectly \\(\\left( i_{e} + i_{ue} = i_{a} \\right)\\) , then the following equation holds true (But the idea is there): {width=\"3.7416666666666667in\" height=\"2.057791994750656in\"}","title":"Inflation Risk Premium"},{"location":"1.%20Introductory/ASA-FM/Test/#other-points","text":"Bonds in the US are usually issued at nominal rates rather than real rates US tax is linked to the inflation rates","title":"Other Points"},{"location":"1.%20Introductory/ASA-FM/Test/#liquidity-risk-premium","text":"All else equal, an investment that is less-liquid (Harder to convert to cash) have higher interest rates Liquidity is defined as the speed and ease at which an asset can be converted to cash, without significant loss in value The risk posed by an ill-liquid investment is that if it has to be quickly converted to cash, it will lose a significant amount of its value Thus, in order to compensate for this loss of value, ill-liquid investments demand higher interest rates","title":"Liquidity Risk Premium"},{"location":"1.%20Introductory/ASA-FM/Test/#financial-calculator-tipsunderline","text":"","title":"[Financial Calculator Tips]{.underline}"},{"location":"1.%20Introductory/ASA-FM/Test/#overview-of-using-the-ba-ii-plus","text":"It is a calculator with pre-programmed financial formulas . While this saves you the time of remembering and writing out formulas, you must still understand how the formulas work to know which to use and what to input Its main usage should be for these financial calculations only. While it can perform more general calculations, it is recommended to stick to a scientific calculator for those","title":"Overview of using the BA II Plus"},{"location":"1.%20Introductory/ASA-FM/Test/#basic-settings-to-toggle-beforehand","text":"By default, the calculator only shows values up to 2 decimal places. Due to rounding errors in intermediate steps, this is not ideal for a math paper. We can change the number of decimal places displayed: [2 nd .]{.underline} \u2192 DEC \u2192 [9]{.underline} This sets the calculator to FLOAT , which means that it displays up to 8 decimal places if available, otherwise up to the number of non-zero places The calculator only allows for one step of calculations at a time and evaluates them immediately. It can be annoying to calculate a value using an equation this way, as one would have to use brackets (which can't be easily seen) to input successive steps. We can change the way the calculator evaluates a series of instructions: [2 nd .]{.underline} \u2192 CHN \u2192 [2 nd ENTER]{.underline} This sets the calculator to Algebraic Operating System which follows the BODMAS format of evaluating equations","title":"Basic settings to toggle beforehand"},{"location":"1.%20Introductory/ASA-FM/Test/#basic-calculator-features","text":"Financial Functions -- Since the calculator already has formulas built in, a large part of the process is about assigning values to these variables. There are two different ways: Value to Variable \u2192 Key in the [VALUES]{.underline} then press the [VARIABLE]{.underline} [CPT]{.underline} \u2192 [TARGET VARIABLE]{.underline} Variable to Value \u2192 Press the [VARIABLE]{.underline} then key in the [VALUES]{.underline} & press [ENTER]{.underline} [TARGET VARIABLE]{.underline} \u2192 [CPT]{.underline} Always ensure that the [TARGET VARIABLE]{.underline} is set to [0]{.underline} In all cases, we can confirm that the value has been properly assigned to the variable when we see a full stop after the value Once we are done with the function, we should use [2 ND CE|C]{.underline} or [2 nd FV]{.underline} to reset all other variables Storing Variables -- It is not practical or memorize intermediate values. We can store them in one of the 9 memory slots and recall them at any time: [VALUE]{.underline} \u2192 [STO]{.underline} \u2192 [1-9]{.underline} (Stores a value in the chosen memory slot) [RCL]{.underline} \u2192 [1-9]{.underline} (Recalls a value from the chosen memory slot) [2 nd 0]{.underline} \u2192 [M1-9]{.underline} \u2192 [VALUE]{.underline} \u2192 [ENTER]{.underline} (View current stored data and manually edit","title":"Basic calculator features"},{"location":"1.%20Introductory/ASA-FM/Test/#positive-and-negative-signs","text":"We use Positive signs for Cash Inflows and Negative signs for Cash Outflows. We can toggle between positive and negative using \\(+ | -\\) We can determine whether they are Inflows and Outflows by choosing whose perspective we want to take - Investor or the Fund Manager As an Investor , you pay an initial amount along with periodic payments. At the end, we receive our returns. [PV]{.underline} and [PMT]{.underline} are thus negative , while [FV]{.underline} is positive. As the Fund Manager , you receive an initial amount along with periodic payments. At the end, you pay back the total returns. [PV]{.underline} and [PMT]{.underline} are thus positive , while [FV]{.underline} is negative","title":"Positive and Negative Signs"},{"location":"1.%20Introductory/ASA-FM/Test/#time-value-of-money_1","text":"The function can be used to determine the present value of any annuities and Bonds [N]{.underline} \u2192 Number of periods As the name suggests, this can represent any time period (Months/Years etc) For perpetuities, we can input any number sufficiently large number (~999) [I/Y]{.underline} \u2192 Effective Interest per period (Integer) [PV]{.underline} \u2192 Present Value [PMT]{.underline} \u2192 Payment for Level Annuity Immediate (Arrears) By default, the option is payments at the end of the period . We can change this by going to [2 nd PMT]{.underline} \u2192 [2 nd ENTER]{.underline} which sets the mode to Beginning of the period instead [FV]{.underline} \u2192 Future Value","title":"Time Value of Money"},{"location":"1.%20Introductory/ASA-FM/Test/#amortization-schedule","text":"This mode can only be accessed after filling in the relevant TVM variables for a bond [2 nd PV]{.underline} \u2192 Amortization Mode P1 \u2192 Starting Period P2 \u2192 Ending Period Both should be the same value to find amortization in a specific period BAL \u2192 Remaining Balance of the Loan after P2 PRN \u2192 Sum of the Principal paid from P1 to P2 INT \u2192 Sum of the interest paid from P1 to P2","title":"Amortization Schedule"},{"location":"1.%20Introductory/ASA-FM/Test/#cashflow-function","text":"[CF]{.underline} \u2192 Cashflow Mode CF0 \u2192 Cashflow at time 0 This is typically the initial investment amount & hence should be a negative value C01 \u2192 Cashflow at time 1 F01 \u2192 Frequency of C01 (How many periods it repeats -- For perpetuities, put 999) \\(\\vdots\\) [NPV]{.underline} \u2192 Net Present Value Mode I \u2192 Interest Rate (Integer, in %) [IRR]{.underline} \u2192 Internal Rate of Return","title":"Cashflow Function"},{"location":"1.%20Introductory/ASA-FM/Test/#interest-rate-conversion-nominal-to-effective","text":"[2 nd 2]{.underline} \u2192 Enter Interest rate conversion mode EFF \u2192 Effective Interest ( EAR - Effective Annual Rate) NOM \u2192 Nominal Interest Rate ( APR -- Annual Percentage Rate) It is always assumed to be compounded 12 times a year unless stated otherwise C/Y \u2192 Compounding Frequency over a year \\((m)\\)","title":"Interest Rate Conversion (Nominal to Effective)"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/1.%20Introduction/","text":"","title":"Introduction"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/2.%20Forwards%20%26%20Futures/","text":"","title":"2. Forwards & Futures"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/3.%20Options/","text":"Options \u00b6 Overview \u00b6 Options are contracts where the contract owner has the right to trade an asset at a fixed price on or before a specified future date. The buyer of contract is known as the Option Holder while the Seller is known as the Option Writer . The buyer pays the seller a Premium at time 0 for fulfilling the contract. The option holder can Exercise their Right to trade an asset while the option writer is Obligated to fulfil the trade. Naturally, option holders will only exercise their right when it is favourable to do so; resulting in a positive payoff . The right to Buy an Asset is known as a Call Option (Calls) while the right to Sell an Asset is known as a Put Option (Puts). Warning It is a common misconception to mix up the buyer and seller of the option with buying and selling the asset. For both Call and Put Options, there will always be a buyer and a seller. The fixed price is known as the Strike Price \\((K)\\) and the future date is known as the Expiration Date \\((T)\\) . There are three types of options, based on when they can be exercised: American Options : Any time on or before the expiration date Bermudan Options : Only during a specified window European Options : Only on the expiration date itself For the purposes of this exam, only European and American Options will be covered in depth. European Option Premiums are denoted in Lower Case \\((c_0, p_0)\\) while American Options are denoted using Upper Case \\((C_0, P_0)\\) . European Options \u00b6 Call Options \u00b6 Consider a Long Call Option : Exercise Do Not Exercise Intuition Cheaper to buy via Call Expensive to buy via Call Scenario Spot is larger than Strike \\((S_T \\gt K)\\) Spot is smaller than Strike \\((S_T \\lt K)\\) Payoff Buy at strike and sell at spot \\((S_T - K)\\) No trade occurs \\((0)\\) Profit Positive Profit \\((S_T - K - c_0)\\) Negative Profit \\((-c_0)\\) This can also be expressed in a piecewise function : \\[ \\begin{aligned} \\text{Payoff}_\\text{Long} &= \\begin{cases} 0 & S_T \\leq K, \\\\ S_T - K & S_T > K \\end{cases} \\\\ &= \\max(S_T \u2013 K, 0) \\\\ \\\\ \\text{Profit}_\\text{Long} &= \\max(S_T \u2013 K, 0) \u2013 c_0 \\end{aligned} \\] Tip The key takeaway is that Options will only be exercised if there are positive payoffs, but positive payoffs do not guarantee that there are positive profits . Consider the possible payoffs and profits for a long call: Payoff Profit Minimum Scenario Not exercised, no trade occurs Premium already paid Minimum Value \\(\\text{Payoff} = 0\\) \\(\\text{Profit} = -c_0\\) Maximum Scenario Exercised, as high as the stock price Premium Already paid Maximum Value \\(\\text{Payoff} = \\infty\\) \\(\\text{Profit }= \\infty - c_0 = \\infty\\) Note There is no limit for the stock price, thus the maximum stock price is best represented using infinity. Thus, Long Call Options have Unlimited Upside \\((\\infty)\\) and Limited Downsides \\((0, -c_0)\\) : Using the zero-sum game property , Short Calls are simply the opposite of their long counterparts: \\[ \\begin{aligned} \\text{Payoff}_\\text{Short} &= \\begin{cases} 0 & S_T \\leq K, \\\\ -(S_T \u2013 K) & S_T > K \\end{cases} \\\\ &= -\\max(S_T \u2013 K, 0) \\\\ \\\\ \\text{Profit}_\\text{Short} &= c_0 - \\max(S_T \u2013 K, 0) \\end{aligned} \\] Unlike their long counterparts, they have Limited Upside \\((0, c_0)\\) and Unlimited Downside \\((-\\infty)\\) : Put Options \u00b6 A Long Put Option is the opposite of a long call as the perspective has been flipped to selling instead: Exercise Do Not Exercise Intuition Better to sell via Put Worse to sell via Put Scenario Spot is smaller than Strike \\((S_T \\lt K)\\) Spot is larger than Strike \\((S_T \\gt K)\\) Payoff Buy at spot and sell at strike \\((K - S_T)\\) No trade occurs \\((0)\\) Profit Positive Profit \\((K - S_T - p_0)\\) Negative Profit \\((-p_0)\\) This can also be expressed in a piecewise function : \\[ \\begin{aligned} \\text{Payoff}_\\text{Long} &= \\begin{cases} K \u2013 S_T & S_T < K, \\\\ 0 & S_T \\geq K \\end{cases} \\\\ &= \\max(K - S_T, 0) \\\\ \\\\ \\text{Profit}_\\text{Long} &= \\max(K - S_T, 0) \u2013 p_0 \\end{aligned} \\] Consider the possible payoffs and profits for a long put: Payoff Profit Minimum Scenario Not exercised, no trade occurs Premium already paid Minimum Value \\(\\text{Payoff} = 0\\) \\(\\text{Profit} = -p_0\\) Maximum Scenario Exercised, as high as the strike price Premium Already paid Maximum Value \\(\\text{Payoff} = K\\) \\(\\text{Profit }= K - p_0\\) Note The lowest possible stock price is 0, which is why the maximum possible payoff is just the strike price \\(K-0 = K\\) . Thus, Long Call Options have higher upside \\((K, K-p_0)\\) and lower Downsides \\((0, -c_0)\\) : Using the zero-sum game property , Short Puts are simply the opposite of their long counterparts: \\[ \\begin{aligned} \\text{Payoff}_\\text{Short} &= \\begin{cases} -(K \u2013 S_T) & S_T < K, \\\\ 0 & S_T \\ge K \\end{cases} \\\\ &= -\\max(K - S_T, 0) \\\\ \\\\ \\text{Profit}_\\text{Short} &= p_0 - \\max(S_T \u2013 K, 0) \\end{aligned} \\] Unlike their long counterparts, they have Lower Upside \\((0, p_0)\\) and Higher Downside \\((-K, p_0 - K)\\) : Moneyness \u00b6 Option Moneyness is a notional indicator about the payoff of a LONG option should it be exercised immediately at the current spot price . It does not matter if the option can or cannot be exercised; it is meant to provide a rough gauge about what the spot price currently is without having to provide its exact value. In the Money (ITM) At the Money (ATM) Out the Money (OTM) Calls Positive Payoff \\((S_T > K)\\) Zero Payoff \\((S_T = K)\\) Negative Payoff \\((S_T < K)\\) Puts Positive Payoff \\((S_T < K)\\) Zero Payoff \\((S_T = K)\\) Negative Payoff \\((S_T = K)\\) Put Call Parity \u00b6 Put Call Parity (PCP) is an equation that relates Calls, Puts and an underlying Forward Contract: \\[ p_0 \\ \u2013 \\ c_0 = Ke^{-rt} \\ \u2013 \\ F_0^P \\] Warning The equation takes the perspective of the initial cashflows of the position, rather than the terminal cashflow, which is what we are more used to working with. Positive Sign : Cashflow in (Revenue from selling ) Negative Sign : Cashflow Out (Cost of buying ) No Arbitrage Proof \u00b6 Consider the following two portfolios which have the same payoffs: One Long Call & One Short Put One Long Forward only \\[ \\begin{aligned} \\text{Payoff} &= \\text{Payoff}_\\text{Long Call} + \\text{Payoff}_\\text{Short Put} \\\\ &= \\max(S_T \\ - \\ K, \\ 0) - \\max(K \\ - \\ S_T, \\ 0) \\\\ &= S_T \\ \u2013 \\ K \\end{aligned} \\] By the Law of One Price , both of the portfolios must be priced the same , resulting in the PCP equation. The price of the first portfolio is simply the difference in premiums of the two options: Pay premium for buying the Call Receive premium selling the Put \\[ \\text{Price}_\\text{Options} = - c_0 + p_0 \\] The price of the second portfolio is harder to determine as the forward price is usually paid at expiration rather than at the inception . Thus, we need to come up with a replicating portfolio to determine the time 0 price of a forward. A forward contract consists of two components: Buying the asset at a fixed price ; akin to selling a Zero Coupon Bond Selling the asset at the future spot price ; akin to buying a Prepaid Forward \\[ \\begin{aligned} \\text{Price}_\\text{Long Forward} &= Ke^{-rt} \u2013 F_0^P \\\\ &= Ke^{-rt} \u2013 S_0 \\cdot e^{rT} \\end{aligned} \\] Note As its name suggests, a prepaid forward is simply a forward contract whose price is paid for at time 0 rather than at expiration. Intuitively, this time 0 price is simply the PV of the price that would have been paid at maturity: \\[ F^P_0 = \\text{PV}(F_0) \\] However, since the forward price depends on the underlying, the prepaid forward price depends on the underlying as well: Prepaid Forward Price Forward Price No Dividends \\(S_0\\) \\(S_0 \\cdot e^{rT}\\) Discrete Dividends \\(S_0 - \\text{PV Dividends}\\) \\(S_0 - \\text{AV Dividends}\\) Continuous Dividends \\(S_0 \\cdot e^{-qt}\\) \\(S_0 \\cdot e^{(r-q)t}\\) For simplicity, the no dividend stock was used in the expression above. Thus, following the law of one price, the price of the two portfolios must be equal: \\[ p_0 \\ \u2013 \\ c_0 = Ke^{-rt} \\ \u2013 \\ F_0^P \\] Synthetic Positions \u00b6 One practical application of the PCP equation is to easily identify and create replicating portfolios for each of its individual components. This can be done by simply re-arranging the equation to match the time 0 cashflow of the position we want to replicate: Position Replicating Portfolio Long Call \\(-c_0 = -p_0 + Ke^{-rt} - F^P_0\\) Long Put \\(-p_0 = -c_0 - Ke^{-rt} + F^P_0\\) Long ZC Bond \\(-Ke^{-rt} = -p_0 + c_0 - F^P_0\\) Long Stock \\(-F^P_0 = - S_0 = -c_0 - Ke^{-rt} + p_0\\) Note For a short selling position, simply re-arrange the equations to obtain a positive expression on the LHS. Risk Free Rate \u00b6 Another application of the PCP equation is to solve for the risk free rates . This can be done when PCP is given for multiple dates \\[ \\begin{align*} p_{0, T} \\ \u2013 \\ c_{0, T} &= Ke^{-rT} \u2013 S_0 \\\\ p_{t, T} \\ \u2013 \\ c_{t, T} &= Ke^{-r(T-t)} \u2013 S_t \\\\ \\\\ \\therefore e^{rt} &= \\frac{Ke^{-rT}}{Ke^{-r(T-t)}} \\\\ &= \\frac{ p_{0, T} \u2013 c_{o, T} + S_0 }{ p_{t, T} \u2013 c_{t, T} + S_t} \\end{align*} \\]","title":"Options"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/3.%20Options/#options","text":"","title":"Options"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/3.%20Options/#overview","text":"Options are contracts where the contract owner has the right to trade an asset at a fixed price on or before a specified future date. The buyer of contract is known as the Option Holder while the Seller is known as the Option Writer . The buyer pays the seller a Premium at time 0 for fulfilling the contract. The option holder can Exercise their Right to trade an asset while the option writer is Obligated to fulfil the trade. Naturally, option holders will only exercise their right when it is favourable to do so; resulting in a positive payoff . The right to Buy an Asset is known as a Call Option (Calls) while the right to Sell an Asset is known as a Put Option (Puts). Warning It is a common misconception to mix up the buyer and seller of the option with buying and selling the asset. For both Call and Put Options, there will always be a buyer and a seller. The fixed price is known as the Strike Price \\((K)\\) and the future date is known as the Expiration Date \\((T)\\) . There are three types of options, based on when they can be exercised: American Options : Any time on or before the expiration date Bermudan Options : Only during a specified window European Options : Only on the expiration date itself For the purposes of this exam, only European and American Options will be covered in depth. European Option Premiums are denoted in Lower Case \\((c_0, p_0)\\) while American Options are denoted using Upper Case \\((C_0, P_0)\\) .","title":"Overview"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/3.%20Options/#european-options","text":"","title":"European Options"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/3.%20Options/#call-options","text":"Consider a Long Call Option : Exercise Do Not Exercise Intuition Cheaper to buy via Call Expensive to buy via Call Scenario Spot is larger than Strike \\((S_T \\gt K)\\) Spot is smaller than Strike \\((S_T \\lt K)\\) Payoff Buy at strike and sell at spot \\((S_T - K)\\) No trade occurs \\((0)\\) Profit Positive Profit \\((S_T - K - c_0)\\) Negative Profit \\((-c_0)\\) This can also be expressed in a piecewise function : \\[ \\begin{aligned} \\text{Payoff}_\\text{Long} &= \\begin{cases} 0 & S_T \\leq K, \\\\ S_T - K & S_T > K \\end{cases} \\\\ &= \\max(S_T \u2013 K, 0) \\\\ \\\\ \\text{Profit}_\\text{Long} &= \\max(S_T \u2013 K, 0) \u2013 c_0 \\end{aligned} \\] Tip The key takeaway is that Options will only be exercised if there are positive payoffs, but positive payoffs do not guarantee that there are positive profits . Consider the possible payoffs and profits for a long call: Payoff Profit Minimum Scenario Not exercised, no trade occurs Premium already paid Minimum Value \\(\\text{Payoff} = 0\\) \\(\\text{Profit} = -c_0\\) Maximum Scenario Exercised, as high as the stock price Premium Already paid Maximum Value \\(\\text{Payoff} = \\infty\\) \\(\\text{Profit }= \\infty - c_0 = \\infty\\) Note There is no limit for the stock price, thus the maximum stock price is best represented using infinity. Thus, Long Call Options have Unlimited Upside \\((\\infty)\\) and Limited Downsides \\((0, -c_0)\\) : Using the zero-sum game property , Short Calls are simply the opposite of their long counterparts: \\[ \\begin{aligned} \\text{Payoff}_\\text{Short} &= \\begin{cases} 0 & S_T \\leq K, \\\\ -(S_T \u2013 K) & S_T > K \\end{cases} \\\\ &= -\\max(S_T \u2013 K, 0) \\\\ \\\\ \\text{Profit}_\\text{Short} &= c_0 - \\max(S_T \u2013 K, 0) \\end{aligned} \\] Unlike their long counterparts, they have Limited Upside \\((0, c_0)\\) and Unlimited Downside \\((-\\infty)\\) :","title":"Call Options"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/3.%20Options/#put-options","text":"A Long Put Option is the opposite of a long call as the perspective has been flipped to selling instead: Exercise Do Not Exercise Intuition Better to sell via Put Worse to sell via Put Scenario Spot is smaller than Strike \\((S_T \\lt K)\\) Spot is larger than Strike \\((S_T \\gt K)\\) Payoff Buy at spot and sell at strike \\((K - S_T)\\) No trade occurs \\((0)\\) Profit Positive Profit \\((K - S_T - p_0)\\) Negative Profit \\((-p_0)\\) This can also be expressed in a piecewise function : \\[ \\begin{aligned} \\text{Payoff}_\\text{Long} &= \\begin{cases} K \u2013 S_T & S_T < K, \\\\ 0 & S_T \\geq K \\end{cases} \\\\ &= \\max(K - S_T, 0) \\\\ \\\\ \\text{Profit}_\\text{Long} &= \\max(K - S_T, 0) \u2013 p_0 \\end{aligned} \\] Consider the possible payoffs and profits for a long put: Payoff Profit Minimum Scenario Not exercised, no trade occurs Premium already paid Minimum Value \\(\\text{Payoff} = 0\\) \\(\\text{Profit} = -p_0\\) Maximum Scenario Exercised, as high as the strike price Premium Already paid Maximum Value \\(\\text{Payoff} = K\\) \\(\\text{Profit }= K - p_0\\) Note The lowest possible stock price is 0, which is why the maximum possible payoff is just the strike price \\(K-0 = K\\) . Thus, Long Call Options have higher upside \\((K, K-p_0)\\) and lower Downsides \\((0, -c_0)\\) : Using the zero-sum game property , Short Puts are simply the opposite of their long counterparts: \\[ \\begin{aligned} \\text{Payoff}_\\text{Short} &= \\begin{cases} -(K \u2013 S_T) & S_T < K, \\\\ 0 & S_T \\ge K \\end{cases} \\\\ &= -\\max(K - S_T, 0) \\\\ \\\\ \\text{Profit}_\\text{Short} &= p_0 - \\max(S_T \u2013 K, 0) \\end{aligned} \\] Unlike their long counterparts, they have Lower Upside \\((0, p_0)\\) and Higher Downside \\((-K, p_0 - K)\\) :","title":"Put Options"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/3.%20Options/#moneyness","text":"Option Moneyness is a notional indicator about the payoff of a LONG option should it be exercised immediately at the current spot price . It does not matter if the option can or cannot be exercised; it is meant to provide a rough gauge about what the spot price currently is without having to provide its exact value. In the Money (ITM) At the Money (ATM) Out the Money (OTM) Calls Positive Payoff \\((S_T > K)\\) Zero Payoff \\((S_T = K)\\) Negative Payoff \\((S_T < K)\\) Puts Positive Payoff \\((S_T < K)\\) Zero Payoff \\((S_T = K)\\) Negative Payoff \\((S_T = K)\\)","title":"Moneyness"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/3.%20Options/#put-call-parity","text":"Put Call Parity (PCP) is an equation that relates Calls, Puts and an underlying Forward Contract: \\[ p_0 \\ \u2013 \\ c_0 = Ke^{-rt} \\ \u2013 \\ F_0^P \\] Warning The equation takes the perspective of the initial cashflows of the position, rather than the terminal cashflow, which is what we are more used to working with. Positive Sign : Cashflow in (Revenue from selling ) Negative Sign : Cashflow Out (Cost of buying )","title":"Put Call Parity"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/3.%20Options/#no-arbitrage-proof","text":"Consider the following two portfolios which have the same payoffs: One Long Call & One Short Put One Long Forward only \\[ \\begin{aligned} \\text{Payoff} &= \\text{Payoff}_\\text{Long Call} + \\text{Payoff}_\\text{Short Put} \\\\ &= \\max(S_T \\ - \\ K, \\ 0) - \\max(K \\ - \\ S_T, \\ 0) \\\\ &= S_T \\ \u2013 \\ K \\end{aligned} \\] By the Law of One Price , both of the portfolios must be priced the same , resulting in the PCP equation. The price of the first portfolio is simply the difference in premiums of the two options: Pay premium for buying the Call Receive premium selling the Put \\[ \\text{Price}_\\text{Options} = - c_0 + p_0 \\] The price of the second portfolio is harder to determine as the forward price is usually paid at expiration rather than at the inception . Thus, we need to come up with a replicating portfolio to determine the time 0 price of a forward. A forward contract consists of two components: Buying the asset at a fixed price ; akin to selling a Zero Coupon Bond Selling the asset at the future spot price ; akin to buying a Prepaid Forward \\[ \\begin{aligned} \\text{Price}_\\text{Long Forward} &= Ke^{-rt} \u2013 F_0^P \\\\ &= Ke^{-rt} \u2013 S_0 \\cdot e^{rT} \\end{aligned} \\] Note As its name suggests, a prepaid forward is simply a forward contract whose price is paid for at time 0 rather than at expiration. Intuitively, this time 0 price is simply the PV of the price that would have been paid at maturity: \\[ F^P_0 = \\text{PV}(F_0) \\] However, since the forward price depends on the underlying, the prepaid forward price depends on the underlying as well: Prepaid Forward Price Forward Price No Dividends \\(S_0\\) \\(S_0 \\cdot e^{rT}\\) Discrete Dividends \\(S_0 - \\text{PV Dividends}\\) \\(S_0 - \\text{AV Dividends}\\) Continuous Dividends \\(S_0 \\cdot e^{-qt}\\) \\(S_0 \\cdot e^{(r-q)t}\\) For simplicity, the no dividend stock was used in the expression above. Thus, following the law of one price, the price of the two portfolios must be equal: \\[ p_0 \\ \u2013 \\ c_0 = Ke^{-rt} \\ \u2013 \\ F_0^P \\]","title":"No Arbitrage Proof"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/3.%20Options/#synthetic-positions","text":"One practical application of the PCP equation is to easily identify and create replicating portfolios for each of its individual components. This can be done by simply re-arranging the equation to match the time 0 cashflow of the position we want to replicate: Position Replicating Portfolio Long Call \\(-c_0 = -p_0 + Ke^{-rt} - F^P_0\\) Long Put \\(-p_0 = -c_0 - Ke^{-rt} + F^P_0\\) Long ZC Bond \\(-Ke^{-rt} = -p_0 + c_0 - F^P_0\\) Long Stock \\(-F^P_0 = - S_0 = -c_0 - Ke^{-rt} + p_0\\) Note For a short selling position, simply re-arrange the equations to obtain a positive expression on the LHS.","title":"Synthetic Positions"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/3.%20Options/#risk-free-rate","text":"Another application of the PCP equation is to solve for the risk free rates . This can be done when PCP is given for multiple dates \\[ \\begin{align*} p_{0, T} \\ \u2013 \\ c_{0, T} &= Ke^{-rT} \u2013 S_0 \\\\ p_{t, T} \\ \u2013 \\ c_{t, T} &= Ke^{-r(T-t)} \u2013 S_t \\\\ \\\\ \\therefore e^{rt} &= \\frac{Ke^{-rT}}{Ke^{-r(T-t)}} \\\\ &= \\frac{ p_{0, T} \u2013 c_{o, T} + S_0 }{ p_{t, T} \u2013 c_{t, T} + S_t} \\end{align*} \\]","title":"Risk Free Rate"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/4.%20Option%20Strategies/","text":"","title":"4. Option Strategies"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/5.%20Binomial%20Model/","text":"Binomial Model \u00b6 Overview \u00b6 The simplest option pricing model is the Binomial Model . All option pricing models attempt to predict the price of the underlying at expiration and thus work backwards to determine the price of the option. As its name suggests, the binomial model assumes that in each period \\((h)\\) , the stock price can only move in two ways - Up \\((u)\\) or Down \\((d)\\) : \\[ \\begin{aligned} S_u &= S_0 \\cdot u \\\\ S_d &= S_0 \\cdot d \\end{aligned} \\] Warning It is a common misconception to think that up or down is used relative to the starting stock price. It is meant to be used relative to one another , thus both can be higher or lower than the starting price but still smaller than the other. \\[ S_u > S_d \\equiv u > d \\] However, it is impossible to know which of the two ways that the stock will take. Thus, the probability of going up is denoted as \\(p\\) while the probability of going down is its complement \\(1-p\\) . Given that there only two outcomes, it can easily illustrated via a probability tree , which is why they are also referred to as Binomial Trees : Single Period Trees \u00b6 The simplest Binomial Model assumes that there is only one period from inception to expiration . Though unrealistic, they allow us to easily understand the core concepts behind binomial trees. SP Replicating Portfolio Method \u00b6 The first way to use a binomial tree is through the Replicating Portfolio Method . As its name suggests, it attempts to create a replicating portfolio that will payoff of the option regardless of which way the stock moves . By the law of one price, the cost of the option is the cost of the portfolio . The replicating portfolio is assumed to consist of Stocks \\((\\delta)\\) and Bonds \\((B)\\) . The payoff of the options are denoted using \\(V\\) . The goal is to set up a simultaneous equation for each of the two ways that the price can move and thus solve for the \\(\\Delta\\) and \\(B\\) : \\[ \\begin{aligned} \\Delta S_0 u + B e^{rT} &= V_u \\\\ \\Delta S_0 d + B e^{rT} &= V_d \\end{aligned} \\] Warning Bonds are usually stated in terms of their face values, but in here \\(B\\) is the price at time 0 . Do not be confused! Subtracting the down scenario from the up scenario, \\[ \\begin{aligned} \\Delta S_0 u + B e^{rT} - (\\Delta S_0 d + B e^{rT}) &= V_u - V_d \\\\ \\Delta S_0 (u - d) &= V_u - V_d \\\\ \\Delta &= \\frac{V_u - V_d}{S_0 (u - d)} \\end{aligned} \\] Substituting it back in, \\[ \\begin{aligned} \\Delta S_0 u + B e^{rT} &= V_u \\\\ \\frac{V_u - V_d}{S_0 (u - d)} \\cdot S_0 u + B e^{rT} &= V_u \\\\ \\frac{V_u - V_d}{(u - d)} \\cdot u + B e^{rT} &= V_u \\\\ B e^{rT} &= \\frac{uV_d - dV_u}{u-d} \\\\ B &= e^{-rT} \\cdot \\frac{uV_d - dV_u}{u-d} \\end{aligned} \\] Thus, the price of the option is equal to the price of the portfolio: \\[ \\text{Option Price} = \\Delta \\cdot S_0 + B \\] Calls Puts Buy Stock \\((\\Delta \\gt 0)\\) Sell Stock ( \\(\\Delta \\gt 0\\) ) Sell Bonds/Borrow Money \\((B \\lt 0)\\) Buy Bonds/Lend money \\((B \\gt 0)\\) SP Risk Neutral Method \u00b6 Given the probabilities assigned earlier, the price of the option should be equal to the PV of the expected payoff : \\[ \\text{Option Price} = e^{-rt} \\cdot [p \\cdot V_u + (1-p) \\cdot V_d] \\] Similarly, the initial stock price should be the PV of the expected final stock prices , which will allow us to solve for \\(p\\) and hence calculate the option price: \\[ \\begin{aligned} S_0 &= e^{-rt} \\cdot [p \\cdot S_0 u + (1 \u2013 p) \\cdot S_0 d] \\\\ e^{rt} &= p \\cdot u + (1-p) \\cdot d \\\\ e^{rt} &= pu + d - pd \\\\ e^{rt} \u2013 d &= p(u \u2013 d) \\\\ p &= \\frac{e^{rt} - d}{u - d} \\\\ \\end{aligned} \\] Generally speaking, this method is preferred to the replicating portfolio one as it is much faster - simply plug in the appropriate values into the formula. Info Since \\(p\\) is a probability, it MUST be between 0 and 1 . Any violation of this would lead to an opportunity for arbitrage: \\[ \\begin{aligned} 0 &< p < 1 \\\\ 0 &< \\frac{e^{rt} - d}{u - d} \\\\ 0 &< e^{rt} \u2013 d < u - d \\\\ d &< e^{rt} < u \\end{aligned} \\] This further drives home the point that \\(u\\) and \\(d\\) do not have to be above or below 1 respectively - they just need to be larger or smaller relative to one another. Multi Period Trees \u00b6 A slightly more realistic binomial tree is one that has multiple periods , as the stock prices tend to change many times before expiration. For simplicity, we will only consider a two period binomial tree , but the concepts easily extend to more periods. We can think of multi-period trees as a single period tree that repeats itself . As such, all factors \\((u, d, p)\\) remain constant throughout the tree . The price of the option can thus be found by recursively solving one node at a time using any of the above methods from the RHS till the starting node is found. MP Replicating Portfolio Method \u00b6 Unfortunately, there is no shortcut for the replicating portfolio method - the components of the portfolio can only be calculated recursively. Note that the intuition of the method changes slightly. The portfolio formed at time 0 is a self-financing portfolio that will be sold each period to exactly purchase a new portfolio at that time , such that the final portfolio leading up to expiration will replicate the payoff. Tip If both 0 then starting must be 0 MP Risk Neutral Method \u00b6 Fortunately, there is a shortcut for the risk neutral method. For a two period model, there are three ending possibilities - Up Up, Up Down & Down Down. Instead of having to recursively solve for intermediate option prices, we can use the Binomial Distribution to directly solve for the overall probability of the ending node and hence immediately solve for the option price. The binomial distribution has two parameters: Number of trials : Number of periods Probability of Success : Probability of moving Up \\[ \\begin{aligned} X &\\sim \\text{Binomial}(n, p) \\\\ \\\\ dd &\\rightarrow K = 0 \\\\ ud &\\rightarrow K = 1 \\\\ uu &\\rightarrow K = 2 \\\\ \\\\ P(X = K) &= \\binom{n}{K} \\cdot p^K \\cdot (1 \u2013 p)^{n-k} \\end{aligned} \\] Warning The first term counts the number of ways that the ending node can be reached. The top and bottom node can only be reached in 1 way each while the middle node can be reached in 2 ways . It is a common mistake to forget to include the 2 for the middle node. Given this shortcut, the risk neutral method is once again preferred for solving multi-period problems, unless explicitly stated otherwise.","title":"Binomial Model"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/5.%20Binomial%20Model/#binomial-model","text":"","title":"Binomial Model"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/5.%20Binomial%20Model/#overview","text":"The simplest option pricing model is the Binomial Model . All option pricing models attempt to predict the price of the underlying at expiration and thus work backwards to determine the price of the option. As its name suggests, the binomial model assumes that in each period \\((h)\\) , the stock price can only move in two ways - Up \\((u)\\) or Down \\((d)\\) : \\[ \\begin{aligned} S_u &= S_0 \\cdot u \\\\ S_d &= S_0 \\cdot d \\end{aligned} \\] Warning It is a common misconception to think that up or down is used relative to the starting stock price. It is meant to be used relative to one another , thus both can be higher or lower than the starting price but still smaller than the other. \\[ S_u > S_d \\equiv u > d \\] However, it is impossible to know which of the two ways that the stock will take. Thus, the probability of going up is denoted as \\(p\\) while the probability of going down is its complement \\(1-p\\) . Given that there only two outcomes, it can easily illustrated via a probability tree , which is why they are also referred to as Binomial Trees :","title":"Overview"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/5.%20Binomial%20Model/#single-period-trees","text":"The simplest Binomial Model assumes that there is only one period from inception to expiration . Though unrealistic, they allow us to easily understand the core concepts behind binomial trees.","title":"Single Period Trees"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/5.%20Binomial%20Model/#sp-replicating-portfolio-method","text":"The first way to use a binomial tree is through the Replicating Portfolio Method . As its name suggests, it attempts to create a replicating portfolio that will payoff of the option regardless of which way the stock moves . By the law of one price, the cost of the option is the cost of the portfolio . The replicating portfolio is assumed to consist of Stocks \\((\\delta)\\) and Bonds \\((B)\\) . The payoff of the options are denoted using \\(V\\) . The goal is to set up a simultaneous equation for each of the two ways that the price can move and thus solve for the \\(\\Delta\\) and \\(B\\) : \\[ \\begin{aligned} \\Delta S_0 u + B e^{rT} &= V_u \\\\ \\Delta S_0 d + B e^{rT} &= V_d \\end{aligned} \\] Warning Bonds are usually stated in terms of their face values, but in here \\(B\\) is the price at time 0 . Do not be confused! Subtracting the down scenario from the up scenario, \\[ \\begin{aligned} \\Delta S_0 u + B e^{rT} - (\\Delta S_0 d + B e^{rT}) &= V_u - V_d \\\\ \\Delta S_0 (u - d) &= V_u - V_d \\\\ \\Delta &= \\frac{V_u - V_d}{S_0 (u - d)} \\end{aligned} \\] Substituting it back in, \\[ \\begin{aligned} \\Delta S_0 u + B e^{rT} &= V_u \\\\ \\frac{V_u - V_d}{S_0 (u - d)} \\cdot S_0 u + B e^{rT} &= V_u \\\\ \\frac{V_u - V_d}{(u - d)} \\cdot u + B e^{rT} &= V_u \\\\ B e^{rT} &= \\frac{uV_d - dV_u}{u-d} \\\\ B &= e^{-rT} \\cdot \\frac{uV_d - dV_u}{u-d} \\end{aligned} \\] Thus, the price of the option is equal to the price of the portfolio: \\[ \\text{Option Price} = \\Delta \\cdot S_0 + B \\] Calls Puts Buy Stock \\((\\Delta \\gt 0)\\) Sell Stock ( \\(\\Delta \\gt 0\\) ) Sell Bonds/Borrow Money \\((B \\lt 0)\\) Buy Bonds/Lend money \\((B \\gt 0)\\)","title":"SP Replicating Portfolio Method"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/5.%20Binomial%20Model/#sp-risk-neutral-method","text":"Given the probabilities assigned earlier, the price of the option should be equal to the PV of the expected payoff : \\[ \\text{Option Price} = e^{-rt} \\cdot [p \\cdot V_u + (1-p) \\cdot V_d] \\] Similarly, the initial stock price should be the PV of the expected final stock prices , which will allow us to solve for \\(p\\) and hence calculate the option price: \\[ \\begin{aligned} S_0 &= e^{-rt} \\cdot [p \\cdot S_0 u + (1 \u2013 p) \\cdot S_0 d] \\\\ e^{rt} &= p \\cdot u + (1-p) \\cdot d \\\\ e^{rt} &= pu + d - pd \\\\ e^{rt} \u2013 d &= p(u \u2013 d) \\\\ p &= \\frac{e^{rt} - d}{u - d} \\\\ \\end{aligned} \\] Generally speaking, this method is preferred to the replicating portfolio one as it is much faster - simply plug in the appropriate values into the formula. Info Since \\(p\\) is a probability, it MUST be between 0 and 1 . Any violation of this would lead to an opportunity for arbitrage: \\[ \\begin{aligned} 0 &< p < 1 \\\\ 0 &< \\frac{e^{rt} - d}{u - d} \\\\ 0 &< e^{rt} \u2013 d < u - d \\\\ d &< e^{rt} < u \\end{aligned} \\] This further drives home the point that \\(u\\) and \\(d\\) do not have to be above or below 1 respectively - they just need to be larger or smaller relative to one another.","title":"SP Risk Neutral Method"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/5.%20Binomial%20Model/#multi-period-trees","text":"A slightly more realistic binomial tree is one that has multiple periods , as the stock prices tend to change many times before expiration. For simplicity, we will only consider a two period binomial tree , but the concepts easily extend to more periods. We can think of multi-period trees as a single period tree that repeats itself . As such, all factors \\((u, d, p)\\) remain constant throughout the tree . The price of the option can thus be found by recursively solving one node at a time using any of the above methods from the RHS till the starting node is found.","title":"Multi Period Trees"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/5.%20Binomial%20Model/#mp-replicating-portfolio-method","text":"Unfortunately, there is no shortcut for the replicating portfolio method - the components of the portfolio can only be calculated recursively. Note that the intuition of the method changes slightly. The portfolio formed at time 0 is a self-financing portfolio that will be sold each period to exactly purchase a new portfolio at that time , such that the final portfolio leading up to expiration will replicate the payoff. Tip If both 0 then starting must be 0","title":"MP Replicating Portfolio Method"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/5.%20Binomial%20Model/#mp-risk-neutral-method","text":"Fortunately, there is a shortcut for the risk neutral method. For a two period model, there are three ending possibilities - Up Up, Up Down & Down Down. Instead of having to recursively solve for intermediate option prices, we can use the Binomial Distribution to directly solve for the overall probability of the ending node and hence immediately solve for the option price. The binomial distribution has two parameters: Number of trials : Number of periods Probability of Success : Probability of moving Up \\[ \\begin{aligned} X &\\sim \\text{Binomial}(n, p) \\\\ \\\\ dd &\\rightarrow K = 0 \\\\ ud &\\rightarrow K = 1 \\\\ uu &\\rightarrow K = 2 \\\\ \\\\ P(X = K) &= \\binom{n}{K} \\cdot p^K \\cdot (1 \u2013 p)^{n-k} \\end{aligned} \\] Warning The first term counts the number of ways that the ending node can be reached. The top and bottom node can only be reached in 1 way each while the middle node can be reached in 2 ways . It is a common mistake to forget to include the 2 for the middle node. Given this shortcut, the risk neutral method is once again preferred for solving multi-period problems, unless explicitly stated otherwise.","title":"MP Risk Neutral Method"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/6.%20Black%20Scholes%20Model/","text":"Black Scholes Model \u00b6 Lognormal Returns \u00b6 Fitting Historical Data \u00b6 Black Scholes Formula \u00b6 Special Cases \u00b6","title":"Black Scholes Model"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/6.%20Black%20Scholes%20Model/#black-scholes-model","text":"","title":"Black Scholes Model"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/6.%20Black%20Scholes%20Model/#lognormal-returns","text":"","title":"Lognormal Returns"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/6.%20Black%20Scholes%20Model/#fitting-historical-data","text":"","title":"Fitting Historical Data"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/6.%20Black%20Scholes%20Model/#black-scholes-formula","text":"","title":"Black Scholes Formula"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/6.%20Black%20Scholes%20Model/#special-cases","text":"","title":"Special Cases"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/7.%20Option%20Greeks/","text":"","title":"7. Option Greeks"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/8.%20Exotics/","text":"","title":"8. Exotics"},{"location":"1.%20Introductory/ASA-IFM/1.%20Derivatives/9.%20Embedded%20Options/","text":"","title":"9. Embedded Options"},{"location":"1.%20Introductory/ASA-IFM/2.%20Financial%20Markets/1.%20Modern%20Portfolio%20Theory/","text":"","title":"1. Modern Portfolio Theory"},{"location":"1.%20Introductory/ASA-IFM/2.%20Financial%20Markets/2.%20Asset%20Pricing%20Models/","text":"","title":"2. Asset Pricing Models"},{"location":"1.%20Introductory/ASA-IFM/2.%20Financial%20Markets/3.%20Efficient%20Market%20Hypothesis/","text":"","title":"3. Efficient Market Hypothesis"},{"location":"1.%20Introductory/ASA-IFM/3.%20Corporate%20Finance/1.%20Raising%20Capital/","text":"","title":"1. Raising Capital"},{"location":"1.%20Introductory/ASA-IFM/3.%20Corporate%20Finance/2.%20Cost%20of%20Capital/","text":"","title":"2. Cost of Capital"},{"location":"1.%20Introductory/ASA-IFM/3.%20Corporate%20Finance/3.%20Risk%20Measures/","text":"","title":"3. Risk Measures"},{"location":"1.%20Introductory/ASA-IFM/3.%20Corporate%20Finance/4.%20Decision%20Making/","text":"","title":"4. Decision Making"},{"location":"2.%20Actuarial%20Mathematics/Overview%20of%20Insurance/","text":"","title":"Overview of Insurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/","text":"Profit Testing \u00b6 Overview \u00b6 In most other industries, the cost of the product is known beforehand, thus it is possible to determine its profitability beforehand. Unfortunately, due to the contingent nature of life assurances and annuities, it is impossible to determine profitability beforehand. Recall that insurers make certain assumptions about the policyholder and thus price the product accordingly. If the policyholder follows these assumptions exactly , then the resulting profit is known as the Expected Profit . In reality, the policyholder will deviate from the assumptions to varying extents, resulting in the Actual Profit . From an accounting perspective, what the insurer recognises as \"profit\" is the excess of actual profit over expected profit, also known as the Gain of the policy: \\[ \\text{Gain} = \\text{Actual Profit} - \\text{Expected Profit} \\] Expected Profit \u00b6 Cashflow Projections \u00b6 We first consider the cashflows that the insurer expects every policy year : At the start of the policy year, they collect premiums and pay expenses They invest the excess amount to earn interest at the end of the policy year They expect some policyholders to die and hence payout claims and any claims expenses at the end of the policy year Alternative Perspective If the insurer pays more expenses than premiums, then they have to borrow money , which means that they pay interest at the end of the policy year instead. The combination of these cashflows are known as the expected net cashflows for that policy year, calculated at the end of the policy year : \\[ \\text{Expected NCF} = (P - e)(1+i) - q_x \\cdot (B+E) \\] Survival Benefits For Annuities where there is instead a survival benefit, the insurer expects some policyholders to remain alive and hence pay them the survival benefits and any survival expenses , thus the payout should be multiplied by the survival probability instead : \\[ \\text{Expected NCF Annuity } = (P - e)(1+i) - p_x \\cdot (B^s+E^s) \\] For Endowments in particular, the survival benefit only occurs in the final year. Thus, in the final year , both the death and survival benefits must be accounted for: \\[ \\text{Expected NCF Final Year EA} = (P - e)(1+i) - q_x \\cdot (B+E) - p_x \\cdot (B^s+E^s) \\] Since the insurer holds reserves that cannot be touched, it can be thought of as an account balance : The reserve held at the beginning of the policy year is the opening balance Since the reserve is invested, it earns interest at the end of the policy year They receive the NCF at the end of the policy year They set up a new reserve at the end of the policy year ( closing balance ) for the expected remaining policyholders Since the closing balance is known, any excess over the closing balance is the expected profit over that policy year: \\[ \\begin{aligned} \\text{Expected Profit} &= {}_{t-1}V \\cdot (1+i) + \\text{Expected NCF} - p_x \\cdot {}_{t}V \\\\ &= ({}_{t-1}V + P - e)(1+i) - q_x \\cdot (B+E) - p_x \\cdot {}_{t}V \\end{aligned} \\] Different Interest Rates The interest earned on the reserves may be different from the interest used to compute the premiums and/or reserves. Be sure to read the question properly. This is equivalent to saying that the expected profit in each policy year is the combination of the NCFs AND the change in reserves for that policy year: \\[ \\begin{aligned} \\text{Expected Profit} &= \\text{Expected NCF} + {}_{t-1}V \\cdot (1+i) - p_x \\cdot {}_{t}V \\\\ &= \\text{Expected NCF} + \\text{Expected change in reserve} \\end{aligned} \\] There are two special cases for the reserves: Beginning of policy - Starting reserve is 0 End of policy - Ending reserve is 0 Profit Vector & Signature \u00b6 If the expected profit is calculated every policy year and collected together, then the resulting vector is known as the Profit Vector . For a typical contract, the profits are usually negative in the first (few) years and are small in magnitude across all years. This can be used to sense check the profit vector calculations. \\[ \\begin{aligned} \\text{PR} &= \\begin{pmatrix} \\text{PR}_1 \\\\ \\text{PR}_2 \\\\ \\vdots \\\\ \\text{PR}_t \\end{pmatrix} \\end{aligned} \\] Calculation Questions Most questions will only require computing the profit vector for a short period of time given the intensity needed to calculate the reserves at various points in time. If required to compute the profit vector for a long duration, the question will usually specify to ignore the change in reserves or simply provide them , which greatly simplifies the required calculations. However, the expected profit for each policy year makes the implicit assumption that the policyholder survives till the BEGINNING of that policy year . Thus, every element of the profit vector is calculated based off varying assumptions. This is not very useful and thus can be corrected by adjusting the profit vector with the probability of survival till the beginning each policy year. The resulting vector is the unconditional profit , known as the Profit Signature . \\[ \\begin{aligned} \\Pi &= \\begin{pmatrix} {}_{0}p_x \\\\ {}_{1}p_x \\\\ \\vdots \\\\ {}_{t-1}p_x \\end{pmatrix} * \\begin{pmatrix} \\text{PR}_1 \\\\ \\text{PR}_2 \\\\ \\vdots \\\\ \\text{PR}_t \\end{pmatrix} \\end{aligned} \\] Recall that \\({}_{0}p_x = 1\\) . Profit Measures \u00b6 Using the profit signature, insurers can gauge the profitability of the products using a variety of metrics. Apart from calculation, it is important to know how to explain how a change in any of the parameters affect these metrics. Net Present Value \u00b6 Net Present Value (NPV) follows its finance definition and is the excess of PV Inflows over Outflows . In this context, it is the sum of the present values of all elements within the profit signature: \\[ \\begin{aligned} \\text{NPV} &= \\left(v^1 \\quad v^2 \\quad \\dots \\quad v^t \\right) \\cdot \\begin{pmatrix} \\Pi_1 \\\\ \\Pi_2 \\\\ \\vdots \\\\ \\Pi_t \\end{pmatrix} \\\\ &= v^1 \\cdot \\Pi_1 + v^2 \\cdot \\Pi_2 + \\dots + v^t \\cdot \\Pi_t \\end{aligned} \\] Net Expected Present Value Note that the NPV is calculated based off the profit signature, NOT the profit vector. Thus, it is more appropriately called the Net Expected Present Value , as survival probabilities are taken into account as well. The interest rate used to discount the cashflows is known as the Discount Rate , and is usually different from the earned interest rate or the one used in pricing/reserving. Internal Rate of Return \u00b6 The Internal Rate of Return (IRR) is the Discount Rate that will result in an NPV of 0 : \\[ \\text{NPV}_\\text{IRR} = 0 \\] The IRR cannot be calculated through an equation and must be solved using numerical methods. However, it can also be calculated using the cashflow function in most financial calculators. Discounted Payback Period \u00b6 The Discounted Payback Period (DPP) is the earliest time in the contract which the NPV becomes positive : \\[ \\text{DPP} = min [t: NPV(t) > 0] \\] It is best calculated by incrementally calculating the NPV at various policy years, stopping when the NPV becomes positive. Profit Margin \u00b6 The Profit Margin ( \\(\\pi\\) ) is the ratio of the NPV to the EPV of premiums: \\[ \\pi = \\frac{\\text{NPV}}{\\text{EPV Premiums}} \\] Note that the EPV of premiums are discounted using the same interest as the NPV . Pricing \u00b6 A more realistic way to price would be to consider the actual cashflows and thus pricing the policy such that the desired profitability is achieved . This is done by leaving the premium as a variable within the profit expressions and solving for it. Alternatively, the current premium and profitability of the policy may be provided. In this scenario, it may be easier to consider the change in premium and the corresponding change in profitability instead of calculating it directly. Reserving \u00b6 Insurers may want to set reserves such that they avoid negative profits throughout the lifetime of the policy. This is done by setting reserves such that the profits are floored at 0 . The reserves are calculated recursively starting from the last policy year : Recall that in the last policy year, the ending reserve is 0 Set the profit to 0 & solve for the starting reserve If the starting reserve is negative, floor it at 0 Use the calculated amount above as the new ending reserve for the previous year & repeat the process until all reserves are calculated Since this process sets negative reserves to 0, it is known as Zeroization Alternatively, the question may provide the current profit for each year, with the profit in some years being negative. The goal is then to calculate the reserves such that the negative profits are floored to 0. The key is to understand that if the profit for all future years are positive, then the starting reserve for that year will always be floored at 0 . Thus, using this, the reserves are recursively calculated: Start from the latest policy year with negative profits Stop once there are no more negative profits There is no need to calculate the reserve for earlier years once all negative profits have been set to 0 because following the initial logic, the reserve for these earlier years will be 0 given that all future profits are non-negative! After Zeroization, recalculate the profit for all years: For years with negative profits where zeroization occurred, the profit is 0 For years with positive profits but were NOT just before negative profit years, the profit is unchanged For years with positive profits but were JUST before negative profit years, the profit is changed The profits have changed because there is now a new starting reserve for the following year with negative profits due to the zeroization. This affects the amount of reserve that has to be set up at the end of the current year, resulting in a new profit. If this new profit turns out to be negative as well , then repeat the process once more. Gains by Source \u00b6 As alluded to earlier, actual profits are simply profits that are based on what actually happens rather than what was assumed. From this and the expected profit, the gain from the policy is determined. The difference in actual versus expected (AvE) assumptions typically comes in three aspects - Expenses, Interest & Mortality. Thus, the gain can also be decomposed into these three aspects to precisely pinpoint which assumptions need tweaking. \\[ \\begin{aligned} \\text{Total Gain} &= \\text{Actual Profit} - \\text{Expected Profit} \\\\ &= \\text{Expense Gain} + \\text{Interest Gain} + \\text{Mortality Gain} \\end{aligned} \\] For this section, Expected and Actual variables are differentiated using \" \\('\\) \". Expense Gain \u00b6 Since expense is a cash outflow , gain is recognised when the expected outflow is larger than the actual outflow. \\[ \\begin{aligned} \\text{Expense Gain} &= \\text{Assumed Expense Profit} - \\text{Actual Expense Profit} \\\\ &= e(1+i) - q_x \\cdot E - (e'(1+i) - q_x \\cdot E') \\\\ &= (e-e')(1+i) - q_x (E-E') \\end{aligned} \\] Note that since only the gain due to expenses are desired, only the expense assumptions go through the AvE process. The rest of the assumptions follow what was expected. Interest Gain \u00b6 On the flipside, since interest is a cash inflow , gain is recognised when the expected inflow is smaller than the actual outflow. \\[ \\begin{aligned} \\text{Interest Gain} &= \\text{Actual Interest Profit} - \\text{Expected Interest Profit} \\\\ &= ({}_{t-1}V + P - e') \\cdot i' - ({}_{t-1}V + P - e') \\cdot i \\\\ &= (i' - i)({}_{t-1}V + P - e') \\end{aligned} \\] Similarly, since only the gain due to interest is desired, only the interest assumptions go through the AvE process. However, since the expense gain was already calculated , the actual expense is directly used. Thus, the order of calculation is important as it determines whether the actual or expected values are used - if the gain has already been calculated, then in all subsequent calculations, only the actual assumptions should be used. TBC the reason for this Mortality Gain \u00b6 Similarly, mortality is a cash outflow , gain is recognised when the expected outflow is larger than the actual outflow. \\[ \\begin{aligned} \\text{Mortality Gain} &= \\text{Assumed Mortality Profit} - \\text{Actual Mortality Profit} \\\\ &= q_x \\cdot (B+E') + (1-q_x) \\cdot {}_{t}V - [q'_x \\cdot (B+E') + (1-q'_x) \\cdot {}_{t}V] \\\\ &= q_x \\cdot (B+E') + {}_{t}V - q_x \\cdot {}_{t}V - [q'_x \\cdot (B+E') + {}_{t}V - q'_x \\cdot {}_{t}V] \\\\ &= q_x \\cdot (B+E') + {}_{t}V - q_x \\cdot {}_{t}V - q'_x \\cdot (B+E') - {}_{t}V + q'_x \\cdot {}_{t}V \\\\ &= (q_x - q'_x)(B+E') - (q_x - q'_x){}_{t}V \\\\ &= (q_x - q'_x)(B+E'-{}_{t}V) \\end{aligned} \\] The second term is also known as the Net Amount at Risk , which represents the additional amount that the insurer has to pay in the event of a claim. In practice, the question may provide the actual number of policyholders (NOP) instead of the probabilities. Thus, we need to solve for the actual probabilities using: \\[ q'_x = \\frac{\\text{NOP}_\\text{Beginning} - \\text{NOP}_\\text{Ending}}{\\text{NOP}_\\text{Beginning}} \\] For annuities, the expression is slightly different (TBC) \\[ \\begin{aligned} \\text{Mortality Gain} &= \\text{Assumed Mortality Profit} - \\text{Actual Mortality Profit} \\\\ &= p_x \\cdot (B+E') + p_x \\cdot {}_{t}V - [p'_x \\cdot (B+E') + p'_x \\cdot {}_{t}V] \\\\ &= p_x \\cdot (B + E' + {}_{t}V) - p'_x (B + E' + {}_{t}V) \\\\ &= (p_x - p'_x)(B + E' + {}_{t}V) \\end{aligned} \\] Per Policy Gain \u00b6 All the above calculations are known as the Per Policy gains - if the actual number of policies are given, then each gain needs to be multiplied by the NOP to determine the total gain made by the insurer.","title":"Profit Testing"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#profit-testing","text":"","title":"Profit Testing"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#overview","text":"In most other industries, the cost of the product is known beforehand, thus it is possible to determine its profitability beforehand. Unfortunately, due to the contingent nature of life assurances and annuities, it is impossible to determine profitability beforehand. Recall that insurers make certain assumptions about the policyholder and thus price the product accordingly. If the policyholder follows these assumptions exactly , then the resulting profit is known as the Expected Profit . In reality, the policyholder will deviate from the assumptions to varying extents, resulting in the Actual Profit . From an accounting perspective, what the insurer recognises as \"profit\" is the excess of actual profit over expected profit, also known as the Gain of the policy: \\[ \\text{Gain} = \\text{Actual Profit} - \\text{Expected Profit} \\]","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#expected-profit","text":"","title":"Expected Profit"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#cashflow-projections","text":"We first consider the cashflows that the insurer expects every policy year : At the start of the policy year, they collect premiums and pay expenses They invest the excess amount to earn interest at the end of the policy year They expect some policyholders to die and hence payout claims and any claims expenses at the end of the policy year Alternative Perspective If the insurer pays more expenses than premiums, then they have to borrow money , which means that they pay interest at the end of the policy year instead. The combination of these cashflows are known as the expected net cashflows for that policy year, calculated at the end of the policy year : \\[ \\text{Expected NCF} = (P - e)(1+i) - q_x \\cdot (B+E) \\] Survival Benefits For Annuities where there is instead a survival benefit, the insurer expects some policyholders to remain alive and hence pay them the survival benefits and any survival expenses , thus the payout should be multiplied by the survival probability instead : \\[ \\text{Expected NCF Annuity } = (P - e)(1+i) - p_x \\cdot (B^s+E^s) \\] For Endowments in particular, the survival benefit only occurs in the final year. Thus, in the final year , both the death and survival benefits must be accounted for: \\[ \\text{Expected NCF Final Year EA} = (P - e)(1+i) - q_x \\cdot (B+E) - p_x \\cdot (B^s+E^s) \\] Since the insurer holds reserves that cannot be touched, it can be thought of as an account balance : The reserve held at the beginning of the policy year is the opening balance Since the reserve is invested, it earns interest at the end of the policy year They receive the NCF at the end of the policy year They set up a new reserve at the end of the policy year ( closing balance ) for the expected remaining policyholders Since the closing balance is known, any excess over the closing balance is the expected profit over that policy year: \\[ \\begin{aligned} \\text{Expected Profit} &= {}_{t-1}V \\cdot (1+i) + \\text{Expected NCF} - p_x \\cdot {}_{t}V \\\\ &= ({}_{t-1}V + P - e)(1+i) - q_x \\cdot (B+E) - p_x \\cdot {}_{t}V \\end{aligned} \\] Different Interest Rates The interest earned on the reserves may be different from the interest used to compute the premiums and/or reserves. Be sure to read the question properly. This is equivalent to saying that the expected profit in each policy year is the combination of the NCFs AND the change in reserves for that policy year: \\[ \\begin{aligned} \\text{Expected Profit} &= \\text{Expected NCF} + {}_{t-1}V \\cdot (1+i) - p_x \\cdot {}_{t}V \\\\ &= \\text{Expected NCF} + \\text{Expected change in reserve} \\end{aligned} \\] There are two special cases for the reserves: Beginning of policy - Starting reserve is 0 End of policy - Ending reserve is 0","title":"Cashflow Projections"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#profit-vector-signature","text":"If the expected profit is calculated every policy year and collected together, then the resulting vector is known as the Profit Vector . For a typical contract, the profits are usually negative in the first (few) years and are small in magnitude across all years. This can be used to sense check the profit vector calculations. \\[ \\begin{aligned} \\text{PR} &= \\begin{pmatrix} \\text{PR}_1 \\\\ \\text{PR}_2 \\\\ \\vdots \\\\ \\text{PR}_t \\end{pmatrix} \\end{aligned} \\] Calculation Questions Most questions will only require computing the profit vector for a short period of time given the intensity needed to calculate the reserves at various points in time. If required to compute the profit vector for a long duration, the question will usually specify to ignore the change in reserves or simply provide them , which greatly simplifies the required calculations. However, the expected profit for each policy year makes the implicit assumption that the policyholder survives till the BEGINNING of that policy year . Thus, every element of the profit vector is calculated based off varying assumptions. This is not very useful and thus can be corrected by adjusting the profit vector with the probability of survival till the beginning each policy year. The resulting vector is the unconditional profit , known as the Profit Signature . \\[ \\begin{aligned} \\Pi &= \\begin{pmatrix} {}_{0}p_x \\\\ {}_{1}p_x \\\\ \\vdots \\\\ {}_{t-1}p_x \\end{pmatrix} * \\begin{pmatrix} \\text{PR}_1 \\\\ \\text{PR}_2 \\\\ \\vdots \\\\ \\text{PR}_t \\end{pmatrix} \\end{aligned} \\] Recall that \\({}_{0}p_x = 1\\) .","title":"Profit Vector &amp; Signature"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#profit-measures","text":"Using the profit signature, insurers can gauge the profitability of the products using a variety of metrics. Apart from calculation, it is important to know how to explain how a change in any of the parameters affect these metrics.","title":"Profit Measures"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#net-present-value","text":"Net Present Value (NPV) follows its finance definition and is the excess of PV Inflows over Outflows . In this context, it is the sum of the present values of all elements within the profit signature: \\[ \\begin{aligned} \\text{NPV} &= \\left(v^1 \\quad v^2 \\quad \\dots \\quad v^t \\right) \\cdot \\begin{pmatrix} \\Pi_1 \\\\ \\Pi_2 \\\\ \\vdots \\\\ \\Pi_t \\end{pmatrix} \\\\ &= v^1 \\cdot \\Pi_1 + v^2 \\cdot \\Pi_2 + \\dots + v^t \\cdot \\Pi_t \\end{aligned} \\] Net Expected Present Value Note that the NPV is calculated based off the profit signature, NOT the profit vector. Thus, it is more appropriately called the Net Expected Present Value , as survival probabilities are taken into account as well. The interest rate used to discount the cashflows is known as the Discount Rate , and is usually different from the earned interest rate or the one used in pricing/reserving.","title":"Net Present Value"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#internal-rate-of-return","text":"The Internal Rate of Return (IRR) is the Discount Rate that will result in an NPV of 0 : \\[ \\text{NPV}_\\text{IRR} = 0 \\] The IRR cannot be calculated through an equation and must be solved using numerical methods. However, it can also be calculated using the cashflow function in most financial calculators.","title":"Internal Rate of Return"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#discounted-payback-period","text":"The Discounted Payback Period (DPP) is the earliest time in the contract which the NPV becomes positive : \\[ \\text{DPP} = min [t: NPV(t) > 0] \\] It is best calculated by incrementally calculating the NPV at various policy years, stopping when the NPV becomes positive.","title":"Discounted Payback Period"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#profit-margin","text":"The Profit Margin ( \\(\\pi\\) ) is the ratio of the NPV to the EPV of premiums: \\[ \\pi = \\frac{\\text{NPV}}{\\text{EPV Premiums}} \\] Note that the EPV of premiums are discounted using the same interest as the NPV .","title":"Profit Margin"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#pricing","text":"A more realistic way to price would be to consider the actual cashflows and thus pricing the policy such that the desired profitability is achieved . This is done by leaving the premium as a variable within the profit expressions and solving for it. Alternatively, the current premium and profitability of the policy may be provided. In this scenario, it may be easier to consider the change in premium and the corresponding change in profitability instead of calculating it directly.","title":"Pricing"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#reserving","text":"Insurers may want to set reserves such that they avoid negative profits throughout the lifetime of the policy. This is done by setting reserves such that the profits are floored at 0 . The reserves are calculated recursively starting from the last policy year : Recall that in the last policy year, the ending reserve is 0 Set the profit to 0 & solve for the starting reserve If the starting reserve is negative, floor it at 0 Use the calculated amount above as the new ending reserve for the previous year & repeat the process until all reserves are calculated Since this process sets negative reserves to 0, it is known as Zeroization Alternatively, the question may provide the current profit for each year, with the profit in some years being negative. The goal is then to calculate the reserves such that the negative profits are floored to 0. The key is to understand that if the profit for all future years are positive, then the starting reserve for that year will always be floored at 0 . Thus, using this, the reserves are recursively calculated: Start from the latest policy year with negative profits Stop once there are no more negative profits There is no need to calculate the reserve for earlier years once all negative profits have been set to 0 because following the initial logic, the reserve for these earlier years will be 0 given that all future profits are non-negative! After Zeroization, recalculate the profit for all years: For years with negative profits where zeroization occurred, the profit is 0 For years with positive profits but were NOT just before negative profit years, the profit is unchanged For years with positive profits but were JUST before negative profit years, the profit is changed The profits have changed because there is now a new starting reserve for the following year with negative profits due to the zeroization. This affects the amount of reserve that has to be set up at the end of the current year, resulting in a new profit. If this new profit turns out to be negative as well , then repeat the process once more.","title":"Reserving"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#gains-by-source","text":"As alluded to earlier, actual profits are simply profits that are based on what actually happens rather than what was assumed. From this and the expected profit, the gain from the policy is determined. The difference in actual versus expected (AvE) assumptions typically comes in three aspects - Expenses, Interest & Mortality. Thus, the gain can also be decomposed into these three aspects to precisely pinpoint which assumptions need tweaking. \\[ \\begin{aligned} \\text{Total Gain} &= \\text{Actual Profit} - \\text{Expected Profit} \\\\ &= \\text{Expense Gain} + \\text{Interest Gain} + \\text{Mortality Gain} \\end{aligned} \\] For this section, Expected and Actual variables are differentiated using \" \\('\\) \".","title":"Gains by Source"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#expense-gain","text":"Since expense is a cash outflow , gain is recognised when the expected outflow is larger than the actual outflow. \\[ \\begin{aligned} \\text{Expense Gain} &= \\text{Assumed Expense Profit} - \\text{Actual Expense Profit} \\\\ &= e(1+i) - q_x \\cdot E - (e'(1+i) - q_x \\cdot E') \\\\ &= (e-e')(1+i) - q_x (E-E') \\end{aligned} \\] Note that since only the gain due to expenses are desired, only the expense assumptions go through the AvE process. The rest of the assumptions follow what was expected.","title":"Expense Gain"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#interest-gain","text":"On the flipside, since interest is a cash inflow , gain is recognised when the expected inflow is smaller than the actual outflow. \\[ \\begin{aligned} \\text{Interest Gain} &= \\text{Actual Interest Profit} - \\text{Expected Interest Profit} \\\\ &= ({}_{t-1}V + P - e') \\cdot i' - ({}_{t-1}V + P - e') \\cdot i \\\\ &= (i' - i)({}_{t-1}V + P - e') \\end{aligned} \\] Similarly, since only the gain due to interest is desired, only the interest assumptions go through the AvE process. However, since the expense gain was already calculated , the actual expense is directly used. Thus, the order of calculation is important as it determines whether the actual or expected values are used - if the gain has already been calculated, then in all subsequent calculations, only the actual assumptions should be used. TBC the reason for this","title":"Interest Gain"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#mortality-gain","text":"Similarly, mortality is a cash outflow , gain is recognised when the expected outflow is larger than the actual outflow. \\[ \\begin{aligned} \\text{Mortality Gain} &= \\text{Assumed Mortality Profit} - \\text{Actual Mortality Profit} \\\\ &= q_x \\cdot (B+E') + (1-q_x) \\cdot {}_{t}V - [q'_x \\cdot (B+E') + (1-q'_x) \\cdot {}_{t}V] \\\\ &= q_x \\cdot (B+E') + {}_{t}V - q_x \\cdot {}_{t}V - [q'_x \\cdot (B+E') + {}_{t}V - q'_x \\cdot {}_{t}V] \\\\ &= q_x \\cdot (B+E') + {}_{t}V - q_x \\cdot {}_{t}V - q'_x \\cdot (B+E') - {}_{t}V + q'_x \\cdot {}_{t}V \\\\ &= (q_x - q'_x)(B+E') - (q_x - q'_x){}_{t}V \\\\ &= (q_x - q'_x)(B+E'-{}_{t}V) \\end{aligned} \\] The second term is also known as the Net Amount at Risk , which represents the additional amount that the insurer has to pay in the event of a claim. In practice, the question may provide the actual number of policyholders (NOP) instead of the probabilities. Thus, we need to solve for the actual probabilities using: \\[ q'_x = \\frac{\\text{NOP}_\\text{Beginning} - \\text{NOP}_\\text{Ending}}{\\text{NOP}_\\text{Beginning}} \\] For annuities, the expression is slightly different (TBC) \\[ \\begin{aligned} \\text{Mortality Gain} &= \\text{Assumed Mortality Profit} - \\text{Actual Mortality Profit} \\\\ &= p_x \\cdot (B+E') + p_x \\cdot {}_{t}V - [p'_x \\cdot (B+E') + p'_x \\cdot {}_{t}V] \\\\ &= p_x \\cdot (B + E' + {}_{t}V) - p'_x (B + E' + {}_{t}V) \\\\ &= (p_x - p'_x)(B + E' + {}_{t}V) \\end{aligned} \\]","title":"Mortality Gain"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/#per-policy-gain","text":"All the above calculations are known as the Per Policy gains - if the actual number of policies are given, then each gain needs to be multiplied by the NOP to determine the total gain made by the insurer.","title":"Per Policy Gain"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/","text":"Survival Models \u00b6 This section assumes some basic knowledge on Probability Theory , which can be found under another set of notes covering a Review of Probability Theory . Overview \u00b6 Survival Models are probability distributions that measure the time to failure of an entity. In an actuarial context, it measures the time to death of a person. From another perspective, it also measures the future lifetime of that person. Note If someone is going to die in 5 years, then the person only has 5 years left to live (future lifetime). Intuitively, the distribution depends heavily on the age of the person, which is denoted using \\((x)\\) , which denotes a life aged \\(x\\) . Continuous Lifetime \u00b6 The distribution of the time to death ( in years ) is represented using the Continuous Random Variable \\(T_{x}\\) , where \\(x\\) represents the age of the person. Note Although we often use integer values for time, it can be expressed in continuous form as well: 1 Year = 12 Months 0.5 Years = 6 Months 0.541 Years = 6 Months & 15 Days Since \\(T_{x}\\) represents the time to death, a person aged \\(x\\) will live for another \\(T_{x}\\) years and then die before age \\(x + T_{x}\\) : The probability that the person dies DURING the next \\(t\\) years before age \\(x+t\\) is given by the CDF of the distribution: \\[ F_{x}(t) = P(T_{x} \\le t) \\] Note The implicit assumption is that the person aged \\(x\\) just turned age \\(x\\) (on their birthday). Thus, dying within the next \\(t\\) years means that they will not live to turn age \\(x+t\\) . The complement of the CDF is the probability that the person dies AFTER the next \\(t\\) years . From another perspective, this also means that the person survives the next \\(t\\) years to age \\(x+t\\) , which is why it is known as the Survival Function . \\[ \\begin{aligned} S_{x}(t) &= P(T_{x} \\gt t) \\\\ &= 1 - P(T_{x} \\lt t) \\\\ &= 1 - F_{x}(t) \\\\ \\end{aligned} \\] Since they are complements of one another, they share similar properties: \\ltcenter\\gt CDF Survival Function Intuition \\(F_{x}(0) = 0\\) \\(S_{x}(0) = 1\\) Humans are currently alive \\(F_{x}(\\infty) = 1\\) \\(S_{x}(\\infty) = 0\\) Humans will inevitably die Non-decreasing function Non-increasing function Probability of death increases with age \\lt/center\\gt Warning The first property is hard to intepret. The probability that a life aged \\(x\\) dies within 0 years before age \\(x+0\\) must be 0, since the person is already aged \\(x\\) . The above two can also be combined into the probability of surviving \\(u\\) years till age \\(x+u\\) and then dying within the following \\(t\\) years before age \\(x+u+t\\) , known as the Probability of Deferred Death . \\[ \\begin{aligned} P(u \\lt T_x \\lt t+u) &= P(T_x \\lt t+u) - P(T_x \\lt u) \\\\ &= [1 - P(T_x \\gt t+u)] - [1 - P(T_x \\gt u)] \\\\ &= P(T_x \\gt u) - P(T_x \\gt t+u) \\\\ &= P(T_x \\gt u) - P(T_x \\gt u) \\cdot P(T_{x+u} \\gt t) \\\\ &= P(T_x \\gt u) [1 - P(T_{x+u} \\gt t)] \\\\ &= P(T_x \\gt u) \\cdot P(T_{x+u} \\lt t) \\\\ &= S_{x}(u) \\cdot F_{x+t}(t) \\end{aligned} \\] Warning Remember that the second term reflects the \"new\" age of the person after surviving \\(u\\) years till age \\(x+u\\) . Using Only Survival Functions , it is represented as the probability that the person survives \\(u\\) years but does not survive past \\(u+t\\) years : \\[ \\begin{aligned} P(u \\lt T_x \\lt t+u) &= P(T_x \\lt t+u) - P(T_x \\lt u) \\\\ &= [1 - P(T_x \\gt t+u)] - [1 - P(T_x \\gt u)] \\\\ &= P(T_x \\gt u) - P(T_x \\gt t+u) \\\\ &= S_{x}(u) - S_{x}(t+u) \\end{aligned} \\] Using Only Death Functions , it is represented as the probability of dying within \\(u+t\\) years but NOT \\(u\\) years: \\[ \\begin{aligned} P(u \\lt T_{x} \\lt u+t) &= P(T_x \\lt u+t) - P(T_x \\lt u) \\\\ &= F_{x} (u+t) - F_{x}(u) \\end{aligned} \\] Newborn Lifetime \u00b6 Note that one implicit assumption made so far is that the person has already survived till age \\(x\\) . In order to further analyze this, we must consider a special case of the distribution for a newborn aged 0 , represented by \\(T_{0}\\) . The future lifetime variable is essentially conditioned on the newborn having survived \\(x\\) years, then dying within the following \\(t\\) years before age \\(x+t\\) : \\[ \\begin{aligned} T_{x} &= T_{0} - x \\mid T_{0} \\gt x \\\\ \\\\ P(T_x \\le t) &= P(T_0 \\le x + t \\mid T_0 \\gt x) \\\\ &= \\frac{P(x \\lt T_0 \\lt x +t)}{P(T_0 \\gt x)} \\\\ &= \\frac{P(T_0 \\lt x +t)-P(T_0 \\lt x)}{P(T_0 \\gt x)} \\end{aligned} \\] When written in terms of the survival function, it can be simplified even further: \\[ \\begin{aligned} P(T_x \\ge t) &= P(T_0 \\ge x + t | T_0 \\gt x) \\\\ &= \\frac{P(T_0 \\gt x + t)}{P(T_0 \\gt x)} \\\\ &= \\frac{S_0(x+t)}{S_0(x)} \\end{aligned} \\] Note Since the surviving till age \\(x\\) is a subset of surviving till age \\(x+t\\) , the numerator can be simplified to just the probability of surviving till age \\(x+t\\) . This leads to the Multiplication Rule of survival probabilities: \\[ \\begin{aligned} S_x(t) &= \\frac{S_0(x+t)}{S_0(x)} \\\\ S_0(x+t) &= S_0(x)S_x(t) \\\\ \\\\ \\therefore S_x(t+u) &= S_x(u) S_{x+u}(t) \\end{aligned} \\] The survival probability can be decomposed into two components: The probability of a person surviving \\(u\\) years The conditional probability of person who has survived \\(u\\) years (new age) surviving another \\(t\\) years Warning This result ONLY applies to the survival function ! This result was used in to prove the deferred death probabilities, thus the same warning applies here - do not forget to update the \"new\" age of the person! Actuarial Notation \u00b6 Given how often the survival function is used, they are often abbreviated using the International Actuarial Notation : \\[ \\begin{aligned} P(T_{x} \\lt t) &= {}_{t}q_{x} \\\\ P(T_{x} \\gt u) &= {}_{t}p_{x} \\end{aligned} \\] Note If \\(t=1\\) , it is omitted - EG. \\({}_{1}q_{x} = q_{x}\\) . Deferred Deaths are expressed using the pipe symbol : \\[ \\begin{aligned} P(u \\lt T_{x} \\lt t+u) &= {}_{u \\mid t}q_{x} \\\\ &= {}_{u}p_{x} \\cdot {}_{t}q_{x+t} \\\\ &= {}_{u}p_{x} - {}_{u+t}p_{x} \\\\ &= {}_{t+u}q_{x} - {}_{u}q_{x} \\end{aligned} \\] Other key results: \\[ \\begin{aligned} {}_{t+u}p_{x} &= {}_{u}p_{x} \\cdot {}_{t}p_{x+u} \\\\ {}_{t}p_{x} &= \\sum^{t}_{u = t} {}_{u|}q_{x} \\end{aligned} \\] Discrete Lifetime \u00b6 Similarly, the distribution of the time to death ( in years ) can also be represented using a Discrete Random Variable \\(K_{x}\\) , where \\(x\\) represents the age of the person. It is also known as the Curtate Future Lifetime as it only contains the integer components of the continuous future lifetime. Info The definition of Curtate is \"shortened\"; it represents how the continuous distribution has been reduced to just its integer components. Another way to remember is that \\(K\\) stands for Kurtate . Mathematically, it is represented as a continuous lifetime variable that has been floored : \\[ K_x = \\lfloor T_x \\rfloor \\] The intepretation of \\(K_{x}\\) has changed slightly - A person aged \\(x\\) will live for another \\(K_x\\) FULL years and then die before age \\(x + K_{x} + 1\\) : Warning It is a very common mistake to misintepret \\(K_x\\) , thus consider the following examples: A person aged \\(x\\) dies within the same year . They have lived for 0 full years since turning \\(x\\) , thus \\(K_x = 0\\) . A person aged \\(x\\) lives till their \\(x+1\\) birthday but dies in that year. They have lived for 1 full year since turning \\(x\\) , thus \\(K_x = 1\\) . \\(K_x = k\\) can also be intepreted more intuitively as the deferred probability of living \\(k\\) years and then dying the following year : \\[ \\begin{aligned} P(K_x = k) &= P(k \\le T_x \\lt k+1) \\\\ &= P(T_x \\lt k+1) - P(T_x \\le k) \\\\ &= {}_{k+1}q_{x} - {}_{k}q_{x} \\\\ &= {}_{k}p_{x} - {}_{k+1}p_{x} \\\\ &= {}_{k}p_{x} - {}_{k}p_{x} \\cdot {}_{1}p_{x+k} \\\\ &= {}_{k}p_{x} \\cdot (1 - {}_{1}p_{x+k}) \\\\ &= {}_{k}p_{x} \\cdot {}_{1}q_{x+k} \\\\ &= {}_{k \\mid 1}q_{x} \\end{aligned} \\] Tip Recall that the probability of deferred death can be represented in three different ways, thus the PDF may take on different formulations: \\[ \\begin{aligned} {}_{u \\mid 1}q_{x} &= {}_{u}p_{x} \\cdot {}_{}q_{x+t} \\\\ &= {}_{u}p_{x} - {}_{u+1}p_{x} \\\\ &= {}_{u+1}q_{x} - {}_{u}q_{x} \\end{aligned} \\] MOVE TO CURTATE? This result can be better expressed in the form of a probability tree: Thus, it can be shown through recursion that the probability of surviving \\(t\\) years is equal to the sum of the probabilities of deferred deaths for every year after: \\[ \\begin{aligned} S_x(1) &= S_x(1)F_{x+1}(1) + S_x(1)S_{x+1}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(1) [S_{x+1}(1)F_{x+2}(1) + S_{x+1}(1)S_{x+2}(1)] \\\\ &= S_x(1)F_{x+1}(1) + S_x(1) S_{x+1}(1)F_{x+2}(1) + S_x(1) S_{x+1}(1)S_{x+2}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(2)F_{x+2}(1) + S_x(2)S_{x+2}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(2)F_{x+2}(1) + \\dots \\\\ \\\\ \\therefore S_x(t) &= \\sum^{t}_{t = 1} S_x(t)F_{x+t}(1) \\end{aligned} \\] Recall that the probability of surviving till a certain age is simply the probability that the person will die sometime after that age. The above expression solidifies this, where the probability of survival is equivalent to the probability of dying in every possible age after . Force of Mortality \u00b6 \\({}_{h}q_{x}\\) is the probability of dying within \\(h\\) years: \\[ \\begin{aligned} {}_{h}q_{x} &= 1 - {}_{h}q_{x} \\\\ &= 1 - \\frac{S_0(x+h)}{S_0(x)} \\\\ &= \\frac{S_0(x) - S_0(x+h)}{S_0(x)} \\end{aligned} \\] The Force of Mortality \\((\\mu_{x})\\) is the probability that (x) dies within an infinitely small \\(h\\) ; in other words, the probability of dying instantly . We consider the rate of death per unit time (this is NOT a probability!): \\[ \\frac{{}_{h}q_{x}}{h} = \\frac{S_0(x) - S_0(x+h)}{h \\cdot S_0(x)} \\] Note The proof uses the first principles of differentiation, which measures the rate of change of \\(y\\) over an infinitely small unit of \\(x\\) , which is why the rate of death had to be considered. As the period of time becomes infinitely small, \\[ \\begin{aligned} \\mu_{x} &= \\lim_{h \\to 0} \\frac{S_0(x) - S_0(x+h)}{S_0(x)h} \\\\ &= \\frac{1}{S_0(x)} \\lim_{h \\to 0} \\frac{S_0(x) - S_0(x+h)}{h} \\\\ &= - \\frac{1}{S_0(x)} \\lim_{h \\to 0} \\frac{S_0(x+h) - S_0(x)}{h} \\\\ &= - \\frac{1}{S_0(x)} \\cdot \\left(\\frac{d}{dx} S_0(x) \\right) \\\\ &= - \\frac{S'_0(x)}{S_0(x)} \\\\ &= - \\frac{d}{dx} \\ln S_0(x) \\end{aligned} \\] Note Since the force of mortality represents the instantaneous rate of death, for extremely small time intervals , it can be used to approximate the probability of death in that interval: \\[ P(T_x \\lt h) \\approx h * \\mu_{x} \\] Using the PDF of the newborn distribution, the force of mortality can be rewritten accordingly: \\[ \\begin{aligned} f_0(x) &= \\frac{d}{dx} F_0(x) \\\\ &= \\frac{d}{dx} (1 - S_0(x)) \\\\ &= -S'_0(x) \\\\ \\\\ \\therefore \\mu_{x} &= \\frac{f_0(x)}{S_0(x)} \\end{aligned} \\] Loosely speaking, it represents the probability of a newborn living till age \\(x\\) and then dying instantly . Generalization \u00b6 The force of mortality is currently defined using the newborn distribution: \\[ \\mu_{y} = - \\frac{\\frac{d}{dy} S_0(y)}{S_0(y)} \\] For (x), \\(y = x + t\\) , where t is a random: \\[ \\begin{aligned} dy &= dx + dt \\\\ &= 0 + dt \\\\ &= dt \\\\ \\\\ \\therefore \\mu_{x+t} &= - \\frac{\\frac{d}{dt} S_0(x+t)}{S_0(x+t)} \\\\ &= - \\frac{d}{dt} \\ln S_0(x+t) \\end{aligned} \\] Since integration is the reverse of differentiation, \\[ \\begin{aligned} \\int^{n}_{0} \\frac{d}{dt} \\ln S_0(x+t) &= - \\int^{n}_{0} \\mu_{x+t} \\\\ [\\ln S_0(x+t)]^{n}_{0} &= - \\int^{n}_{0} \\mu_{x+t} \\\\ \\ln S_0(x+n) - \\ln S_0(x) &= - \\int^{n}_{0} \\mu_{x+t} \\\\ \\ln \\frac{S_0(x+n)}{\\ln S_0(x)} &= - \\int^{n}_{0} \\mu_{x+t} \\\\ \\ln S_{x}(n) &= - \\int^{n}_{0} \\mu_{x+t} \\\\ S_{x}(n) &= e^{- \\int^{n}_{0} \\mu_{x+t}} \\\\ {}_{n}p_{x} &= e^{- \\int^{n}_{0} \\mu_{x+t}} \\end{aligned} \\] Warning Most of the time, we want \\(\\mu_{x+t}\\) but the question provides \\(\\mu_{x}\\) . Need to convert? Following a similar generalization (proof not shown), the PDF can also be expressed using the force of mortality: \\[ \\begin{aligned} \\mu_{x} &= \\frac{f_0(x)}{S_0(x)} \\\\ f_0(x) &= S_0(x) \\cdot \\mu_{x} \\\\ f_x(t) &= S_x(t) \\cdot \\mu_{x+t} \\\\ f_x(t) &= {}_{t}p_{x} \\cdot \\mu_{x+t} \\end{aligned} \\] Risk Adjustments \u00b6 There are people who have a higher risk of dying than the average person - EG. Smokers. The first method to account for their increased risk is to treat them as an older person (higher risk), known as an Age Rating . An \\(n\\) year age rating means that a person aged \\(x\\) should be treated as if they had the mortality of a person aged \\(x+n\\) . Another method would be to simply scale the mortality rates by a factor to increase it: \\[ {}_{}q'_{x} = c \\cdot {}_{}q'_{x} \\] The last method would be to adjust the force of mortality instead, by either adding or multiplying a constant to it: \\[ \\begin{aligned} {}_{}p'_{x} &= e^{- (\\int^{n}_{0} \\mu_{x+t} + k)} \\\\ &= e^{- \\int^{n}_{0} \\mu_{x+t} - kn} \\\\ &= e^{- \\int^{n}_{0} \\mu_{x+t}} \\cdot e^{-kn} \\\\ &= {}_{}p'_{x} \\cdot e^{-kn} \\\\ \\\\ {}_{}p'_{x} &= e^{- k\\int^{n}_{0} \\mu_{x+t}} \\\\ &= (e^{- \\int^{n}_{0} \\mu_{x+t}})^k \\\\ &= ({}_{}p_{x})^k \\end{aligned} \\] Moments \u00b6 Continuous Moments \u00b6 The first raw moment of the distribution is known as the Complete Expectation of Life , which can be calculated via first principles: \\[ E(T_{x}) = \\int_{0}^{\\infty} t \\cdot f_x(t) \\\\ \\] Note that integrating this expression directly requires integration by parts , which can be time consuming, this it can be simplified using the Survival Function Method . Recall that in the previous section we found that the PDF and the survival function are related: \\[ \\begin{aligned} f_{x}(t) &= - \\frac{d}{dt} S_{x}(t) \\\\ \\therefore \\int f_{x}(t) &= -S_{x}(t) \\end{aligned} \\] This result can be used to simplify the integration by parts: \\[ \\begin{aligned} E(T_{x}) &= \\Bigl[t \\cdot \\int f_x(t) \\Bigr]_0^\\infty - \\int_{0}^{\\infty} \\int f_x(t) \\cdot 1 \\\\ &= \\Bigl[t \\cdot (-S_X(t)) \\Bigr]_0^\\infty - \\int_{0}^{\\infty} (-S_X(x)) \\\\ &= tS_X(\\infty) - tS_X(0) + \\int_{0}^{\\infty} S_X(x) \\\\ &= \\infty \\cdot (0) - 0 \\cdot 1 + \\int_{0}^{\\infty} S_X(x) \\\\ &= \\int_{0}^{\\infty} S_X(x) \\\\ &= \\int_0^\\infty {}_{t}p_x \\end{aligned} \\] The same logic can be applied to the second raw moment as well: \\[ \\begin{aligned} E(T^2_x) &= \\int_{0}^{\\infty} t^2 \\cdot f_x(t) \\\\ &= \\Bigl[t^2 \\cdot \\int f_x(t) \\Bigr]_0^\\infty - \\int_{0}^{\\infty} \\int f_x(t) \\cdot 2t \\\\ &= \\Bigl[t^2 \\cdot S_x(t)\\Bigr]_0^\\infty - \\int_{0}^{\\infty} 2t \\cdot S_x(t) \\\\ &= t^2 \\cdot S_x(\\infty) - t^2 \\cdot S_x(0) - \\int_{0}^{\\infty} 2t \\cdot S_x(t) \\\\ &= (\\infty)^2 \\cdot 0 - 0^2 \\cdot 1 - \\int_{0}^{\\infty} 2t \\cdot S_x(t) \\\\ &= \\int_{0}^{\\infty} 2t \\cdot {}_{t}p_x \\end{aligned} \\] Tip More generally, the survival function method can be expressed as the following: \\[ \\begin{aligned} \\int_{0}^{\\infty} g(x) \\cdot f_x(t) &= \\int_{0}^{\\infty} g'(x) \\cdot S_x(t) \\end{aligned} \\] The Variance is simply the difference of the first two raw moments with no special simplification : \\[ \\begin{aligned} Var(T_x) &= E(T^2_x) - [E(T_x)]^2 \\\\ &= 2 \\int_{0}^{\\infty} t \\cdot {}_{t}p_x - \\left(\\int_0^\\infty {}_{t}p_x \\right)^2 \\end{aligned} \\] Discrete Moments \u00b6 Similarly, the expectation of the discrete distribution is known as the Expectation of Curtate Lifetime . \\[ \\begin{aligned} E(K_x) &= \\sum_{k=0}^{\\infty} k \\cdot {}_{k \\mid 1}q_{x} \\\\ &= \\sum_{k=0}^{\\infty} k ({}_{k}p_{x} - {}_{k+1}p_{x}) \\\\ &= 1 ({}_{1}p_{x} - {}_{2}p_{x}) + 2 ({}_{2}p_{x} - {}_{3}p_{x}) + 3 ({}_{3}p_{x} - {}_{4}p_{x}) + ... \\\\ &= {}_{1}p_{x} - {}_{2}p_{x} + 2 {}_{2}p_{x} - 2{}_{3}p_{x} + 3 {}_{3}p_{x} - 3 {}_{4}p_{x} \\\\ &= {}_{1}p_{x} + {}_{2}p_{x} + {}_{3}p_{x} + ... \\\\ &= \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\end{aligned} \\] Note Summation starts from 1 as the first term is cancelled Probability tree perspective The second moment can be calculated in a similar fashion: \\[ \\begin{aligned} E(K^2_x) &= \\sum_{k=0}^{\\infty} k^2 ({}_{k}p_{x} - {}_{k+1}p_{x}) \\\\ &= 1 ({}_{1}p_{x} - {}_{2}p_{x}) + 4 ({}_{2}p_{x} - {}_{3}p_{x}) + 9 ({}_{3}p_{x} - {}_{4}p_{x}) + ... \\\\ &= {}_{1}p_{x} - {}_{2}p_{x} + 4{}_{2}p_{x} - 4{}_{3}p_{x} + 9 {}_{3}p_{x} - 9 {}_{4}p_{x} \\\\ &= {}_{1}p_{x} + 3{}_{2}p_{x} + 5 {}_{3}p_{x} + ... \\\\ &= \\sum_{k=1}^{\\infty} (2k-1) {}_{k}p_{x} \\end{aligned} \\] Similarly, the variance has no special simplifications : \\[ \\begin{aligned} Var(K_x) &= E(K_x^2) - [E(K_x)]^2 \\\\ &= \\sum_{k=1}^{\\infty} (2k-1) {}_{k}p_{x} - \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\end{aligned} \\] Temporary Expectation \u00b6 If the future lifetime variable is artifically limited to \\(n\\) years, then the expectation is known as the n-year temporary complete/curtate expectation of life . It is intepreted as the number of years that the person is expected to live out of the next \\(n\\) years ONLY. If the person lives past \\(n\\) years, then only \\(n\\) years are recorded. Thus, it can be represented using a minimum function : \\[ \\min (T_x, n) = \\begin{cases} T_x,& T_x \\lt n \\\\ n,& T_x \\gt n \\end{cases} \\] In order to easily represent and distinguish it from the full counterpart, the following actuarial notation is used: \\ltcenter\\gt Complete Curtate Full \\(\\mathring{e}_{x}\\) \\(e_{x}\\) Temporary \\(\\mathring{e}_{x:\\enclose{actuarial}{n}}\\) \\(e_{x:\\enclose{actuarial}{n}}\\) \\lt/center\\gt \\[ \\begin{aligned} \\mathring{e}_{x} &= \\int^{\\infty}_0 {}_{t}p_x \\\\ \\mathring{e}_{x:\\enclose{actuarial}{n}} &= \\int^{n}_0 {}_{t}p_x \\\\ \\\\ e_{x} &= \\sum^{\\infty}_0 {}_{k}p_x \\\\ e_{x:\\enclose{actuarial}{n}} &= \\sum^{n}_0 {}_{k}p_x \\end{aligned} \\] Decomposition \u00b6 The complete expectation can be decomposed into two components: Term Expectation at the current age representing the \"early\" years Complete Expectation at a future age represenging the \"later\" years \\[ \\begin{aligned} \\mathring{e}_x &\\approx \\mathring{e}_{x:\\enclose{actuarial}{n}} + \\mathring{e}_{x+n} \\\\ e_x &\\approx e_{x:\\enclose{actuarial}{n}} + e_{x+n} \\end{aligned} \\] However, the above makes an implicit assumption that the person will survive the first n years. Thus, the second term needs to account for the probability of surviving those n years: \\[ \\begin{aligned} \\mathring{e}_x &= \\mathring{e}_{x:\\enclose{actuarial}{n}} + {}_{n}p_{x} \\cdot \\mathring{e}_{x+n} \\\\ e_x &= e_{x:\\enclose{actuarial}{n}} + {}_{n}p_{x} \\cdot e_{x+n} \\end{aligned} \\] Similarly, a temporary expectation can be further decomposed into more temporary expectations: \\[ \\begin{aligned} \\mathring{e}_{x:\\enclose{actuarial}{n}} &= \\mathring{e}_{x:\\enclose{actuarial}{m}} + {}_{n}p_{x} \\cdot \\mathring{e}_{x+m+n} \\\\ e_{x:\\enclose{actuarial}{n}} &= e_{x:\\enclose{actuarial}{m}} + {}_{n}p_{x} \\cdot e_{x+m+n} \\end{aligned} \\] For the curtate expectation only , if \\(n=1\\) , it leads to a recursion : \\[ \\begin{aligned} e_x &= e_{x:\\enclose{actuarial}{1}} + {}_{}p_{x} \\cdot e_{x+1} \\\\ &= {}_{}p_{x} + {}_{}p_x \\cdot e_{x+1} \\\\ &= {}_{}p_{x} (1 + e_{x+1}) \\end{aligned} \\] Discrete Continuous Link \u00b6 Note that the two expectations are similar to one another: Continuous Expectation - Area under survival function Discrete Expectation - Right Riemann Sum of the area under the survival function Recall from Calculus that the area under a curve can be estimated through the Trapezium Rule , which states that the area is approximately equal to the sum of the area of discrete trapeziums formed under the curve. \\[ \\begin{aligned} \\int^b_a f(x) & \\approx \\sum \\text{Area of Trapezium} \\\\ & \\approx \\sum \\frac{1}{2} h \\left[f(a+kh) + f(a+(k+1)h) \\right] \\\\ & \\approx \\frac{h}{2}[f(a) + f(a+h)] + \\frac{h}{2}[f(a+h) + f(a+2h)] + \\dots + \\frac{h}{2}[f(b-h) + f(b)] \\\\ & \\approx \\frac{h}{2} f(a) + h[f(a+h) + f(a+2h) + \\dots + f(b-h)] + \\frac{h}{2} f(b) \\\\ & \\approx h[f(a+h) + f(a+2h) + \\dots + f(b-h)] + \\frac{h}{2} [f(a)+f(b)] \\\\ & \\approx h[f(a) + f(a+h) + \\dots + f(b)] - \\frac{h}{2} [f(a)+f(b)] \\\\ & \\approx h[f(a) + f(a+h) + \\dots + f(b-h)] - \\frac{h}{2} [f(a)-f(b)] \\\\ & \\approx h[f(a+h) + f(a+2h) + \\dots + f(b)] + \\frac{h}{2} [f(a)-f(b)] \\end{aligned} \\] The proof relies on the fact that other than \\(f(a)\\) and \\(f(b)\\) , all other terms are repeated twice . The last four lines are just different variations that showcase this property: Both \\(f(a)\\) and \\(f(b)\\) excluded from main expression Both \\(f(a)\\) and \\(f(b)\\) included in main expression but subtracted Only \\(f(a)\\) included in main expression; Main expression is a Left Riemman Sum Only \\(f(b)\\) included in main expression; Main expression is a Right Riemman Sum Since the discrete expectation is a right riemann sum, the last expression of the trapezoidal rule should be used. This allows the continuous expectation to be expressed using the discrete expectation, assuming \\(h=1\\) : \\[ \\begin{aligned} \\mathring{e}_x &= \\int^{\\infty}_{0} S_x(t) \\\\ & \\approx h[f(a+h) + f(a+2h) + \\dots + f(b)] + \\frac{h}{2} [f(a)-f(b)] \\\\ & \\approx (1) [S_x(1) + S_x(2) + \\dots + S_x(\\infty)] + \\frac{1}{2} [S_x(0)-S_x(\\infty)] \\\\ & \\approx [S_x(1) + S_x(2) + \\dots + S_x(\\infty)] + \\frac{1}{2} [1-0] \\\\ & \\approx e_x + \\frac{1}{2} \\end{aligned} \\] An alternative way to view the above is that the area of the trapezoid is the sum of the riemann rectangles and a triangle . Euler Maclaurin Formula \u00b6 Note that the Trapezoidal Rule is not perfect - there is an inherent error in trying to approximate a curve using a line. \\[ \\begin{aligned} \\int^b_a f(x) & \\approx \\sum (\\text{Area of Trapezium} + \\text{Error})\\\\ \\end{aligned} \\] The error can be positive or negative, depending on the shape of the curve in that interval: The error term is calculated by taking the difference between the integral and the trapezium, and is generalized using the taylor series. The final result is known as the Euler Maclaurin Formula : \\[ \\sum \\text{Error} = \\frac{h^2}{12} [f'(a)-f'(b)] \\] \\gt Given that a taylor series was used, the error can be expressed as the sum of many different terms, but higher powers are ignored . Thus, the trapezoidal approximation for the expectation can be made more precise: \\[ \\begin{aligned} \\mathring{e}_x & \\approx e_x + \\frac{1}{2} + \\frac{1^2}{12} [S'(0)-S'(\\infty)] \\\\ & \\approx e_x + \\frac{1}{2} + \\frac{1}{12} [S'(0)] \\end{aligned} \\] The above will rarely be used in this manner - it is more than sufficient to know the relationship between the two expectations of life. However, it sets as a strong foundation to understand the Woolhouse Approximation in the life annuities section. Parametric Survival Models \u00b6 Given the importance of \\(\\mu_x\\) , several mathematical functions have been made to describe the force of mortality, known as a Parametric Survival Model . Exponential Distribution \u00b6 When the force of mortality is constant \\(\\mu_{x} = \\mu\\) , the PDF can be shown to be exponential: \\[ \\begin{aligned} {}_{}p_x &= e^{- \\int^{t}_{0} \\mu} \\\\ &= e^{- \\mu t}\\\\ \\\\ f(x) &= {}_{}p_x \\cdot \\mu \\\\ &= \\mu \\cdot e^{- \\mu t} \\\\ \\\\ \\therefore T_{x} &\\sim \\text{Exponential}(\\mu) \\\\ E(T_x) &= \\frac{1}{\\mu} \\\\ Var (T_x) &= \\frac{1}{\\mu^2} \\end{aligned} \\] The key is to remember that distribution is Memoryless , which means that the age of the person does not matter: \\[ \\begin{aligned} {}_{n}p_{x} &= {}_{n}p_{x+m} \\\\ \\mathring{e}_{x} &= \\mathring{e}_{x+m} \\\\ \\end{aligned} \\] This leads to an interesting relationship between the full and temporary expectation: \\[ \\begin{aligned} \\mathring{e}_x &= \\mathring{e}_{x:\\enclose{actuarial}{n}} + {}_{n}p_{x} \\cdot \\mathring{e}_{x+n} \\\\ \\mathring{e}_x &= \\mathring{e}_{x:\\enclose{actuarial}{n}} + {}_{n}p_{x} \\cdot \\mathring{e}_{x} \\\\ \\mathring{e}_{x:\\enclose{actuarial}{n}} &= \\mathring{e}_x - {}_{n}p_{x} \\cdot \\mathring{e}_{x} \\\\ \\mathring{e}_{x:\\enclose{actuarial}{n}} &= \\mathring{e}_x \\cdot (1 - {}_{n}p_{x}) \\end{aligned} \\] De Moivre's Model \u00b6 De Moivre's Law assumes that the distribution of a NEWBORN is uniformly distributed between 0 and \\(\\omega\\) . Note \\(\\omega\\) is known as the Limiting Age . It is the age that everyone is expected to die by. \\[ T_{0} \\sim \\text{Uniform}(0, \\omega) \\] It can be adjusted to fit the distribution of a person aged \\(x\\) : $$ \\begin{aligned} T_{x} &= T_{0} - x \\mid T_{0} \\gt x \\ \\therefore T_{x} &\\sim \\text{Uniform}(0, \\omega-x) \\ \\ f(x) &= \\frac{1}{\\omega-x} \\ E(T_{x}) &= \\frac{\\omega-x}{2} \\ Var(T_{x}) &= \\frac{(\\omega-x)^2}{12} \\end{aligned} $$s Gompertz Model \u00b6 Both the above distributions make unrealistic assumptions: Exponential Distribution : Assume age does not matter Uniform Distribution : Assume equal number of people die eacg tear A more realistic model is the Gompertz Model , which suggests that mortality increases with age : \\[ \\mu_{x} = Bc^{x} \\\\ \\] Thus, the corresponding survival function can be calculated: \\[ \\begin{aligned} {}_{t}p_{x} &= e^{-\\int^{n}_{0} Bc^{x+s}} \\\\ &= e^{-Bc^{x} \\int^{n}_{0} c^{s}} \\\\ &= e^{-Bc^{x} [\\frac{c^s}{\\ln c}]^t_0} \\\\ &= e^{-Bc^{x} [\\frac{c^t}{\\ln c} - \\frac{c^0}{\\ln c}]} \\\\ &= e^{\\frac{-Bc^{x}}{\\ln c} (c^t - 1)} \\end{aligned} \\] Makeham Gompertz Model \u00b6 Makeham added a constant \\(A\\) into the Gompertz model, resulting in the Makeham Gompertz Model : \\[ \\mu_{x} = A + Bc^{x} \\] \\(A\\) represents the age independent mortality - dying from reasons unrelated to age, such as from accidents or natural catastrophes. \\[ \\begin{aligned} S_x(t) &= e^{- \\int^{t}_{0} A + Bc^{x+t}} \\\\ &= e^{\\int^{t}_{0} -A - Bc^{x+t}} \\\\ &= e^{\\left[-At - \\frac{B}{\\ln c} c^{x+t} \\right]^{t}_{0}} \\\\ &= e^{\\left[-At - \\frac{B}{\\ln c} c^{x+t} \\right] - \\left[-\\frac{B}{\\ln c} c^{x} \\right]} \\\\ &= e^{-At - \\frac{B}{\\ln c} c^{x+t} + \\frac{B}{\\ln c} c^{x}} \\\\ &= e^{-At - \\frac{B}{\\ln c} c^x [c^t - 1]} \\\\ &= e^{-At} \\cdot e^{- \\frac{B}{\\ln c} c^x [c^t - 1]} \\end{aligned} \\]","title":"Survival Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#survival-models","text":"This section assumes some basic knowledge on Probability Theory , which can be found under another set of notes covering a Review of Probability Theory .","title":"Survival Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#overview","text":"Survival Models are probability distributions that measure the time to failure of an entity. In an actuarial context, it measures the time to death of a person. From another perspective, it also measures the future lifetime of that person. Note If someone is going to die in 5 years, then the person only has 5 years left to live (future lifetime). Intuitively, the distribution depends heavily on the age of the person, which is denoted using \\((x)\\) , which denotes a life aged \\(x\\) .","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#continuous-lifetime","text":"The distribution of the time to death ( in years ) is represented using the Continuous Random Variable \\(T_{x}\\) , where \\(x\\) represents the age of the person. Note Although we often use integer values for time, it can be expressed in continuous form as well: 1 Year = 12 Months 0.5 Years = 6 Months 0.541 Years = 6 Months & 15 Days Since \\(T_{x}\\) represents the time to death, a person aged \\(x\\) will live for another \\(T_{x}\\) years and then die before age \\(x + T_{x}\\) : The probability that the person dies DURING the next \\(t\\) years before age \\(x+t\\) is given by the CDF of the distribution: \\[ F_{x}(t) = P(T_{x} \\le t) \\] Note The implicit assumption is that the person aged \\(x\\) just turned age \\(x\\) (on their birthday). Thus, dying within the next \\(t\\) years means that they will not live to turn age \\(x+t\\) . The complement of the CDF is the probability that the person dies AFTER the next \\(t\\) years . From another perspective, this also means that the person survives the next \\(t\\) years to age \\(x+t\\) , which is why it is known as the Survival Function . \\[ \\begin{aligned} S_{x}(t) &= P(T_{x} \\gt t) \\\\ &= 1 - P(T_{x} \\lt t) \\\\ &= 1 - F_{x}(t) \\\\ \\end{aligned} \\] Since they are complements of one another, they share similar properties: \\ltcenter\\gt CDF Survival Function Intuition \\(F_{x}(0) = 0\\) \\(S_{x}(0) = 1\\) Humans are currently alive \\(F_{x}(\\infty) = 1\\) \\(S_{x}(\\infty) = 0\\) Humans will inevitably die Non-decreasing function Non-increasing function Probability of death increases with age \\lt/center\\gt Warning The first property is hard to intepret. The probability that a life aged \\(x\\) dies within 0 years before age \\(x+0\\) must be 0, since the person is already aged \\(x\\) . The above two can also be combined into the probability of surviving \\(u\\) years till age \\(x+u\\) and then dying within the following \\(t\\) years before age \\(x+u+t\\) , known as the Probability of Deferred Death . \\[ \\begin{aligned} P(u \\lt T_x \\lt t+u) &= P(T_x \\lt t+u) - P(T_x \\lt u) \\\\ &= [1 - P(T_x \\gt t+u)] - [1 - P(T_x \\gt u)] \\\\ &= P(T_x \\gt u) - P(T_x \\gt t+u) \\\\ &= P(T_x \\gt u) - P(T_x \\gt u) \\cdot P(T_{x+u} \\gt t) \\\\ &= P(T_x \\gt u) [1 - P(T_{x+u} \\gt t)] \\\\ &= P(T_x \\gt u) \\cdot P(T_{x+u} \\lt t) \\\\ &= S_{x}(u) \\cdot F_{x+t}(t) \\end{aligned} \\] Warning Remember that the second term reflects the \"new\" age of the person after surviving \\(u\\) years till age \\(x+u\\) . Using Only Survival Functions , it is represented as the probability that the person survives \\(u\\) years but does not survive past \\(u+t\\) years : \\[ \\begin{aligned} P(u \\lt T_x \\lt t+u) &= P(T_x \\lt t+u) - P(T_x \\lt u) \\\\ &= [1 - P(T_x \\gt t+u)] - [1 - P(T_x \\gt u)] \\\\ &= P(T_x \\gt u) - P(T_x \\gt t+u) \\\\ &= S_{x}(u) - S_{x}(t+u) \\end{aligned} \\] Using Only Death Functions , it is represented as the probability of dying within \\(u+t\\) years but NOT \\(u\\) years: \\[ \\begin{aligned} P(u \\lt T_{x} \\lt u+t) &= P(T_x \\lt u+t) - P(T_x \\lt u) \\\\ &= F_{x} (u+t) - F_{x}(u) \\end{aligned} \\]","title":"Continuous Lifetime"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#newborn-lifetime","text":"Note that one implicit assumption made so far is that the person has already survived till age \\(x\\) . In order to further analyze this, we must consider a special case of the distribution for a newborn aged 0 , represented by \\(T_{0}\\) . The future lifetime variable is essentially conditioned on the newborn having survived \\(x\\) years, then dying within the following \\(t\\) years before age \\(x+t\\) : \\[ \\begin{aligned} T_{x} &= T_{0} - x \\mid T_{0} \\gt x \\\\ \\\\ P(T_x \\le t) &= P(T_0 \\le x + t \\mid T_0 \\gt x) \\\\ &= \\frac{P(x \\lt T_0 \\lt x +t)}{P(T_0 \\gt x)} \\\\ &= \\frac{P(T_0 \\lt x +t)-P(T_0 \\lt x)}{P(T_0 \\gt x)} \\end{aligned} \\] When written in terms of the survival function, it can be simplified even further: \\[ \\begin{aligned} P(T_x \\ge t) &= P(T_0 \\ge x + t | T_0 \\gt x) \\\\ &= \\frac{P(T_0 \\gt x + t)}{P(T_0 \\gt x)} \\\\ &= \\frac{S_0(x+t)}{S_0(x)} \\end{aligned} \\] Note Since the surviving till age \\(x\\) is a subset of surviving till age \\(x+t\\) , the numerator can be simplified to just the probability of surviving till age \\(x+t\\) . This leads to the Multiplication Rule of survival probabilities: \\[ \\begin{aligned} S_x(t) &= \\frac{S_0(x+t)}{S_0(x)} \\\\ S_0(x+t) &= S_0(x)S_x(t) \\\\ \\\\ \\therefore S_x(t+u) &= S_x(u) S_{x+u}(t) \\end{aligned} \\] The survival probability can be decomposed into two components: The probability of a person surviving \\(u\\) years The conditional probability of person who has survived \\(u\\) years (new age) surviving another \\(t\\) years Warning This result ONLY applies to the survival function ! This result was used in to prove the deferred death probabilities, thus the same warning applies here - do not forget to update the \"new\" age of the person!","title":"Newborn Lifetime"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#actuarial-notation","text":"Given how often the survival function is used, they are often abbreviated using the International Actuarial Notation : \\[ \\begin{aligned} P(T_{x} \\lt t) &= {}_{t}q_{x} \\\\ P(T_{x} \\gt u) &= {}_{t}p_{x} \\end{aligned} \\] Note If \\(t=1\\) , it is omitted - EG. \\({}_{1}q_{x} = q_{x}\\) . Deferred Deaths are expressed using the pipe symbol : \\[ \\begin{aligned} P(u \\lt T_{x} \\lt t+u) &= {}_{u \\mid t}q_{x} \\\\ &= {}_{u}p_{x} \\cdot {}_{t}q_{x+t} \\\\ &= {}_{u}p_{x} - {}_{u+t}p_{x} \\\\ &= {}_{t+u}q_{x} - {}_{u}q_{x} \\end{aligned} \\] Other key results: \\[ \\begin{aligned} {}_{t+u}p_{x} &= {}_{u}p_{x} \\cdot {}_{t}p_{x+u} \\\\ {}_{t}p_{x} &= \\sum^{t}_{u = t} {}_{u|}q_{x} \\end{aligned} \\]","title":"Actuarial Notation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#discrete-lifetime","text":"Similarly, the distribution of the time to death ( in years ) can also be represented using a Discrete Random Variable \\(K_{x}\\) , where \\(x\\) represents the age of the person. It is also known as the Curtate Future Lifetime as it only contains the integer components of the continuous future lifetime. Info The definition of Curtate is \"shortened\"; it represents how the continuous distribution has been reduced to just its integer components. Another way to remember is that \\(K\\) stands for Kurtate . Mathematically, it is represented as a continuous lifetime variable that has been floored : \\[ K_x = \\lfloor T_x \\rfloor \\] The intepretation of \\(K_{x}\\) has changed slightly - A person aged \\(x\\) will live for another \\(K_x\\) FULL years and then die before age \\(x + K_{x} + 1\\) : Warning It is a very common mistake to misintepret \\(K_x\\) , thus consider the following examples: A person aged \\(x\\) dies within the same year . They have lived for 0 full years since turning \\(x\\) , thus \\(K_x = 0\\) . A person aged \\(x\\) lives till their \\(x+1\\) birthday but dies in that year. They have lived for 1 full year since turning \\(x\\) , thus \\(K_x = 1\\) . \\(K_x = k\\) can also be intepreted more intuitively as the deferred probability of living \\(k\\) years and then dying the following year : \\[ \\begin{aligned} P(K_x = k) &= P(k \\le T_x \\lt k+1) \\\\ &= P(T_x \\lt k+1) - P(T_x \\le k) \\\\ &= {}_{k+1}q_{x} - {}_{k}q_{x} \\\\ &= {}_{k}p_{x} - {}_{k+1}p_{x} \\\\ &= {}_{k}p_{x} - {}_{k}p_{x} \\cdot {}_{1}p_{x+k} \\\\ &= {}_{k}p_{x} \\cdot (1 - {}_{1}p_{x+k}) \\\\ &= {}_{k}p_{x} \\cdot {}_{1}q_{x+k} \\\\ &= {}_{k \\mid 1}q_{x} \\end{aligned} \\] Tip Recall that the probability of deferred death can be represented in three different ways, thus the PDF may take on different formulations: \\[ \\begin{aligned} {}_{u \\mid 1}q_{x} &= {}_{u}p_{x} \\cdot {}_{}q_{x+t} \\\\ &= {}_{u}p_{x} - {}_{u+1}p_{x} \\\\ &= {}_{u+1}q_{x} - {}_{u}q_{x} \\end{aligned} \\] MOVE TO CURTATE? This result can be better expressed in the form of a probability tree: Thus, it can be shown through recursion that the probability of surviving \\(t\\) years is equal to the sum of the probabilities of deferred deaths for every year after: \\[ \\begin{aligned} S_x(1) &= S_x(1)F_{x+1}(1) + S_x(1)S_{x+1}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(1) [S_{x+1}(1)F_{x+2}(1) + S_{x+1}(1)S_{x+2}(1)] \\\\ &= S_x(1)F_{x+1}(1) + S_x(1) S_{x+1}(1)F_{x+2}(1) + S_x(1) S_{x+1}(1)S_{x+2}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(2)F_{x+2}(1) + S_x(2)S_{x+2}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(2)F_{x+2}(1) + \\dots \\\\ \\\\ \\therefore S_x(t) &= \\sum^{t}_{t = 1} S_x(t)F_{x+t}(1) \\end{aligned} \\] Recall that the probability of surviving till a certain age is simply the probability that the person will die sometime after that age. The above expression solidifies this, where the probability of survival is equivalent to the probability of dying in every possible age after .","title":"Discrete Lifetime"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#force-of-mortality","text":"\\({}_{h}q_{x}\\) is the probability of dying within \\(h\\) years: \\[ \\begin{aligned} {}_{h}q_{x} &= 1 - {}_{h}q_{x} \\\\ &= 1 - \\frac{S_0(x+h)}{S_0(x)} \\\\ &= \\frac{S_0(x) - S_0(x+h)}{S_0(x)} \\end{aligned} \\] The Force of Mortality \\((\\mu_{x})\\) is the probability that (x) dies within an infinitely small \\(h\\) ; in other words, the probability of dying instantly . We consider the rate of death per unit time (this is NOT a probability!): \\[ \\frac{{}_{h}q_{x}}{h} = \\frac{S_0(x) - S_0(x+h)}{h \\cdot S_0(x)} \\] Note The proof uses the first principles of differentiation, which measures the rate of change of \\(y\\) over an infinitely small unit of \\(x\\) , which is why the rate of death had to be considered. As the period of time becomes infinitely small, \\[ \\begin{aligned} \\mu_{x} &= \\lim_{h \\to 0} \\frac{S_0(x) - S_0(x+h)}{S_0(x)h} \\\\ &= \\frac{1}{S_0(x)} \\lim_{h \\to 0} \\frac{S_0(x) - S_0(x+h)}{h} \\\\ &= - \\frac{1}{S_0(x)} \\lim_{h \\to 0} \\frac{S_0(x+h) - S_0(x)}{h} \\\\ &= - \\frac{1}{S_0(x)} \\cdot \\left(\\frac{d}{dx} S_0(x) \\right) \\\\ &= - \\frac{S'_0(x)}{S_0(x)} \\\\ &= - \\frac{d}{dx} \\ln S_0(x) \\end{aligned} \\] Note Since the force of mortality represents the instantaneous rate of death, for extremely small time intervals , it can be used to approximate the probability of death in that interval: \\[ P(T_x \\lt h) \\approx h * \\mu_{x} \\] Using the PDF of the newborn distribution, the force of mortality can be rewritten accordingly: \\[ \\begin{aligned} f_0(x) &= \\frac{d}{dx} F_0(x) \\\\ &= \\frac{d}{dx} (1 - S_0(x)) \\\\ &= -S'_0(x) \\\\ \\\\ \\therefore \\mu_{x} &= \\frac{f_0(x)}{S_0(x)} \\end{aligned} \\] Loosely speaking, it represents the probability of a newborn living till age \\(x\\) and then dying instantly .","title":"Force of Mortality"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#generalization","text":"The force of mortality is currently defined using the newborn distribution: \\[ \\mu_{y} = - \\frac{\\frac{d}{dy} S_0(y)}{S_0(y)} \\] For (x), \\(y = x + t\\) , where t is a random: \\[ \\begin{aligned} dy &= dx + dt \\\\ &= 0 + dt \\\\ &= dt \\\\ \\\\ \\therefore \\mu_{x+t} &= - \\frac{\\frac{d}{dt} S_0(x+t)}{S_0(x+t)} \\\\ &= - \\frac{d}{dt} \\ln S_0(x+t) \\end{aligned} \\] Since integration is the reverse of differentiation, \\[ \\begin{aligned} \\int^{n}_{0} \\frac{d}{dt} \\ln S_0(x+t) &= - \\int^{n}_{0} \\mu_{x+t} \\\\ [\\ln S_0(x+t)]^{n}_{0} &= - \\int^{n}_{0} \\mu_{x+t} \\\\ \\ln S_0(x+n) - \\ln S_0(x) &= - \\int^{n}_{0} \\mu_{x+t} \\\\ \\ln \\frac{S_0(x+n)}{\\ln S_0(x)} &= - \\int^{n}_{0} \\mu_{x+t} \\\\ \\ln S_{x}(n) &= - \\int^{n}_{0} \\mu_{x+t} \\\\ S_{x}(n) &= e^{- \\int^{n}_{0} \\mu_{x+t}} \\\\ {}_{n}p_{x} &= e^{- \\int^{n}_{0} \\mu_{x+t}} \\end{aligned} \\] Warning Most of the time, we want \\(\\mu_{x+t}\\) but the question provides \\(\\mu_{x}\\) . Need to convert? Following a similar generalization (proof not shown), the PDF can also be expressed using the force of mortality: \\[ \\begin{aligned} \\mu_{x} &= \\frac{f_0(x)}{S_0(x)} \\\\ f_0(x) &= S_0(x) \\cdot \\mu_{x} \\\\ f_x(t) &= S_x(t) \\cdot \\mu_{x+t} \\\\ f_x(t) &= {}_{t}p_{x} \\cdot \\mu_{x+t} \\end{aligned} \\]","title":"Generalization"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#risk-adjustments","text":"There are people who have a higher risk of dying than the average person - EG. Smokers. The first method to account for their increased risk is to treat them as an older person (higher risk), known as an Age Rating . An \\(n\\) year age rating means that a person aged \\(x\\) should be treated as if they had the mortality of a person aged \\(x+n\\) . Another method would be to simply scale the mortality rates by a factor to increase it: \\[ {}_{}q'_{x} = c \\cdot {}_{}q'_{x} \\] The last method would be to adjust the force of mortality instead, by either adding or multiplying a constant to it: \\[ \\begin{aligned} {}_{}p'_{x} &= e^{- (\\int^{n}_{0} \\mu_{x+t} + k)} \\\\ &= e^{- \\int^{n}_{0} \\mu_{x+t} - kn} \\\\ &= e^{- \\int^{n}_{0} \\mu_{x+t}} \\cdot e^{-kn} \\\\ &= {}_{}p'_{x} \\cdot e^{-kn} \\\\ \\\\ {}_{}p'_{x} &= e^{- k\\int^{n}_{0} \\mu_{x+t}} \\\\ &= (e^{- \\int^{n}_{0} \\mu_{x+t}})^k \\\\ &= ({}_{}p_{x})^k \\end{aligned} \\]","title":"Risk Adjustments"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#moments","text":"","title":"Moments"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#continuous-moments","text":"The first raw moment of the distribution is known as the Complete Expectation of Life , which can be calculated via first principles: \\[ E(T_{x}) = \\int_{0}^{\\infty} t \\cdot f_x(t) \\\\ \\] Note that integrating this expression directly requires integration by parts , which can be time consuming, this it can be simplified using the Survival Function Method . Recall that in the previous section we found that the PDF and the survival function are related: \\[ \\begin{aligned} f_{x}(t) &= - \\frac{d}{dt} S_{x}(t) \\\\ \\therefore \\int f_{x}(t) &= -S_{x}(t) \\end{aligned} \\] This result can be used to simplify the integration by parts: \\[ \\begin{aligned} E(T_{x}) &= \\Bigl[t \\cdot \\int f_x(t) \\Bigr]_0^\\infty - \\int_{0}^{\\infty} \\int f_x(t) \\cdot 1 \\\\ &= \\Bigl[t \\cdot (-S_X(t)) \\Bigr]_0^\\infty - \\int_{0}^{\\infty} (-S_X(x)) \\\\ &= tS_X(\\infty) - tS_X(0) + \\int_{0}^{\\infty} S_X(x) \\\\ &= \\infty \\cdot (0) - 0 \\cdot 1 + \\int_{0}^{\\infty} S_X(x) \\\\ &= \\int_{0}^{\\infty} S_X(x) \\\\ &= \\int_0^\\infty {}_{t}p_x \\end{aligned} \\] The same logic can be applied to the second raw moment as well: \\[ \\begin{aligned} E(T^2_x) &= \\int_{0}^{\\infty} t^2 \\cdot f_x(t) \\\\ &= \\Bigl[t^2 \\cdot \\int f_x(t) \\Bigr]_0^\\infty - \\int_{0}^{\\infty} \\int f_x(t) \\cdot 2t \\\\ &= \\Bigl[t^2 \\cdot S_x(t)\\Bigr]_0^\\infty - \\int_{0}^{\\infty} 2t \\cdot S_x(t) \\\\ &= t^2 \\cdot S_x(\\infty) - t^2 \\cdot S_x(0) - \\int_{0}^{\\infty} 2t \\cdot S_x(t) \\\\ &= (\\infty)^2 \\cdot 0 - 0^2 \\cdot 1 - \\int_{0}^{\\infty} 2t \\cdot S_x(t) \\\\ &= \\int_{0}^{\\infty} 2t \\cdot {}_{t}p_x \\end{aligned} \\] Tip More generally, the survival function method can be expressed as the following: \\[ \\begin{aligned} \\int_{0}^{\\infty} g(x) \\cdot f_x(t) &= \\int_{0}^{\\infty} g'(x) \\cdot S_x(t) \\end{aligned} \\] The Variance is simply the difference of the first two raw moments with no special simplification : \\[ \\begin{aligned} Var(T_x) &= E(T^2_x) - [E(T_x)]^2 \\\\ &= 2 \\int_{0}^{\\infty} t \\cdot {}_{t}p_x - \\left(\\int_0^\\infty {}_{t}p_x \\right)^2 \\end{aligned} \\]","title":"Continuous Moments"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#discrete-moments","text":"Similarly, the expectation of the discrete distribution is known as the Expectation of Curtate Lifetime . \\[ \\begin{aligned} E(K_x) &= \\sum_{k=0}^{\\infty} k \\cdot {}_{k \\mid 1}q_{x} \\\\ &= \\sum_{k=0}^{\\infty} k ({}_{k}p_{x} - {}_{k+1}p_{x}) \\\\ &= 1 ({}_{1}p_{x} - {}_{2}p_{x}) + 2 ({}_{2}p_{x} - {}_{3}p_{x}) + 3 ({}_{3}p_{x} - {}_{4}p_{x}) + ... \\\\ &= {}_{1}p_{x} - {}_{2}p_{x} + 2 {}_{2}p_{x} - 2{}_{3}p_{x} + 3 {}_{3}p_{x} - 3 {}_{4}p_{x} \\\\ &= {}_{1}p_{x} + {}_{2}p_{x} + {}_{3}p_{x} + ... \\\\ &= \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\end{aligned} \\] Note Summation starts from 1 as the first term is cancelled Probability tree perspective The second moment can be calculated in a similar fashion: \\[ \\begin{aligned} E(K^2_x) &= \\sum_{k=0}^{\\infty} k^2 ({}_{k}p_{x} - {}_{k+1}p_{x}) \\\\ &= 1 ({}_{1}p_{x} - {}_{2}p_{x}) + 4 ({}_{2}p_{x} - {}_{3}p_{x}) + 9 ({}_{3}p_{x} - {}_{4}p_{x}) + ... \\\\ &= {}_{1}p_{x} - {}_{2}p_{x} + 4{}_{2}p_{x} - 4{}_{3}p_{x} + 9 {}_{3}p_{x} - 9 {}_{4}p_{x} \\\\ &= {}_{1}p_{x} + 3{}_{2}p_{x} + 5 {}_{3}p_{x} + ... \\\\ &= \\sum_{k=1}^{\\infty} (2k-1) {}_{k}p_{x} \\end{aligned} \\] Similarly, the variance has no special simplifications : \\[ \\begin{aligned} Var(K_x) &= E(K_x^2) - [E(K_x)]^2 \\\\ &= \\sum_{k=1}^{\\infty} (2k-1) {}_{k}p_{x} - \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\end{aligned} \\]","title":"Discrete Moments"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#temporary-expectation","text":"If the future lifetime variable is artifically limited to \\(n\\) years, then the expectation is known as the n-year temporary complete/curtate expectation of life . It is intepreted as the number of years that the person is expected to live out of the next \\(n\\) years ONLY. If the person lives past \\(n\\) years, then only \\(n\\) years are recorded. Thus, it can be represented using a minimum function : \\[ \\min (T_x, n) = \\begin{cases} T_x,& T_x \\lt n \\\\ n,& T_x \\gt n \\end{cases} \\] In order to easily represent and distinguish it from the full counterpart, the following actuarial notation is used: \\ltcenter\\gt Complete Curtate Full \\(\\mathring{e}_{x}\\) \\(e_{x}\\) Temporary \\(\\mathring{e}_{x:\\enclose{actuarial}{n}}\\) \\(e_{x:\\enclose{actuarial}{n}}\\) \\lt/center\\gt \\[ \\begin{aligned} \\mathring{e}_{x} &= \\int^{\\infty}_0 {}_{t}p_x \\\\ \\mathring{e}_{x:\\enclose{actuarial}{n}} &= \\int^{n}_0 {}_{t}p_x \\\\ \\\\ e_{x} &= \\sum^{\\infty}_0 {}_{k}p_x \\\\ e_{x:\\enclose{actuarial}{n}} &= \\sum^{n}_0 {}_{k}p_x \\end{aligned} \\]","title":"Temporary Expectation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#decomposition","text":"The complete expectation can be decomposed into two components: Term Expectation at the current age representing the \"early\" years Complete Expectation at a future age represenging the \"later\" years \\[ \\begin{aligned} \\mathring{e}_x &\\approx \\mathring{e}_{x:\\enclose{actuarial}{n}} + \\mathring{e}_{x+n} \\\\ e_x &\\approx e_{x:\\enclose{actuarial}{n}} + e_{x+n} \\end{aligned} \\] However, the above makes an implicit assumption that the person will survive the first n years. Thus, the second term needs to account for the probability of surviving those n years: \\[ \\begin{aligned} \\mathring{e}_x &= \\mathring{e}_{x:\\enclose{actuarial}{n}} + {}_{n}p_{x} \\cdot \\mathring{e}_{x+n} \\\\ e_x &= e_{x:\\enclose{actuarial}{n}} + {}_{n}p_{x} \\cdot e_{x+n} \\end{aligned} \\] Similarly, a temporary expectation can be further decomposed into more temporary expectations: \\[ \\begin{aligned} \\mathring{e}_{x:\\enclose{actuarial}{n}} &= \\mathring{e}_{x:\\enclose{actuarial}{m}} + {}_{n}p_{x} \\cdot \\mathring{e}_{x+m+n} \\\\ e_{x:\\enclose{actuarial}{n}} &= e_{x:\\enclose{actuarial}{m}} + {}_{n}p_{x} \\cdot e_{x+m+n} \\end{aligned} \\] For the curtate expectation only , if \\(n=1\\) , it leads to a recursion : \\[ \\begin{aligned} e_x &= e_{x:\\enclose{actuarial}{1}} + {}_{}p_{x} \\cdot e_{x+1} \\\\ &= {}_{}p_{x} + {}_{}p_x \\cdot e_{x+1} \\\\ &= {}_{}p_{x} (1 + e_{x+1}) \\end{aligned} \\]","title":"Decomposition"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#discrete-continuous-link","text":"Note that the two expectations are similar to one another: Continuous Expectation - Area under survival function Discrete Expectation - Right Riemann Sum of the area under the survival function Recall from Calculus that the area under a curve can be estimated through the Trapezium Rule , which states that the area is approximately equal to the sum of the area of discrete trapeziums formed under the curve. \\[ \\begin{aligned} \\int^b_a f(x) & \\approx \\sum \\text{Area of Trapezium} \\\\ & \\approx \\sum \\frac{1}{2} h \\left[f(a+kh) + f(a+(k+1)h) \\right] \\\\ & \\approx \\frac{h}{2}[f(a) + f(a+h)] + \\frac{h}{2}[f(a+h) + f(a+2h)] + \\dots + \\frac{h}{2}[f(b-h) + f(b)] \\\\ & \\approx \\frac{h}{2} f(a) + h[f(a+h) + f(a+2h) + \\dots + f(b-h)] + \\frac{h}{2} f(b) \\\\ & \\approx h[f(a+h) + f(a+2h) + \\dots + f(b-h)] + \\frac{h}{2} [f(a)+f(b)] \\\\ & \\approx h[f(a) + f(a+h) + \\dots + f(b)] - \\frac{h}{2} [f(a)+f(b)] \\\\ & \\approx h[f(a) + f(a+h) + \\dots + f(b-h)] - \\frac{h}{2} [f(a)-f(b)] \\\\ & \\approx h[f(a+h) + f(a+2h) + \\dots + f(b)] + \\frac{h}{2} [f(a)-f(b)] \\end{aligned} \\] The proof relies on the fact that other than \\(f(a)\\) and \\(f(b)\\) , all other terms are repeated twice . The last four lines are just different variations that showcase this property: Both \\(f(a)\\) and \\(f(b)\\) excluded from main expression Both \\(f(a)\\) and \\(f(b)\\) included in main expression but subtracted Only \\(f(a)\\) included in main expression; Main expression is a Left Riemman Sum Only \\(f(b)\\) included in main expression; Main expression is a Right Riemman Sum Since the discrete expectation is a right riemann sum, the last expression of the trapezoidal rule should be used. This allows the continuous expectation to be expressed using the discrete expectation, assuming \\(h=1\\) : \\[ \\begin{aligned} \\mathring{e}_x &= \\int^{\\infty}_{0} S_x(t) \\\\ & \\approx h[f(a+h) + f(a+2h) + \\dots + f(b)] + \\frac{h}{2} [f(a)-f(b)] \\\\ & \\approx (1) [S_x(1) + S_x(2) + \\dots + S_x(\\infty)] + \\frac{1}{2} [S_x(0)-S_x(\\infty)] \\\\ & \\approx [S_x(1) + S_x(2) + \\dots + S_x(\\infty)] + \\frac{1}{2} [1-0] \\\\ & \\approx e_x + \\frac{1}{2} \\end{aligned} \\] An alternative way to view the above is that the area of the trapezoid is the sum of the riemann rectangles and a triangle .","title":"Discrete Continuous Link"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#euler-maclaurin-formula","text":"Note that the Trapezoidal Rule is not perfect - there is an inherent error in trying to approximate a curve using a line. \\[ \\begin{aligned} \\int^b_a f(x) & \\approx \\sum (\\text{Area of Trapezium} + \\text{Error})\\\\ \\end{aligned} \\] The error can be positive or negative, depending on the shape of the curve in that interval: The error term is calculated by taking the difference between the integral and the trapezium, and is generalized using the taylor series. The final result is known as the Euler Maclaurin Formula : \\[ \\sum \\text{Error} = \\frac{h^2}{12} [f'(a)-f'(b)] \\] \\gt Given that a taylor series was used, the error can be expressed as the sum of many different terms, but higher powers are ignored . Thus, the trapezoidal approximation for the expectation can be made more precise: \\[ \\begin{aligned} \\mathring{e}_x & \\approx e_x + \\frac{1}{2} + \\frac{1^2}{12} [S'(0)-S'(\\infty)] \\\\ & \\approx e_x + \\frac{1}{2} + \\frac{1}{12} [S'(0)] \\end{aligned} \\] The above will rarely be used in this manner - it is more than sufficient to know the relationship between the two expectations of life. However, it sets as a strong foundation to understand the Woolhouse Approximation in the life annuities section.","title":"Euler Maclaurin Formula"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#parametric-survival-models","text":"Given the importance of \\(\\mu_x\\) , several mathematical functions have been made to describe the force of mortality, known as a Parametric Survival Model .","title":"Parametric Survival Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#exponential-distribution","text":"When the force of mortality is constant \\(\\mu_{x} = \\mu\\) , the PDF can be shown to be exponential: \\[ \\begin{aligned} {}_{}p_x &= e^{- \\int^{t}_{0} \\mu} \\\\ &= e^{- \\mu t}\\\\ \\\\ f(x) &= {}_{}p_x \\cdot \\mu \\\\ &= \\mu \\cdot e^{- \\mu t} \\\\ \\\\ \\therefore T_{x} &\\sim \\text{Exponential}(\\mu) \\\\ E(T_x) &= \\frac{1}{\\mu} \\\\ Var (T_x) &= \\frac{1}{\\mu^2} \\end{aligned} \\] The key is to remember that distribution is Memoryless , which means that the age of the person does not matter: \\[ \\begin{aligned} {}_{n}p_{x} &= {}_{n}p_{x+m} \\\\ \\mathring{e}_{x} &= \\mathring{e}_{x+m} \\\\ \\end{aligned} \\] This leads to an interesting relationship between the full and temporary expectation: \\[ \\begin{aligned} \\mathring{e}_x &= \\mathring{e}_{x:\\enclose{actuarial}{n}} + {}_{n}p_{x} \\cdot \\mathring{e}_{x+n} \\\\ \\mathring{e}_x &= \\mathring{e}_{x:\\enclose{actuarial}{n}} + {}_{n}p_{x} \\cdot \\mathring{e}_{x} \\\\ \\mathring{e}_{x:\\enclose{actuarial}{n}} &= \\mathring{e}_x - {}_{n}p_{x} \\cdot \\mathring{e}_{x} \\\\ \\mathring{e}_{x:\\enclose{actuarial}{n}} &= \\mathring{e}_x \\cdot (1 - {}_{n}p_{x}) \\end{aligned} \\]","title":"Exponential Distribution"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#de-moivres-model","text":"De Moivre's Law assumes that the distribution of a NEWBORN is uniformly distributed between 0 and \\(\\omega\\) . Note \\(\\omega\\) is known as the Limiting Age . It is the age that everyone is expected to die by. \\[ T_{0} \\sim \\text{Uniform}(0, \\omega) \\] It can be adjusted to fit the distribution of a person aged \\(x\\) : $$ \\begin{aligned} T_{x} &= T_{0} - x \\mid T_{0} \\gt x \\ \\therefore T_{x} &\\sim \\text{Uniform}(0, \\omega-x) \\ \\ f(x) &= \\frac{1}{\\omega-x} \\ E(T_{x}) &= \\frac{\\omega-x}{2} \\ Var(T_{x}) &= \\frac{(\\omega-x)^2}{12} \\end{aligned} $$s","title":"De Moivre's Model"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#gompertz-model","text":"Both the above distributions make unrealistic assumptions: Exponential Distribution : Assume age does not matter Uniform Distribution : Assume equal number of people die eacg tear A more realistic model is the Gompertz Model , which suggests that mortality increases with age : \\[ \\mu_{x} = Bc^{x} \\\\ \\] Thus, the corresponding survival function can be calculated: \\[ \\begin{aligned} {}_{t}p_{x} &= e^{-\\int^{n}_{0} Bc^{x+s}} \\\\ &= e^{-Bc^{x} \\int^{n}_{0} c^{s}} \\\\ &= e^{-Bc^{x} [\\frac{c^s}{\\ln c}]^t_0} \\\\ &= e^{-Bc^{x} [\\frac{c^t}{\\ln c} - \\frac{c^0}{\\ln c}]} \\\\ &= e^{\\frac{-Bc^{x}}{\\ln c} (c^t - 1)} \\end{aligned} \\]","title":"Gompertz Model"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#makeham-gompertz-model","text":"Makeham added a constant \\(A\\) into the Gompertz model, resulting in the Makeham Gompertz Model : \\[ \\mu_{x} = A + Bc^{x} \\] \\(A\\) represents the age independent mortality - dying from reasons unrelated to age, such as from accidents or natural catastrophes. \\[ \\begin{aligned} S_x(t) &= e^{- \\int^{t}_{0} A + Bc^{x+t}} \\\\ &= e^{\\int^{t}_{0} -A - Bc^{x+t}} \\\\ &= e^{\\left[-At - \\frac{B}{\\ln c} c^{x+t} \\right]^{t}_{0}} \\\\ &= e^{\\left[-At - \\frac{B}{\\ln c} c^{x+t} \\right] - \\left[-\\frac{B}{\\ln c} c^{x} \\right]} \\\\ &= e^{-At - \\frac{B}{\\ln c} c^{x+t} + \\frac{B}{\\ln c} c^{x}} \\\\ &= e^{-At - \\frac{B}{\\ln c} c^x [c^t - 1]} \\\\ &= e^{-At} \\cdot e^{- \\frac{B}{\\ln c} c^x [c^t - 1]} \\end{aligned} \\]","title":"Makeham Gompertz Model"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/","text":"Life Tables \u00b6 Overview \u00b6 Life Tables existed long before the survival models in the previous section. The mortality functions were presented in tabular form, where the probabilities and expectations were then calculated from. Both the survival model and life table are equivalent ways of obtaining the same results. Basic Life Table \u00b6 The life table is constructed based on the mortality of a group of people known as the Cohort . The initial age of the group people is known as the Starting Age , denoted by \\(\\alpha\\) . The initial number of people in the cohort is known as the Radix , which is usually a large round number (EG. 1000) Conversely, the maximum age is known as the Terminal Age , denoted by \\(\\omega\\) . Everybody in the cohort is expected to gradually die by the terminal age . The number of people alive from the cohort at age \\(x\\) is denoted as \\(\\ell_x\\) : \\[ \\begin{aligned} \\ell_0 &= 1000 \\\\ \\ell_{\\omega} &= 0 \\\\ \\\\ \\therefore \\ell_{x+t} &< \\ell_{x} \\end{aligned} \\] From this basic life table, several important values can be calculated: The expected number of deaths over a period, \\({}_{n}d_{x}\\) The probability of survival past a period, \\({}_{n}p_{x}\\) The probability of death within a period, \\({}_{n}q_{x}\\) The deferred probability of death within a period, \\({}_{s|t}q_{x}\\) \\[ \\begin{aligned} {}_{n}d_{x} &= \\ell_{x} - \\ell_{x+n} \\\\ {}_{n}p_{x} &= \\frac{\\ell_{x+t}}{\\ell_{x}} \\\\ {}_{n}q_{x} &= \\frac{{}_{n}d_{x}}{\\ell_{x}} \\\\ {}_{s|t}q_{x} &= \\frac{{}_{t}d_{x+s}}{l_{x}} \\\\ \\end{aligned} \\] Thus, it can be seen that the probabilities are simply the proportion of people who died/survived over the a given period. Fractional Age Assumptions \u00b6 One limitation of the life table is that it is computed at discrete ages while many problems require probabilities for non-discrete ages. Thus, several assumptions about the life table that allows non-discrete values to be interpolated from the discrete ones, known as Fractional Age Assumptions . Uniform Distribution of Deaths \u00b6 The Uniform Distribution of Deaths (UDD) is an assumption that allows for Linear Interpolation between discrete ages. It assumes that there is a uniform distribution of deaths between ages such that the probability of survival decreases linearly over the year. The UDD assumption is less appropriate for older individuals as the probability of death begins to increase exponentially, even from month to month. Let the fractional age be \\(s\\) . Thus, the probability using fractional ages is simply the weighted average of the discrete points: \\[ \\begin{aligned} {}_{s}p_{x} &= (1-s) {}_{0}p_{x} + s {}_{1}p_{x} \\\\ &= (1-s)\\frac{\\ell_{x}}{\\ell_{x}} + s \\frac{\\ell_{x+1}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x}-s\\ell_{x} + s\\ell_{x+1}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x} - s(\\ell_{x}-\\ell_{x+1})}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x} - s d_{x}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x}}{\\ell_{x}} - s\\frac{d_{x}}{\\ell_{x}} \\\\ &= 1 - s q_{x} \\\\ \\\\ \\therefore {}_{s}q_{x} &= s \\cdot q_x \\end{aligned} \\] Note that \\({}_{0}p_{x}=1\\) as it is the probability of surviving in that moment; which should be guaranteed. Based on this, the PDF and Force of Mortality can also be determined: \\[ \\begin{aligned} F_x(s) &= s \\cdot q_x \\\\ \\\\ f_x(s) &= \\frac{d}{ds} (s \\cdot q_x) \\\\ &= q_x \\\\ \\\\ \\mu_{x+t} &= \\frac{f_x(s)}{S_x(s)} \\\\ &= \\frac{q_x}{1-sq_x} \\end{aligned} \\] Constant Force of Mortality \u00b6 The Constant Force of Mortality is an assumption that allows for Exponential Interpolation between discrete ages. It assumes that there is a constant force of mortality between ages, such that the survival probability decreases exponentially over the year. \\[ \\begin{aligned} {}_{1}p_{x} &= e^{-\\int^{x+1}_{x} \\mu_c} \\\\ &= e^{-[t\\mu_c]^{x+1}_{x}} \\\\ &= e^{-[(x+1)\\mu_c - (x)\\mu_c ]} \\\\ &= e^{-[\\mu_c (x+1-x)]} \\\\ &= e^{-\\mu_c} \\\\ \\mu_c &= - \\ln p_{x} \\\\ \\\\ \\therefore {}_{s}p_{x} &= e^{-\\int^{x+s}_{x} \\mu_c} \\\\ &= e^{-s\\mu_c} \\\\ &= e^{s\\ln p_{x}} \\\\ &= e^{\\ln (p_{x})^{s}} \\\\ &= (p_{x})^{s} \\\\ \\\\ \\therefore {}_{s}q_{x} &= 1 - {}_{s}p_{x} \\\\ &= 1 - (p_{x})^{s} \\\\ &= 1 - [(1-q_{x})]^{s} \\end{aligned} \\] Select & Ultimate Mortality \u00b6 Mortality rates for the general population versus individuals who buy life insurance tend to be different. Generally speaking, people who purchase insurance tend to be richer and thus have better mortality rates than the general population . Within the individuals who purchase life insurance, those who have recently purchased a policy tend to have better mortality . This is because these individuals would have gone through Medical Underwriting and thus is expected to be in better health. The extent of the better mortality decreases over time and after a few years, they should experience the same mortality as the rest of the individuals who purchased life insurance. The duration of time is dependent on the rigorousness of the underwriting process . Formally speaking, the individuals who purchased life insurance were selected by the underwriting process and thus the better mortality experienced is known as Select Mortality . Similarly, the time whereby the select mortality is better is known as the Select Period . The select mortality ultimately converges with the non-select group, known as the Ultimate Mortality . In actuarial notation, subscript \\([x]\\) is used to distinguish select mortality from ultimate mortality, where the select period is denoted as \\(d\\) : \\[ \\begin{aligned} \\begin{cases} q_{[x]+t} < q_{x+t},& t < d \\\\ q_{[x]+t} = q_{x+t},& t \\ge d \\end{cases} \\end{aligned} \\] Note that the definition of \\(x\\) is dependent on the questions: \\(x\\) is the select age - \\([x], [x]+1, [x]+2, ...\\) \\(x\\) is the ultimate age - \\([x], [x-1], [x-2], ...\\)","title":"Life Tables"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#life-tables","text":"","title":"Life Tables"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#overview","text":"Life Tables existed long before the survival models in the previous section. The mortality functions were presented in tabular form, where the probabilities and expectations were then calculated from. Both the survival model and life table are equivalent ways of obtaining the same results.","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#basic-life-table","text":"The life table is constructed based on the mortality of a group of people known as the Cohort . The initial age of the group people is known as the Starting Age , denoted by \\(\\alpha\\) . The initial number of people in the cohort is known as the Radix , which is usually a large round number (EG. 1000) Conversely, the maximum age is known as the Terminal Age , denoted by \\(\\omega\\) . Everybody in the cohort is expected to gradually die by the terminal age . The number of people alive from the cohort at age \\(x\\) is denoted as \\(\\ell_x\\) : \\[ \\begin{aligned} \\ell_0 &= 1000 \\\\ \\ell_{\\omega} &= 0 \\\\ \\\\ \\therefore \\ell_{x+t} &< \\ell_{x} \\end{aligned} \\] From this basic life table, several important values can be calculated: The expected number of deaths over a period, \\({}_{n}d_{x}\\) The probability of survival past a period, \\({}_{n}p_{x}\\) The probability of death within a period, \\({}_{n}q_{x}\\) The deferred probability of death within a period, \\({}_{s|t}q_{x}\\) \\[ \\begin{aligned} {}_{n}d_{x} &= \\ell_{x} - \\ell_{x+n} \\\\ {}_{n}p_{x} &= \\frac{\\ell_{x+t}}{\\ell_{x}} \\\\ {}_{n}q_{x} &= \\frac{{}_{n}d_{x}}{\\ell_{x}} \\\\ {}_{s|t}q_{x} &= \\frac{{}_{t}d_{x+s}}{l_{x}} \\\\ \\end{aligned} \\] Thus, it can be seen that the probabilities are simply the proportion of people who died/survived over the a given period.","title":"Basic Life Table"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#fractional-age-assumptions","text":"One limitation of the life table is that it is computed at discrete ages while many problems require probabilities for non-discrete ages. Thus, several assumptions about the life table that allows non-discrete values to be interpolated from the discrete ones, known as Fractional Age Assumptions .","title":"Fractional Age Assumptions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#uniform-distribution-of-deaths","text":"The Uniform Distribution of Deaths (UDD) is an assumption that allows for Linear Interpolation between discrete ages. It assumes that there is a uniform distribution of deaths between ages such that the probability of survival decreases linearly over the year. The UDD assumption is less appropriate for older individuals as the probability of death begins to increase exponentially, even from month to month. Let the fractional age be \\(s\\) . Thus, the probability using fractional ages is simply the weighted average of the discrete points: \\[ \\begin{aligned} {}_{s}p_{x} &= (1-s) {}_{0}p_{x} + s {}_{1}p_{x} \\\\ &= (1-s)\\frac{\\ell_{x}}{\\ell_{x}} + s \\frac{\\ell_{x+1}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x}-s\\ell_{x} + s\\ell_{x+1}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x} - s(\\ell_{x}-\\ell_{x+1})}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x} - s d_{x}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x}}{\\ell_{x}} - s\\frac{d_{x}}{\\ell_{x}} \\\\ &= 1 - s q_{x} \\\\ \\\\ \\therefore {}_{s}q_{x} &= s \\cdot q_x \\end{aligned} \\] Note that \\({}_{0}p_{x}=1\\) as it is the probability of surviving in that moment; which should be guaranteed. Based on this, the PDF and Force of Mortality can also be determined: \\[ \\begin{aligned} F_x(s) &= s \\cdot q_x \\\\ \\\\ f_x(s) &= \\frac{d}{ds} (s \\cdot q_x) \\\\ &= q_x \\\\ \\\\ \\mu_{x+t} &= \\frac{f_x(s)}{S_x(s)} \\\\ &= \\frac{q_x}{1-sq_x} \\end{aligned} \\]","title":"Uniform Distribution of Deaths"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#constant-force-of-mortality","text":"The Constant Force of Mortality is an assumption that allows for Exponential Interpolation between discrete ages. It assumes that there is a constant force of mortality between ages, such that the survival probability decreases exponentially over the year. \\[ \\begin{aligned} {}_{1}p_{x} &= e^{-\\int^{x+1}_{x} \\mu_c} \\\\ &= e^{-[t\\mu_c]^{x+1}_{x}} \\\\ &= e^{-[(x+1)\\mu_c - (x)\\mu_c ]} \\\\ &= e^{-[\\mu_c (x+1-x)]} \\\\ &= e^{-\\mu_c} \\\\ \\mu_c &= - \\ln p_{x} \\\\ \\\\ \\therefore {}_{s}p_{x} &= e^{-\\int^{x+s}_{x} \\mu_c} \\\\ &= e^{-s\\mu_c} \\\\ &= e^{s\\ln p_{x}} \\\\ &= e^{\\ln (p_{x})^{s}} \\\\ &= (p_{x})^{s} \\\\ \\\\ \\therefore {}_{s}q_{x} &= 1 - {}_{s}p_{x} \\\\ &= 1 - (p_{x})^{s} \\\\ &= 1 - [(1-q_{x})]^{s} \\end{aligned} \\]","title":"Constant Force of Mortality"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#select-ultimate-mortality","text":"Mortality rates for the general population versus individuals who buy life insurance tend to be different. Generally speaking, people who purchase insurance tend to be richer and thus have better mortality rates than the general population . Within the individuals who purchase life insurance, those who have recently purchased a policy tend to have better mortality . This is because these individuals would have gone through Medical Underwriting and thus is expected to be in better health. The extent of the better mortality decreases over time and after a few years, they should experience the same mortality as the rest of the individuals who purchased life insurance. The duration of time is dependent on the rigorousness of the underwriting process . Formally speaking, the individuals who purchased life insurance were selected by the underwriting process and thus the better mortality experienced is known as Select Mortality . Similarly, the time whereby the select mortality is better is known as the Select Period . The select mortality ultimately converges with the non-select group, known as the Ultimate Mortality . In actuarial notation, subscript \\([x]\\) is used to distinguish select mortality from ultimate mortality, where the select period is denoted as \\(d\\) : \\[ \\begin{aligned} \\begin{cases} q_{[x]+t} < q_{x+t},& t < d \\\\ q_{[x]+t} = q_{x+t},& t \\ge d \\end{cases} \\end{aligned} \\] Note that the definition of \\(x\\) is dependent on the questions: \\(x\\) is the select age - \\([x], [x]+1, [x]+2, ...\\) \\(x\\) is the ultimate age - \\([x], [x-1], [x-2], ...\\)","title":"Select &amp; Ultimate Mortality"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/","text":"Life Assurances \u00b6 Overview \u00b6 Life Assurances are contracts that promise to pay out a benefit if the insured event occurs in the future . The value of a life assurance must reflect these two aspects: Uncertainty of cashflows - Expected Value , based on Survival Models Time value of money - Present Value , based on Interest Theory Thus, the value of a life assurance is the Expected Present Value (EPV) of the promised cashflows. Payable Discretely \u00b6 The simplest form of life assurance pays the benefits at the end of the year that the insured event occurs. While not common in practice, it provides a simple framework to understand the core concepts. For an assurance payable discretely, \\(K_x\\) is used as the survival model . Thus, the present value of the benefit payable in each period is \\(v^{K_x + 1}\\) . The EPV is the Triple Product Summation of the various components: Amount of benefit, \\(B\\) Discounting of the benefit, \\(v^{K_x + 1}\\) Probability of paying the benefit, \\({}_{K_x}p_{x} {}_{}q_{x + K_x}\\) \\[ \\begin{aligned} EPV &= \\sum B \\cdot v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\\\ &= \\sum B \\cdot v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] For simplicity, all of the proofs in this section will use \\(B=1\\) . This also has the benefit of allowing the EPVs to be easily scaled for any level of \\(B\\) . Actuarial Notation \u00b6 Similar to the survival model, given how often these values are calculated, they are abbreviated using the International Actuarial Notation as well. \\(A\\) represents the first moment (expectation) of the present value of a contract where a benefit of 1 is payable discretely when the status fails . The subscript \\(x\\) is the age of the policyholder, known as the Life Status . It fails when the policyholder dies . An additional subscript \\(:\\enclose{actuarial}{n}\\) is the duration that the assurance remains valid, known as the Duration Status . It fails if the policyholder survives past the policyterm. A \\(1\\) above the status indicates that the assurance pays only pays if that particular status fails first . If nothing is indicated, then the assurance pays whichever status fails first. If a particular status is omitted, then the assurance is not dependent on that status. For instance, omitting the duration status means that the contract is only dependent on the age of the policyholder. An assurance that begins \\(n\\) years later is known as a Deferred Assurance , which is denoted by \\({}_{n|}A\\) , following the same notation as deferred probabilities. Thus, putting everything together, the EPV of each assurance can be denoted as follows: Whole Life Assurance - Payable whenever policyholder dies; \\(A_x\\) Term Assurance - Payable only if policyholder dies during assurance period; \\(A^{1}_{x:\\enclose{actuarial}{n}}\\) Pure Endowment - Payable only if policyholder survives past assurance period; \\(A^{\\>\\>\\> 1}_{x:\\enclose{actuarial}{n}}\\) Endowment Assurance - Payable whichever of the above two occur first; \\(A_{x:\\enclose{actuarial}{n}}\\) Deferred Whole Life Assurance - Payable only if policyholder dies after \\(n\\) years; \\({}_{n|}A_x\\) Pure Endowments can also be expressed as \\({}_{n}E_x\\) for easier typesetting. Whole Life Assurance \u00b6 Whole Life Assurances cover the insured indefinitely and thus will pay out whenever the insured dies. Let WL be the random variable denoting the PV of the death benefit: \\[ \\begin{aligned} \\text{WL} &= v^{K_x + 1} \\\\ K_x &= 0, 1, \\dots, \\infty \\end{aligned} \\] Technically speaking, the upper limit of \\(K_x\\) should \\(\\omega-x\\) as it represents the maximum attainable age in the discrete survival model. However, since the \\({}_{k}p_{x} = 0\\) for all \\(k \\ge \\omega-x\\) , it does not matter what the upper limit is as long as it is larger than \\(\\omega-x\\) . Thus, \\(\\infty\\) is used for conciseness instead. The EPV is the Expectation/First Moment of the WL random variable: \\[ \\begin{aligned} E(\\text{WL}) &= \\sum^\\infty_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ A_{x} &= \\sum^\\infty_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Another commonly used metric is the Variance of the contract. In order to get it, the Second Moment must first be determined: \\[ \\begin{aligned} E({\\text{WL}}^2) &= \\sum^\\infty_{K_x = 0} \\left(v^{K_x + 1}\\right)^2 \\cdot {}_{K_x|}q_{x} \\\\ &= \\sum^\\infty_{K_x = 0} \\left((v^2)^{K_x + 1}\\right) \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Notice that the expression can be simplified to a form that looks almost identical to the first moment - with the only difference being that is uses \\(v^2\\) instead of \\(v\\) . Thus, the second moment is simply the first moment evaluated at a higher interest rate \\(i^*=(1+i)^2-1\\) , such that \\(v^* = v^2\\) . Generally, the k-th moment is denoted as \\({}^{k}A_x\\) , where the \\(k\\) is the multiplier on the interest rate used to evaluate the moment, \\(i^*=(1+i)^k-1\\) . It can also be written as \\(A_x |_{i = (1+i)^2-1}\\) using non-actuarial notation, which will come in handy for more complicated expressions. The Second Moment and hence Variance can be shown as: \\[ \\begin{aligned} E({\\text{WL}}^2) &= \\sum^\\infty_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ {}^{2} A_{x} &= \\sum^\\infty_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ \\\\ \\therefore Var(\\text{WL}) &= {}^{2} A_{x} - (A_{x})^2 \\end{aligned} \\] Note that the actual benefit must also be squared - this is likely to result in a relatively large value for the second moment and variance. Term Assurance \u00b6 Term Assurance covers the insured for a specified period \\(n\\) and only pays out if the insured dies within that period. NOTHING is paid out if the insured survives beyond that. A WL Assurance can be thought of as a special Term Assurance with infinite coverage. Let TA be the random variable denoting the PV of the death benefit: \\[ \\begin{aligned} \\text{TA} &= \\begin{cases} v^{K_x + 1} ,& K_x = 0, 1, \\dots, n-1 \\\\ 0 ,& K_x \\ge n \\end{cases} \\\\ &= {}_{\\{K_x \\le n-1\\}} v^{K_x+1} \\end{aligned} \\] \\(\\{K_x \\le n-1\\}\\) is known as an Indicator Function , which is a binary variable that takes 1 if the condition is true and 0 if the condition if false . It provides a concise way to express a piecewise function in a single expression. The EPV is the expectation of the TA random variable: \\[ \\begin{aligned} E(\\text{TA}) &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + 0 \\cdot {}_{n}p_{x} \\\\ A^1_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Similar as before, the second moment and thus Variance can be calculated as the following: \\[ \\begin{aligned} E({\\text{TA}}^2) &= \\sum^{n-1}_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} + 0^2 \\cdot {}_{n}p_{x} \\\\ {}^{2} A^1_{x:\\enclose{actuarial}{n}} &= \\sum^\\infty_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ \\\\ \\therefore Var(\\text{TA}) &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} - (A^1_{x:\\enclose{actuarial}{n}})^2 \\end{aligned} \\] Pure Endowment \u00b6 Pure Endowments are a special kind of contract that instead only pays out if the insured survives past a specified period \\(n\\) . NOTHING is paid out if the insured dies before that. Let PE be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{PE} &= \\begin{cases} 0 ,& K_x = 0, 1, 2 \\dots, n \\\\ v^n ,& K_x \\ge n \\end{cases} \\end{aligned} \\] The EPV is the expectation of the PE random variable. However, note that the probability of surviving past the period is given by a single probability \\({}_{n}p_{x}\\) : \\[ \\begin{aligned} E(\\text{PE}) &= 0 \\cdot {}_{n}q_{x} + v^n {}_{n}p_{x} \\\\ {}_{n}E_x &= v^n {}_{n}p_{x} \\end{aligned} \\] Similar as before, the second moment and thus Variance can be calculated: \\[ \\begin{aligned} E({\\text{PE}}^2) &= 0^2 \\cdot {}_{n}q_{x} + (v^2)^n {}_{n}p_{x} \\\\ {}^{2}_{n}E_x &= (v^*)^n {}_{n}p_{x} \\\\ \\\\ \\therefore Var(\\text{PE}) &= {}^{2}_{n}E_x - ({}_{n}E_x)^2 \\end{aligned} \\] Endowment Assurance \u00b6 Endowment Assurances are a combination of term assurances and pure endowments: Term Assurance - Pays out if the insured dies within the period Pure Endowment - Pays out if the insured survives past the period Thus, endowment assurances WILL pay out no matter what Let EA be the random variable denoting the PV of the benefits: \\[ \\begin{aligned} \\text{EA} &= \\begin{cases} v^{K_x + 1} ,& K_x = 0, 1, \\dots, n-1 \\\\ v^n ,& K_x \\ge n \\end{cases} \\\\ &= v^{min(K_x + 1, n)} \\end{aligned} \\] The EPV is the expectation of the EA random variable: \\[ \\begin{aligned} E(\\text{EA}) &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n {}_{n}p_{x} \\\\ A_{x:\\enclose{actuarial}{n}} &= A^1_{x:\\enclose{actuarial}{n}} + {}_{n}E_{x} \\end{aligned} \\] Note that the in the final year of the contract, the assurance will pay \\(v^n\\) regardless of the outcome - TA pays if they die while PE pays if they survive. Thus, a simplification can be made to the EPV: \\[ \\begin{aligned} A_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n {}_{n}p_{x} \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} + v^n {}_{n-1}p_{x} {}_{}q_{x + n - 1} + v^n {}_{n}p_{x} \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n ({}_{n-1}p_{x} {}_{}q_{x + n - 1} + {}_{n}p_{x}) \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n {}_{n-1}p_{x} \\end{aligned} \\] Similar as before, the second moment and thus Variance can be calculated: \\[ \\begin{aligned} E(\\text{EA}^2) &= \\sum^{n-1}_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} + (v^2)^n {}_{n}p_{x} \\\\ {}^{2} A_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} + (v^*)^n {}_{n}p_{x} \\\\ &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\\\ \\\\ \\therefore Var(\\text{EA}) &= {}^{2} A_{x:\\enclose{actuarial}{n}} - \\left(A_{x:\\enclose{actuarial}{n}}\\right)^2 \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left(A^1_{x:\\enclose{actuarial}{n}} - ({}_{n}E_x) \\right)^2 \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left[(A^1_{x:\\enclose{actuarial}{n}})^2 + 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) + ({}_{n}E_x)^2 \\right] \\end{aligned} \\] The same outcome can be reached using a slightly different approach: \\[ \\begin{aligned} Var(EA) &= Var(\\text{TA} + \\text{PE}) \\\\ &= Var(\\text{TA}) + Var(\\text{PE}) + 2 Cov(\\text{TA}, \\text{PE}) \\\\ &= Var(\\text{TA}) + Var(\\text{PE}) + 2 [E(\\text{TA} \\cdot \\text{PE}) - E(\\text{TA}) \\cdot E(\\text{PE})] \\end{aligned} \\] Consider the distribution of TA and PE: \\[ \\begin{aligned} TA \\cdot PE &= \\begin{cases} v^{K_x + 1} \\cdot 0, K_x \\lt n \\\\ 0 \\cdot v^n, K_x \\ge n \\end{cases} \\\\ &= \\begin{cases} 0, K_x \\lt n \\\\ 0, K_x \\ge n \\end{cases} \\\\ \\\\ \\therefore E(\\text{TA} \\cdot \\text{PE}) &= 0 \\cdot {}_{n}q_x + 0 \\cdot {}_{n}p_x \\\\ &= 0 \\end{aligned} \\] This results in the same variance as before: \\[ \\begin{aligned} Var(EA) &= Var(\\text{TA}) + Var(\\text{PE}) + 2 [E(\\text{TA} \\cdot \\text{PE}) - E(\\text{TA}) \\cdot E(\\text{PE})] \\\\ &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} - (A^1_{x:\\enclose{actuarial}{n}})^2 + {}^{2}_{n}E_x - ({}_{n}E_x)^2 - 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left[(A^1_{x:\\enclose{actuarial}{n}})^2 + 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) + ({}_{n}E_x)^2 \\right] \\\\ \\end{aligned} \\] While this process might seem redundant, it provides an easy to understand example of how the variance of a combination of assurances is derived by considering the covariance. Deferred Assurances \u00b6 Deferred Assurances are variations of any of the above assurances, where the assurance starts \\(n\\) years later rather than immediately. While any assurance can be deferred, the most common is the Deferred Whole Life . Deferred WL \u00b6 Let DWL be the random variable denoting the PV of the benefits of a deferred whole life assurance: \\[ \\begin{aligned} \\text{DWL} &= \\begin{cases} 0,& K_x = 0, 1, 2, \\dots, n-1 \\\\ v^{K_x + 1} ,& K_x \\ge n \\end{cases} \\\\ &= {}_{\\{K_x \\ge n\\}} v^{K_x + 1} \\end{aligned} \\] The EPV is the expectation of the DWL random variable: \\[ \\begin{aligned} E(\\text{DWL}) &= 0 \\cdot {}_{n}p_{x} + \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ {}_{n|} A_{x} &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} - \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ &= A_x - A^1_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] Alternatively, since a DWL is simply a WL assurance issued \\(n\\) years later, the EPV of the DWL is equivalent to the EPV of a WL issued at age \\(x+n\\) after adjusting for interest and survival : \\[ \\begin{aligned} {}_{n|} A_{x} &= \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ &= \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x}p_{x} \\cdot q_{x+K_x} \\\\ &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1 + n} \\cdot {}_{K_x+n}p_{x} \\cdot q_{x+K_x+n} \\\\ &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} v^{n} \\cdot {}_{n}p_{x} {}_{K_x}p_{x+n} \\cdot q_{x+K_x+n} \\\\ &= v^n {}_{n} p_{x} \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x+n} \\\\ &= {}_{n}E_{x} \\cdot A_{x+n} \\\\ \\end{aligned} \\] PEs can be use as a discount factor for functions that takes mortality into consideration . If provided by the life table, this reduces the need for computation. When going \"back\" in time, it must reflect that the policyholder will eventually survive till the current age, which is why the probability of surviving the period must be multiplied. This allows a TA to be expressed as the difference of two WL assurances issued at different times : \\[ \\begin{aligned} {}_{n|} A_{x} &= {}_{n}E_{x} * A_{x+n} \\\\ A_x - A^1_{x:\\enclose{actuarial}{n}} &= {}_{n}E_{x} * A_{x+n} \\\\ A^1_{x:\\enclose{actuarial}{n}} &= A_{x} - {}_{n}E_{x} * A_{x+n} \\\\ \\end{aligned} \\] This result is extremely important as this is the main method of calculating the EPV of a TA as the values for the WL can be easily found in the SULT. If the interest is not 0.05 or if mortality does NOT follow the SULT, then the EPV of a TA must be calculated manually. However, there is usually a catch that allows the EPV to be easily calculated manually: Different Interest : Term of the contract is short (EG. 3 Years) Different Mortality : Mortality can be simplified (EG. Becomes a constant) It is rare to have problems that have both different interest and mortality. In such cases, it is likely that an adjusted mortality table is provided. The second moment of a TA is still an expectation, thus the above method applies to it as well: \\[ \\begin{aligned} {}^{2} A^1_{x:\\enclose{actuarial}{n}} &= {}^{2} A_{x} - {}^{2}_{n} E_{x} \\cdot {}^{2} A_{x+n} \\end{aligned} \\] The key intuition is understanding that the discounting factor should be squared as well, which is why \\({}^{2}_{n} E_{x}\\) is used instead. Finally, the variance of a DWL can be easily calculated by applying previous results: \\[ \\begin{aligned} Var (DWL) &= {}^{2}_{n|}A_x - ({}_{n|}A_x)^2 \\\\ &= \\left({}^{2}A_x - {}^{2}A^1_{x:\\enclose{actuarial}{n}}\\right) - \\left(A_x - A^1_{x:\\enclose{actuarial}{n}}\\right)^2 \\\\ &= \\left({}^{2}A_x - {}^{2}A^1_{x:\\enclose{actuarial}{n}}\\right) + \\left[(A_x)^2 - 2(A_x)(A^1_{x:\\enclose{actuarial}{n}}) + (A^1_{x:\\enclose{actuarial}{n}})^2 \\right] \\end{aligned} \\] Deferred TA \u00b6 Another less commonly used deferred assurance is the Deferred Term Assurance . As both a deferred and term assurance, both results apply to it: \\[ \\begin{aligned} {}_{k|}A^1_{x:\\enclose{actuarial}{n}} &= {}_{k}E_{x} \\cdot A^1_{x+k:\\enclose{actuarial}{n}} \\\\ &= {}_{k}E_{x} \\cdot (A_{x+k} - {}_{n}E_{x} \\cdot A_{x+k+n}) \\end{aligned} \\] Although rarely used, this result can be tricky due to the different PEs used to discount - one is for the deferred assurance while the other is for the term. Alternatively, it can be used as a building block to decompose a regular assurance. An \\(n\\) year term assurance can be thought of as the sum of \\(n\\) deferred TAs , each with a one year term: \\[ \\begin{aligned} A^1_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{k=0} {}_{k|}A^1_{x:\\enclose{actuarial}{1}} \\\\ A_{x} &= \\sum^{\\infty}_{k=0} {}_{k|}A_{x} \\end{aligned} \\] Since WLs are just term assurances with infinite coverage, it can be extended to WLs as well if needed. This result will come in handy in later sections. Recursions \u00b6 The EPV of each contract can also be expressed through backwards recursion , where it is calculated as a function of itself. Consider the WL random variable: If the policyholder dies in the year, then a benefit of 1 is paid at the end of the year. If the policyholder survives past the year, then the policyholder will die in some future year. The PV of the benefit at the end of the year is given is the EPV of the same contract but at that future time ; \\(A_{x+1}\\) . \\[ \\begin{aligned} WL &= \\begin{cases} v \\cdot 1 ,& {}_{}q_{x} \\\\ v \\cdot A_{x+1} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore A_x = v{}_{}q_{x} + v{}_{}p_{x}A_{x+1} \\] The same exercise can be shown for the TA random variable. The main difference is understanding how the age & duration changes: \\(x+1\\) reflects the new age of the policyholder (same as WL) \\(n-1\\) reflects that one year of coverage has passed (not applicable for WL) \\[ \\begin{aligned} TA &= \\begin{cases} v \\cdot 1 ,& {}_{}q_{x} \\\\ v \\cdot A^1_{x+1:\\enclose{actuarial}{n-1}} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore A^1_{x:\\enclose{actuarial}{n}} = v{}_{}q_{x} + v{}_{}p_{x}A^1_{x+1:\\enclose{actuarial}{n-1}} \\] Note that since it requires a term policy with a reduced term , this recursion is not particularly useful as it is difficult to obtain it. The PE variable is similar, with the main difference being that the policyholder will receive nothing if the policyholder dies . Thus, only the second component of the recursion remains: \\[ \\begin{aligned} PE &= \\begin{cases} 0 ,& {}_{}q_{x} \\\\ v \\cdot {}_{n-1}E_{x+1} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore {}_{n}E_{x} = v{}_{}p_{x} \\cdot {}_{n-1}E_{x+1} \\] EA is omitted from this section as it is simply the combination of a TA and PE. These identities are most useful in a spreadsheet setting where the calculations can be easily repeated to fill up an entire life table. However, even then it is necessary to have a starting point for the recusions to occur. The most common starting point is the terminal age as the EPVs can be intuitively determined since the policyholder will inevitably die at the end of the year: \\[ \\begin{aligned} A_{\\omega-1} &= v \\\\ A^{\\> \\> 1}_{\\omega-1:\\enclose{actuarial}{n}} &= v \\\\ {}_{n}E_{\\omega-1} &= 0 \\\\ A^{1}_{\\omega-1:\\enclose{actuarial}{n}} &= v \\end{aligned} \\] Intuitions \u00b6 Although the exam questions are mostly computational, it is good to have an understanding of how the different EPVs compare against one another to serve as a sort of \"sense check\" against the SULT provided to determine if the answer is in the right direction. Same Assurance \u00b6 Recall that the probability of death is an increasing function with age. The death benefit is more likely to be paid out to an older policyholder - in other words, they receive the death benefit \"sooner\" than a younger policyholder. Thus, an older policyholder has larger expected cashflows that are discounted less (due to receiving it sooner), which results in a higher EPV than a younger policyholder, all else equal: \\[ \\begin{aligned} A_{x+k} & \\gt A_{x} \\\\ A^{\\> \\> 1} _{x+k:\\enclose{actuarial}{n}} & \\gt A^{\\> \\> 1}_{x+k:\\enclose{actuarial}{n}} \\end{aligned} \\] Conversely, the probability of survival is a decreasing function with age. The survival benefit is less likely to be paid out to an older policyholder - smaller expected cashflows. Regardless of the age of the policyholder, the survival benefit is paid at the same time ( same discounting ). Thus, since an older policyholder has smaller expected cashflows , it has a lower EPV than a younger policyholder: \\[ {}_{n}E_{x+k} \\le {}_{n}E_{x} \\] Endowment Assurances have both a death and survival component , thus the comparison is a combination of the two: A younger policyholder is more likely to survive and receive the survival benefit at the end of the term (discounted more) An older policyholder is more likely to die and receive the death benefit during the term (discounted less) Assuming that the difference in expected cashflows are negligible , then an older policyholder would have an higher EPV due to the lower discounting : \\[ A_{x+k:\\enclose{actuarial}{n}} \\gt A_{x:\\enclose{actuarial}{n}} \\] Naturally, all else equal, assurances with a lower interest rate are discounted less and thus have a higher EPV . If a seperate mortality table with EPVs are given, it is likely that the question is not using an interest rate of 5%. Different Assurances \u00b6 At a young age where the probability of death is low, all else equal (where applicable), the EPVs of each assurance rank as follows: TA will have the smallest EPV . Although their benefits are paid out sooner, the expected benefits are small as the probability of death is small. WL has the next largest EPV . They have the same benefits as term in the short run, but have large expected benefits in the future . However, these large benefits are heavily discounted , still resuling in a small EPV. PE has the next largest EPV . Given the high probability of survival, the expected benefits are large . EA has the largest EPV . Since it is a combination of TA and PE, it is naturally the highest. \\[ \\begin{aligned} A^{1}_{30:\\enclose{actuarial}{n}} < A_{30} < {}_{n}E_{30} < A_{30:\\enclose{actuarial}{n}} \\\\ \\end{aligned} \\] At an old age where the probability of death is high, all else equal (where applicable), the EPVs of each assurance rank as follows: PE will have the smallest EPV . Given the low probability of survival, the expected benefits are are small . TA will have the next largest EPV . Given the high probability of death, the expected benefits are high . EA will have the next largest EPV . Since it is combination of TA and PE, it is naturally higher than both of them. WL has the largest EPV . Given the inevitable death of the policyholder, the expected benefits are the highest . \\[ \\begin{aligned} {}_{n}E_{100} < A^{1}_{100:\\enclose{actuarial}{n}} < A_{100:\\enclose{actuarial}{n}} < A_{100} \\\\ \\end{aligned} \\] As the policyholder approaches the terminal age, the EPVs tend to one another: \\[ \\begin{aligned} x &\\to \\omega \\\\ E(\\text{PE}) &\\to 0 \\\\ E(\\text{EA}) &\\to E(\\text{TA}) \\\\ E(\\text{TA}) &\\to E(\\text{WL}) \\end{aligned} \\] TA tends to WL whenever the end of the term exceeds the terminal age - thus the cashflows and hence EPV for both assurances become identical. Probabilities and Percentiles \u00b6 Apart from just calculating the Expectation and Variance, the probabilities and hence percentiles of the contract benefits can be calculated as well. The former refers to calculating the probability that the random variable is at most some value \\(u\\) . In other words, that the PV ( NOT the EPV! ) is at most \\(u\\) . \\[ \\begin{aligned} \\text{WL} &\\le u \\\\ v^{K_x+1} &\\le u \\\\ (K_x+1) &\\ln v \\le \\ln u \\\\ K_x+1 &\\ge \\frac{\\ln u}{\\ln v} \\\\ K_x & \\ge \\frac{\\ln u}{\\ln v} - 1 \\\\ \\end{aligned} \\] Note that it is a common mistake to forget to flip the inequality sign as \\(\\ln v \\lt 0\\) . To avoid this error, it is advised to plot a graph to remember that \\(K_x\\) should be larger than the calculated value: The RHS of the expression is unlikely to be an integer. However, \\(K_x\\) can only take integer values. Thus, the values can rounded up to the nearest whole number (EG. 5): \\[ \\begin{aligned} P(K_x \\ge \\frac{\\ln u}{\\ln v} - 1) &= P(K_x \\ge 5) \\\\ &= P(T_x \\ge 5) \\\\ &= {}_{5}p_x \\end{aligned} \\] The latter refers to calculating the percentile of the random variable , which is the smallest value of the RV that results in the specified probability. This process is the opposite of the previous one - it involves solving for \\(T_x\\) , converting it to \\(K_x\\) and then subsituting it back into the RV, which results in the associated percentile. Payable Continuously \u00b6 In practice, life assurances pay benefits as soon as the insured event occurs , thus is akin to paying out continuously throughout the year. Notice that paying out continuously is actually a special case of a contract that pays out \\(m\\) times a year , where \\(m\\) tends to infinity. \\(m=4\\) ; Quarterly Payments \\(m=12\\) ; Monthly Payments \\(m=\\infty\\) ; Continuous Payments Thus, despite the header stating \"payable continously\", this section will instead cover paying out \\(m\\) times a year , which can be used to derive the continuous case. Note Questions with Assurances that pay out \\(m\\) times a year are rare - if anything, the continuous case will be directly tested instead. However, the \\(m\\) times a year case is still covered because the ideas can be extended to Annuities , where such questions are common. The survival model used must now reflect the probability of death at fractional ages . Intuitively, it can be thought of as the probability living till a discrete age and then dying within a sub-period within that year: \\[ \\begin{aligned} K_x + \\frac{j+1}{m}, \\>\\> j = 0, 1, 2, ..., m-1 \\end{aligned} \\] Thus, the corresponding random variable representing the PV of the benefits: \\[ \\text{PV} = v^{K_x + \\frac{j+1}{m}} \\] The EPV of an assurance payable \\(m\\) times a year can be denoted with the \\((m)\\) superscript with the same notations as before: \\[ A^{(m)}_{x} = \\sum^{\\infty}_{K_x=0} v^{K_x} {}_{K_x}p_x \\cdot \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}} {}_{\\frac{j}{m}}p_{x+K_x} \\cdot {}_{\\frac{1}{m}} q_{x+K_x+\\frac{j}{m}} \\\\ \\] Annual Benefit Note that the benefit used is still the ANNUAL BENEFIT . To obtain the actual benefit payable per period , then it should be divided by the number of periods; \\(\\frac{B}{m}\\) . Unfortunately, most life tables do not naturally provide probabilities for death at fractional ages, thus the EPVs must be approximated from the discrete case , which will be covered later. Continuous Case \u00b6 As \\(m \\to \\infty\\) , then the survival model becomes \\(T_x\\) and hence random variable is: \\[ \\begin{aligned} \\text{PV} &= v^{T_x} \\\\ &= e^{-\\delta \\cdot T_x} \\end{aligned} \\] The EPV of an assurance payable continously is denoted with the \\(\\bar{}\\) accent with the same notations as before: \\[ \\begin{aligned} \\bar{A}_x &= \\int^{\\infty}_{0} v^{T_x} \\cdot f_x(t) \\\\ &= \\int^{\\infty}_{0} e^{-\\delta \\cdot T_x} \\cdot {}_{t}p_{x} \\mu_{x+t} \\end{aligned} \\] Recall that the second term represents the probability of living to a certain age \\(x+t\\) and then dying in an infinitely small time after that (definition of force of interest). Thus, although the EPV looks very different, it is still fundamentally a Triple Product Summation of the same components: Amount of Benefit, 1 Discounting of the Benefit, \\(e^{-\\delta \\cdot T_x}\\) Probability of paying the Benefit, \\({}_{t}p_{x} \\mu_{x+t}\\) Unlike the payable \\(m\\) times case, the EPVs can be calculated directly if the survival distribution is provided. However, since they are special cases of the payable \\(m\\) times case, it can also be calculated through approximation from the discrete case. The same logic can be applied to all other assurances, EXCEPT for PEs . This is because they pay out at a fixed time , thus there is NO such thing as a payable \\(m\\) times PE or a continuously payable PE. Uniform Distribution of Deaths \u00b6 Recall that the UDD assumption can be used to approximate survival probabilities of fractional ages from discrete probabilities. Using this approach, the Continuous EPVs can be approximated from the Discrete ones. Assuming UDD between integer ages, \\[ {}_{\\frac{j}{m}}p_{x+K_x} \\cdot {}_{\\frac{1}{m}} q_{x+K_x+\\frac{j}{m}} \\approx \\frac{1}{m} \\cdot q_{x+K_x} \\] Thus, the EPV of a continuous contract can be expressed as a function of the discrete case: \\[ \\begin{aligned} A^{(m)}_{x} &= \\sum^{\\infty}_{K_x=0} v^{K_x} {}_{K_x}p_x \\cdot \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}} {}_{\\frac{j}{m}}p_{x+K_x} \\cdot {}_{\\frac{1}{m}} q_{x+K_x+\\frac{j}{m}} \\\\ & \\approx \\sum^{\\infty}_{K_x=0} v^{K_x} {}_{K_x}p_x \\cdot \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}} \\frac{1}{m} \\cdot q_{x+K_x} \\\\ & \\approx \\sum^{\\infty}_{K_x=0} v^{K_x+1} {}_{K_x}p_x q_{x+K_x} \\cdot \\frac{1}{m} \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}-1} \\\\ & \\approx A_x \\cdot \\frac{v^{-1}}{m} \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}} \\\\ & \\approx A_x \\cdot \\frac{1+i}{m} \\cdot \\frac{v^{\\frac{1}{m}}(1-v)}{1-v^{\\frac{1}{m}}} \\\\ & \\approx A_x \\cdot \\frac{1+i}{m} \\cdot \\frac{v^{\\frac{1}{m}}(1-v)}{v^{\\frac{1}{m}}[(1+i)^{m}-1]} \\\\ & \\approx A_x \\cdot \\frac{1+i}{m} \\cdot \\frac{1-v}{(1+i)^{m}-1} \\\\ & \\approx A_x \\cdot \\frac{1+i-1}{m[(1+i)^{m}-1]} \\\\ & \\approx \\frac{i}{i^{(m)}} \\cdot A_x \\end{aligned} \\] As \\(m \\to \\infty\\) , \\[ \\begin{aligned} i^{(m)} &\\to \\delta, \\\\ A^{(m)}_{x} &\\to \\bar{A}_x, \\\\ \\therefore \\bar{A}_x &= \\frac{i}{\\delta} A_x \\end{aligned} \\] Claims Acceleration Approach \u00b6 Given that claims occur every \\(\\frac{1}{m}\\) period, then on average , the claims within a year occur at \\(\\frac{m+1}{2m}\\) : \\[ \\begin{aligned} \\text{Average Death} &= \\frac{\\frac{1}{m} + \\frac{m}{m}}{2} \\\\ &= \\frac{\\frac{1+m}{m}}{2} \\\\ &= \\frac{m+1}{2m} \\end{aligned} \\] Thus, the EPV of a continuous contract can be expressed as a function of the discrete case: \\[ \\begin{aligned} A^{(m)}_{x} & \\approx \\sum^{\\infty}_{K_x = 0} v^{K_x + \\frac{m+1}{2m}} {}_{K_x|} q_{x} \\\\ & \\approx v^{\\frac{m+1}{2m}-1} \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} {}_{K_x|} q_{x} \\\\ & \\approx v^{\\frac{m+1-2m}{2m}} A_x \\\\ & \\approx v^{\\frac{-m+1}{2m}} A_x \\\\ & \\approx v^{-\\frac{m-1}{2m}} A_x \\\\ & \\approx (1+i)^{\\frac{m-1}{2m}} A_x \\end{aligned} \\] As \\(m \\to \\infty\\) , \\[ \\begin{aligned} m &\\to \\infty, \\\\ \\frac{m-1}{2m} &\\to \\frac{1}{2}, \\\\ A^{(m)}_{x} &\\to \\bar{A}_x, \\\\ \\therefore \\bar{A}_x &= (1+i)^{\\frac{1}{2}} A_x \\end{aligned} \\] This is known as the Claims Acceleration Approach as the claims are paid out earlier on in the year as compared to the end. Generally speaking, this approach is preferred as it only has one parameter compared to UDD which has two. Common Pitfalls \u00b6 Recall that there are no continuous PEs - thus, be very careful when approximating an EA as ONLY the TA portion needs to be approximated ! This means that we cannot directly apply an approximation to the EA values provided in the SULT - they must be broken down into the TA and EA components first. If calculating manually, it is always advised to calculate the TA and EA components seperately before combining into an EA to prevent falling into the trap of directly approximating the EA. \\[ \\begin{aligned} \\bar{A}_{x:\\enclose{actuarial}{n}} & \\ne \\frac{i}{\\delta} A_{x:\\enclose{actuarial}{n}} \\\\ \\bar{A}_{x:\\enclose{actuarial}{n}} & = \\frac{i}{\\delta} A^{1}_{x:\\enclose{actuarial}{n}} + {}_{n}E_x \\end{aligned} \\] Similarly, if approximating the second moment for variance, note that the NEW interest rate must be used in the approximation term as well: \\[ \\begin{aligned} {}^{2} \\bar{A}_x &= \\frac{i^*}{\\delta^*} \\cdot {}^{2} A_x \\end{aligned} \\] Note Although the above two points were illustrated with UDD, they apply to the claims acceleration approach as well. Lastly, the two approaches should produce similar results which differ by a few decimal places. However, these differences are enlarged when dealing with large benefits as these decimal places will be brought forward, resulting in seemingly large differences - do not be alarmed!","title":"Life Assurances"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#life-assurances","text":"","title":"Life Assurances"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#overview","text":"Life Assurances are contracts that promise to pay out a benefit if the insured event occurs in the future . The value of a life assurance must reflect these two aspects: Uncertainty of cashflows - Expected Value , based on Survival Models Time value of money - Present Value , based on Interest Theory Thus, the value of a life assurance is the Expected Present Value (EPV) of the promised cashflows.","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#payable-discretely","text":"The simplest form of life assurance pays the benefits at the end of the year that the insured event occurs. While not common in practice, it provides a simple framework to understand the core concepts. For an assurance payable discretely, \\(K_x\\) is used as the survival model . Thus, the present value of the benefit payable in each period is \\(v^{K_x + 1}\\) . The EPV is the Triple Product Summation of the various components: Amount of benefit, \\(B\\) Discounting of the benefit, \\(v^{K_x + 1}\\) Probability of paying the benefit, \\({}_{K_x}p_{x} {}_{}q_{x + K_x}\\) \\[ \\begin{aligned} EPV &= \\sum B \\cdot v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\\\ &= \\sum B \\cdot v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] For simplicity, all of the proofs in this section will use \\(B=1\\) . This also has the benefit of allowing the EPVs to be easily scaled for any level of \\(B\\) .","title":"Payable Discretely"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#actuarial-notation","text":"Similar to the survival model, given how often these values are calculated, they are abbreviated using the International Actuarial Notation as well. \\(A\\) represents the first moment (expectation) of the present value of a contract where a benefit of 1 is payable discretely when the status fails . The subscript \\(x\\) is the age of the policyholder, known as the Life Status . It fails when the policyholder dies . An additional subscript \\(:\\enclose{actuarial}{n}\\) is the duration that the assurance remains valid, known as the Duration Status . It fails if the policyholder survives past the policyterm. A \\(1\\) above the status indicates that the assurance pays only pays if that particular status fails first . If nothing is indicated, then the assurance pays whichever status fails first. If a particular status is omitted, then the assurance is not dependent on that status. For instance, omitting the duration status means that the contract is only dependent on the age of the policyholder. An assurance that begins \\(n\\) years later is known as a Deferred Assurance , which is denoted by \\({}_{n|}A\\) , following the same notation as deferred probabilities. Thus, putting everything together, the EPV of each assurance can be denoted as follows: Whole Life Assurance - Payable whenever policyholder dies; \\(A_x\\) Term Assurance - Payable only if policyholder dies during assurance period; \\(A^{1}_{x:\\enclose{actuarial}{n}}\\) Pure Endowment - Payable only if policyholder survives past assurance period; \\(A^{\\>\\>\\> 1}_{x:\\enclose{actuarial}{n}}\\) Endowment Assurance - Payable whichever of the above two occur first; \\(A_{x:\\enclose{actuarial}{n}}\\) Deferred Whole Life Assurance - Payable only if policyholder dies after \\(n\\) years; \\({}_{n|}A_x\\) Pure Endowments can also be expressed as \\({}_{n}E_x\\) for easier typesetting.","title":"Actuarial Notation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#whole-life-assurance","text":"Whole Life Assurances cover the insured indefinitely and thus will pay out whenever the insured dies. Let WL be the random variable denoting the PV of the death benefit: \\[ \\begin{aligned} \\text{WL} &= v^{K_x + 1} \\\\ K_x &= 0, 1, \\dots, \\infty \\end{aligned} \\] Technically speaking, the upper limit of \\(K_x\\) should \\(\\omega-x\\) as it represents the maximum attainable age in the discrete survival model. However, since the \\({}_{k}p_{x} = 0\\) for all \\(k \\ge \\omega-x\\) , it does not matter what the upper limit is as long as it is larger than \\(\\omega-x\\) . Thus, \\(\\infty\\) is used for conciseness instead. The EPV is the Expectation/First Moment of the WL random variable: \\[ \\begin{aligned} E(\\text{WL}) &= \\sum^\\infty_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ A_{x} &= \\sum^\\infty_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Another commonly used metric is the Variance of the contract. In order to get it, the Second Moment must first be determined: \\[ \\begin{aligned} E({\\text{WL}}^2) &= \\sum^\\infty_{K_x = 0} \\left(v^{K_x + 1}\\right)^2 \\cdot {}_{K_x|}q_{x} \\\\ &= \\sum^\\infty_{K_x = 0} \\left((v^2)^{K_x + 1}\\right) \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Notice that the expression can be simplified to a form that looks almost identical to the first moment - with the only difference being that is uses \\(v^2\\) instead of \\(v\\) . Thus, the second moment is simply the first moment evaluated at a higher interest rate \\(i^*=(1+i)^2-1\\) , such that \\(v^* = v^2\\) . Generally, the k-th moment is denoted as \\({}^{k}A_x\\) , where the \\(k\\) is the multiplier on the interest rate used to evaluate the moment, \\(i^*=(1+i)^k-1\\) . It can also be written as \\(A_x |_{i = (1+i)^2-1}\\) using non-actuarial notation, which will come in handy for more complicated expressions. The Second Moment and hence Variance can be shown as: \\[ \\begin{aligned} E({\\text{WL}}^2) &= \\sum^\\infty_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ {}^{2} A_{x} &= \\sum^\\infty_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ \\\\ \\therefore Var(\\text{WL}) &= {}^{2} A_{x} - (A_{x})^2 \\end{aligned} \\] Note that the actual benefit must also be squared - this is likely to result in a relatively large value for the second moment and variance.","title":"Whole Life Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#term-assurance","text":"Term Assurance covers the insured for a specified period \\(n\\) and only pays out if the insured dies within that period. NOTHING is paid out if the insured survives beyond that. A WL Assurance can be thought of as a special Term Assurance with infinite coverage. Let TA be the random variable denoting the PV of the death benefit: \\[ \\begin{aligned} \\text{TA} &= \\begin{cases} v^{K_x + 1} ,& K_x = 0, 1, \\dots, n-1 \\\\ 0 ,& K_x \\ge n \\end{cases} \\\\ &= {}_{\\{K_x \\le n-1\\}} v^{K_x+1} \\end{aligned} \\] \\(\\{K_x \\le n-1\\}\\) is known as an Indicator Function , which is a binary variable that takes 1 if the condition is true and 0 if the condition if false . It provides a concise way to express a piecewise function in a single expression. The EPV is the expectation of the TA random variable: \\[ \\begin{aligned} E(\\text{TA}) &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + 0 \\cdot {}_{n}p_{x} \\\\ A^1_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Similar as before, the second moment and thus Variance can be calculated as the following: \\[ \\begin{aligned} E({\\text{TA}}^2) &= \\sum^{n-1}_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} + 0^2 \\cdot {}_{n}p_{x} \\\\ {}^{2} A^1_{x:\\enclose{actuarial}{n}} &= \\sum^\\infty_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ \\\\ \\therefore Var(\\text{TA}) &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} - (A^1_{x:\\enclose{actuarial}{n}})^2 \\end{aligned} \\]","title":"Term Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#pure-endowment","text":"Pure Endowments are a special kind of contract that instead only pays out if the insured survives past a specified period \\(n\\) . NOTHING is paid out if the insured dies before that. Let PE be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{PE} &= \\begin{cases} 0 ,& K_x = 0, 1, 2 \\dots, n \\\\ v^n ,& K_x \\ge n \\end{cases} \\end{aligned} \\] The EPV is the expectation of the PE random variable. However, note that the probability of surviving past the period is given by a single probability \\({}_{n}p_{x}\\) : \\[ \\begin{aligned} E(\\text{PE}) &= 0 \\cdot {}_{n}q_{x} + v^n {}_{n}p_{x} \\\\ {}_{n}E_x &= v^n {}_{n}p_{x} \\end{aligned} \\] Similar as before, the second moment and thus Variance can be calculated: \\[ \\begin{aligned} E({\\text{PE}}^2) &= 0^2 \\cdot {}_{n}q_{x} + (v^2)^n {}_{n}p_{x} \\\\ {}^{2}_{n}E_x &= (v^*)^n {}_{n}p_{x} \\\\ \\\\ \\therefore Var(\\text{PE}) &= {}^{2}_{n}E_x - ({}_{n}E_x)^2 \\end{aligned} \\]","title":"Pure Endowment"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#endowment-assurance","text":"Endowment Assurances are a combination of term assurances and pure endowments: Term Assurance - Pays out if the insured dies within the period Pure Endowment - Pays out if the insured survives past the period Thus, endowment assurances WILL pay out no matter what Let EA be the random variable denoting the PV of the benefits: \\[ \\begin{aligned} \\text{EA} &= \\begin{cases} v^{K_x + 1} ,& K_x = 0, 1, \\dots, n-1 \\\\ v^n ,& K_x \\ge n \\end{cases} \\\\ &= v^{min(K_x + 1, n)} \\end{aligned} \\] The EPV is the expectation of the EA random variable: \\[ \\begin{aligned} E(\\text{EA}) &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n {}_{n}p_{x} \\\\ A_{x:\\enclose{actuarial}{n}} &= A^1_{x:\\enclose{actuarial}{n}} + {}_{n}E_{x} \\end{aligned} \\] Note that the in the final year of the contract, the assurance will pay \\(v^n\\) regardless of the outcome - TA pays if they die while PE pays if they survive. Thus, a simplification can be made to the EPV: \\[ \\begin{aligned} A_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n {}_{n}p_{x} \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} + v^n {}_{n-1}p_{x} {}_{}q_{x + n - 1} + v^n {}_{n}p_{x} \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n ({}_{n-1}p_{x} {}_{}q_{x + n - 1} + {}_{n}p_{x}) \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n {}_{n-1}p_{x} \\end{aligned} \\] Similar as before, the second moment and thus Variance can be calculated: \\[ \\begin{aligned} E(\\text{EA}^2) &= \\sum^{n-1}_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} + (v^2)^n {}_{n}p_{x} \\\\ {}^{2} A_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} + (v^*)^n {}_{n}p_{x} \\\\ &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\\\ \\\\ \\therefore Var(\\text{EA}) &= {}^{2} A_{x:\\enclose{actuarial}{n}} - \\left(A_{x:\\enclose{actuarial}{n}}\\right)^2 \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left(A^1_{x:\\enclose{actuarial}{n}} - ({}_{n}E_x) \\right)^2 \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left[(A^1_{x:\\enclose{actuarial}{n}})^2 + 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) + ({}_{n}E_x)^2 \\right] \\end{aligned} \\] The same outcome can be reached using a slightly different approach: \\[ \\begin{aligned} Var(EA) &= Var(\\text{TA} + \\text{PE}) \\\\ &= Var(\\text{TA}) + Var(\\text{PE}) + 2 Cov(\\text{TA}, \\text{PE}) \\\\ &= Var(\\text{TA}) + Var(\\text{PE}) + 2 [E(\\text{TA} \\cdot \\text{PE}) - E(\\text{TA}) \\cdot E(\\text{PE})] \\end{aligned} \\] Consider the distribution of TA and PE: \\[ \\begin{aligned} TA \\cdot PE &= \\begin{cases} v^{K_x + 1} \\cdot 0, K_x \\lt n \\\\ 0 \\cdot v^n, K_x \\ge n \\end{cases} \\\\ &= \\begin{cases} 0, K_x \\lt n \\\\ 0, K_x \\ge n \\end{cases} \\\\ \\\\ \\therefore E(\\text{TA} \\cdot \\text{PE}) &= 0 \\cdot {}_{n}q_x + 0 \\cdot {}_{n}p_x \\\\ &= 0 \\end{aligned} \\] This results in the same variance as before: \\[ \\begin{aligned} Var(EA) &= Var(\\text{TA}) + Var(\\text{PE}) + 2 [E(\\text{TA} \\cdot \\text{PE}) - E(\\text{TA}) \\cdot E(\\text{PE})] \\\\ &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} - (A^1_{x:\\enclose{actuarial}{n}})^2 + {}^{2}_{n}E_x - ({}_{n}E_x)^2 - 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left[(A^1_{x:\\enclose{actuarial}{n}})^2 + 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) + ({}_{n}E_x)^2 \\right] \\\\ \\end{aligned} \\] While this process might seem redundant, it provides an easy to understand example of how the variance of a combination of assurances is derived by considering the covariance.","title":"Endowment Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#deferred-assurances","text":"Deferred Assurances are variations of any of the above assurances, where the assurance starts \\(n\\) years later rather than immediately. While any assurance can be deferred, the most common is the Deferred Whole Life .","title":"Deferred Assurances"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#deferred-wl","text":"Let DWL be the random variable denoting the PV of the benefits of a deferred whole life assurance: \\[ \\begin{aligned} \\text{DWL} &= \\begin{cases} 0,& K_x = 0, 1, 2, \\dots, n-1 \\\\ v^{K_x + 1} ,& K_x \\ge n \\end{cases} \\\\ &= {}_{\\{K_x \\ge n\\}} v^{K_x + 1} \\end{aligned} \\] The EPV is the expectation of the DWL random variable: \\[ \\begin{aligned} E(\\text{DWL}) &= 0 \\cdot {}_{n}p_{x} + \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ {}_{n|} A_{x} &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} - \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ &= A_x - A^1_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] Alternatively, since a DWL is simply a WL assurance issued \\(n\\) years later, the EPV of the DWL is equivalent to the EPV of a WL issued at age \\(x+n\\) after adjusting for interest and survival : \\[ \\begin{aligned} {}_{n|} A_{x} &= \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ &= \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x}p_{x} \\cdot q_{x+K_x} \\\\ &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1 + n} \\cdot {}_{K_x+n}p_{x} \\cdot q_{x+K_x+n} \\\\ &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} v^{n} \\cdot {}_{n}p_{x} {}_{K_x}p_{x+n} \\cdot q_{x+K_x+n} \\\\ &= v^n {}_{n} p_{x} \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x+n} \\\\ &= {}_{n}E_{x} \\cdot A_{x+n} \\\\ \\end{aligned} \\] PEs can be use as a discount factor for functions that takes mortality into consideration . If provided by the life table, this reduces the need for computation. When going \"back\" in time, it must reflect that the policyholder will eventually survive till the current age, which is why the probability of surviving the period must be multiplied. This allows a TA to be expressed as the difference of two WL assurances issued at different times : \\[ \\begin{aligned} {}_{n|} A_{x} &= {}_{n}E_{x} * A_{x+n} \\\\ A_x - A^1_{x:\\enclose{actuarial}{n}} &= {}_{n}E_{x} * A_{x+n} \\\\ A^1_{x:\\enclose{actuarial}{n}} &= A_{x} - {}_{n}E_{x} * A_{x+n} \\\\ \\end{aligned} \\] This result is extremely important as this is the main method of calculating the EPV of a TA as the values for the WL can be easily found in the SULT. If the interest is not 0.05 or if mortality does NOT follow the SULT, then the EPV of a TA must be calculated manually. However, there is usually a catch that allows the EPV to be easily calculated manually: Different Interest : Term of the contract is short (EG. 3 Years) Different Mortality : Mortality can be simplified (EG. Becomes a constant) It is rare to have problems that have both different interest and mortality. In such cases, it is likely that an adjusted mortality table is provided. The second moment of a TA is still an expectation, thus the above method applies to it as well: \\[ \\begin{aligned} {}^{2} A^1_{x:\\enclose{actuarial}{n}} &= {}^{2} A_{x} - {}^{2}_{n} E_{x} \\cdot {}^{2} A_{x+n} \\end{aligned} \\] The key intuition is understanding that the discounting factor should be squared as well, which is why \\({}^{2}_{n} E_{x}\\) is used instead. Finally, the variance of a DWL can be easily calculated by applying previous results: \\[ \\begin{aligned} Var (DWL) &= {}^{2}_{n|}A_x - ({}_{n|}A_x)^2 \\\\ &= \\left({}^{2}A_x - {}^{2}A^1_{x:\\enclose{actuarial}{n}}\\right) - \\left(A_x - A^1_{x:\\enclose{actuarial}{n}}\\right)^2 \\\\ &= \\left({}^{2}A_x - {}^{2}A^1_{x:\\enclose{actuarial}{n}}\\right) + \\left[(A_x)^2 - 2(A_x)(A^1_{x:\\enclose{actuarial}{n}}) + (A^1_{x:\\enclose{actuarial}{n}})^2 \\right] \\end{aligned} \\]","title":"Deferred WL"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#deferred-ta","text":"Another less commonly used deferred assurance is the Deferred Term Assurance . As both a deferred and term assurance, both results apply to it: \\[ \\begin{aligned} {}_{k|}A^1_{x:\\enclose{actuarial}{n}} &= {}_{k}E_{x} \\cdot A^1_{x+k:\\enclose{actuarial}{n}} \\\\ &= {}_{k}E_{x} \\cdot (A_{x+k} - {}_{n}E_{x} \\cdot A_{x+k+n}) \\end{aligned} \\] Although rarely used, this result can be tricky due to the different PEs used to discount - one is for the deferred assurance while the other is for the term. Alternatively, it can be used as a building block to decompose a regular assurance. An \\(n\\) year term assurance can be thought of as the sum of \\(n\\) deferred TAs , each with a one year term: \\[ \\begin{aligned} A^1_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{k=0} {}_{k|}A^1_{x:\\enclose{actuarial}{1}} \\\\ A_{x} &= \\sum^{\\infty}_{k=0} {}_{k|}A_{x} \\end{aligned} \\] Since WLs are just term assurances with infinite coverage, it can be extended to WLs as well if needed. This result will come in handy in later sections.","title":"Deferred TA"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#recursions","text":"The EPV of each contract can also be expressed through backwards recursion , where it is calculated as a function of itself. Consider the WL random variable: If the policyholder dies in the year, then a benefit of 1 is paid at the end of the year. If the policyholder survives past the year, then the policyholder will die in some future year. The PV of the benefit at the end of the year is given is the EPV of the same contract but at that future time ; \\(A_{x+1}\\) . \\[ \\begin{aligned} WL &= \\begin{cases} v \\cdot 1 ,& {}_{}q_{x} \\\\ v \\cdot A_{x+1} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore A_x = v{}_{}q_{x} + v{}_{}p_{x}A_{x+1} \\] The same exercise can be shown for the TA random variable. The main difference is understanding how the age & duration changes: \\(x+1\\) reflects the new age of the policyholder (same as WL) \\(n-1\\) reflects that one year of coverage has passed (not applicable for WL) \\[ \\begin{aligned} TA &= \\begin{cases} v \\cdot 1 ,& {}_{}q_{x} \\\\ v \\cdot A^1_{x+1:\\enclose{actuarial}{n-1}} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore A^1_{x:\\enclose{actuarial}{n}} = v{}_{}q_{x} + v{}_{}p_{x}A^1_{x+1:\\enclose{actuarial}{n-1}} \\] Note that since it requires a term policy with a reduced term , this recursion is not particularly useful as it is difficult to obtain it. The PE variable is similar, with the main difference being that the policyholder will receive nothing if the policyholder dies . Thus, only the second component of the recursion remains: \\[ \\begin{aligned} PE &= \\begin{cases} 0 ,& {}_{}q_{x} \\\\ v \\cdot {}_{n-1}E_{x+1} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore {}_{n}E_{x} = v{}_{}p_{x} \\cdot {}_{n-1}E_{x+1} \\] EA is omitted from this section as it is simply the combination of a TA and PE. These identities are most useful in a spreadsheet setting where the calculations can be easily repeated to fill up an entire life table. However, even then it is necessary to have a starting point for the recusions to occur. The most common starting point is the terminal age as the EPVs can be intuitively determined since the policyholder will inevitably die at the end of the year: \\[ \\begin{aligned} A_{\\omega-1} &= v \\\\ A^{\\> \\> 1}_{\\omega-1:\\enclose{actuarial}{n}} &= v \\\\ {}_{n}E_{\\omega-1} &= 0 \\\\ A^{1}_{\\omega-1:\\enclose{actuarial}{n}} &= v \\end{aligned} \\]","title":"Recursions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#intuitions","text":"Although the exam questions are mostly computational, it is good to have an understanding of how the different EPVs compare against one another to serve as a sort of \"sense check\" against the SULT provided to determine if the answer is in the right direction.","title":"Intuitions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#same-assurance","text":"Recall that the probability of death is an increasing function with age. The death benefit is more likely to be paid out to an older policyholder - in other words, they receive the death benefit \"sooner\" than a younger policyholder. Thus, an older policyholder has larger expected cashflows that are discounted less (due to receiving it sooner), which results in a higher EPV than a younger policyholder, all else equal: \\[ \\begin{aligned} A_{x+k} & \\gt A_{x} \\\\ A^{\\> \\> 1} _{x+k:\\enclose{actuarial}{n}} & \\gt A^{\\> \\> 1}_{x+k:\\enclose{actuarial}{n}} \\end{aligned} \\] Conversely, the probability of survival is a decreasing function with age. The survival benefit is less likely to be paid out to an older policyholder - smaller expected cashflows. Regardless of the age of the policyholder, the survival benefit is paid at the same time ( same discounting ). Thus, since an older policyholder has smaller expected cashflows , it has a lower EPV than a younger policyholder: \\[ {}_{n}E_{x+k} \\le {}_{n}E_{x} \\] Endowment Assurances have both a death and survival component , thus the comparison is a combination of the two: A younger policyholder is more likely to survive and receive the survival benefit at the end of the term (discounted more) An older policyholder is more likely to die and receive the death benefit during the term (discounted less) Assuming that the difference in expected cashflows are negligible , then an older policyholder would have an higher EPV due to the lower discounting : \\[ A_{x+k:\\enclose{actuarial}{n}} \\gt A_{x:\\enclose{actuarial}{n}} \\] Naturally, all else equal, assurances with a lower interest rate are discounted less and thus have a higher EPV . If a seperate mortality table with EPVs are given, it is likely that the question is not using an interest rate of 5%.","title":"Same Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#different-assurances","text":"At a young age where the probability of death is low, all else equal (where applicable), the EPVs of each assurance rank as follows: TA will have the smallest EPV . Although their benefits are paid out sooner, the expected benefits are small as the probability of death is small. WL has the next largest EPV . They have the same benefits as term in the short run, but have large expected benefits in the future . However, these large benefits are heavily discounted , still resuling in a small EPV. PE has the next largest EPV . Given the high probability of survival, the expected benefits are large . EA has the largest EPV . Since it is a combination of TA and PE, it is naturally the highest. \\[ \\begin{aligned} A^{1}_{30:\\enclose{actuarial}{n}} < A_{30} < {}_{n}E_{30} < A_{30:\\enclose{actuarial}{n}} \\\\ \\end{aligned} \\] At an old age where the probability of death is high, all else equal (where applicable), the EPVs of each assurance rank as follows: PE will have the smallest EPV . Given the low probability of survival, the expected benefits are are small . TA will have the next largest EPV . Given the high probability of death, the expected benefits are high . EA will have the next largest EPV . Since it is combination of TA and PE, it is naturally higher than both of them. WL has the largest EPV . Given the inevitable death of the policyholder, the expected benefits are the highest . \\[ \\begin{aligned} {}_{n}E_{100} < A^{1}_{100:\\enclose{actuarial}{n}} < A_{100:\\enclose{actuarial}{n}} < A_{100} \\\\ \\end{aligned} \\] As the policyholder approaches the terminal age, the EPVs tend to one another: \\[ \\begin{aligned} x &\\to \\omega \\\\ E(\\text{PE}) &\\to 0 \\\\ E(\\text{EA}) &\\to E(\\text{TA}) \\\\ E(\\text{TA}) &\\to E(\\text{WL}) \\end{aligned} \\] TA tends to WL whenever the end of the term exceeds the terminal age - thus the cashflows and hence EPV for both assurances become identical.","title":"Different Assurances"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#probabilities-and-percentiles","text":"Apart from just calculating the Expectation and Variance, the probabilities and hence percentiles of the contract benefits can be calculated as well. The former refers to calculating the probability that the random variable is at most some value \\(u\\) . In other words, that the PV ( NOT the EPV! ) is at most \\(u\\) . \\[ \\begin{aligned} \\text{WL} &\\le u \\\\ v^{K_x+1} &\\le u \\\\ (K_x+1) &\\ln v \\le \\ln u \\\\ K_x+1 &\\ge \\frac{\\ln u}{\\ln v} \\\\ K_x & \\ge \\frac{\\ln u}{\\ln v} - 1 \\\\ \\end{aligned} \\] Note that it is a common mistake to forget to flip the inequality sign as \\(\\ln v \\lt 0\\) . To avoid this error, it is advised to plot a graph to remember that \\(K_x\\) should be larger than the calculated value: The RHS of the expression is unlikely to be an integer. However, \\(K_x\\) can only take integer values. Thus, the values can rounded up to the nearest whole number (EG. 5): \\[ \\begin{aligned} P(K_x \\ge \\frac{\\ln u}{\\ln v} - 1) &= P(K_x \\ge 5) \\\\ &= P(T_x \\ge 5) \\\\ &= {}_{5}p_x \\end{aligned} \\] The latter refers to calculating the percentile of the random variable , which is the smallest value of the RV that results in the specified probability. This process is the opposite of the previous one - it involves solving for \\(T_x\\) , converting it to \\(K_x\\) and then subsituting it back into the RV, which results in the associated percentile.","title":"Probabilities and Percentiles"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#payable-continuously","text":"In practice, life assurances pay benefits as soon as the insured event occurs , thus is akin to paying out continuously throughout the year. Notice that paying out continuously is actually a special case of a contract that pays out \\(m\\) times a year , where \\(m\\) tends to infinity. \\(m=4\\) ; Quarterly Payments \\(m=12\\) ; Monthly Payments \\(m=\\infty\\) ; Continuous Payments Thus, despite the header stating \"payable continously\", this section will instead cover paying out \\(m\\) times a year , which can be used to derive the continuous case. Note Questions with Assurances that pay out \\(m\\) times a year are rare - if anything, the continuous case will be directly tested instead. However, the \\(m\\) times a year case is still covered because the ideas can be extended to Annuities , where such questions are common. The survival model used must now reflect the probability of death at fractional ages . Intuitively, it can be thought of as the probability living till a discrete age and then dying within a sub-period within that year: \\[ \\begin{aligned} K_x + \\frac{j+1}{m}, \\>\\> j = 0, 1, 2, ..., m-1 \\end{aligned} \\] Thus, the corresponding random variable representing the PV of the benefits: \\[ \\text{PV} = v^{K_x + \\frac{j+1}{m}} \\] The EPV of an assurance payable \\(m\\) times a year can be denoted with the \\((m)\\) superscript with the same notations as before: \\[ A^{(m)}_{x} = \\sum^{\\infty}_{K_x=0} v^{K_x} {}_{K_x}p_x \\cdot \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}} {}_{\\frac{j}{m}}p_{x+K_x} \\cdot {}_{\\frac{1}{m}} q_{x+K_x+\\frac{j}{m}} \\\\ \\] Annual Benefit Note that the benefit used is still the ANNUAL BENEFIT . To obtain the actual benefit payable per period , then it should be divided by the number of periods; \\(\\frac{B}{m}\\) . Unfortunately, most life tables do not naturally provide probabilities for death at fractional ages, thus the EPVs must be approximated from the discrete case , which will be covered later.","title":"Payable Continuously"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#continuous-case","text":"As \\(m \\to \\infty\\) , then the survival model becomes \\(T_x\\) and hence random variable is: \\[ \\begin{aligned} \\text{PV} &= v^{T_x} \\\\ &= e^{-\\delta \\cdot T_x} \\end{aligned} \\] The EPV of an assurance payable continously is denoted with the \\(\\bar{}\\) accent with the same notations as before: \\[ \\begin{aligned} \\bar{A}_x &= \\int^{\\infty}_{0} v^{T_x} \\cdot f_x(t) \\\\ &= \\int^{\\infty}_{0} e^{-\\delta \\cdot T_x} \\cdot {}_{t}p_{x} \\mu_{x+t} \\end{aligned} \\] Recall that the second term represents the probability of living to a certain age \\(x+t\\) and then dying in an infinitely small time after that (definition of force of interest). Thus, although the EPV looks very different, it is still fundamentally a Triple Product Summation of the same components: Amount of Benefit, 1 Discounting of the Benefit, \\(e^{-\\delta \\cdot T_x}\\) Probability of paying the Benefit, \\({}_{t}p_{x} \\mu_{x+t}\\) Unlike the payable \\(m\\) times case, the EPVs can be calculated directly if the survival distribution is provided. However, since they are special cases of the payable \\(m\\) times case, it can also be calculated through approximation from the discrete case. The same logic can be applied to all other assurances, EXCEPT for PEs . This is because they pay out at a fixed time , thus there is NO such thing as a payable \\(m\\) times PE or a continuously payable PE.","title":"Continuous Case"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#uniform-distribution-of-deaths","text":"Recall that the UDD assumption can be used to approximate survival probabilities of fractional ages from discrete probabilities. Using this approach, the Continuous EPVs can be approximated from the Discrete ones. Assuming UDD between integer ages, \\[ {}_{\\frac{j}{m}}p_{x+K_x} \\cdot {}_{\\frac{1}{m}} q_{x+K_x+\\frac{j}{m}} \\approx \\frac{1}{m} \\cdot q_{x+K_x} \\] Thus, the EPV of a continuous contract can be expressed as a function of the discrete case: \\[ \\begin{aligned} A^{(m)}_{x} &= \\sum^{\\infty}_{K_x=0} v^{K_x} {}_{K_x}p_x \\cdot \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}} {}_{\\frac{j}{m}}p_{x+K_x} \\cdot {}_{\\frac{1}{m}} q_{x+K_x+\\frac{j}{m}} \\\\ & \\approx \\sum^{\\infty}_{K_x=0} v^{K_x} {}_{K_x}p_x \\cdot \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}} \\frac{1}{m} \\cdot q_{x+K_x} \\\\ & \\approx \\sum^{\\infty}_{K_x=0} v^{K_x+1} {}_{K_x}p_x q_{x+K_x} \\cdot \\frac{1}{m} \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}-1} \\\\ & \\approx A_x \\cdot \\frac{v^{-1}}{m} \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}} \\\\ & \\approx A_x \\cdot \\frac{1+i}{m} \\cdot \\frac{v^{\\frac{1}{m}}(1-v)}{1-v^{\\frac{1}{m}}} \\\\ & \\approx A_x \\cdot \\frac{1+i}{m} \\cdot \\frac{v^{\\frac{1}{m}}(1-v)}{v^{\\frac{1}{m}}[(1+i)^{m}-1]} \\\\ & \\approx A_x \\cdot \\frac{1+i}{m} \\cdot \\frac{1-v}{(1+i)^{m}-1} \\\\ & \\approx A_x \\cdot \\frac{1+i-1}{m[(1+i)^{m}-1]} \\\\ & \\approx \\frac{i}{i^{(m)}} \\cdot A_x \\end{aligned} \\] As \\(m \\to \\infty\\) , \\[ \\begin{aligned} i^{(m)} &\\to \\delta, \\\\ A^{(m)}_{x} &\\to \\bar{A}_x, \\\\ \\therefore \\bar{A}_x &= \\frac{i}{\\delta} A_x \\end{aligned} \\]","title":"Uniform Distribution of Deaths"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#claims-acceleration-approach","text":"Given that claims occur every \\(\\frac{1}{m}\\) period, then on average , the claims within a year occur at \\(\\frac{m+1}{2m}\\) : \\[ \\begin{aligned} \\text{Average Death} &= \\frac{\\frac{1}{m} + \\frac{m}{m}}{2} \\\\ &= \\frac{\\frac{1+m}{m}}{2} \\\\ &= \\frac{m+1}{2m} \\end{aligned} \\] Thus, the EPV of a continuous contract can be expressed as a function of the discrete case: \\[ \\begin{aligned} A^{(m)}_{x} & \\approx \\sum^{\\infty}_{K_x = 0} v^{K_x + \\frac{m+1}{2m}} {}_{K_x|} q_{x} \\\\ & \\approx v^{\\frac{m+1}{2m}-1} \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} {}_{K_x|} q_{x} \\\\ & \\approx v^{\\frac{m+1-2m}{2m}} A_x \\\\ & \\approx v^{\\frac{-m+1}{2m}} A_x \\\\ & \\approx v^{-\\frac{m-1}{2m}} A_x \\\\ & \\approx (1+i)^{\\frac{m-1}{2m}} A_x \\end{aligned} \\] As \\(m \\to \\infty\\) , \\[ \\begin{aligned} m &\\to \\infty, \\\\ \\frac{m-1}{2m} &\\to \\frac{1}{2}, \\\\ A^{(m)}_{x} &\\to \\bar{A}_x, \\\\ \\therefore \\bar{A}_x &= (1+i)^{\\frac{1}{2}} A_x \\end{aligned} \\] This is known as the Claims Acceleration Approach as the claims are paid out earlier on in the year as compared to the end. Generally speaking, this approach is preferred as it only has one parameter compared to UDD which has two.","title":"Claims Acceleration Approach"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#common-pitfalls","text":"Recall that there are no continuous PEs - thus, be very careful when approximating an EA as ONLY the TA portion needs to be approximated ! This means that we cannot directly apply an approximation to the EA values provided in the SULT - they must be broken down into the TA and EA components first. If calculating manually, it is always advised to calculate the TA and EA components seperately before combining into an EA to prevent falling into the trap of directly approximating the EA. \\[ \\begin{aligned} \\bar{A}_{x:\\enclose{actuarial}{n}} & \\ne \\frac{i}{\\delta} A_{x:\\enclose{actuarial}{n}} \\\\ \\bar{A}_{x:\\enclose{actuarial}{n}} & = \\frac{i}{\\delta} A^{1}_{x:\\enclose{actuarial}{n}} + {}_{n}E_x \\end{aligned} \\] Similarly, if approximating the second moment for variance, note that the NEW interest rate must be used in the approximation term as well: \\[ \\begin{aligned} {}^{2} \\bar{A}_x &= \\frac{i^*}{\\delta^*} \\cdot {}^{2} A_x \\end{aligned} \\] Note Although the above two points were illustrated with UDD, they apply to the claims acceleration approach as well. Lastly, the two approaches should produce similar results which differ by a few decimal places. However, these differences are enlarged when dealing with large benefits as these decimal places will be brought forward, resulting in seemingly large differences - do not be alarmed!","title":"Common Pitfalls"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/","text":"Life Annuities \u00b6 Overview \u00b6 Life Annuity contracts promise to pay out a stream of benefits in the future for as long the policyholder remains alive. Similar to life insurance, the benefits can be paid discretely or continuously and can be valued through their EPV. Note that these are different from the annuities covered in Exam FM. The payments for those annuities are made regardless of the life of the policyholder, known as Annuity Certain . They serve as the foundation to understanding Life Annuities. Review: Annuities Certain \u00b6 There are two types of payment structures: Period Start Period End Paid in Advance Paid in Arrears Annuity Due Annuity Immediate \\(\\ddot{a}_{\\enclose{actuarial}{n}}\\) \\(a_{\\enclose{actuarial}{n}}\\) The overall PV of the annuity is the sum of the PV of the stream of payments: \\[ \\begin{aligned} a_{\\enclose{actuarial}{n}} &= v + v^2 + v^3 + \\dots + v^n \\\\ &= \\frac{v-v^{n+1}}{1-v} \\\\ &= \\sum^n_{k=1} v^k \\\\ &= \\frac{v(1-v^n)}{1-v} \\\\ &= \\frac{1-v^n}{i} \\\\ \\\\ \\ddot{a}_{\\enclose{actuarial}{n}} &= 1 + v + v^2 + \\dots + v^n \\\\ &= \\sum^{n-1}_{k=0} v^k \\\\ &= \\frac{1- v^{n}}{1-v} \\\\ &= \\frac{1- v^{n}}{d} \\end{aligned} \\] Notice that the payments simply differ by one period and hence one discounting factor: \\[ \\begin{aligned} \\ddot{a}_{\\enclose{actuarial}{n}} &= 1 + v + v^2 + v^3 + \\dots + v^n \\\\ &= (1+i) (v + v^2 + v^3 + \\dots + v^n) \\\\ &= (1+i) \\sum^{n}_{k=1} v^k \\\\ &= (1+i) a_{\\enclose{actuarial}{n}} \\end{aligned} \\] From another perspective, only the payments at the end points \\(t=0\\) and \\(t=n\\) are different: \\[ \\begin{aligned} \\ddot{a}_{\\enclose{actuarial}{n}} &= \\sum^{n-1}_0 v^k \\\\ &= v^0 + \\sum^{n-1}_{k=1} v^k \\\\ &= 1 + a_{\\enclose{actuarial}{n-1}} \\\\ &= 1 + a_{\\enclose{actuarial}{n}} - v^n \\end{aligned} \\] Payable Discretely \u00b6 If the contract pays out benefits discretely, then \\(k\\) is used as the survival model. Since a life annuity pays out a stream of benefits, the PV of the contract at every time period can be slightly confusing. It is important to remember the following: The PV is calculated with respect to time 0 The PV is the sum of the PV of all the payments the individual is expected to live \\[ \\begin{aligned} PV &= \\begin{cases} a_{\\enclose{actuarial}{K_x}}, & \\text{Life Annuity Immediate} \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x + 1}}, & \\text{Life Annuity Due} \\end{cases} \\end{aligned} \\] Thus, the EPV of the contract is the sum product of the PV of the benefit in each period and the probability of death in that period: \\[ \\begin{aligned} EPV &= \\begin{cases} \\text{Life Annuity Immediate} &=\\ \\sum a_{\\enclose{actuarial}{k}} \\cdot {}_{k|}q_{x} \\\\ \\text{Life Annuity Due} &= \\sum \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} \\end{cases} \\end{aligned} \\] For the remainder of the section, to be concise, only the formulas for Annuity Due will be given. The corresponding formulas for Annuity Immediates can be easily calculated from it. Actuarial Notation \u00b6 Most of the notation for Life Insurance applies to Life Annuities as well. The key difference is that \\({}^{k}a\\) is used to represent the k-th moment of the present value of a contract where a benefit of 1 is paid discretely for as long as the status does NOT fail . \\(A\\) stands for Assurance while \\(a\\) stands for Annuity. Whole Life Annuity \u00b6 Whole Life Annuities covers the insured indefinitely and thus will pay out for as long as the insured survives . Let WL be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{WL}_{\\text{Due}} &=\\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\end{aligned} \\] Thus, the EPV is the Expectation of the WLA random variable: \\[ \\begin{aligned} E(\\text{WLA}_{\\text{Due}}) &= \\sum^{\\infty}_{k = 0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} \\\\ \\ddot{a}_{x} &= \\sum^{\\infty}_{k = 0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} \\end{aligned} \\] The EPV can be furthered simplified, allowing a life annuity to be viewed as the sum of a series of pure endowments : \\[ \\begin{aligned} \\ddot{a}_{x} &= \\sum^{\\infty}_{k = 0} \\ddot{a}_{\\enclose{actuarial}{k}} \\cdot {}_{k|}q_{x} \\\\ &=\\sum^{\\infty}_{k = 0} \\left(\\sum^{n-1}_{j=0} v^j \\right) \\cdot {}_{k|}q_{x} \\\\ &= v^0 \\cdot {}_{0|}q_{x} + (v^0 + v^1) \\cdot {}_{1|}q_{x} + (v^0 + v^1 + v^2) \\cdot {}_{2|}q_{x} + \\dots \\\\ &= v^0 ({}_{0|}q_{x} + {}_{1|}q_{x} + \\dots) + v^1 ({}_{1|}q_{x} + {}_{2|}q_{x} + \\dots) + v^2 ({}_{2|}q_{x} + {}_{3|}q_{x} + \\dots) + \\dots \\\\ &= \\sum^{\\infty}_{j=0} v^j \\cdot \\left(\\sum^{\\infty}_{k = j} {}_{k|}q_{x} \\right) \\\\ &= \\sum^{\\infty}_{j=0} v^j \\cdot {}_{j}p_{x} \\end{aligned} \\] Note that the above formula can be used to easily prove the relationships between different kinds of annuities, but is NOT a proper definition of an annuity - it hence cannot be adjusted to compute the variance. The second moment and variance for all life annuities will be covered in a later section. Temporary Annuity \u00b6 Temporary Life Annuities covers the insured for a specified period \\(n\\) and thus pays out for as long as the insured survives during that period only . Let TA be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{TA}_\\text{Due} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{K_x+1}},& k = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{n}},& k \\ge n \\\\ \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{min(K_x + 1, n)}} \\end{aligned} \\] Thus, the EPV is the expectation of the TA random variable, which can be simplified using the same approach as before: \\[ \\begin{aligned} E(\\text{TA}_\\text{Due}) &= \\sum^{n-1}_{k = 0} \\ddot{a}_{\\enclose{actuarial}{k}} \\cdot {}_{k|}q_x + \\ddot{a}_{\\enclose{actuarial}{n}} \\cdot {}_{n}p_x \\\\ \\ddot{a}_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{j=0} v^j \\cdot {}_{j}p_{x} \\end{aligned} \\] Note the change in limits for the simplified approach - an annuity is the combination of pure endowments for as long as the annuity lasts . This gives rise to a very simple sense check - the EPV of an \\(n\\) year temporary annuity should NOT be larger than \\(n\\) as there are \\(n\\) payments of 1 which are discounted. Deferred Annuities \u00b6 Deferred Annuities are variations of the above contracts where the coverage is deferred by \\(n\\) years . While any contract can be deferred, the most useful is the Deferred Whole Life Annuity . Let DWL be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{DWL}_{\\text{Due}} &= \\begin{cases} 0, & k = 0, 1, 2, \\dots, n-1 \\\\ v^n \\cdot \\ddot{a}_{\\enclose{actuarial}{K_x+1-n}}, & k \\ge n \\end{cases} \\end{aligned} \\] Intuitively, the RV can also be expressed as the difference between two certain annuities: \\[ \\begin{aligned} \\text{DWL}_{\\text{Due}} &= v^n \\cdot \\ddot{a}_{\\enclose{actuarial}{k+1-n}} \\\\ &= v^n \\cdot \\frac{1-v^{k+1-n}}{d} \\\\ &= \\frac{v^{n}-v^{k+1}}{d} \\\\ &= \\frac{(1-v^{k+1}) - (1-v^{n})}{d} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{k+1}} -\\ddot{a}_{\\enclose{actuarial}{n}} \\\\ \\\\ \\therefore \\text{DWL}_{\\text{Due}} &= \\begin{cases} 0, & k = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{k+1}} -\\ddot{a}_{\\enclose{actuarial}{n}} , & k \\ge n \\end{cases} \\end{aligned} \\] Thus, the EPV is the expectation of the DWL random variable: \\[ \\begin{aligned} E(\\text{DWL}_{\\text{Due}}) &= \\sum^{\\infty}_{k=n} (\\ddot{a}_{\\enclose{actuarial}{k+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{k|}q_{x} \\\\ {}_{n|}\\ddot{a}_{x} &= \\sum^{\\infty}_{k=0} (\\ddot{a}_{\\enclose{actuarial}{k+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{k|}q_{x} - \\sum^{n-1}_{k=0} (\\ddot{a}_{\\enclose{actuarial}{k+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{k|}q_{x} \\\\ &= \\sum^{\\infty}_{k=0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} - \\sum^{\\infty}_{k=0} \\ddot{a}_{\\enclose{actuarial}{n}} \\cdot {}_{k|}q_{x} - \\sum^{n-1}_{k=0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} + \\sum^{n-1}_{k=0} \\ddot{a}_{\\enclose{actuarial}{n}} \\cdot {}_{k|}q_{x} \\\\ &= \\sum^{\\infty}_{k=0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} - \\sum^{n-1}_{k=0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} \\\\ &= \\ddot{a}_x - \\ddot{a}_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] Similar to before, the EPV can be shown to be a sum of pure endowments: \\[ {}_{n|}\\ddot{a}_{x} = \\sum^{\\infty}_{j=n} v^j \\cdot {}_{j}p_{x} \\] Since a DWL is effectively a WL that is issued \\(n\\) years later, the EPV of a DWL is the EPV of the WL, adjusted for both interest and survival: \\[ \\begin{aligned} {}_{n|}a_{x} &= \\sum^{\\infty}_{j=n} v^j \\cdot {}_{j}p_{x} \\\\ &= \\sum^{\\infty}_{j=0} v^{j+n} \\cdot {}_{j+n}p_{x} \\\\ &= \\sum^{\\infty}_{j=0} v^{j} v^{n} \\cdot {}_{n}p_{x} {}_{j}p_{x+n} \\\\ &= v^{n} \\cdot {}_{n}p_{x} \\sum^{\\infty}_{j=0} v^{j} \\cdot {}_{j}p_{x+n} \\\\ &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\end{aligned} \\] This allows a TA to be expressed as the difference of two WL annuities issued at different times : \\[ \\begin{aligned} {}_{n|}\\ddot{a}_{x} &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\\\ \\ddot{a}_x - \\ddot{a}_{x:\\enclose{actuarial}{n}} &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\\\ \\ddot{a}_{x:\\enclose{actuarial}{n}} &= \\ddot{a}_x - {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\end{aligned} \\] Guaranteed Annuities \u00b6 Guaranteed Life Annuities are whole life annuities with the benefits in the first \\(n\\) years being guaranteed - they will be paid out even if the insured dies during this period. Let GA be the random variable denoting the PV of the survival benefit. Since the payments are guaranteed, they can be represented using a certain annuity : \\[ \\begin{aligned} \\text{GA}_{\\text{Due}} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& k = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}},& k \\ge n \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{max(K_x+1,n)}} \\end{aligned} \\] The random variable can be manipulated to be easier to work with: \\[ \\begin{aligned} \\text{GA}_{\\text{Due}} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}},& K_x \\ge n \\end{cases} \\\\ &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}} + \\ddot{a}_{\\enclose{actuarial}{n}},& K_x \\ge n \\end{cases} \\\\ &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}} \\\\ \\ddot{a}_{\\enclose{actuarial}{n}} \\end{cases} + \\begin{cases} 0,& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}},& K_x \\ge n \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{n}} + \\begin{cases} 0,& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}},& K_x \\ge n \\end{cases} \\end{aligned} \\] Thus, the EPV can be more easily calculated: \\[ \\begin{aligned} E(\\text{GA}_{\\text{Due}}) &= \\ddot{a}_{\\enclose{actuarial}{n}} + 0 \\cdot {}_{n}q_x + \\sum^{\\infty}_{k = n} (\\ddot{a}_{\\enclose{actuarial}{k+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{k|}q_{x}\\\\ \\ddot{a}_{\\overline{x:\\enclose{actuarial}{n}}} &= \\ddot{a}_{\\enclose{actuarial}{n}} + {}_{n} \\ddot{a}_{x} \\end{aligned} \\] Note that a Guaranteed Annuity is denoted via an additional bar above the status. Assurances and Annuities \u00b6 Consider the random variable for the PV Whole Life Assurance and Annuity Due. Notice that both RVs are related through the certain annuity formula: \\[ \\begin{aligned} \\text{WL} &= v^{K_x + 1} \\\\ \\text{WLA}_{Due} &= \\ddot{a}_{\\enclose{actuarial}{K_x+1}} = \\frac{1-v^{K_x + 1}}{d} \\end{aligned} \\] This is why Annuity Due, rather Annuity Immediates, are provided in the SULT. Thus, given the EPV of a life annuity, the EPV of a life assurance can be determined, vice-versa: \\[ \\begin{aligned} \\text{WLA} &= \\frac{1-\\text{WL}}{d} \\\\ E(\\text{WLA}_{Due}) &= \\frac{1-E(\\text{WL})}{d} \\\\ \\ddot{a}_x &= \\frac{1-A_x}{d} \\\\ d \\ddot{a}_x &= 1 - A_x \\\\ A_x &= 1 - d \\ddot{a}_x \\end{aligned} \\] The same relationship can be shown for a temporary annuity and an Endowment Assurance, NOT a term assurance : \\[ \\begin{aligned} A_{x:\\enclose{actuarial}{n}} &= 1 - d \\ddot{a}_{x:\\enclose{actuarial}{n}} \\\\ \\end{aligned} \\] It is a very common mistake to confuse the two as both are denoted by TA. However, it is the endowment assurance that shares the same random variable as the temporary annuity. It also makes sense when comparing the actuarial notation - both share the same subscript. Variance \u00b6 However, the second moment is slightly different as it must reflect the new interest rate used. The same logic can be applied for the Variance as well: \\[ \\begin{aligned} \\text{WLA}_{Due} &= \\frac{1-\\text{WL}}{d} \\\\ Var(\\text{WLA}_{Due}) &= Var \\left( \\frac{1-\\text{WL}}{d} \\right) \\\\ Var(\\text{WLA}_{Due}) &= \\frac{1}{d^2} var(\\text{WL}) \\\\ Var(\\text{WLA}_{Due}) &= \\frac{1}{d^2} \\left({}_{}^2 A_x - (A_x)^2 \\right) \\end{aligned} \\] Var(\\text{TA}_{Due}) &= \\frac{1}{d^2} \\left({}_{}^2 A_{x:\\enclose{actuarial}{n}} - (A_{x:\\enclose{actuarial}{n}})^2 \\right) WLDue and WL Immediate same variance but DWLDue and DWL immediate different variance because it is dependent on survival GA = Certain + Deferred Var (GA) = Var (Deferred) Problem is Var(Deferred) Immediate and Due \u00b6 Recall that for Certain Annuities, the only difference between Due and Immediate ones are the payments at the start and end times. This relationship can be extended for Life Annuities as well. For WL annuities, since the end point is infinity , the difference in the end points can be ignored. Thus, the only difference is the first payment of 1, which can be easily accounted for: \\[ a_x = \\ddot{a}_x - 1 \\] Note This section uses \\(a_x\\) as the focus, as it is assumed that if needed, the due versions will be calculated and then converted to immediate , rather than calculating immediate annuities directly. Unfortunately, the difference at the end cannot be ignored for TA annuities: \\[ a_{x:\\enclose{actuarial}{n}} = \\ddot{a}_{x:\\enclose{actuarial}{n}} - 1 + {}_{n}E_x \\] There is no need to memorize this expression as it can be easily derived using the WL conversion: \\[ \\begin{aligned} a_{x:\\enclose{actuarial}{n}} &= a_{x} - {}_{n}E_x a_{x+n} \\\\ &= \\ddot{a}_{x} - 1 - {}_{n}E_x (\\ddot{a}_{x+n} - 1) \\\\ &= \\ddot{a}_{x} - 1 - {}_{n}E_x \\cdot \\ddot{a}_{x+n} + {}_{n}E_x \\\\ &= \\ddot{a}_{x} - {}_{n}E_x \\cdot \\ddot{a}_{x+n} - 1 + {}_{n}E_x \\\\ &= \\ddot{a}_{x:\\enclose{actuarial}{n}} - 1 + {}_{n}E_x \\end{aligned} \\] For variance, the random variables (which are certain annuities) are used instead: \\[ \\begin{aligned} Var(\\text{WL}_{Immediate}) &= Var(a_{\\enclose{actuarial}{k}}) \\\\ &= Var(\\ddot{a}_{\\enclose{actuarial}{k+1}}-1) \\\\ &= Var(\\ddot{a}_{\\enclose{actuarial}{k+1}}) \\\\ &= Var(\\text{WL}_{Due}) \\\\ \\\\ Var(\\text{TA}_{Immediate}) &= Var(a_{\\enclose{actuarial}{min(k,n)}}) \\\\ &= Var(\\ddot{a}_{\\enclose{actuarial}{min(k+1,n+1)}}-1) \\\\ &= Var(\\ddot{a}_{\\enclose{actuarial}{min(k+1,n+1)}}) \\\\ &= Var \\left( \\frac{1-v^{min(k+1,n+1)}}{d} \\right) \\\\ &= \\frac{1}{d^2} Var(v^{min(k+1,n+1)}) \\\\ &= \\frac{1}{d^2} \\left({}_{2}A_{x:\\enclose{actuarial}{n+1}} - (A_{x:\\enclose{actuarial}{n+1}})^2 \\right) \\end{aligned} \\] Thus, the variance of a WL annuity is the same for both Immediate and Due but NOT for a TA annuity. Recursions \u00b6 Following the same logic as assurances, Annuities can also be recursively expressed as a function of itself: If the policyholder dies, they would have only received the benefit of 1 at the start of the year If the policyholder survives, they would receive the additional benefits of the future periods The benefit of 1 is received REGARDLESS of death or survival as it was paid at the start of the year \\[ \\begin{aligned} \\text{WL}_\\text{Due} &= \\begin{cases} 1,& q_x \\\\ 1 + v\\ddot{a}_{x+1} ,& p_x \\end{cases} \\\\ &= 1 + \\begin{cases} 0,& q_x \\\\ v\\ddot{a}_{x+1} ,& p_x \\end{cases} \\\\ \\\\ \\therefore \\ddot{a}_{x} &= 1 + vp_x\\ddot{a}_{x+1} \\end{aligned} \\] The same can be shown for TAs, but remember that the remaining duration of the policy must decrease as well: \\[ \\ddot{a}_{x:\\enclose{actuarial}{n}} = 1 + vp_x \\ddot{a}_{x+1:\\enclose{actuarial}{n-1}} \\] Intuitions \u00b6 Similar to assurances, several intuitions can be made about the EPV of various annuities to serve as a sort of \"sense check\" against the SULT provided to determine if the answer is in the right direction. Same Annuity \u00b6 Since annuities are all contingent on the survival of the policyholder, only one case needs to be considered. The probability of survival is a decreasing function with age. The benefits are less likely to be paid out to an older policyholder, resulting in smaller expected cashflows . Since the cashflows are discounted the same amount, an older policyholder will have a lower EPV than a younger one: \\[ \\ddot{a}_{x+n} \\lt \\ddot{a}_{x} \\] As shown previously, Annuity Dues are always smaller than Immediates as the cashflows occur earlier and are hence discounted less . \\[ a_x < \\ddot{a}_x \\] Naturally, all else equal, annuities with a lower interest rate are discounted less and thus have a higher EPV . Different Annuities \u00b6 Since annuities are all contingent on the survival of the policyholder, the age of the policyholder for comparison does not matter. TA has the smallest EPV as it can only pay for a maximum of \\(n\\) years, while WLs and GA can pay indefinitely. WL is always smaller than GA as its payments in the first \\(n\\) years are not guaranteed; the payments after that are identical. \\[ \\ddot{a}_{x:\\enclose{actuarial}{n}} < \\ddot{a}_x < \\ddot{a}_{\\bar{x:\\enclose{actuarial}{n}}} \\] Immediate VS Due \u00b6 Consider two temporary life annuity with a term of \\(n\\) years: Annuity Immediate issued at age \\(x\\) Annuity Due issued at age \\(x+1\\) Both have the same cashflows : However, both of them are valued at different times: Annuity Immediate valued at age \\(x\\) Annuity Due valued at age \\(x+1\\) Thus, the PV of the cashflows are NOT the same : Thus, although they have the same cashflows, the annuity due has a larger EPV : \\[ \\begin{aligned} a_{x:\\enclose{actuarial}{n}} &= \\sum^n_{j=1} v^j {}_{j}p_{x} \\\\ \\\\ \\ddot{a}_{x+1:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{j=0} v^j {}_{j}p_{x+1} \\\\ &= \\frac{1}{vp_{x}} \\sum^{n-1}_{j=0} v^{j+1} {}_{j}p_{x+1} p_{x} \\\\ &= \\frac{1}{vp_{x}} \\sum^{n-1}_{j=0} v^{j+1} {}_{j+1}p_{x} \\\\ &= \\frac{1}{vp_{x}} \\sum^{n}_{j=1} v^{j} {}_{j}p_{x} \\\\ &= \\underbrace{\\frac{1}{vp_{x}}}_{>1} \\cdot a_{x:\\enclose{actuarial}{n}} \\\\ \\\\ \\therefore \\ddot{a}_{x+1:\\enclose{actuarial}{n}} &> a_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] This approach might seem long winded, as it seems that it is sufficient to simply compare the cashflows of \\(1 > v^n\\) . However, that ignores the probabilities , which is properly accounted for in the above approach. Probabilities and Percentiles \u00b6 Similar to assurances, apart from just calculating the Expectation and Variance, the probabilities and hence percentiles of the contract benefits can be calculated as well. \\[ \\begin{aligned} \\text{WL}_\\text{Due} &\\le u \\\\ \\frac{1-v^{k+1}}{d} &\\le u \\\\ 1-v^{k+1} &\\le ud \\\\ v^{k+1} &\\ge 1-ud \\\\ (k+1) \\ln v &\\ge \\ln(1-ud) \\\\ k+1 &\\le \\frac{\\ln(1-ud)}{\\ln v} \\\\ k &\\le \\frac{\\ln(1-ud)}{\\ln v} - 1 \\\\ \\end{aligned} \\] The common mistake of not flipping the inequality sign is relevant in annuities as well. To avoid this error, it is advised to plot a graph to remember that \\(k\\) should be smaller than the calculated value: The RHS of the expression is unlikely to be an integer. However, \\(k\\) can only take integer values. Thus, the values can rounded DOWN to the nearest whole number (EG. 4): \\[ \\begin{aligned} P(k \\le \\frac{\\ln(1-ud)}{\\ln v} - 1) &= P(k \\le 4) \\\\ &= P(T_x \\le 4) \\\\ &= {}_{4}p_x \\end{aligned} \\] The latter refers to calculating the percentile of the random variable , which is the smallest value of the RV that results in the specified probability. This process is the opposite of the previous one - it involves solving for \\(T_x\\) , converting it to \\(k\\) and then subsituting it back into the RV, which results in the associated percentile. Payable Continuously \u00b6 Similar to assurances, annuities can also be payable \\(m\\) times a year or payable continuously. Most of the key ideas carry over from the assurances, thus this section will mainly focus on the approximations from discrete annuities. However, something unique to annuities is that the conversion between due and immediates is slightly different because of the differences in cashflows: \\[ a^{(m)}_x = \\ddot{a}^{(m)}_x - \\frac{1}{m} \\] Uniform Distribution of Deaths \u00b6 Using the relationship between Assurances and Annuities, a continuous WL annuity can also be expressed in the form of a continuous assurance: \\[ \\begin{aligned} \\ddot{a}^{(m)}_x &= \\frac{1 - A^{(m)}_x}{d^{(m)}} \\end{aligned} \\] Assuming UDD, the continuous assurance and hence the overarching continuous annuity can be simplified: \\[ \\begin{aligned} \\ddot{a}^{(m)}_x &= \\frac{1 - A^{(m)}_x}{d^{(m)}} \\\\ &= \\frac{1 - \\frac{i}{i^{(m)}}A_x}{d^{(m)}} \\\\ &= \\frac{\\frac{i^{(m)}-iA_x}{i^{(m)}}}{d^{(m)}} \\\\ &= \\frac{i^{(m)}-iA_x}{i^{(m)}d^{(m)}} \\\\ &= \\frac{i^{(m)}-i(1-d\\ddot{a}_x)}{i^{(m)}d^{(m)}} \\\\ &= \\frac{i^{(m)}-i+id\\ddot{a}_x}{i^{(m)}d^{(m)}} \\\\ &= \\frac{id}{i^{(m)}d^{(m)}} \\ddot{a}_x - \\frac{i-i^{(m)}}{i^{(m)}d^{(m)}} \\\\ &= \\alpha(m)\\ddot{a}_x - \\beta(m) \\\\ \\\\ \\therefore \\alpha(m) &= \\frac{id}{i^{(m)}d^{(m)}} \\\\ \\\\ \\therefore \\beta(m) &= \\frac{i-i^{(m)}}{i^{(m)}d^{(m)}} \\end{aligned} \\] Parameter Calculation Recall that nominal and effective rates are linked using: \\[ \\begin{aligned} i^{(m)} &= m[(1+i)^\\frac{1}{m}-1] \\\\ d^{(m)} &= m[1-(1+d)^\\frac{1}{m}] \\end{aligned} \\] However, for \\(i=0.05\\) , \\(\\alpha\\) and \\(\\beta\\) are provided inside the SULT for common values of \\(m\\) , thus there is no need to waste precious time calculating them. An adjustment can be made for temporary annuities: \\[ \\begin{aligned} \\ddot{a}^{(m)}_{x:\\enclose{actuarial}{n}} &= \\ddot{a}^{(m)}_{x} - {}_{n}E_x \\ddot{a}^{(m)}_{x+n} \\\\ &= \\alpha(m)\\ddot{a}_x - \\beta(m) - {}_{n}E_x [\\alpha(m)\\ddot{a}_{x:\\enclose{actuarial}{n}} - \\beta(m)] \\\\ &= \\alpha(m)\\ddot{a}_x - \\beta(m) - {}_{n}E_x \\alpha(m)\\ddot{a}_{x:\\enclose{actuarial}{n}} - {}_{n}E_x \\beta(m) \\\\ &= \\alpha(m) [\\ddot{a}_x - {}_{n}E_x \\ddot{a}_{x:\\enclose{actuarial}{n}}] - \\beta(m) [1-{}_{n}E_x] \\\\ &= \\alpha(m) \\ddot{a}_{x:\\enclose{actuarial}{n}} - \\beta(m) [1-{}_{n}E_x] \\end{aligned} \\] Woolhouse Approximation \u00b6 Since a continuous annuity can be expressed as a sum of discrete pure endowments , the trapezoidal rule and euler-macluarin correction can be used to approximate its EPV. The key difference from before is understanding that the EPV of an annuity due is a LEFT riemann sum as there is a payment at time 0. \\[ \\begin{aligned} \\bar{a}_x &= \\int^{\\infty}_0 v^t {}_{t}p_x \\\\ & \\approx h[f(a) + f(a+h) + \\dots + f(b-h)] - \\frac{h}{2} [f(a)-f(b)] + \\frac{h^2}{12} [f'(a)-f'(b)] \\end{aligned} \\] The first derivatives can be found below: \\[ \\begin{aligned} f'(n) &= \\frac{d}{dt} (e^{-\\delta n} {}_{n}p_x) \\\\ &= {}_{n}p_x \\delta e^{-\\delta n} - e^{-\\delta n} {}_{n}p_x \\mu_{x+n} \\\\ &= e^{-\\delta n} {}_{n}p_x (\\delta + \\mu_{x+n}) \\\\ &= {}_{n}E_x (\\delta + \\mu_{x+n}) \\\\ \\\\ \\therefore f'(0) &= -(\\delta + \\mu_{x}) \\\\ \\therefore f'(\\infty) &= 0 \\end{aligned} \\] Assuming \\(h=1\\) , \\[ \\begin{aligned} \\bar{a}_x & \\approx (1) [{}_{0}E_x + {}_{1}E_x + \\dots + {}_{\\infty-1}E_x] - \\frac{1}{2} [{}_{0}E_x-{}_{\\infty-1}E_x] + \\frac{1}{12} [-(\\delta + \\mu_{x}) - 0] \\\\ & \\approx [1 + {}_{n}E_1 + \\dots] - \\frac{1}{2} [1-0] - \\frac{1}{12} (\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12} (\\delta + \\mu_{x}) \\end{aligned} \\] Assuming \\(h=\\frac{1}{m}\\) instead, \\[ \\begin{aligned} \\bar{a}_x & \\approx \\left(\\frac{1}{m} \\right) [{}_{0}E_x + {}_{\\frac{1}{m}}E_x + \\dots + {}_{\\infty-\\frac{1}{m}}E_x] - \\frac{1}{2m} [{}_{0}E_x-{}_{\\infty-\\frac{1}{m}}E_x] + \\frac{1}{12m^2} [-(\\delta + \\mu_{x}) - 0] \\\\ & \\approx \\left(\\frac{1}{m} \\right) [1 + {}_{\\frac{1}{m}}E_x + \\dots] - \\frac{1}{2m} [1-0] - \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}^{(m)}_x - \\frac{1}{2m} - \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\end{aligned} \\] Combining both together, \\[ \\begin{aligned} \\bar{a}_x &= \\bar{a}_x \\\\ \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12} (\\delta + \\mu_{x}) & \\approx \\ddot{a}^{(m)}_x - \\frac{1}{2m} - \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\\\ \\ddot{a}^{(m)}_x & \\approx \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12} (\\delta + \\mu_{x}) + \\frac{1}{2m} + \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}_x - (\\frac{1}{2}-\\frac{1}{2m}) - (\\frac{1}{12} - \\frac{1}{12m^2})(\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}_x - \\frac{m-1}{2m} - \\frac{m^2-1}{12m^2}(\\delta + \\mu_{x}) \\end{aligned} \\] This process is known as the Woolhouse Approximation . If the error term is omitted, then it is known as the 2-term Woolhouse Approximation else it is known as the 3-term Woolhouse Approximation . The 3-term woolhouse approximation is extremely accurate due to its mathematical roots. It so much more accurate than UDD such that when calculating a continuous assurance , it is advisable to calculate the corresponding annuity using the 3 term approximation first and then convert it, for the best results. Unfortunately, the drawback is that it requires knowledge about the force of mortality, which is not commonly provided in life tables. Thus, although less accurate, the 2 term approach can be easily used in any situation. Term Approximations \u00b6 Unlike assurances, the approximations for annuities have multiple components. Thus, the approximations for term annuities are slightly different from the ones used for WL. However, there is NO need to remember a seperate expression for it, it can be calculated by converting the WL to a TA: \\[ \\begin{aligned} a^{(m)}_{x:\\enclose{actuarial}{n}} &= a^{(m)}_{x} - {}_{n}E_{x} a^{(m)}_{x+n} \\\\ &= \\alpha(m) \\ddot{a}_{x} - \\beta(m) - {}_{n}E_{x} (\\alpha(m) \\ddot{a}_{x+n} - \\beta(m)) \\\\ &= \\alpha(m) \\ddot{a}_{x} - \\beta(m) - {}_{n}E_{x} \\alpha(m) \\ddot{a}_{x+n} + {}_{n}E_{x} \\beta(m) \\\\ &= \\alpha(m) (\\ddot{a}_{x} - {}_{n}E_{x} \\ddot{a}_{x+n}) - \\beta(m) (1-{}_{n}E_{x}) \\\\ &= \\alpha(m) (\\ddot{a}_{x+n}) - \\beta(m) (1-{}_{n}E_{x}) \\end{aligned} \\] A similar approach can be taken for the woolhouse approximation, resulting in the following: \\[ a^{(m)}_{x:\\enclose{actuarial}{n}} = \\ddot{a}_{x+n} - \\frac{m-1}{2m} (1-{}_{n}E_{x}) \\] For simplicity, the three term approach is not shown.","title":"Life Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#life-annuities","text":"","title":"Life Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#overview","text":"Life Annuity contracts promise to pay out a stream of benefits in the future for as long the policyholder remains alive. Similar to life insurance, the benefits can be paid discretely or continuously and can be valued through their EPV. Note that these are different from the annuities covered in Exam FM. The payments for those annuities are made regardless of the life of the policyholder, known as Annuity Certain . They serve as the foundation to understanding Life Annuities.","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#review-annuities-certain","text":"There are two types of payment structures: Period Start Period End Paid in Advance Paid in Arrears Annuity Due Annuity Immediate \\(\\ddot{a}_{\\enclose{actuarial}{n}}\\) \\(a_{\\enclose{actuarial}{n}}\\) The overall PV of the annuity is the sum of the PV of the stream of payments: \\[ \\begin{aligned} a_{\\enclose{actuarial}{n}} &= v + v^2 + v^3 + \\dots + v^n \\\\ &= \\frac{v-v^{n+1}}{1-v} \\\\ &= \\sum^n_{k=1} v^k \\\\ &= \\frac{v(1-v^n)}{1-v} \\\\ &= \\frac{1-v^n}{i} \\\\ \\\\ \\ddot{a}_{\\enclose{actuarial}{n}} &= 1 + v + v^2 + \\dots + v^n \\\\ &= \\sum^{n-1}_{k=0} v^k \\\\ &= \\frac{1- v^{n}}{1-v} \\\\ &= \\frac{1- v^{n}}{d} \\end{aligned} \\] Notice that the payments simply differ by one period and hence one discounting factor: \\[ \\begin{aligned} \\ddot{a}_{\\enclose{actuarial}{n}} &= 1 + v + v^2 + v^3 + \\dots + v^n \\\\ &= (1+i) (v + v^2 + v^3 + \\dots + v^n) \\\\ &= (1+i) \\sum^{n}_{k=1} v^k \\\\ &= (1+i) a_{\\enclose{actuarial}{n}} \\end{aligned} \\] From another perspective, only the payments at the end points \\(t=0\\) and \\(t=n\\) are different: \\[ \\begin{aligned} \\ddot{a}_{\\enclose{actuarial}{n}} &= \\sum^{n-1}_0 v^k \\\\ &= v^0 + \\sum^{n-1}_{k=1} v^k \\\\ &= 1 + a_{\\enclose{actuarial}{n-1}} \\\\ &= 1 + a_{\\enclose{actuarial}{n}} - v^n \\end{aligned} \\]","title":"Review: Annuities Certain"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#payable-discretely","text":"If the contract pays out benefits discretely, then \\(k\\) is used as the survival model. Since a life annuity pays out a stream of benefits, the PV of the contract at every time period can be slightly confusing. It is important to remember the following: The PV is calculated with respect to time 0 The PV is the sum of the PV of all the payments the individual is expected to live \\[ \\begin{aligned} PV &= \\begin{cases} a_{\\enclose{actuarial}{K_x}}, & \\text{Life Annuity Immediate} \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x + 1}}, & \\text{Life Annuity Due} \\end{cases} \\end{aligned} \\] Thus, the EPV of the contract is the sum product of the PV of the benefit in each period and the probability of death in that period: \\[ \\begin{aligned} EPV &= \\begin{cases} \\text{Life Annuity Immediate} &=\\ \\sum a_{\\enclose{actuarial}{k}} \\cdot {}_{k|}q_{x} \\\\ \\text{Life Annuity Due} &= \\sum \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} \\end{cases} \\end{aligned} \\] For the remainder of the section, to be concise, only the formulas for Annuity Due will be given. The corresponding formulas for Annuity Immediates can be easily calculated from it.","title":"Payable Discretely"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#actuarial-notation","text":"Most of the notation for Life Insurance applies to Life Annuities as well. The key difference is that \\({}^{k}a\\) is used to represent the k-th moment of the present value of a contract where a benefit of 1 is paid discretely for as long as the status does NOT fail . \\(A\\) stands for Assurance while \\(a\\) stands for Annuity.","title":"Actuarial Notation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#whole-life-annuity","text":"Whole Life Annuities covers the insured indefinitely and thus will pay out for as long as the insured survives . Let WL be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{WL}_{\\text{Due}} &=\\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\end{aligned} \\] Thus, the EPV is the Expectation of the WLA random variable: \\[ \\begin{aligned} E(\\text{WLA}_{\\text{Due}}) &= \\sum^{\\infty}_{k = 0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} \\\\ \\ddot{a}_{x} &= \\sum^{\\infty}_{k = 0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} \\end{aligned} \\] The EPV can be furthered simplified, allowing a life annuity to be viewed as the sum of a series of pure endowments : \\[ \\begin{aligned} \\ddot{a}_{x} &= \\sum^{\\infty}_{k = 0} \\ddot{a}_{\\enclose{actuarial}{k}} \\cdot {}_{k|}q_{x} \\\\ &=\\sum^{\\infty}_{k = 0} \\left(\\sum^{n-1}_{j=0} v^j \\right) \\cdot {}_{k|}q_{x} \\\\ &= v^0 \\cdot {}_{0|}q_{x} + (v^0 + v^1) \\cdot {}_{1|}q_{x} + (v^0 + v^1 + v^2) \\cdot {}_{2|}q_{x} + \\dots \\\\ &= v^0 ({}_{0|}q_{x} + {}_{1|}q_{x} + \\dots) + v^1 ({}_{1|}q_{x} + {}_{2|}q_{x} + \\dots) + v^2 ({}_{2|}q_{x} + {}_{3|}q_{x} + \\dots) + \\dots \\\\ &= \\sum^{\\infty}_{j=0} v^j \\cdot \\left(\\sum^{\\infty}_{k = j} {}_{k|}q_{x} \\right) \\\\ &= \\sum^{\\infty}_{j=0} v^j \\cdot {}_{j}p_{x} \\end{aligned} \\] Note that the above formula can be used to easily prove the relationships between different kinds of annuities, but is NOT a proper definition of an annuity - it hence cannot be adjusted to compute the variance. The second moment and variance for all life annuities will be covered in a later section.","title":"Whole Life Annuity"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#temporary-annuity","text":"Temporary Life Annuities covers the insured for a specified period \\(n\\) and thus pays out for as long as the insured survives during that period only . Let TA be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{TA}_\\text{Due} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{K_x+1}},& k = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{n}},& k \\ge n \\\\ \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{min(K_x + 1, n)}} \\end{aligned} \\] Thus, the EPV is the expectation of the TA random variable, which can be simplified using the same approach as before: \\[ \\begin{aligned} E(\\text{TA}_\\text{Due}) &= \\sum^{n-1}_{k = 0} \\ddot{a}_{\\enclose{actuarial}{k}} \\cdot {}_{k|}q_x + \\ddot{a}_{\\enclose{actuarial}{n}} \\cdot {}_{n}p_x \\\\ \\ddot{a}_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{j=0} v^j \\cdot {}_{j}p_{x} \\end{aligned} \\] Note the change in limits for the simplified approach - an annuity is the combination of pure endowments for as long as the annuity lasts . This gives rise to a very simple sense check - the EPV of an \\(n\\) year temporary annuity should NOT be larger than \\(n\\) as there are \\(n\\) payments of 1 which are discounted.","title":"Temporary Annuity"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#deferred-annuities","text":"Deferred Annuities are variations of the above contracts where the coverage is deferred by \\(n\\) years . While any contract can be deferred, the most useful is the Deferred Whole Life Annuity . Let DWL be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{DWL}_{\\text{Due}} &= \\begin{cases} 0, & k = 0, 1, 2, \\dots, n-1 \\\\ v^n \\cdot \\ddot{a}_{\\enclose{actuarial}{K_x+1-n}}, & k \\ge n \\end{cases} \\end{aligned} \\] Intuitively, the RV can also be expressed as the difference between two certain annuities: \\[ \\begin{aligned} \\text{DWL}_{\\text{Due}} &= v^n \\cdot \\ddot{a}_{\\enclose{actuarial}{k+1-n}} \\\\ &= v^n \\cdot \\frac{1-v^{k+1-n}}{d} \\\\ &= \\frac{v^{n}-v^{k+1}}{d} \\\\ &= \\frac{(1-v^{k+1}) - (1-v^{n})}{d} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{k+1}} -\\ddot{a}_{\\enclose{actuarial}{n}} \\\\ \\\\ \\therefore \\text{DWL}_{\\text{Due}} &= \\begin{cases} 0, & k = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{k+1}} -\\ddot{a}_{\\enclose{actuarial}{n}} , & k \\ge n \\end{cases} \\end{aligned} \\] Thus, the EPV is the expectation of the DWL random variable: \\[ \\begin{aligned} E(\\text{DWL}_{\\text{Due}}) &= \\sum^{\\infty}_{k=n} (\\ddot{a}_{\\enclose{actuarial}{k+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{k|}q_{x} \\\\ {}_{n|}\\ddot{a}_{x} &= \\sum^{\\infty}_{k=0} (\\ddot{a}_{\\enclose{actuarial}{k+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{k|}q_{x} - \\sum^{n-1}_{k=0} (\\ddot{a}_{\\enclose{actuarial}{k+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{k|}q_{x} \\\\ &= \\sum^{\\infty}_{k=0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} - \\sum^{\\infty}_{k=0} \\ddot{a}_{\\enclose{actuarial}{n}} \\cdot {}_{k|}q_{x} - \\sum^{n-1}_{k=0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} + \\sum^{n-1}_{k=0} \\ddot{a}_{\\enclose{actuarial}{n}} \\cdot {}_{k|}q_{x} \\\\ &= \\sum^{\\infty}_{k=0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} - \\sum^{n-1}_{k=0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} \\\\ &= \\ddot{a}_x - \\ddot{a}_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] Similar to before, the EPV can be shown to be a sum of pure endowments: \\[ {}_{n|}\\ddot{a}_{x} = \\sum^{\\infty}_{j=n} v^j \\cdot {}_{j}p_{x} \\] Since a DWL is effectively a WL that is issued \\(n\\) years later, the EPV of a DWL is the EPV of the WL, adjusted for both interest and survival: \\[ \\begin{aligned} {}_{n|}a_{x} &= \\sum^{\\infty}_{j=n} v^j \\cdot {}_{j}p_{x} \\\\ &= \\sum^{\\infty}_{j=0} v^{j+n} \\cdot {}_{j+n}p_{x} \\\\ &= \\sum^{\\infty}_{j=0} v^{j} v^{n} \\cdot {}_{n}p_{x} {}_{j}p_{x+n} \\\\ &= v^{n} \\cdot {}_{n}p_{x} \\sum^{\\infty}_{j=0} v^{j} \\cdot {}_{j}p_{x+n} \\\\ &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\end{aligned} \\] This allows a TA to be expressed as the difference of two WL annuities issued at different times : \\[ \\begin{aligned} {}_{n|}\\ddot{a}_{x} &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\\\ \\ddot{a}_x - \\ddot{a}_{x:\\enclose{actuarial}{n}} &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\\\ \\ddot{a}_{x:\\enclose{actuarial}{n}} &= \\ddot{a}_x - {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\end{aligned} \\]","title":"Deferred Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#guaranteed-annuities","text":"Guaranteed Life Annuities are whole life annuities with the benefits in the first \\(n\\) years being guaranteed - they will be paid out even if the insured dies during this period. Let GA be the random variable denoting the PV of the survival benefit. Since the payments are guaranteed, they can be represented using a certain annuity : \\[ \\begin{aligned} \\text{GA}_{\\text{Due}} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& k = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}},& k \\ge n \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{max(K_x+1,n)}} \\end{aligned} \\] The random variable can be manipulated to be easier to work with: \\[ \\begin{aligned} \\text{GA}_{\\text{Due}} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}},& K_x \\ge n \\end{cases} \\\\ &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}} + \\ddot{a}_{\\enclose{actuarial}{n}},& K_x \\ge n \\end{cases} \\\\ &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}} \\\\ \\ddot{a}_{\\enclose{actuarial}{n}} \\end{cases} + \\begin{cases} 0,& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}},& K_x \\ge n \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{n}} + \\begin{cases} 0,& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}},& K_x \\ge n \\end{cases} \\end{aligned} \\] Thus, the EPV can be more easily calculated: \\[ \\begin{aligned} E(\\text{GA}_{\\text{Due}}) &= \\ddot{a}_{\\enclose{actuarial}{n}} + 0 \\cdot {}_{n}q_x + \\sum^{\\infty}_{k = n} (\\ddot{a}_{\\enclose{actuarial}{k+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{k|}q_{x}\\\\ \\ddot{a}_{\\overline{x:\\enclose{actuarial}{n}}} &= \\ddot{a}_{\\enclose{actuarial}{n}} + {}_{n} \\ddot{a}_{x} \\end{aligned} \\] Note that a Guaranteed Annuity is denoted via an additional bar above the status.","title":"Guaranteed Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#assurances-and-annuities","text":"Consider the random variable for the PV Whole Life Assurance and Annuity Due. Notice that both RVs are related through the certain annuity formula: \\[ \\begin{aligned} \\text{WL} &= v^{K_x + 1} \\\\ \\text{WLA}_{Due} &= \\ddot{a}_{\\enclose{actuarial}{K_x+1}} = \\frac{1-v^{K_x + 1}}{d} \\end{aligned} \\] This is why Annuity Due, rather Annuity Immediates, are provided in the SULT. Thus, given the EPV of a life annuity, the EPV of a life assurance can be determined, vice-versa: \\[ \\begin{aligned} \\text{WLA} &= \\frac{1-\\text{WL}}{d} \\\\ E(\\text{WLA}_{Due}) &= \\frac{1-E(\\text{WL})}{d} \\\\ \\ddot{a}_x &= \\frac{1-A_x}{d} \\\\ d \\ddot{a}_x &= 1 - A_x \\\\ A_x &= 1 - d \\ddot{a}_x \\end{aligned} \\] The same relationship can be shown for a temporary annuity and an Endowment Assurance, NOT a term assurance : \\[ \\begin{aligned} A_{x:\\enclose{actuarial}{n}} &= 1 - d \\ddot{a}_{x:\\enclose{actuarial}{n}} \\\\ \\end{aligned} \\] It is a very common mistake to confuse the two as both are denoted by TA. However, it is the endowment assurance that shares the same random variable as the temporary annuity. It also makes sense when comparing the actuarial notation - both share the same subscript.","title":"Assurances and Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#variance","text":"However, the second moment is slightly different as it must reflect the new interest rate used. The same logic can be applied for the Variance as well: \\[ \\begin{aligned} \\text{WLA}_{Due} &= \\frac{1-\\text{WL}}{d} \\\\ Var(\\text{WLA}_{Due}) &= Var \\left( \\frac{1-\\text{WL}}{d} \\right) \\\\ Var(\\text{WLA}_{Due}) &= \\frac{1}{d^2} var(\\text{WL}) \\\\ Var(\\text{WLA}_{Due}) &= \\frac{1}{d^2} \\left({}_{}^2 A_x - (A_x)^2 \\right) \\end{aligned} \\] Var(\\text{TA}_{Due}) &= \\frac{1}{d^2} \\left({}_{}^2 A_{x:\\enclose{actuarial}{n}} - (A_{x:\\enclose{actuarial}{n}})^2 \\right) WLDue and WL Immediate same variance but DWLDue and DWL immediate different variance because it is dependent on survival GA = Certain + Deferred Var (GA) = Var (Deferred) Problem is Var(Deferred)","title":"Variance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#immediate-and-due","text":"Recall that for Certain Annuities, the only difference between Due and Immediate ones are the payments at the start and end times. This relationship can be extended for Life Annuities as well. For WL annuities, since the end point is infinity , the difference in the end points can be ignored. Thus, the only difference is the first payment of 1, which can be easily accounted for: \\[ a_x = \\ddot{a}_x - 1 \\] Note This section uses \\(a_x\\) as the focus, as it is assumed that if needed, the due versions will be calculated and then converted to immediate , rather than calculating immediate annuities directly. Unfortunately, the difference at the end cannot be ignored for TA annuities: \\[ a_{x:\\enclose{actuarial}{n}} = \\ddot{a}_{x:\\enclose{actuarial}{n}} - 1 + {}_{n}E_x \\] There is no need to memorize this expression as it can be easily derived using the WL conversion: \\[ \\begin{aligned} a_{x:\\enclose{actuarial}{n}} &= a_{x} - {}_{n}E_x a_{x+n} \\\\ &= \\ddot{a}_{x} - 1 - {}_{n}E_x (\\ddot{a}_{x+n} - 1) \\\\ &= \\ddot{a}_{x} - 1 - {}_{n}E_x \\cdot \\ddot{a}_{x+n} + {}_{n}E_x \\\\ &= \\ddot{a}_{x} - {}_{n}E_x \\cdot \\ddot{a}_{x+n} - 1 + {}_{n}E_x \\\\ &= \\ddot{a}_{x:\\enclose{actuarial}{n}} - 1 + {}_{n}E_x \\end{aligned} \\] For variance, the random variables (which are certain annuities) are used instead: \\[ \\begin{aligned} Var(\\text{WL}_{Immediate}) &= Var(a_{\\enclose{actuarial}{k}}) \\\\ &= Var(\\ddot{a}_{\\enclose{actuarial}{k+1}}-1) \\\\ &= Var(\\ddot{a}_{\\enclose{actuarial}{k+1}}) \\\\ &= Var(\\text{WL}_{Due}) \\\\ \\\\ Var(\\text{TA}_{Immediate}) &= Var(a_{\\enclose{actuarial}{min(k,n)}}) \\\\ &= Var(\\ddot{a}_{\\enclose{actuarial}{min(k+1,n+1)}}-1) \\\\ &= Var(\\ddot{a}_{\\enclose{actuarial}{min(k+1,n+1)}}) \\\\ &= Var \\left( \\frac{1-v^{min(k+1,n+1)}}{d} \\right) \\\\ &= \\frac{1}{d^2} Var(v^{min(k+1,n+1)}) \\\\ &= \\frac{1}{d^2} \\left({}_{2}A_{x:\\enclose{actuarial}{n+1}} - (A_{x:\\enclose{actuarial}{n+1}})^2 \\right) \\end{aligned} \\] Thus, the variance of a WL annuity is the same for both Immediate and Due but NOT for a TA annuity.","title":"Immediate and Due"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#recursions","text":"Following the same logic as assurances, Annuities can also be recursively expressed as a function of itself: If the policyholder dies, they would have only received the benefit of 1 at the start of the year If the policyholder survives, they would receive the additional benefits of the future periods The benefit of 1 is received REGARDLESS of death or survival as it was paid at the start of the year \\[ \\begin{aligned} \\text{WL}_\\text{Due} &= \\begin{cases} 1,& q_x \\\\ 1 + v\\ddot{a}_{x+1} ,& p_x \\end{cases} \\\\ &= 1 + \\begin{cases} 0,& q_x \\\\ v\\ddot{a}_{x+1} ,& p_x \\end{cases} \\\\ \\\\ \\therefore \\ddot{a}_{x} &= 1 + vp_x\\ddot{a}_{x+1} \\end{aligned} \\] The same can be shown for TAs, but remember that the remaining duration of the policy must decrease as well: \\[ \\ddot{a}_{x:\\enclose{actuarial}{n}} = 1 + vp_x \\ddot{a}_{x+1:\\enclose{actuarial}{n-1}} \\]","title":"Recursions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#intuitions","text":"Similar to assurances, several intuitions can be made about the EPV of various annuities to serve as a sort of \"sense check\" against the SULT provided to determine if the answer is in the right direction.","title":"Intuitions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#same-annuity","text":"Since annuities are all contingent on the survival of the policyholder, only one case needs to be considered. The probability of survival is a decreasing function with age. The benefits are less likely to be paid out to an older policyholder, resulting in smaller expected cashflows . Since the cashflows are discounted the same amount, an older policyholder will have a lower EPV than a younger one: \\[ \\ddot{a}_{x+n} \\lt \\ddot{a}_{x} \\] As shown previously, Annuity Dues are always smaller than Immediates as the cashflows occur earlier and are hence discounted less . \\[ a_x < \\ddot{a}_x \\] Naturally, all else equal, annuities with a lower interest rate are discounted less and thus have a higher EPV .","title":"Same Annuity"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#different-annuities","text":"Since annuities are all contingent on the survival of the policyholder, the age of the policyholder for comparison does not matter. TA has the smallest EPV as it can only pay for a maximum of \\(n\\) years, while WLs and GA can pay indefinitely. WL is always smaller than GA as its payments in the first \\(n\\) years are not guaranteed; the payments after that are identical. \\[ \\ddot{a}_{x:\\enclose{actuarial}{n}} < \\ddot{a}_x < \\ddot{a}_{\\bar{x:\\enclose{actuarial}{n}}} \\]","title":"Different Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#immediate-vs-due","text":"Consider two temporary life annuity with a term of \\(n\\) years: Annuity Immediate issued at age \\(x\\) Annuity Due issued at age \\(x+1\\) Both have the same cashflows : However, both of them are valued at different times: Annuity Immediate valued at age \\(x\\) Annuity Due valued at age \\(x+1\\) Thus, the PV of the cashflows are NOT the same : Thus, although they have the same cashflows, the annuity due has a larger EPV : \\[ \\begin{aligned} a_{x:\\enclose{actuarial}{n}} &= \\sum^n_{j=1} v^j {}_{j}p_{x} \\\\ \\\\ \\ddot{a}_{x+1:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{j=0} v^j {}_{j}p_{x+1} \\\\ &= \\frac{1}{vp_{x}} \\sum^{n-1}_{j=0} v^{j+1} {}_{j}p_{x+1} p_{x} \\\\ &= \\frac{1}{vp_{x}} \\sum^{n-1}_{j=0} v^{j+1} {}_{j+1}p_{x} \\\\ &= \\frac{1}{vp_{x}} \\sum^{n}_{j=1} v^{j} {}_{j}p_{x} \\\\ &= \\underbrace{\\frac{1}{vp_{x}}}_{>1} \\cdot a_{x:\\enclose{actuarial}{n}} \\\\ \\\\ \\therefore \\ddot{a}_{x+1:\\enclose{actuarial}{n}} &> a_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] This approach might seem long winded, as it seems that it is sufficient to simply compare the cashflows of \\(1 > v^n\\) . However, that ignores the probabilities , which is properly accounted for in the above approach.","title":"Immediate VS Due"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#probabilities-and-percentiles","text":"Similar to assurances, apart from just calculating the Expectation and Variance, the probabilities and hence percentiles of the contract benefits can be calculated as well. \\[ \\begin{aligned} \\text{WL}_\\text{Due} &\\le u \\\\ \\frac{1-v^{k+1}}{d} &\\le u \\\\ 1-v^{k+1} &\\le ud \\\\ v^{k+1} &\\ge 1-ud \\\\ (k+1) \\ln v &\\ge \\ln(1-ud) \\\\ k+1 &\\le \\frac{\\ln(1-ud)}{\\ln v} \\\\ k &\\le \\frac{\\ln(1-ud)}{\\ln v} - 1 \\\\ \\end{aligned} \\] The common mistake of not flipping the inequality sign is relevant in annuities as well. To avoid this error, it is advised to plot a graph to remember that \\(k\\) should be smaller than the calculated value: The RHS of the expression is unlikely to be an integer. However, \\(k\\) can only take integer values. Thus, the values can rounded DOWN to the nearest whole number (EG. 4): \\[ \\begin{aligned} P(k \\le \\frac{\\ln(1-ud)}{\\ln v} - 1) &= P(k \\le 4) \\\\ &= P(T_x \\le 4) \\\\ &= {}_{4}p_x \\end{aligned} \\] The latter refers to calculating the percentile of the random variable , which is the smallest value of the RV that results in the specified probability. This process is the opposite of the previous one - it involves solving for \\(T_x\\) , converting it to \\(k\\) and then subsituting it back into the RV, which results in the associated percentile.","title":"Probabilities and Percentiles"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#payable-continuously","text":"Similar to assurances, annuities can also be payable \\(m\\) times a year or payable continuously. Most of the key ideas carry over from the assurances, thus this section will mainly focus on the approximations from discrete annuities. However, something unique to annuities is that the conversion between due and immediates is slightly different because of the differences in cashflows: \\[ a^{(m)}_x = \\ddot{a}^{(m)}_x - \\frac{1}{m} \\]","title":"Payable Continuously"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#uniform-distribution-of-deaths","text":"Using the relationship between Assurances and Annuities, a continuous WL annuity can also be expressed in the form of a continuous assurance: \\[ \\begin{aligned} \\ddot{a}^{(m)}_x &= \\frac{1 - A^{(m)}_x}{d^{(m)}} \\end{aligned} \\] Assuming UDD, the continuous assurance and hence the overarching continuous annuity can be simplified: \\[ \\begin{aligned} \\ddot{a}^{(m)}_x &= \\frac{1 - A^{(m)}_x}{d^{(m)}} \\\\ &= \\frac{1 - \\frac{i}{i^{(m)}}A_x}{d^{(m)}} \\\\ &= \\frac{\\frac{i^{(m)}-iA_x}{i^{(m)}}}{d^{(m)}} \\\\ &= \\frac{i^{(m)}-iA_x}{i^{(m)}d^{(m)}} \\\\ &= \\frac{i^{(m)}-i(1-d\\ddot{a}_x)}{i^{(m)}d^{(m)}} \\\\ &= \\frac{i^{(m)}-i+id\\ddot{a}_x}{i^{(m)}d^{(m)}} \\\\ &= \\frac{id}{i^{(m)}d^{(m)}} \\ddot{a}_x - \\frac{i-i^{(m)}}{i^{(m)}d^{(m)}} \\\\ &= \\alpha(m)\\ddot{a}_x - \\beta(m) \\\\ \\\\ \\therefore \\alpha(m) &= \\frac{id}{i^{(m)}d^{(m)}} \\\\ \\\\ \\therefore \\beta(m) &= \\frac{i-i^{(m)}}{i^{(m)}d^{(m)}} \\end{aligned} \\] Parameter Calculation Recall that nominal and effective rates are linked using: \\[ \\begin{aligned} i^{(m)} &= m[(1+i)^\\frac{1}{m}-1] \\\\ d^{(m)} &= m[1-(1+d)^\\frac{1}{m}] \\end{aligned} \\] However, for \\(i=0.05\\) , \\(\\alpha\\) and \\(\\beta\\) are provided inside the SULT for common values of \\(m\\) , thus there is no need to waste precious time calculating them. An adjustment can be made for temporary annuities: \\[ \\begin{aligned} \\ddot{a}^{(m)}_{x:\\enclose{actuarial}{n}} &= \\ddot{a}^{(m)}_{x} - {}_{n}E_x \\ddot{a}^{(m)}_{x+n} \\\\ &= \\alpha(m)\\ddot{a}_x - \\beta(m) - {}_{n}E_x [\\alpha(m)\\ddot{a}_{x:\\enclose{actuarial}{n}} - \\beta(m)] \\\\ &= \\alpha(m)\\ddot{a}_x - \\beta(m) - {}_{n}E_x \\alpha(m)\\ddot{a}_{x:\\enclose{actuarial}{n}} - {}_{n}E_x \\beta(m) \\\\ &= \\alpha(m) [\\ddot{a}_x - {}_{n}E_x \\ddot{a}_{x:\\enclose{actuarial}{n}}] - \\beta(m) [1-{}_{n}E_x] \\\\ &= \\alpha(m) \\ddot{a}_{x:\\enclose{actuarial}{n}} - \\beta(m) [1-{}_{n}E_x] \\end{aligned} \\]","title":"Uniform Distribution of Deaths"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#woolhouse-approximation","text":"Since a continuous annuity can be expressed as a sum of discrete pure endowments , the trapezoidal rule and euler-macluarin correction can be used to approximate its EPV. The key difference from before is understanding that the EPV of an annuity due is a LEFT riemann sum as there is a payment at time 0. \\[ \\begin{aligned} \\bar{a}_x &= \\int^{\\infty}_0 v^t {}_{t}p_x \\\\ & \\approx h[f(a) + f(a+h) + \\dots + f(b-h)] - \\frac{h}{2} [f(a)-f(b)] + \\frac{h^2}{12} [f'(a)-f'(b)] \\end{aligned} \\] The first derivatives can be found below: \\[ \\begin{aligned} f'(n) &= \\frac{d}{dt} (e^{-\\delta n} {}_{n}p_x) \\\\ &= {}_{n}p_x \\delta e^{-\\delta n} - e^{-\\delta n} {}_{n}p_x \\mu_{x+n} \\\\ &= e^{-\\delta n} {}_{n}p_x (\\delta + \\mu_{x+n}) \\\\ &= {}_{n}E_x (\\delta + \\mu_{x+n}) \\\\ \\\\ \\therefore f'(0) &= -(\\delta + \\mu_{x}) \\\\ \\therefore f'(\\infty) &= 0 \\end{aligned} \\] Assuming \\(h=1\\) , \\[ \\begin{aligned} \\bar{a}_x & \\approx (1) [{}_{0}E_x + {}_{1}E_x + \\dots + {}_{\\infty-1}E_x] - \\frac{1}{2} [{}_{0}E_x-{}_{\\infty-1}E_x] + \\frac{1}{12} [-(\\delta + \\mu_{x}) - 0] \\\\ & \\approx [1 + {}_{n}E_1 + \\dots] - \\frac{1}{2} [1-0] - \\frac{1}{12} (\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12} (\\delta + \\mu_{x}) \\end{aligned} \\] Assuming \\(h=\\frac{1}{m}\\) instead, \\[ \\begin{aligned} \\bar{a}_x & \\approx \\left(\\frac{1}{m} \\right) [{}_{0}E_x + {}_{\\frac{1}{m}}E_x + \\dots + {}_{\\infty-\\frac{1}{m}}E_x] - \\frac{1}{2m} [{}_{0}E_x-{}_{\\infty-\\frac{1}{m}}E_x] + \\frac{1}{12m^2} [-(\\delta + \\mu_{x}) - 0] \\\\ & \\approx \\left(\\frac{1}{m} \\right) [1 + {}_{\\frac{1}{m}}E_x + \\dots] - \\frac{1}{2m} [1-0] - \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}^{(m)}_x - \\frac{1}{2m} - \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\end{aligned} \\] Combining both together, \\[ \\begin{aligned} \\bar{a}_x &= \\bar{a}_x \\\\ \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12} (\\delta + \\mu_{x}) & \\approx \\ddot{a}^{(m)}_x - \\frac{1}{2m} - \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\\\ \\ddot{a}^{(m)}_x & \\approx \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12} (\\delta + \\mu_{x}) + \\frac{1}{2m} + \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}_x - (\\frac{1}{2}-\\frac{1}{2m}) - (\\frac{1}{12} - \\frac{1}{12m^2})(\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}_x - \\frac{m-1}{2m} - \\frac{m^2-1}{12m^2}(\\delta + \\mu_{x}) \\end{aligned} \\] This process is known as the Woolhouse Approximation . If the error term is omitted, then it is known as the 2-term Woolhouse Approximation else it is known as the 3-term Woolhouse Approximation . The 3-term woolhouse approximation is extremely accurate due to its mathematical roots. It so much more accurate than UDD such that when calculating a continuous assurance , it is advisable to calculate the corresponding annuity using the 3 term approximation first and then convert it, for the best results. Unfortunately, the drawback is that it requires knowledge about the force of mortality, which is not commonly provided in life tables. Thus, although less accurate, the 2 term approach can be easily used in any situation.","title":"Woolhouse Approximation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#term-approximations","text":"Unlike assurances, the approximations for annuities have multiple components. Thus, the approximations for term annuities are slightly different from the ones used for WL. However, there is NO need to remember a seperate expression for it, it can be calculated by converting the WL to a TA: \\[ \\begin{aligned} a^{(m)}_{x:\\enclose{actuarial}{n}} &= a^{(m)}_{x} - {}_{n}E_{x} a^{(m)}_{x+n} \\\\ &= \\alpha(m) \\ddot{a}_{x} - \\beta(m) - {}_{n}E_{x} (\\alpha(m) \\ddot{a}_{x+n} - \\beta(m)) \\\\ &= \\alpha(m) \\ddot{a}_{x} - \\beta(m) - {}_{n}E_{x} \\alpha(m) \\ddot{a}_{x+n} + {}_{n}E_{x} \\beta(m) \\\\ &= \\alpha(m) (\\ddot{a}_{x} - {}_{n}E_{x} \\ddot{a}_{x+n}) - \\beta(m) (1-{}_{n}E_{x}) \\\\ &= \\alpha(m) (\\ddot{a}_{x+n}) - \\beta(m) (1-{}_{n}E_{x}) \\end{aligned} \\] A similar approach can be taken for the woolhouse approximation, resulting in the following: \\[ a^{(m)}_{x:\\enclose{actuarial}{n}} = \\ddot{a}_{x+n} - \\frac{m-1}{2m} (1-{}_{n}E_{x}) \\] For simplicity, the three term approach is not shown.","title":"Term Approximations"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/","text":"Variable Benefits \u00b6 Overview \u00b6 Previously, the benefit of the assurances were assumed to be fixed at 1 for simplicity. In practice, it is common to have assurances where the benefits change over time . Geometric Contracts \u00b6 If the benefits change by a common factor each period, then it is known as a Geometric contract. The benefit of the policy starts at 1 in the first year and changes by \\((1+b)\\) each period: If \\(b>0\\) , then the benefits are increasing If \\(b<0\\) , then the benefits are decreasing \\[ \\text{Geometric Benefit} = (1+b)^{K_x} \\] Note that the power is \\(K_x\\) and NOT \\(K_x+1\\) to reflect that the benefit starts at 1 when \\(K_x=0\\) . Geometric Assurance \u00b6 Let \\(\\text{Geom WL}\\) be the random variable denoting the PV of a Geometric Assurance. \\[ \\text{Geom WL} = (1+b)^{K_x} v^{K_x+1} \\] Thus, the EPV of a WL Geometric Assurance can be calculated as: \\[ \\begin{aligned} \\text{EPV Geom WL} &= \\sum^{\\infty}_{k=0} (1+b)^{k} v^{k+1} {}_{k|}q_x \\\\ &= (1+b)^{-1} \\sum^{\\infty}_{k=0} (1+b)^{k+1} v^{k+1} {}_{k|}q_x \\\\ &= \\frac{1}{1+b} \\sum^{\\infty}_{k=0} \\left(\\frac{1+b}{1+i}\\right)^{k+1} {}_{k|}q_x \\\\ &= \\frac{1}{1+b} \\sum^{\\infty}_{k=0} (v')^{k+1} {}_{k|}q_x \\\\ &= \\frac{1}{1+b} A_x |_{i=\\frac{1+i}{1+b}-1} \\end{aligned} \\] Note that this means that the term \\(A_x |_{i=\\frac{1+i}{1+b}-1}\\) by itself is a Geometric Assurance that starts with payments of \\((1+b)\\) , which is why \\(\\frac{1}{1+b}\\) is required to scale it down by one factor such that it starts at one. In practice, most questions will provide a value of \\(b\\) such that \\((i=\\frac{1+i}{1+b}-1)\\) simplifies to a nice percentage . However, this usually means that the value of \\(b\\) provided is some complicated number, so do not be taken aback! The expression follows similar intuition to the second moment - it is simply a regular EPV expression evaluated at a different interest rate . Unfortunately, there is no specified actuarial notation for this niche case. One common approach is to denote it using \\(A_x^i\\) . Geometric Annuity \u00b6 Let \\(\\text{EPV Geom WL}_\\text{Due}\\) be the random variable denoting the PV of a Geometric Annuity Due. Thus, the EPV of a Geometric Annuity Due can be calculated as: \\[ \\begin{aligned} \\text{EPV Geom WL}_\\text{Due} &= \\sum^{\\infty}_{k=0} (1+c)^k v^k {}_{k}p_x \\\\ &= \\sum^{\\infty}_{k=0} \\left(\\frac{1+c}{1+i}\\right)^k {}_{k}p_x \\\\ &= \\ddot{a}_x|_{i = \\frac{1+i}{1+c}-1} \\end{aligned} \\] Continuous Geometric \u00b6 The EPV of continuously payable geometric contracts can be calculated using the usual UDD, Claims Acceleration or Woolhouse Approximations. However, despite having a \"new\" interest rate, the original interest rate is used in the continuous approximation parameters. Warning This is different from the second moment where the new interest rate must be reflected in the approximation as well. Take note of the different treatments. Arithmetic Contracts \u00b6 If the benefits change by a fixed constant each period, then it is instead known as an Arithmetic contract. For simplicity, the change each period is assumed to be 1: Arithmetically Increasing - Increases by 1 each period Arithmetically Decreasing - Decreased by 1 each period Since a WL contract lasts forever, the benefits of a decreasing WL contract would inevitably become negative , which do not make sense. Thus, WLs can only be arithmetically increasing contracts . \\[ \\text{Arithmetically Increasing Benefit} = K_x + 1 \\] Arithmetic Assurance \u00b6 Let \\(\\text{Arith WL}\\) be the random variable denoting the PV of an arithmetically increasing WL Assurance: \\[ \\text{Arith WL} = (K_x + 1) v^{K_x+1} \\] Thus, the EPV of a WL Arithmetic Assurance can be shown to be: \\[ \\begin{aligned} \\text{EPV Arith WL} &= \\sum^{\\infty}_{k=0} (k+1) v^{k+1} {}_{k|}q_x \\\\ (IA)_x &= \\sum^{\\infty}_{k=0} (k+1) v^{k+1} {}_{k|}q_x \\end{aligned} \\] \\((IA)_x\\) is the actuarial notation for the EPV of a contract with arithmetically increasing benefits starting at 1 and increasing by 1 each period. It can also be shown to be a sum of deferred WLs , each deferred by one period from the previous. This creates a step-like benefit which creates the increasing effect of the assurance: \\[ \\begin{aligned} (IA)_x &= {}_{0|}A_x + {}_{1|}A_x + {}_{2|}A_x + ... \\\\ &= \\sum^{\\infty}_{K_x = 0} {}_{K_x|}A_x \\end{aligned} \\] In practice, the magnitude of the change is unlikely to be one. Thus, the EPV must be scaled to match the actual change. This can be problematic if the starting benefit is NOT the same as the change . Consider a product with benefit \\(B\\) that increases by \\(k\\) each year. Due to the definition of the IWL variable, the starting benefit and change each period are equal - we cannot simply multiply \\(B\\) or \\(k\\) to the IWL EPV. Thus, we must split this product into a fixed component with \\(B-k\\) benefits and a variable component that starts and increases by \\(k\\) each period. The sum of these two components results in a benefit of \\(B, B+k, B+2k, \\dots\\) Arithmetic Annuities \u00b6 Let \\(\\text{Arith WL}_\\text{Due}\\) be the random variable denoting the PV of an arithmetically increasing annuity. The PV can be denoted by an arithmetically increasing annuity certain : \\[ \\begin{aligned} \\text{Arith WL}_\\text{Due} &= (I\\ddot{a})_{\\enclose{actuarial}{n}} \\\\ &= \\sum^{n-1}_{k=0} v^k \\cdot \\ddot{a}_{\\enclose{actuarial}{n-k}} \\\\ &= \\sum^{n-1}_{k=0} v^k \\cdot \\frac{1-v^{n-k}}{d} \\\\ &= \\sum^{n-1}_{k=0} \\frac{v^k - v^n}{d} \\\\ &= \\frac{1}{d} \\left(\\sum^{n-1}_{k=0} v^k - \\sum^{n-1}_{k=0} v^n \\right) \\\\ &= \\frac{1}{d} (\\ddot{a}_{\\enclose{actuarial}{n}} - nv^n) \\\\ &= \\frac{\\ddot{a}_{\\enclose{actuarial}{n}} - nv^n}{d} \\end{aligned} \\] Similarly, the EPV of an arithmetically increasing WL annuity Due can be calculated as: \\[ \\begin{aligned} \\text{EPV Arith WL}_\\text{Due} &= \\sum^{\\infty}_{k=0} (I\\ddot{a})_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_x \\\\ (I\\ddot{a})_x &= \\sum^{\\infty}_{k=0} (k+1) v^k {}_{k}p_x \\\\ \\end{aligned} \\] Alternatively, it can be calculated from an assurance instead: \\[ \\begin{aligned} \\text{Arith WL}_\\text{Due} &= \\frac{\\ddot{a}_{\\enclose{actuarial}{k+1}} - (k+1)v^{k+1}}{d} \\\\ E(\\text{Arith WL}_\\text{Due}) &= \\frac{E(\\ddot{a}_{\\enclose{actuarial}{k+1}}) - E((k+1)v^{k+1})}{d} \\\\ (I\\ddot{a})_x &= \\frac{\\ddot{a}_x - (IA)_x}{d} \\end{aligned} \\] Notice that the main difference between an increasing annuity due and immediate is that the immediate case now lags the due by 1 every period . This difference can be accounted for using a level annuity due: \\[ \\begin{aligned} (I\\ddot{a})_x &= \\sum^{\\infty}_{k=0} (k+1) v^k {}_{k}p_{x} \\\\ &= v^{0} {}_{0}p_{x} + 2v^{1} {}_{1}p_{x} + 3v^{2} {}_{2}p_{x} + \\dots \\\\ &= (v^{1} {}_{1}p_{x} + 2v^{2} {}_{2}p_{x} + \\dots) + (v^{0} {}_{0}p_{x} + v^{1} {}_{1}p_{x} + v^{2} {}_{2}p_{x} + \\dots) \\\\ &= (Ia)_x + \\ddot{a}_x \\\\ (Ia)_x &= (I\\ddot{a})_x - \\ddot{a}_x \\end{aligned} \\] Arithmetic Term/Temporary \u00b6 For contract with level benefits , term/temporary contracts can be expressed as a difference of two WLs issued at different times because the benefits for the time after the specified term would cancel out . For contracts with variable benefits, this is slightly more complicated as the benefits after the specified term do NOT cancel out : The \"earlier\" WL would have increased significantly from its starting value The \"later\" WL would only be at its starting value Thus, an additional expression needs to be subtracted in order to remove the remaining benefit past the specified term. This can be done using a level benefit contract with the a benefit equal to the remaining amount . Thus, the EPV of an Increasing Term Assurance can be calculated as the following: \\[ \\begin{aligned} \\text{EPV Arith TA} &= \\sum^{n-1}_{K_x=0} (k+1) v^{K_x+1} {}_{k|}q_x \\\\ (IA)^1_{x:\\enclose{actuarial}{n}} &= (IA)_x - {}_{n}E_x [(IA)_{x+n} + nA_{x+n}] \\end{aligned} \\] Thus, the EPV of an Increasing Term Annuity can be calculated as the following: \\[ \\begin{aligned} \\text{EPV Arith TA}_\\text{Due} &= \\sum^{n-1}_{k=0} (I\\ddot{a})_{\\enclose{actuarial}{n}} \\cdot {}_{k|}q_x \\\\ (I\\ddot{a})_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{k=0} (k+1) v^k {}_{k}p_x \\\\ &= (I\\ddot{a})_x - {}_{n}E_x[(I\\ddot{a})_{x+n}+n\\ddot{a}_{x+n}] \\end{aligned} \\] Due to Immediate Conversion As mentioned in the Life Annuity section, it is not useful to memorize the conversion from Due to Immediate for TAs. Thus, simply express the ITA in the form of IWLs and then perform the conversion there. Decreasing Benefits \u00b6 As mentioned previously, another key feature of term/temporary contracts is that they can have decreasing benefits . This is because the benefit of the policy can be set such that it would exactly decrease to 0 by the end of the policy term. This is done by setting a starting benefit of \\(n\\) and decreasing by \\(1\\) each period : \\[ \\text{Arithmetically Decreasing Benefit} = n-k \\] Thus, the EPV of a Decreasing Term Assurance can be calculated as the following: \\[ \\begin{aligned} \\text{EPV Arith Decreasing TA} &= \\sum^{\\infty}_{k=0} (n-k) v^{k+1} {}_{k|}q_x \\\\ (D_{\\enclose{actuarial}{n}}A)_{x:\\enclose{actuarial}{n}} &= \\sum^{\\infty}_{k=0} (n-k+1-1) v^{k+1} {}_{k|}q_x \\\\ &= \\sum^{\\infty}_{k=0} (n+1) v^{k+1} {}_{k|}q_x - \\sum^{\\infty}_{k=0} (k+1) v^{k+1} {}_{k|}q_x \\\\ &= (n+1)A^1_{x:\\enclose{actuarial}{n}} - (IA)^1_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] \\((DA)\\) is the actuarial notation for an Assurance with Arithmetically decreasing benefits starting at \\(n\\) and decreasing by 1 each period. Similarly, the EPV of a decreasing temporary annuity can be calculated as the following: \\[ (D_{\\enclose{actuarial}{n}}\\ddot{a})_{x:\\enclose{actuarial}{n}} = (n+1)\\ddot{a}_{x:\\enclose{actuarial}{n}} - (I\\ddot{a})_{x:\\enclose{actuarial}{n}} \\] Continuous Arithmetic \u00b6 Similar to Geometric Contracts, the EPV of a continuously payable Arithmetic Contracts can be calculated using the usual approximations. However, take note that for TAs, the added term should also be continuous : \\[ \\begin{aligned} (I\\bar{A})_{x:\\enclose{actuarial}{n}} &= (I\\bar{A})_x - {}_{n}E_x[(I\\bar{A})_{x+n}+ n\\bar{A}_{x+n}] \\\\ (I\\bar{a})_{x:\\enclose{actuarial}{n}} &= (I\\bar{a})_x - {}_{n}E_x[(I\\bar{a})_{x+n}+ n\\bar{a}_{x+n}] \\end{aligned} \\] Increasing Woolhouse Approximation For IWL Annuities, the woolhouse approximation is slightly different: \\[ (I\\bar{a})_{x} = (I\\ddot{a})_{x} - \\frac{1}{2} \\ddot{a}_x \\] Continuous arithmetic contracts could also refer to a contract that not only payable continuously, but changes continuously as well. The benefits change at a constant rate throughout the period , such that the total change is still equal to 1. For Assurances , it can be understood from a graphical approach: \\[ \\begin{aligned} (\\bar{I}\\bar{A})_{x} &= (I\\bar{A})_{x} - \\frac{1}{2} \\bar{A}_x \\end{aligned} \\] For Annuities, it can be derived using the woolhouse approximation : \\[ \\begin{aligned} (\\bar{I}\\bar{a})_x &= (Ia)_x + \\frac{1}{12} \\\\ &= (I\\ddot{a})_x - \\ddot{a}_x + \\frac{1}{12} \\end{aligned} \\] Note that these approximations are ONLY applicable for WL policies . For continously increasing TAs, convert them to WLs first before applying the approximations. Variable Recursions \u00b6 Since the benefit payable at the end of the first year is still 1 , the recursion for variable contracts are similar to the level ones. Variable Assurances \u00b6 Consider the two scenarios: If the policyholder dies in the current year with probability \\(q_x\\) , then a benefit of 1 is paid at the end of the year If the policyholder lives past the current year with probability \\(p_x\\) , then the they will receive the EPV of a policy with benefits \\(2, 3, \\dots\\) or \\((1+b)^1, (1+b)^2, \\dots\\) at some future time The second component must be the EPV of an assurance that STARTS with an increased benefit of \\((1+b)\\) or \\(2\\) . The issue is that the random variable is currently defined as a contract with benefits starting at 1 , which is why the recursive formula is different from the level case . For Geometric Assurances, \\(A_{x+1}|_{i=\\frac{1+i}{1+b}-1}\\) is already an Geometric Assurance that starts at \\((1+b)\\) , thus no adjustment is needed for the RHS: \\[ \\text{EPV Geom WL}_{x} = vq_x + vp_x A_{x+1}|_{i=\\frac{1+i}{1+b}-1} \\] For Arithmetic Assurances, an EPV representing an additional benefit of 1 each period must be added to the second component so that the RHS expression starts from 2 and increases by an additional 1 each period. \\[ (IA)_{x} = vq_x + vp_x[(IA)_{x+1} + A_{x+1}] \\] Variable Annuities \u00b6 Recall that a benefit of 1 is already received at the beginning of the year regardless of whether the policyholder lives or dies. If the policyholder dies with probability \\(q_x\\) , then they receive nothing extra If the policyholder lives past the current year with probability \\(p_x\\) , then they will receive benefits that are larger than 1 at some future time For Geometric Annuities, TBC. \\[ \\begin{aligned} \\text{EPV Geom WL}_{\\text{Due},x} = 1 + vp_x \\cdot (1+c) \\left(\\ddot{a}_{x+1}|_{i = \\frac{1+i}{1+c}-1} \\right) \\end{aligned} \\] For Arithmetic Annuities, the same adjustment as before must be made: \\[ (I\\ddot{a})_x = 1 + vp_x[(I\\ddot{a})_{x+1} + \\ddot{a}_{x+1}] \\] In Force Recursions \u00b6 Previously, we have only considered the EPV for a newly issued policy at age \\(x\\) . However, now we want to consider the EPV of a policy that has already been in force for some period of time \\(t\\) . For policies with level benefits , the EPV of an already in force policy is exactly the same as the EPV of a newly issued policy at the current age with the same expiration date . This means that the same recursion can be used for both newly issued policies and inforce policies. However, this is NOT the case for policies with variable benefits as the inforce policy would have a benefit of \\(t\\) or \\((1+b)^{t-1}\\) while a newly issued policy would have benefit of 1. This means that a seperate recursion must be defined for inforce variable policies. For simplicity, we will consider an in force arithmetically increasing WL Assurance : If the policyholder dies with probability \\(q_{x+t-1}\\) , they will receive a benefit of \\(t\\) at the end of the year If the poliycholder survives with probability \\(p_{x+t-1}\\) , they will receive the EPV of a contract with benefits \\(t+1, t+2, \\dots\\) at some future time \\[ RHS = vq_{x+t-1} \\cdot t + vp_{x+t-1} [(IA)_{x+t} + tA_{x+t}] \\] Since (IA) is used to denote a policy with benefits starting at 1, an additional term must be added to the LHS to show that the in force policy has benefits of t: \\[ LHS = (IA)_{x+t-1} + (t-1) A_{x+t-1} \\] The same exercise can be performed for Annuities as well, resulting in the following: \\[ \\begin{aligned} (IA)_{x+t-1} + (t-1) A_{x+t-1} &= vq_{x+t-1} \\cdot t + vp_{x+t-1} [(IA)_{x+t} + tA_{x+t}] \\\\ (I\\ddot{a})_{x+t-1} + (t-1) \\ddot{a}_{x+t-1} &= t + vp_{x+t-1} [(I\\ddot{a})_{x+t} + t\\ddot{a}_{x+t}] \\end{aligned} \\] Term/Temporary Policies The same can be shown for Variable Term/Temporary policies, but remember that the remaining duration of the policy must decrease as well. Variance and Second Moment \u00b6 Consider the second moment of both types of variable assurances: \\[ \\begin{aligned} E\\left[(\\text{Arith WL})^2\\right] &= \\sum^{\\infty}_{K_x=0} (k+1)^2 (v^2)^{K_x+1} {}_{k|}q_x \\\\ \\\\ E\\left[(\\text{Geom WL})^2\\right] &= \\sum^{\\infty}_{K_x=0} (1+b)^{2K_x} (v^2)^{K_x+1} {}_{k|}q_x \\end{aligned} \\] Notice that the usual approximation of \\(v=v^2\\) is insufficient to solve for the second moment due to the squared benefit. Thus, there are no \\({}^{2}A\\) defined for variable assurances.","title":"Variable Benefits"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#variable-benefits","text":"","title":"Variable Benefits"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#overview","text":"Previously, the benefit of the assurances were assumed to be fixed at 1 for simplicity. In practice, it is common to have assurances where the benefits change over time .","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#geometric-contracts","text":"If the benefits change by a common factor each period, then it is known as a Geometric contract. The benefit of the policy starts at 1 in the first year and changes by \\((1+b)\\) each period: If \\(b>0\\) , then the benefits are increasing If \\(b<0\\) , then the benefits are decreasing \\[ \\text{Geometric Benefit} = (1+b)^{K_x} \\] Note that the power is \\(K_x\\) and NOT \\(K_x+1\\) to reflect that the benefit starts at 1 when \\(K_x=0\\) .","title":"Geometric Contracts"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#geometric-assurance","text":"Let \\(\\text{Geom WL}\\) be the random variable denoting the PV of a Geometric Assurance. \\[ \\text{Geom WL} = (1+b)^{K_x} v^{K_x+1} \\] Thus, the EPV of a WL Geometric Assurance can be calculated as: \\[ \\begin{aligned} \\text{EPV Geom WL} &= \\sum^{\\infty}_{k=0} (1+b)^{k} v^{k+1} {}_{k|}q_x \\\\ &= (1+b)^{-1} \\sum^{\\infty}_{k=0} (1+b)^{k+1} v^{k+1} {}_{k|}q_x \\\\ &= \\frac{1}{1+b} \\sum^{\\infty}_{k=0} \\left(\\frac{1+b}{1+i}\\right)^{k+1} {}_{k|}q_x \\\\ &= \\frac{1}{1+b} \\sum^{\\infty}_{k=0} (v')^{k+1} {}_{k|}q_x \\\\ &= \\frac{1}{1+b} A_x |_{i=\\frac{1+i}{1+b}-1} \\end{aligned} \\] Note that this means that the term \\(A_x |_{i=\\frac{1+i}{1+b}-1}\\) by itself is a Geometric Assurance that starts with payments of \\((1+b)\\) , which is why \\(\\frac{1}{1+b}\\) is required to scale it down by one factor such that it starts at one. In practice, most questions will provide a value of \\(b\\) such that \\((i=\\frac{1+i}{1+b}-1)\\) simplifies to a nice percentage . However, this usually means that the value of \\(b\\) provided is some complicated number, so do not be taken aback! The expression follows similar intuition to the second moment - it is simply a regular EPV expression evaluated at a different interest rate . Unfortunately, there is no specified actuarial notation for this niche case. One common approach is to denote it using \\(A_x^i\\) .","title":"Geometric Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#geometric-annuity","text":"Let \\(\\text{EPV Geom WL}_\\text{Due}\\) be the random variable denoting the PV of a Geometric Annuity Due. Thus, the EPV of a Geometric Annuity Due can be calculated as: \\[ \\begin{aligned} \\text{EPV Geom WL}_\\text{Due} &= \\sum^{\\infty}_{k=0} (1+c)^k v^k {}_{k}p_x \\\\ &= \\sum^{\\infty}_{k=0} \\left(\\frac{1+c}{1+i}\\right)^k {}_{k}p_x \\\\ &= \\ddot{a}_x|_{i = \\frac{1+i}{1+c}-1} \\end{aligned} \\]","title":"Geometric Annuity"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#continuous-geometric","text":"The EPV of continuously payable geometric contracts can be calculated using the usual UDD, Claims Acceleration or Woolhouse Approximations. However, despite having a \"new\" interest rate, the original interest rate is used in the continuous approximation parameters. Warning This is different from the second moment where the new interest rate must be reflected in the approximation as well. Take note of the different treatments.","title":"Continuous Geometric"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#arithmetic-contracts","text":"If the benefits change by a fixed constant each period, then it is instead known as an Arithmetic contract. For simplicity, the change each period is assumed to be 1: Arithmetically Increasing - Increases by 1 each period Arithmetically Decreasing - Decreased by 1 each period Since a WL contract lasts forever, the benefits of a decreasing WL contract would inevitably become negative , which do not make sense. Thus, WLs can only be arithmetically increasing contracts . \\[ \\text{Arithmetically Increasing Benefit} = K_x + 1 \\]","title":"Arithmetic Contracts"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#arithmetic-assurance","text":"Let \\(\\text{Arith WL}\\) be the random variable denoting the PV of an arithmetically increasing WL Assurance: \\[ \\text{Arith WL} = (K_x + 1) v^{K_x+1} \\] Thus, the EPV of a WL Arithmetic Assurance can be shown to be: \\[ \\begin{aligned} \\text{EPV Arith WL} &= \\sum^{\\infty}_{k=0} (k+1) v^{k+1} {}_{k|}q_x \\\\ (IA)_x &= \\sum^{\\infty}_{k=0} (k+1) v^{k+1} {}_{k|}q_x \\end{aligned} \\] \\((IA)_x\\) is the actuarial notation for the EPV of a contract with arithmetically increasing benefits starting at 1 and increasing by 1 each period. It can also be shown to be a sum of deferred WLs , each deferred by one period from the previous. This creates a step-like benefit which creates the increasing effect of the assurance: \\[ \\begin{aligned} (IA)_x &= {}_{0|}A_x + {}_{1|}A_x + {}_{2|}A_x + ... \\\\ &= \\sum^{\\infty}_{K_x = 0} {}_{K_x|}A_x \\end{aligned} \\] In practice, the magnitude of the change is unlikely to be one. Thus, the EPV must be scaled to match the actual change. This can be problematic if the starting benefit is NOT the same as the change . Consider a product with benefit \\(B\\) that increases by \\(k\\) each year. Due to the definition of the IWL variable, the starting benefit and change each period are equal - we cannot simply multiply \\(B\\) or \\(k\\) to the IWL EPV. Thus, we must split this product into a fixed component with \\(B-k\\) benefits and a variable component that starts and increases by \\(k\\) each period. The sum of these two components results in a benefit of \\(B, B+k, B+2k, \\dots\\)","title":"Arithmetic Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#arithmetic-annuities","text":"Let \\(\\text{Arith WL}_\\text{Due}\\) be the random variable denoting the PV of an arithmetically increasing annuity. The PV can be denoted by an arithmetically increasing annuity certain : \\[ \\begin{aligned} \\text{Arith WL}_\\text{Due} &= (I\\ddot{a})_{\\enclose{actuarial}{n}} \\\\ &= \\sum^{n-1}_{k=0} v^k \\cdot \\ddot{a}_{\\enclose{actuarial}{n-k}} \\\\ &= \\sum^{n-1}_{k=0} v^k \\cdot \\frac{1-v^{n-k}}{d} \\\\ &= \\sum^{n-1}_{k=0} \\frac{v^k - v^n}{d} \\\\ &= \\frac{1}{d} \\left(\\sum^{n-1}_{k=0} v^k - \\sum^{n-1}_{k=0} v^n \\right) \\\\ &= \\frac{1}{d} (\\ddot{a}_{\\enclose{actuarial}{n}} - nv^n) \\\\ &= \\frac{\\ddot{a}_{\\enclose{actuarial}{n}} - nv^n}{d} \\end{aligned} \\] Similarly, the EPV of an arithmetically increasing WL annuity Due can be calculated as: \\[ \\begin{aligned} \\text{EPV Arith WL}_\\text{Due} &= \\sum^{\\infty}_{k=0} (I\\ddot{a})_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_x \\\\ (I\\ddot{a})_x &= \\sum^{\\infty}_{k=0} (k+1) v^k {}_{k}p_x \\\\ \\end{aligned} \\] Alternatively, it can be calculated from an assurance instead: \\[ \\begin{aligned} \\text{Arith WL}_\\text{Due} &= \\frac{\\ddot{a}_{\\enclose{actuarial}{k+1}} - (k+1)v^{k+1}}{d} \\\\ E(\\text{Arith WL}_\\text{Due}) &= \\frac{E(\\ddot{a}_{\\enclose{actuarial}{k+1}}) - E((k+1)v^{k+1})}{d} \\\\ (I\\ddot{a})_x &= \\frac{\\ddot{a}_x - (IA)_x}{d} \\end{aligned} \\] Notice that the main difference between an increasing annuity due and immediate is that the immediate case now lags the due by 1 every period . This difference can be accounted for using a level annuity due: \\[ \\begin{aligned} (I\\ddot{a})_x &= \\sum^{\\infty}_{k=0} (k+1) v^k {}_{k}p_{x} \\\\ &= v^{0} {}_{0}p_{x} + 2v^{1} {}_{1}p_{x} + 3v^{2} {}_{2}p_{x} + \\dots \\\\ &= (v^{1} {}_{1}p_{x} + 2v^{2} {}_{2}p_{x} + \\dots) + (v^{0} {}_{0}p_{x} + v^{1} {}_{1}p_{x} + v^{2} {}_{2}p_{x} + \\dots) \\\\ &= (Ia)_x + \\ddot{a}_x \\\\ (Ia)_x &= (I\\ddot{a})_x - \\ddot{a}_x \\end{aligned} \\]","title":"Arithmetic Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#arithmetic-termtemporary","text":"For contract with level benefits , term/temporary contracts can be expressed as a difference of two WLs issued at different times because the benefits for the time after the specified term would cancel out . For contracts with variable benefits, this is slightly more complicated as the benefits after the specified term do NOT cancel out : The \"earlier\" WL would have increased significantly from its starting value The \"later\" WL would only be at its starting value Thus, an additional expression needs to be subtracted in order to remove the remaining benefit past the specified term. This can be done using a level benefit contract with the a benefit equal to the remaining amount . Thus, the EPV of an Increasing Term Assurance can be calculated as the following: \\[ \\begin{aligned} \\text{EPV Arith TA} &= \\sum^{n-1}_{K_x=0} (k+1) v^{K_x+1} {}_{k|}q_x \\\\ (IA)^1_{x:\\enclose{actuarial}{n}} &= (IA)_x - {}_{n}E_x [(IA)_{x+n} + nA_{x+n}] \\end{aligned} \\] Thus, the EPV of an Increasing Term Annuity can be calculated as the following: \\[ \\begin{aligned} \\text{EPV Arith TA}_\\text{Due} &= \\sum^{n-1}_{k=0} (I\\ddot{a})_{\\enclose{actuarial}{n}} \\cdot {}_{k|}q_x \\\\ (I\\ddot{a})_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{k=0} (k+1) v^k {}_{k}p_x \\\\ &= (I\\ddot{a})_x - {}_{n}E_x[(I\\ddot{a})_{x+n}+n\\ddot{a}_{x+n}] \\end{aligned} \\] Due to Immediate Conversion As mentioned in the Life Annuity section, it is not useful to memorize the conversion from Due to Immediate for TAs. Thus, simply express the ITA in the form of IWLs and then perform the conversion there.","title":"Arithmetic Term/Temporary"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#decreasing-benefits","text":"As mentioned previously, another key feature of term/temporary contracts is that they can have decreasing benefits . This is because the benefit of the policy can be set such that it would exactly decrease to 0 by the end of the policy term. This is done by setting a starting benefit of \\(n\\) and decreasing by \\(1\\) each period : \\[ \\text{Arithmetically Decreasing Benefit} = n-k \\] Thus, the EPV of a Decreasing Term Assurance can be calculated as the following: \\[ \\begin{aligned} \\text{EPV Arith Decreasing TA} &= \\sum^{\\infty}_{k=0} (n-k) v^{k+1} {}_{k|}q_x \\\\ (D_{\\enclose{actuarial}{n}}A)_{x:\\enclose{actuarial}{n}} &= \\sum^{\\infty}_{k=0} (n-k+1-1) v^{k+1} {}_{k|}q_x \\\\ &= \\sum^{\\infty}_{k=0} (n+1) v^{k+1} {}_{k|}q_x - \\sum^{\\infty}_{k=0} (k+1) v^{k+1} {}_{k|}q_x \\\\ &= (n+1)A^1_{x:\\enclose{actuarial}{n}} - (IA)^1_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] \\((DA)\\) is the actuarial notation for an Assurance with Arithmetically decreasing benefits starting at \\(n\\) and decreasing by 1 each period. Similarly, the EPV of a decreasing temporary annuity can be calculated as the following: \\[ (D_{\\enclose{actuarial}{n}}\\ddot{a})_{x:\\enclose{actuarial}{n}} = (n+1)\\ddot{a}_{x:\\enclose{actuarial}{n}} - (I\\ddot{a})_{x:\\enclose{actuarial}{n}} \\]","title":"Decreasing Benefits"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#continuous-arithmetic","text":"Similar to Geometric Contracts, the EPV of a continuously payable Arithmetic Contracts can be calculated using the usual approximations. However, take note that for TAs, the added term should also be continuous : \\[ \\begin{aligned} (I\\bar{A})_{x:\\enclose{actuarial}{n}} &= (I\\bar{A})_x - {}_{n}E_x[(I\\bar{A})_{x+n}+ n\\bar{A}_{x+n}] \\\\ (I\\bar{a})_{x:\\enclose{actuarial}{n}} &= (I\\bar{a})_x - {}_{n}E_x[(I\\bar{a})_{x+n}+ n\\bar{a}_{x+n}] \\end{aligned} \\] Increasing Woolhouse Approximation For IWL Annuities, the woolhouse approximation is slightly different: \\[ (I\\bar{a})_{x} = (I\\ddot{a})_{x} - \\frac{1}{2} \\ddot{a}_x \\] Continuous arithmetic contracts could also refer to a contract that not only payable continuously, but changes continuously as well. The benefits change at a constant rate throughout the period , such that the total change is still equal to 1. For Assurances , it can be understood from a graphical approach: \\[ \\begin{aligned} (\\bar{I}\\bar{A})_{x} &= (I\\bar{A})_{x} - \\frac{1}{2} \\bar{A}_x \\end{aligned} \\] For Annuities, it can be derived using the woolhouse approximation : \\[ \\begin{aligned} (\\bar{I}\\bar{a})_x &= (Ia)_x + \\frac{1}{12} \\\\ &= (I\\ddot{a})_x - \\ddot{a}_x + \\frac{1}{12} \\end{aligned} \\] Note that these approximations are ONLY applicable for WL policies . For continously increasing TAs, convert them to WLs first before applying the approximations.","title":"Continuous Arithmetic"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#variable-recursions","text":"Since the benefit payable at the end of the first year is still 1 , the recursion for variable contracts are similar to the level ones.","title":"Variable Recursions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#variable-assurances","text":"Consider the two scenarios: If the policyholder dies in the current year with probability \\(q_x\\) , then a benefit of 1 is paid at the end of the year If the policyholder lives past the current year with probability \\(p_x\\) , then the they will receive the EPV of a policy with benefits \\(2, 3, \\dots\\) or \\((1+b)^1, (1+b)^2, \\dots\\) at some future time The second component must be the EPV of an assurance that STARTS with an increased benefit of \\((1+b)\\) or \\(2\\) . The issue is that the random variable is currently defined as a contract with benefits starting at 1 , which is why the recursive formula is different from the level case . For Geometric Assurances, \\(A_{x+1}|_{i=\\frac{1+i}{1+b}-1}\\) is already an Geometric Assurance that starts at \\((1+b)\\) , thus no adjustment is needed for the RHS: \\[ \\text{EPV Geom WL}_{x} = vq_x + vp_x A_{x+1}|_{i=\\frac{1+i}{1+b}-1} \\] For Arithmetic Assurances, an EPV representing an additional benefit of 1 each period must be added to the second component so that the RHS expression starts from 2 and increases by an additional 1 each period. \\[ (IA)_{x} = vq_x + vp_x[(IA)_{x+1} + A_{x+1}] \\]","title":"Variable Assurances"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#variable-annuities","text":"Recall that a benefit of 1 is already received at the beginning of the year regardless of whether the policyholder lives or dies. If the policyholder dies with probability \\(q_x\\) , then they receive nothing extra If the policyholder lives past the current year with probability \\(p_x\\) , then they will receive benefits that are larger than 1 at some future time For Geometric Annuities, TBC. \\[ \\begin{aligned} \\text{EPV Geom WL}_{\\text{Due},x} = 1 + vp_x \\cdot (1+c) \\left(\\ddot{a}_{x+1}|_{i = \\frac{1+i}{1+c}-1} \\right) \\end{aligned} \\] For Arithmetic Annuities, the same adjustment as before must be made: \\[ (I\\ddot{a})_x = 1 + vp_x[(I\\ddot{a})_{x+1} + \\ddot{a}_{x+1}] \\]","title":"Variable Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#in-force-recursions","text":"Previously, we have only considered the EPV for a newly issued policy at age \\(x\\) . However, now we want to consider the EPV of a policy that has already been in force for some period of time \\(t\\) . For policies with level benefits , the EPV of an already in force policy is exactly the same as the EPV of a newly issued policy at the current age with the same expiration date . This means that the same recursion can be used for both newly issued policies and inforce policies. However, this is NOT the case for policies with variable benefits as the inforce policy would have a benefit of \\(t\\) or \\((1+b)^{t-1}\\) while a newly issued policy would have benefit of 1. This means that a seperate recursion must be defined for inforce variable policies. For simplicity, we will consider an in force arithmetically increasing WL Assurance : If the policyholder dies with probability \\(q_{x+t-1}\\) , they will receive a benefit of \\(t\\) at the end of the year If the poliycholder survives with probability \\(p_{x+t-1}\\) , they will receive the EPV of a contract with benefits \\(t+1, t+2, \\dots\\) at some future time \\[ RHS = vq_{x+t-1} \\cdot t + vp_{x+t-1} [(IA)_{x+t} + tA_{x+t}] \\] Since (IA) is used to denote a policy with benefits starting at 1, an additional term must be added to the LHS to show that the in force policy has benefits of t: \\[ LHS = (IA)_{x+t-1} + (t-1) A_{x+t-1} \\] The same exercise can be performed for Annuities as well, resulting in the following: \\[ \\begin{aligned} (IA)_{x+t-1} + (t-1) A_{x+t-1} &= vq_{x+t-1} \\cdot t + vp_{x+t-1} [(IA)_{x+t} + tA_{x+t}] \\\\ (I\\ddot{a})_{x+t-1} + (t-1) \\ddot{a}_{x+t-1} &= t + vp_{x+t-1} [(I\\ddot{a})_{x+t} + t\\ddot{a}_{x+t}] \\end{aligned} \\] Term/Temporary Policies The same can be shown for Variable Term/Temporary policies, but remember that the remaining duration of the policy must decrease as well.","title":"In Force Recursions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#variance-and-second-moment","text":"Consider the second moment of both types of variable assurances: \\[ \\begin{aligned} E\\left[(\\text{Arith WL})^2\\right] &= \\sum^{\\infty}_{K_x=0} (k+1)^2 (v^2)^{K_x+1} {}_{k|}q_x \\\\ \\\\ E\\left[(\\text{Geom WL})^2\\right] &= \\sum^{\\infty}_{K_x=0} (1+b)^{2K_x} (v^2)^{K_x+1} {}_{k|}q_x \\end{aligned} \\] Notice that the usual approximation of \\(v=v^2\\) is insufficient to solve for the second moment due to the squared benefit. Thus, there are no \\({}^{2}A\\) defined for variable assurances.","title":"Variance and Second Moment"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Premiums/","text":"Premiums \u00b6 Overview \u00b6 Unlike other products, the cost of a Life Assurance or Annuity are not known when it is issued, as it is impossible to precisely predict when an individual will die or how long they will live for. Premiums are the amount that the insurance company charges for a Life Assurance or Annuity. They aim to charge a sufficiently high premium such that they expect to at least break even on the sale of the policy on an EPV basis . Premiums are always paid in advance , but the frequency of payment depends on the type of premium: Single Premium Limited Premium Regular Premium Single Lump Sum Recurring for a fixed period Recurring as long as the contract is valid Use dollar amount Modelled using TA Annuities Modelled using WL Annuities If premiums are paid in arrears instead, the policyholder may refuse to pay premiums on the flawed grounds that since they did not die in the period, they did not utilize the coverage and hence should not pay. There are two types of premiums that can be calculated: Net Premiums, \\(P\\) - Excluding expenses; based on benefits only Gross Premiums, \\(G\\) - Including expenses; based on BOTH benefits, expenses and other cashflows The key thing to notice is that the components of net premiums are fixed while gross premiums differ from insurer to insurer. Thus, this section will only show formulas for net premiums . Equivalence Principle Approach \u00b6 To determine the breakeven point of the insurer, the expected amount of loss must first be defined. Thus, let \\(L_x\\) be the random variable denoting the loss of the policy: If \\(L_x\\) is positive, then the policy is losing money If \\(L_x\\) is negative, then the policy is making money The intepretation is reversed since \\(L\\) represents an outflow rather than an inflow! \\[ \\begin{aligned} L_x &= \\text{PV Outflow} - \\text{PV Inflows} \\end{aligned} \\] The goal is to solve for the premiums such that there is no expected loss . This is known as the Equivalence Principle approach as it sets the EPVs to be equal: \\[ \\begin{aligned} E(L_x) &= 0 \\\\ \\text{EPV Outflow} &= \\text{EPV Inflow} \\end{aligned} \\] The loss variable for Net and Gross can be differentiated using superscripts: Net Premium : \\(L^{B}_{x}\\) , where \\(B\\) represents the **B**enefits only Gross Premium : \\(L^{B+E}_{x}\\) , where \\(B+E\\) represents **B**enefits and **E**xpenses For Net Premiums only, \\[ \\begin{aligned} E \\left(L^{B}_{x} \\right) &= 0 \\\\ \\text{EPV Benefits} &= \\text{EPV Premiums} \\\\ B \\cdot A_x &= P \\cdot \\ddot{a}_x \\\\ P &= B \\cdot \\frac{A_x}{\\ddot{a}_x} \\end{aligned} \\] Note that several implicit assumptions were made: Premiums are payable annually Benefits are payable discretely If premiums or benefits were payable continously, then the formula would use the corresponding continuous assurance or annuity. However, note that even when calculating this way \\(P\\) will still represent the Annual Net Premium , thus we must still divide to obtain the appropriate monthly or quarterly premium . Portfolio Percentile Approach \u00b6 If there are a large number of policies within the insurer's portfolio, then the insurer can consider the breakeven on a portfolio of policies rather than a single policy. Let \\(Y\\) be the random variable denoting the aggregate loss on a portfolio of \\(n\\) iid policies: \\[ \\begin{aligned} Y & \\sim iid \\\\ E(Y) &= n \\cdot E(L) \\\\ Var(Y) &= n \\cdot Var(L) \\\\ \\end{aligned} \\] If the insurer has a large portfolio of policies ( \\(n\\) is sufficiently large), then through Central Limit Theorem , \\[ Y \\sim N(E(Y), Var(Y)) \\] The premium is then set such the probability of profit is set a pre-determined level close to 1: \\[ \\begin{aligned} P(Y < 0) &= \\alpha \\\\ P\\left(\\frac{Y - E(Y)}{\\sqrt{Var (Y)}} \\le \\frac{0 - E(Y)}{\\sqrt{Var (Y)}}\\right) &= \\alpha \\\\ P\\left(Z \\le \\frac{- E(Y)}{\\sqrt{Var (Y)}}\\right) &= \\alpha \\\\ P\\left(Z \\le \\frac{- n \\cdot E(L)}{n \\cdot \\sqrt{Var (L)}}\\right) &= \\alpha \\\\ P\\left(Z \\le \\frac{- E(L)}{\\sqrt{Var (L)}}\\right) &= \\alpha \\end{aligned} \\] Thus, only the expectation and variance of an individual policy is required. For net premiums only, \\[ \\begin{aligned} L_x &= B \\cdot \\text{WL} - P \\cdot \\text{WL}_{Due} \\\\ &= B \\cdot \\text{WL} - P \\cdot \\frac{1- \\text{WL}}{d} \\\\ &= B \\cdot \\text{WL} - P \\left(\\frac{1}{d}-\\frac{\\text{WL}}{d} \\right) \\\\ &= B \\cdot \\text{WL} - P \\left[\\left(\\frac{1}{d}\\right) + P\\left(\\frac{\\text{WL}}{d}\\right) \\right] \\\\ &= \\text{WL} \\left(B + \\frac{P}{d} \\right) - \\frac{P}{d} \\\\ \\\\ \\therefore E(L_x) &= E \\left[\\text{WL} \\left(B + \\frac{P}{d} \\right) - \\frac{P}{d} \\right] \\\\ &= A_x \\left(B + \\frac{P}{d} \\right) - \\frac{P}{d} \\\\ \\\\ \\therefore Var(L_x) &= Var\\left[\\text{WL} \\left(B + \\frac{P}{d} \\right) - \\frac{P}{d} \\right] \\\\ &= Var \\left[\\text{WL} \\left(B + \\frac{P}{d} \\right) \\right] \\\\ &= \\left(B + \\frac{P}{d} \\right)^2 \\cdot Var(\\text{WL}) \\\\ &= \\left(B + \\frac{P}{d} \\right)^2 \\cdot \\left[{}^{2}A_x - (A_x)^2 \\right] \\end{aligned} \\] The premiums determined this way will always be larger than the premiums determined through the equivalence principle: \\[ \\begin{aligned} P_{\\text{Portfolio Percentile}} &\\ge P_{\\text{Equivalence Principle}} \\\\ \\\\ \\text{As n} &\\to \\infty \\\\ P_{\\text{Portfolio Percentile}} &\\to P_{\\text{Equivalence Principle}} \\end{aligned} \\] Gross Premiums \u00b6 What makes gross premiums so much trickier than net premiums is that there are many different types of expenses , where every question can ask for a unique combination of them. Expenses can be differentiated in terms of when they are charged: Acquisition Expenses Renewal Expenses Termination Expense At policy inception During policy lifetime At policy termination Single payment at time 0 Recurring payments from time 1 onwards Single payment at unknown time Denoted by \\(e_0\\) Denoted by \\(e_t\\) Denoted by \\(E\\) Directly use amount Corresponding Annuity Functions Corresponding Assurance Function EG. Commissions expenses EG. Administrative Expenses EG. Claims expenses Renewal Expenses can also inflate over time, where they increase by a factpr of \\((1+r)\\) each year. In this case, they are modelled using a geometrically increasing annuity instead. IMPORTANT : Acquisition expenses are usually higher than renewal ones as agent commissions are mostly front-loaded in the first year to give agents cash faster, incentivizing them to sell more. They can also be differentiated in terms of how much they charge: Overhead Expenses Direct Expenses Shared among all policies Borne by a specific policy Fixed amount Percentage of policy premium or benefit Spread out among all policies Borne only by that policy EG. Office Rental EG. Sales Commission or Underwriting Expenses Note that gross premiums can also include other cashflows, such as a pre-determined profit margin . Since these are rare and highly dependent on the situation, they will not be covered in this section. Expense Calculations \u00b6 Consider a policy that only has acquisition and renewal expenses , denoted by A and R respectively: The PV of the expenses can be easily calculated by using an annuity immediate: \\[ \\text{EPV Expenses} = A + R \\cdot a_x \\] While this is not challenging per se, it is slightly inconvenient as there is a need to use an annuity immediate while premiums use annuity dues - this would make it harder to simplify or more computationally intensive. This can be worked around by splitting the acquisition expense , such that one part of it forms the first payment for the renewal expenses, allowing an annuity due to be used instead: \\[ \\begin{aligned} \\text{EPV Expenses} &= A + R \\cdot a_x \\\\ &= A - R + R + R \\cdot a_x \\\\ &= A - R + R \\cdot \\ddot{a}_x \\\\ &= A + R \\cdot (\\ddot{a}_x - 1) \\end{aligned} \\] However, note that the simplification does NOT hold true if expense inflation is taken into account - the level annuity immediate CANNOT simply be replaced with the geometric annuity. This is because the expenses only start to inflate from the third payment onwards: Expense Premiums \u00b6 The difference between gross and net premiums represent the amount of premiums needed to cover just the expenses, known as the Expense Premiums , \\(E\\) : \\[ \\begin{aligned} G &= P + E \\\\ E &= G - P \\end{aligned} \\] It is also known as the Expense Loading as it represents the amount that is added ( loaded ) on to the net premiums to cover expenses.","title":"Premiums"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Premiums/#premiums","text":"","title":"Premiums"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Premiums/#overview","text":"Unlike other products, the cost of a Life Assurance or Annuity are not known when it is issued, as it is impossible to precisely predict when an individual will die or how long they will live for. Premiums are the amount that the insurance company charges for a Life Assurance or Annuity. They aim to charge a sufficiently high premium such that they expect to at least break even on the sale of the policy on an EPV basis . Premiums are always paid in advance , but the frequency of payment depends on the type of premium: Single Premium Limited Premium Regular Premium Single Lump Sum Recurring for a fixed period Recurring as long as the contract is valid Use dollar amount Modelled using TA Annuities Modelled using WL Annuities If premiums are paid in arrears instead, the policyholder may refuse to pay premiums on the flawed grounds that since they did not die in the period, they did not utilize the coverage and hence should not pay. There are two types of premiums that can be calculated: Net Premiums, \\(P\\) - Excluding expenses; based on benefits only Gross Premiums, \\(G\\) - Including expenses; based on BOTH benefits, expenses and other cashflows The key thing to notice is that the components of net premiums are fixed while gross premiums differ from insurer to insurer. Thus, this section will only show formulas for net premiums .","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Premiums/#equivalence-principle-approach","text":"To determine the breakeven point of the insurer, the expected amount of loss must first be defined. Thus, let \\(L_x\\) be the random variable denoting the loss of the policy: If \\(L_x\\) is positive, then the policy is losing money If \\(L_x\\) is negative, then the policy is making money The intepretation is reversed since \\(L\\) represents an outflow rather than an inflow! \\[ \\begin{aligned} L_x &= \\text{PV Outflow} - \\text{PV Inflows} \\end{aligned} \\] The goal is to solve for the premiums such that there is no expected loss . This is known as the Equivalence Principle approach as it sets the EPVs to be equal: \\[ \\begin{aligned} E(L_x) &= 0 \\\\ \\text{EPV Outflow} &= \\text{EPV Inflow} \\end{aligned} \\] The loss variable for Net and Gross can be differentiated using superscripts: Net Premium : \\(L^{B}_{x}\\) , where \\(B\\) represents the **B**enefits only Gross Premium : \\(L^{B+E}_{x}\\) , where \\(B+E\\) represents **B**enefits and **E**xpenses For Net Premiums only, \\[ \\begin{aligned} E \\left(L^{B}_{x} \\right) &= 0 \\\\ \\text{EPV Benefits} &= \\text{EPV Premiums} \\\\ B \\cdot A_x &= P \\cdot \\ddot{a}_x \\\\ P &= B \\cdot \\frac{A_x}{\\ddot{a}_x} \\end{aligned} \\] Note that several implicit assumptions were made: Premiums are payable annually Benefits are payable discretely If premiums or benefits were payable continously, then the formula would use the corresponding continuous assurance or annuity. However, note that even when calculating this way \\(P\\) will still represent the Annual Net Premium , thus we must still divide to obtain the appropriate monthly or quarterly premium .","title":"Equivalence Principle Approach"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Premiums/#portfolio-percentile-approach","text":"If there are a large number of policies within the insurer's portfolio, then the insurer can consider the breakeven on a portfolio of policies rather than a single policy. Let \\(Y\\) be the random variable denoting the aggregate loss on a portfolio of \\(n\\) iid policies: \\[ \\begin{aligned} Y & \\sim iid \\\\ E(Y) &= n \\cdot E(L) \\\\ Var(Y) &= n \\cdot Var(L) \\\\ \\end{aligned} \\] If the insurer has a large portfolio of policies ( \\(n\\) is sufficiently large), then through Central Limit Theorem , \\[ Y \\sim N(E(Y), Var(Y)) \\] The premium is then set such the probability of profit is set a pre-determined level close to 1: \\[ \\begin{aligned} P(Y < 0) &= \\alpha \\\\ P\\left(\\frac{Y - E(Y)}{\\sqrt{Var (Y)}} \\le \\frac{0 - E(Y)}{\\sqrt{Var (Y)}}\\right) &= \\alpha \\\\ P\\left(Z \\le \\frac{- E(Y)}{\\sqrt{Var (Y)}}\\right) &= \\alpha \\\\ P\\left(Z \\le \\frac{- n \\cdot E(L)}{n \\cdot \\sqrt{Var (L)}}\\right) &= \\alpha \\\\ P\\left(Z \\le \\frac{- E(L)}{\\sqrt{Var (L)}}\\right) &= \\alpha \\end{aligned} \\] Thus, only the expectation and variance of an individual policy is required. For net premiums only, \\[ \\begin{aligned} L_x &= B \\cdot \\text{WL} - P \\cdot \\text{WL}_{Due} \\\\ &= B \\cdot \\text{WL} - P \\cdot \\frac{1- \\text{WL}}{d} \\\\ &= B \\cdot \\text{WL} - P \\left(\\frac{1}{d}-\\frac{\\text{WL}}{d} \\right) \\\\ &= B \\cdot \\text{WL} - P \\left[\\left(\\frac{1}{d}\\right) + P\\left(\\frac{\\text{WL}}{d}\\right) \\right] \\\\ &= \\text{WL} \\left(B + \\frac{P}{d} \\right) - \\frac{P}{d} \\\\ \\\\ \\therefore E(L_x) &= E \\left[\\text{WL} \\left(B + \\frac{P}{d} \\right) - \\frac{P}{d} \\right] \\\\ &= A_x \\left(B + \\frac{P}{d} \\right) - \\frac{P}{d} \\\\ \\\\ \\therefore Var(L_x) &= Var\\left[\\text{WL} \\left(B + \\frac{P}{d} \\right) - \\frac{P}{d} \\right] \\\\ &= Var \\left[\\text{WL} \\left(B + \\frac{P}{d} \\right) \\right] \\\\ &= \\left(B + \\frac{P}{d} \\right)^2 \\cdot Var(\\text{WL}) \\\\ &= \\left(B + \\frac{P}{d} \\right)^2 \\cdot \\left[{}^{2}A_x - (A_x)^2 \\right] \\end{aligned} \\] The premiums determined this way will always be larger than the premiums determined through the equivalence principle: \\[ \\begin{aligned} P_{\\text{Portfolio Percentile}} &\\ge P_{\\text{Equivalence Principle}} \\\\ \\\\ \\text{As n} &\\to \\infty \\\\ P_{\\text{Portfolio Percentile}} &\\to P_{\\text{Equivalence Principle}} \\end{aligned} \\]","title":"Portfolio Percentile Approach"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Premiums/#gross-premiums","text":"What makes gross premiums so much trickier than net premiums is that there are many different types of expenses , where every question can ask for a unique combination of them. Expenses can be differentiated in terms of when they are charged: Acquisition Expenses Renewal Expenses Termination Expense At policy inception During policy lifetime At policy termination Single payment at time 0 Recurring payments from time 1 onwards Single payment at unknown time Denoted by \\(e_0\\) Denoted by \\(e_t\\) Denoted by \\(E\\) Directly use amount Corresponding Annuity Functions Corresponding Assurance Function EG. Commissions expenses EG. Administrative Expenses EG. Claims expenses Renewal Expenses can also inflate over time, where they increase by a factpr of \\((1+r)\\) each year. In this case, they are modelled using a geometrically increasing annuity instead. IMPORTANT : Acquisition expenses are usually higher than renewal ones as agent commissions are mostly front-loaded in the first year to give agents cash faster, incentivizing them to sell more. They can also be differentiated in terms of how much they charge: Overhead Expenses Direct Expenses Shared among all policies Borne by a specific policy Fixed amount Percentage of policy premium or benefit Spread out among all policies Borne only by that policy EG. Office Rental EG. Sales Commission or Underwriting Expenses Note that gross premiums can also include other cashflows, such as a pre-determined profit margin . Since these are rare and highly dependent on the situation, they will not be covered in this section.","title":"Gross Premiums"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Premiums/#expense-calculations","text":"Consider a policy that only has acquisition and renewal expenses , denoted by A and R respectively: The PV of the expenses can be easily calculated by using an annuity immediate: \\[ \\text{EPV Expenses} = A + R \\cdot a_x \\] While this is not challenging per se, it is slightly inconvenient as there is a need to use an annuity immediate while premiums use annuity dues - this would make it harder to simplify or more computationally intensive. This can be worked around by splitting the acquisition expense , such that one part of it forms the first payment for the renewal expenses, allowing an annuity due to be used instead: \\[ \\begin{aligned} \\text{EPV Expenses} &= A + R \\cdot a_x \\\\ &= A - R + R + R \\cdot a_x \\\\ &= A - R + R \\cdot \\ddot{a}_x \\\\ &= A + R \\cdot (\\ddot{a}_x - 1) \\end{aligned} \\] However, note that the simplification does NOT hold true if expense inflation is taken into account - the level annuity immediate CANNOT simply be replaced with the geometric annuity. This is because the expenses only start to inflate from the third payment onwards:","title":"Expense Calculations"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Premiums/#expense-premiums","text":"The difference between gross and net premiums represent the amount of premiums needed to cover just the expenses, known as the Expense Premiums , \\(E\\) : \\[ \\begin{aligned} G &= P + E \\\\ E &= G - P \\end{aligned} \\] It is also known as the Expense Loading as it represents the amount that is added ( loaded ) on to the net premiums to cover expenses.","title":"Expense Premiums"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/","text":"Reserves \u00b6 Overview \u00b6 Most life insurance contracts charge level premiums - charging the same amount throughout the lifetime of the policy. When the policyholder is young, the probability of death is small, thus the premiums collected are larger than the expected outflow in the year, leading to a surplus . When the policyholder is older, the probability of death is high, thus the premiums collected are smaller than the expected outflow in the year, leading to a deficit . Given the inevitable deficit, the surplus in the earlier years cannot be recognized as a profit and are instead pooled together and safely invested into an account, from which the deficit in later years will draw from. It is a common misconception to think that each policy has its own account. Following this logic, the insurer would not have enough funds to pay an insured who dies shortly after death. In order to determine the adequacy of the account, insurers will calculate the size of the expected loss of ALL policies and compare it against the (hopefully larger) account value. The expected loss is known as the Reserve of the policy because it represents an amount inside the pooled account that is reserved for paying the benefits of the policy. Prospective Approach \u00b6 There are two approaches to calculate the reserves: Prospective Approach - Based on Present Values of future cashflows Retrospective Approach - Based on Future Values of past cashflows (Not covered) Similar to premiums, there are also two types of reserves, depending on what cashflows are being considered: Net Premium Reserves - Net Premiums and Benefits Gross Premium Reserves - Gross Premiums, Benefits and Expenses While this looks similar to premium calculation, note that the Gross Reserves do NOT include any other cashflows (EG. Profit Margin). This is intuitive, as reserves should only be to meet essential cashflows . Reserves are calculated at the beginning of the policy year : AFTER any year end cashflows from the previous period (Claims & Claim Expenses) BEFORE any beginning of year cashflows from the current period (Premiums & Renewal Expenses) This means that the reserves are the additional amount needed to cover claims from the current year onwards, including the current year. Beginning VS End Recall that the beginning of the policy year is also the end of the previous policy year - avoid confusion by drawing out the timelines. Questions can use either phrasing, so be sure to read very carefully. Let \\({}_{t-1}V\\) represent the reserve at the beginning of policy year \\(t\\) : \\[ \\begin{aligned} {}_{t-1}V &= E(L_{x+(t-1)}) \\\\ &= \\text{PV Benefits}_{x+(t-1)} - \\text{PV Premiums}_{x+(t-1)} \\end{aligned} \\] Note that reserves only consider FUTURE cashflows. For single premium or deferred products, there is no need to consider the premiums after the payment period is over. Similar to premiums, we will only define formulas for the Net Premium Reserves . They can be simplified into an expression involving only the following, which allows it to be easily calculated in situations where limited information is provided: Annuities only Assurances only Annuities and Premiums Assurance and Premiums Case 1: Annuities Only \u00b6 \\[ \\begin{aligned} {}_{t-1}V &= A_{x+(t-1)} - P \\ddot{a}_{x+(t-1)} \\\\ &= A_{x+(t-1)} - \\frac{A_x}{\\ddot{a}_x} \\cdot \\ddot{a}_{x+(t-1)} \\\\ &= (1-\\delta\\ddot{a}_{x+(t-1)}) - \\frac{1-\\delta\\ddot{a}_{x}}{\\ddot{a}_{x}} \\cdot \\ddot{a}_{x+(t-1)} \\\\ &= 1-\\delta\\ddot{a}_{x+(t-1)} - \\left (\\frac{1}{\\ddot{a}_x}-\\delta \\right) \\cdot \\ddot{a}_{x+(t-1)} \\\\ &= 1-\\delta\\ddot{a}_{x+(t-1)} - \\frac{\\ddot{a}_{x+(t-1)}}{\\ddot{a}_x} + \\delta\\ddot{a}_{x+(t-1)} \\\\ &= 1 - \\frac{\\ddot{a}_{x+(t-1)}}{\\ddot{a}_x} \\end{aligned} \\] Case 2: Assurances Only \u00b6 \\[ \\begin{aligned} {}_{t-1}V &= A_{x+(t-1)} - P \\ddot{a}_{x+(t-1)} \\\\ &= A_{x+(t-1)} - \\frac{A_x}{\\ddot{a}_x} \\cdot \\ddot{a}_{x+(t-1)} \\\\ &= A_{x+(t-1)} - \\frac{A_x}{\\frac{1-A_x}{d}} \\cdot \\frac{1-A_{x+(t-1)}}{d} \\\\ &= A_{x+(t-1)} - \\frac{A_x}{1-A_x} \\cdot (1-A_{x+(t-1)}) \\\\ &= \\frac{A_{x+(t-1)} \\cdot (1-A_{x}) - A_x(1-A_{x+(t-1)})}{1-A_x} \\\\ &= \\frac{A_{x+(t-1)} - A_{x}A_{x+(t-1)} - A_x + A_x A_{x+(t-1)}}{1-A_x} \\\\ &= \\frac{A_{x+(t-1)} - A_{x}}{1 - A_{x}} \\end{aligned} \\] Case 3: Annuities & Premiums \u00b6 \\[ \\begin{aligned} {}_{t}V &= A_{x+(t-1)} - P_x \\cdot \\ddot{a}_{x+(t-1)} \\\\ &= \\ddot{a}_{x+(t-1)} \\cdot \\left(\\frac{A_{x+(t-1)}}{\\ddot{a}_{x+(t-1)}} - P_{x} \\right) \\\\ &= \\ddot{a}_{x+(t-1)} \\cdot \\left(P_{x+(t-1)} - P_{x} \\right) \\end{aligned} \\] Case 4: Assurances & Premiums \u00b6 \\[ \\begin{aligned} {}_{t}V &= A_{x+(t-1)} - P_x \\cdot \\ddot{a}_{x+(t-1)} \\\\ &= A_{x+(t-1)} \\left(1 - P_x \\cdot \\frac{\\ddot{a}_{x+(t-1)}}{A_{x+(t-1)}} \\right) \\\\ &= A_{x+(t-1)} \\left(1 - P_x \\cdot \\frac{1}{\\frac{A_{x+(t-1)}}{\\ddot{a}_{x+(t-1)}}} \\right) \\\\ &= A_{x+(t-1)} \\left(1 - \\frac{P_x}{P_{x+(t-1)}} \\right) \\end{aligned} \\] Reserves & Premiums \u00b6 The calculation of reserves and premiums both involve the expectation of the random variable \\(L\\) : Premiums Reserves \\(E(L)\\) given as 0 \\(E(L)\\) is the target \\(P\\) is the target \\(P\\) is given Calculated at age \\(x\\) Calculated at age \\(x+t\\) If net premium reserves are calculated at the start of the policy ( \\(t=0\\) ), then following the equivalence principle, the reserves will be 0 . This is intuitive, as the premiums are set such that they expect to exactly cover the losses of the policy. However, this is not the case for gross premiums. This is because gross premiums include provisions for other cashflows like profit margins which are not considered during the reserve calculation. Thus, gross premiums collect more than enough to cover the expected losses, resulting in a negative reserve at time 0 . Technically speaking, not all gross premiums include profit margins. If they do not, then the gross premium reserve will also be 0. \\[ \\begin{aligned} {}_{0}V^{Net} &= 0 \\\\ {}_{0}V^{Gross} &\\le 0 \\end{aligned} \\] Note that a newly issued policy for another individual aged \\(x+t\\) uses the exact same \\(L\\) as the reserve, \\(L_{x+t}\\) . This is why some of the expressions previously involve \\(P_{x+t}\\) , which is the premium of this newly issued policy. Red Herrings \u00b6 Premium and Reserve calculations are similar, so it is easy to get them mixed up with each other. The confusing part is that some questions requires us to compute the premiums ourselves before calculating the reserves. Given how similar the calculations are, this may mistakenly cause us to use the same cashflows for both, when the reserves should be different. Even if the gross premiums are provided, some information exclusively used to compute gross premiums may still be provided as a red herring to intentionally confuse us: Acquisition Expenses - Reserves are usually calculated after inception; no need for it Profit Margin - Reserves DO NOT consider profit margin Premium Frequency - Reserves use annual premiums; monthly premiums may be required or provided Note that the reverse is also possible, where gross premiums are provided but the question actually requires us to calculate net premiums, which can cause us to mistakenly use the gross premium instead. Recursions \u00b6 Following the same logic as assurances and annuities, net premium reserves can also be recursively expressed as a function of itself: If the policyholder dies, the insurer would have to pay a benefit of 1 at the end of the year If the policyholder survives, the insurer would have to keep a reserve for future payments Premiums are received REGARDLESS of death or survival as it was paid at the start of the year \\[ \\begin{aligned} {}_{t}V^{\\text{Net}} &= - P + \\begin{cases} v, & q_{x+t} \\\\ v \\cdot {}_{t+1}V, & p_{x+t} \\end{cases} \\\\ \\\\ \\therefore {}_{t}V^{\\text{Net}} &= - P + q_{x+t} \\cdot v + p_{x+t} \\cdot v \\cdot {}_{t+1}V \\end{aligned} \\] Gross premium reserves are similar as any expenses are always paid at the beginning of the year with premiums: \\[ {}_{t}V^{\\text{Gross}} = (E-G) + q_{x+t} \\cdot v + p_{x+t} \\cdot v \\cdot {}_{t+1}V \\] Modified Net Premium Reserves \u00b6 The method and basis for calculating the reserve is based on the prevailing regulation at that time and region. In the US, reserves are calculated using net premiums due to its simplicity at the cost of some accuracy loss. However, net premium reserves tend to be higher than gross premium reserves . This is due to the negative expense reserves, as the excess expense premiums help to offset the future benefits as well. Since net premiums reserves do not recognize expenses, they do not have this benefit! \\[ \\begin{aligned} {}_{t}v^{\\text{Net}} &= {}_{t}v^{\\text{Gross}} + \\underbrace{{}_{t}v^{\\text{Expense}}}_{\\text{-ve}} \\\\ {}_{t}v^{\\text{Net}} &\\gt {}_{t}v^{\\text{Gross}} \\end{aligned} \\] This means that insurers are holding too much reserves under this valuation basis. This is problematic because reserves can only be invested in low risk assets, which typically have lower returns relative to what the capital would have usually be invested in. This incurs an opportunity cost in investment income . To account for this, US regulators allow for insurers to use a modified net premium reserve instead that implicitly accounts for expenses while retaining the simplicity of a net premium approach. Expense Reserves \u00b6 Following expense premiums, the difference between the gross and net premium reserves represent the additional amount needed to cover the expenses, known as the Expense Reserves . \\[ \\begin{aligned} {}_{t}V^{\\text{Expense}} &= {}_{t}V^{\\text{Gross}} - {}_{t}V^{\\text{Net}} \\\\ &= \\text{EPV(Benefits)} + \\text{EPV(Expenses)} - \\text{EPV(Gross Premium)} - \\text{EPV(Benfits)} + \\text{EPV(Net Premiums)} \\\\ &= \\text{EPV(Expenses)} - (\\text{EPV(Gross Premium)} - \\text{EPV(Net Premiums)}) \\\\ &= \\text{EPV(Expenses)} - \\text{EPV(Expense Premium)} \\end{aligned} \\] The expense reserve is 0 at time 0 . However, the expense reserve is typically negative for all other years. This is because the expense premiums are typically smaller than the acquisition expenses , resulting in a positive loss in the first year, known as the New Business Strain . On the other hand, expense premiums are typically higher than the renewal expenses , resulting in a negative loss in all subsequent years, recovering the initial loss over time. This is known as the Deferred Acquisition Cost as the recovery of the acquisition expenses are deferred to all future years. At time 1, all future expense premiums will be larger than the corresponding renewal expenses, resulting in a negative expense reserve from that time onwards: \\[ \\begin{aligned} \\text{Expense Premium} &\\gt \\text{Renewal Expense} \\\\ \\text{EPV(Expense Premium)} &\\gt \\text{EPV(Renewal Expense)} \\\\ {}_{t}V^{\\text{Expense}} < 0 \\end{aligned} \\] Full Preliminary Term \u00b6 Most methods of calculating modified reserves involve using a modified premium to calculate the reserves. In general, these modified premiums are non-level , consisting of two (or more) components: First Year Premiums , \\(\\alpha\\) Subsequent Premiums , \\(\\beta\\) Where \\(\\beta \\gt P \\gt \\alpha\\) Since \\(\\beta \\gt P\\) , the insurer recognises more premiums for the same amount benefit, resulting in a smaller reserve . This allows the insurer to use their capital more productively. \\[ {}_{t}v^{\\text{Modified}} < {}_{t}v^{\\text{Net}} \\] The real reason is due to acquisition expenses, but im not sure how to explain There are many methods of calculating modified reserves, but for the purposes of this exam, we will only be considering the Full Preliminary Term method. The policy can be thought of as being split into two components: One year TA with single premiums of \\(\\alpha\\) The original policy issued one year later with one less year of coverage with recurring premiums of \\(\\beta\\) For the one year TA, following the equivalence principle approach, \\[ \\begin{aligned} {}_{0}V^{\\text{FPT}} &= 0 \\\\ \\alpha &= A^{1}_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] For the original policy issued one year later with one year less of coverage , following the equivalence principle approach, \\[ \\begin{aligned} {}_{1}V^{\\text{FPT}} &= 0 \\\\ A_{x+1} - \\beta \\ddot{a}_{x+1} &= 0 \\\\ \\beta &= \\frac{A_{x+1}}{\\ddot{a}_{x+1}} \\end{aligned} \\] Special Case: Term & Endowments For Term & Endowment contracts, the one less year of coverage must be accounted for: \\[ \\beta = \\frac{A_{x+1:\\enclose{actuarial}{n-1}}}{\\ddot{a}_{x+1:\\enclose{actuarial}{n-1}}} \\] Thus, the modified reserves at all later times follows the same formula as before, simply using \\(\\beta\\) instead: \\[ \\begin{aligned} {}_{t}V^{\\text{FPT}} &= \\begin{cases} 0, & t = 0 \\\\ 0, & t = 1 \\\\ A_{x+t} - \\beta \\ddot{a}_{x+t}, & t > 1 \\end{cases} \\end{aligned} \\] Thus, the FPT reserves are equivalent to the net premium reserve for a policy that is actually issued one year later with one less year of coverage: \\[ \\therefore {}_{t}V^{\\text{FPT}} = {}_{t-1}V^{\\text{Net}}_{\\text{One year later, one less year}} \\] Interim Reserves \u00b6","title":"Reserves"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#reserves","text":"","title":"Reserves"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#overview","text":"Most life insurance contracts charge level premiums - charging the same amount throughout the lifetime of the policy. When the policyholder is young, the probability of death is small, thus the premiums collected are larger than the expected outflow in the year, leading to a surplus . When the policyholder is older, the probability of death is high, thus the premiums collected are smaller than the expected outflow in the year, leading to a deficit . Given the inevitable deficit, the surplus in the earlier years cannot be recognized as a profit and are instead pooled together and safely invested into an account, from which the deficit in later years will draw from. It is a common misconception to think that each policy has its own account. Following this logic, the insurer would not have enough funds to pay an insured who dies shortly after death. In order to determine the adequacy of the account, insurers will calculate the size of the expected loss of ALL policies and compare it against the (hopefully larger) account value. The expected loss is known as the Reserve of the policy because it represents an amount inside the pooled account that is reserved for paying the benefits of the policy.","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#prospective-approach","text":"There are two approaches to calculate the reserves: Prospective Approach - Based on Present Values of future cashflows Retrospective Approach - Based on Future Values of past cashflows (Not covered) Similar to premiums, there are also two types of reserves, depending on what cashflows are being considered: Net Premium Reserves - Net Premiums and Benefits Gross Premium Reserves - Gross Premiums, Benefits and Expenses While this looks similar to premium calculation, note that the Gross Reserves do NOT include any other cashflows (EG. Profit Margin). This is intuitive, as reserves should only be to meet essential cashflows . Reserves are calculated at the beginning of the policy year : AFTER any year end cashflows from the previous period (Claims & Claim Expenses) BEFORE any beginning of year cashflows from the current period (Premiums & Renewal Expenses) This means that the reserves are the additional amount needed to cover claims from the current year onwards, including the current year. Beginning VS End Recall that the beginning of the policy year is also the end of the previous policy year - avoid confusion by drawing out the timelines. Questions can use either phrasing, so be sure to read very carefully. Let \\({}_{t-1}V\\) represent the reserve at the beginning of policy year \\(t\\) : \\[ \\begin{aligned} {}_{t-1}V &= E(L_{x+(t-1)}) \\\\ &= \\text{PV Benefits}_{x+(t-1)} - \\text{PV Premiums}_{x+(t-1)} \\end{aligned} \\] Note that reserves only consider FUTURE cashflows. For single premium or deferred products, there is no need to consider the premiums after the payment period is over. Similar to premiums, we will only define formulas for the Net Premium Reserves . They can be simplified into an expression involving only the following, which allows it to be easily calculated in situations where limited information is provided: Annuities only Assurances only Annuities and Premiums Assurance and Premiums","title":"Prospective Approach"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#case-1-annuities-only","text":"\\[ \\begin{aligned} {}_{t-1}V &= A_{x+(t-1)} - P \\ddot{a}_{x+(t-1)} \\\\ &= A_{x+(t-1)} - \\frac{A_x}{\\ddot{a}_x} \\cdot \\ddot{a}_{x+(t-1)} \\\\ &= (1-\\delta\\ddot{a}_{x+(t-1)}) - \\frac{1-\\delta\\ddot{a}_{x}}{\\ddot{a}_{x}} \\cdot \\ddot{a}_{x+(t-1)} \\\\ &= 1-\\delta\\ddot{a}_{x+(t-1)} - \\left (\\frac{1}{\\ddot{a}_x}-\\delta \\right) \\cdot \\ddot{a}_{x+(t-1)} \\\\ &= 1-\\delta\\ddot{a}_{x+(t-1)} - \\frac{\\ddot{a}_{x+(t-1)}}{\\ddot{a}_x} + \\delta\\ddot{a}_{x+(t-1)} \\\\ &= 1 - \\frac{\\ddot{a}_{x+(t-1)}}{\\ddot{a}_x} \\end{aligned} \\]","title":"Case 1: Annuities Only"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#case-2-assurances-only","text":"\\[ \\begin{aligned} {}_{t-1}V &= A_{x+(t-1)} - P \\ddot{a}_{x+(t-1)} \\\\ &= A_{x+(t-1)} - \\frac{A_x}{\\ddot{a}_x} \\cdot \\ddot{a}_{x+(t-1)} \\\\ &= A_{x+(t-1)} - \\frac{A_x}{\\frac{1-A_x}{d}} \\cdot \\frac{1-A_{x+(t-1)}}{d} \\\\ &= A_{x+(t-1)} - \\frac{A_x}{1-A_x} \\cdot (1-A_{x+(t-1)}) \\\\ &= \\frac{A_{x+(t-1)} \\cdot (1-A_{x}) - A_x(1-A_{x+(t-1)})}{1-A_x} \\\\ &= \\frac{A_{x+(t-1)} - A_{x}A_{x+(t-1)} - A_x + A_x A_{x+(t-1)}}{1-A_x} \\\\ &= \\frac{A_{x+(t-1)} - A_{x}}{1 - A_{x}} \\end{aligned} \\]","title":"Case 2: Assurances Only"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#case-3-annuities-premiums","text":"\\[ \\begin{aligned} {}_{t}V &= A_{x+(t-1)} - P_x \\cdot \\ddot{a}_{x+(t-1)} \\\\ &= \\ddot{a}_{x+(t-1)} \\cdot \\left(\\frac{A_{x+(t-1)}}{\\ddot{a}_{x+(t-1)}} - P_{x} \\right) \\\\ &= \\ddot{a}_{x+(t-1)} \\cdot \\left(P_{x+(t-1)} - P_{x} \\right) \\end{aligned} \\]","title":"Case 3: Annuities &amp; Premiums"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#case-4-assurances-premiums","text":"\\[ \\begin{aligned} {}_{t}V &= A_{x+(t-1)} - P_x \\cdot \\ddot{a}_{x+(t-1)} \\\\ &= A_{x+(t-1)} \\left(1 - P_x \\cdot \\frac{\\ddot{a}_{x+(t-1)}}{A_{x+(t-1)}} \\right) \\\\ &= A_{x+(t-1)} \\left(1 - P_x \\cdot \\frac{1}{\\frac{A_{x+(t-1)}}{\\ddot{a}_{x+(t-1)}}} \\right) \\\\ &= A_{x+(t-1)} \\left(1 - \\frac{P_x}{P_{x+(t-1)}} \\right) \\end{aligned} \\]","title":"Case 4: Assurances &amp; Premiums"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#reserves-premiums","text":"The calculation of reserves and premiums both involve the expectation of the random variable \\(L\\) : Premiums Reserves \\(E(L)\\) given as 0 \\(E(L)\\) is the target \\(P\\) is the target \\(P\\) is given Calculated at age \\(x\\) Calculated at age \\(x+t\\) If net premium reserves are calculated at the start of the policy ( \\(t=0\\) ), then following the equivalence principle, the reserves will be 0 . This is intuitive, as the premiums are set such that they expect to exactly cover the losses of the policy. However, this is not the case for gross premiums. This is because gross premiums include provisions for other cashflows like profit margins which are not considered during the reserve calculation. Thus, gross premiums collect more than enough to cover the expected losses, resulting in a negative reserve at time 0 . Technically speaking, not all gross premiums include profit margins. If they do not, then the gross premium reserve will also be 0. \\[ \\begin{aligned} {}_{0}V^{Net} &= 0 \\\\ {}_{0}V^{Gross} &\\le 0 \\end{aligned} \\] Note that a newly issued policy for another individual aged \\(x+t\\) uses the exact same \\(L\\) as the reserve, \\(L_{x+t}\\) . This is why some of the expressions previously involve \\(P_{x+t}\\) , which is the premium of this newly issued policy.","title":"Reserves &amp; Premiums"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#red-herrings","text":"Premium and Reserve calculations are similar, so it is easy to get them mixed up with each other. The confusing part is that some questions requires us to compute the premiums ourselves before calculating the reserves. Given how similar the calculations are, this may mistakenly cause us to use the same cashflows for both, when the reserves should be different. Even if the gross premiums are provided, some information exclusively used to compute gross premiums may still be provided as a red herring to intentionally confuse us: Acquisition Expenses - Reserves are usually calculated after inception; no need for it Profit Margin - Reserves DO NOT consider profit margin Premium Frequency - Reserves use annual premiums; monthly premiums may be required or provided Note that the reverse is also possible, where gross premiums are provided but the question actually requires us to calculate net premiums, which can cause us to mistakenly use the gross premium instead.","title":"Red Herrings"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#recursions","text":"Following the same logic as assurances and annuities, net premium reserves can also be recursively expressed as a function of itself: If the policyholder dies, the insurer would have to pay a benefit of 1 at the end of the year If the policyholder survives, the insurer would have to keep a reserve for future payments Premiums are received REGARDLESS of death or survival as it was paid at the start of the year \\[ \\begin{aligned} {}_{t}V^{\\text{Net}} &= - P + \\begin{cases} v, & q_{x+t} \\\\ v \\cdot {}_{t+1}V, & p_{x+t} \\end{cases} \\\\ \\\\ \\therefore {}_{t}V^{\\text{Net}} &= - P + q_{x+t} \\cdot v + p_{x+t} \\cdot v \\cdot {}_{t+1}V \\end{aligned} \\] Gross premium reserves are similar as any expenses are always paid at the beginning of the year with premiums: \\[ {}_{t}V^{\\text{Gross}} = (E-G) + q_{x+t} \\cdot v + p_{x+t} \\cdot v \\cdot {}_{t+1}V \\]","title":"Recursions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#modified-net-premium-reserves","text":"The method and basis for calculating the reserve is based on the prevailing regulation at that time and region. In the US, reserves are calculated using net premiums due to its simplicity at the cost of some accuracy loss. However, net premium reserves tend to be higher than gross premium reserves . This is due to the negative expense reserves, as the excess expense premiums help to offset the future benefits as well. Since net premiums reserves do not recognize expenses, they do not have this benefit! \\[ \\begin{aligned} {}_{t}v^{\\text{Net}} &= {}_{t}v^{\\text{Gross}} + \\underbrace{{}_{t}v^{\\text{Expense}}}_{\\text{-ve}} \\\\ {}_{t}v^{\\text{Net}} &\\gt {}_{t}v^{\\text{Gross}} \\end{aligned} \\] This means that insurers are holding too much reserves under this valuation basis. This is problematic because reserves can only be invested in low risk assets, which typically have lower returns relative to what the capital would have usually be invested in. This incurs an opportunity cost in investment income . To account for this, US regulators allow for insurers to use a modified net premium reserve instead that implicitly accounts for expenses while retaining the simplicity of a net premium approach.","title":"Modified Net Premium Reserves"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#expense-reserves","text":"Following expense premiums, the difference between the gross and net premium reserves represent the additional amount needed to cover the expenses, known as the Expense Reserves . \\[ \\begin{aligned} {}_{t}V^{\\text{Expense}} &= {}_{t}V^{\\text{Gross}} - {}_{t}V^{\\text{Net}} \\\\ &= \\text{EPV(Benefits)} + \\text{EPV(Expenses)} - \\text{EPV(Gross Premium)} - \\text{EPV(Benfits)} + \\text{EPV(Net Premiums)} \\\\ &= \\text{EPV(Expenses)} - (\\text{EPV(Gross Premium)} - \\text{EPV(Net Premiums)}) \\\\ &= \\text{EPV(Expenses)} - \\text{EPV(Expense Premium)} \\end{aligned} \\] The expense reserve is 0 at time 0 . However, the expense reserve is typically negative for all other years. This is because the expense premiums are typically smaller than the acquisition expenses , resulting in a positive loss in the first year, known as the New Business Strain . On the other hand, expense premiums are typically higher than the renewal expenses , resulting in a negative loss in all subsequent years, recovering the initial loss over time. This is known as the Deferred Acquisition Cost as the recovery of the acquisition expenses are deferred to all future years. At time 1, all future expense premiums will be larger than the corresponding renewal expenses, resulting in a negative expense reserve from that time onwards: \\[ \\begin{aligned} \\text{Expense Premium} &\\gt \\text{Renewal Expense} \\\\ \\text{EPV(Expense Premium)} &\\gt \\text{EPV(Renewal Expense)} \\\\ {}_{t}V^{\\text{Expense}} < 0 \\end{aligned} \\]","title":"Expense Reserves"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#full-preliminary-term","text":"Most methods of calculating modified reserves involve using a modified premium to calculate the reserves. In general, these modified premiums are non-level , consisting of two (or more) components: First Year Premiums , \\(\\alpha\\) Subsequent Premiums , \\(\\beta\\) Where \\(\\beta \\gt P \\gt \\alpha\\) Since \\(\\beta \\gt P\\) , the insurer recognises more premiums for the same amount benefit, resulting in a smaller reserve . This allows the insurer to use their capital more productively. \\[ {}_{t}v^{\\text{Modified}} < {}_{t}v^{\\text{Net}} \\] The real reason is due to acquisition expenses, but im not sure how to explain There are many methods of calculating modified reserves, but for the purposes of this exam, we will only be considering the Full Preliminary Term method. The policy can be thought of as being split into two components: One year TA with single premiums of \\(\\alpha\\) The original policy issued one year later with one less year of coverage with recurring premiums of \\(\\beta\\) For the one year TA, following the equivalence principle approach, \\[ \\begin{aligned} {}_{0}V^{\\text{FPT}} &= 0 \\\\ \\alpha &= A^{1}_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] For the original policy issued one year later with one year less of coverage , following the equivalence principle approach, \\[ \\begin{aligned} {}_{1}V^{\\text{FPT}} &= 0 \\\\ A_{x+1} - \\beta \\ddot{a}_{x+1} &= 0 \\\\ \\beta &= \\frac{A_{x+1}}{\\ddot{a}_{x+1}} \\end{aligned} \\] Special Case: Term & Endowments For Term & Endowment contracts, the one less year of coverage must be accounted for: \\[ \\beta = \\frac{A_{x+1:\\enclose{actuarial}{n-1}}}{\\ddot{a}_{x+1:\\enclose{actuarial}{n-1}}} \\] Thus, the modified reserves at all later times follows the same formula as before, simply using \\(\\beta\\) instead: \\[ \\begin{aligned} {}_{t}V^{\\text{FPT}} &= \\begin{cases} 0, & t = 0 \\\\ 0, & t = 1 \\\\ A_{x+t} - \\beta \\ddot{a}_{x+t}, & t > 1 \\end{cases} \\end{aligned} \\] Thus, the FPT reserves are equivalent to the net premium reserve for a policy that is actually issued one year later with one less year of coverage: \\[ \\therefore {}_{t}V^{\\text{FPT}} = {}_{t-1}V^{\\text{Net}}_{\\text{One year later, one less year}} \\]","title":"Full Preliminary Term"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#interim-reserves","text":"","title":"Interim Reserves"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/","text":"Model Estimation \u00b6 Overview \u00b6 Previously, mortality rates were obtained through some parametric distribution. However, this requires us to make an assumption about the population, which may not always hold true. Thus, a better method would be to study the underlying population directly to estimate the mortality rates. Complete Data \u00b6 Consider a study where a group of individuals of the same age are observed till they die. The number of years taken for each individual to die are recorded as observations in the study. Each study has a total of \\(n\\) individuals starting at age \\(x\\) , thus the observations are denoted as \\(t_1, t_2, ..., t_n\\) . Individual Data \u00b6 If each indvidual has an observation tagged to them (similar to what was described earlier), then Individual Data is provided. Equal probability is assigned to each observation, thus the probability of an individual aged \\(x\\) surviving \\(k\\) years is simply the ratio of the number of observations larger than \\(k\\) to the total number of observations: \\[ {}_{t}\\hat{p}_x = \\frac{\\text{# of individuals survive past k}}{n} \\] Grouped Data \u00b6 If the population of the study is large, then the data is usually Grouped into ranges for convenience. The number of observations within each range is also provided. Ranges are concisely expressed using the bracket notation - () denotes that the bound is exlcusive while [] denotes that it is inclusive. For instance, (15, 20] denoted that the time to death is strictly larger than 15 but less than or equal to 20; \\(15 \\lt 5 \\le 20\\) . If \\(k\\) is the boundary of any of the ranges , then the probability of surviving \\(k\\) years is calculated the same way as the individual data - it is the ratio of the number of observations to the total number: \\[ {}_{t}\\hat{p}_x = \\frac{\\text{# of observations larger than k}}{n} \\] However, if \\(k\\) is within the range, by assuming that values within the range are uniformly distributed , then the probability can be linearly interpolated from the probabilities at the boundary: Consider the range \\((a,b]\\) : \\[ {}_{t}\\hat{p}_x = \\frac{k-a}{b-a} \\cdot \\hat{S}_x(a) + \\frac{b-k}{b-a} \\cdot \\hat{S}_x(b) \\] The interpolation follows the same intuition as the fractional age assumption, where the uniform assumption results in a linear graph of probabilities: Incomplete Data \u00b6 In practice, due to various constraints, it is impossible to observe an entire group of individuals till they die. Thus, for some observations, the recorded time to death is not the exact time to death , known as Incomplete Data . Censoring \u00b6 Censoring refers to when the value of the observation is only partially known . For instance, if the study stops prematurely, the time to deaths for the individuals alive will be recorded at the time the study ends. However, in reality, they die at some future time, resulting in a larger time to death than what was recorded . Right Censored - Actual value is larger than the recorded value Left Censored - Actual value is smaller than the recorded value Naturally, since time to death is an increasing function, the only Right Censored data is relevant in an actuarial context Generally speaking, there are two types for censored data: Type 1 Censoring - Study is stopped at a pre-determined time Type 2 Censoring - Study is stopped a certain number of deaths The remaining individuals still in the study at the point of stopping are right censored Censored data are usually denoted by a '+' superscript. Interval censoring is grouped data, random censoring Truncation \u00b6 Truncation refers to when the sample does not contain certain observations outside of some range. This is not to be confused with its mathematical definition of rounding. For instance, consider a study that wishes to study the mortality for people aged 50. However, only people aged 52 are able to be found. Naturally, then the times to death from age 50 for these individuals will always be at least 2 ; it will never be smaller than 2 . Right Truncated - No observations with values larger than cutoff Left Truncated - No observations with values smaller than cutoff Naturally, since time to death is an increasing function, the only Left Truncated data is relevant in an actuarial context Truncation and Censoring are often confused with one another as both result in observations within a given range. Truncation is when observations outside the range are not recorded at all while Censoring still records the observations , with the caveat that it is a floor/ceiling. Kaplan Meier Estimation \u00b6 Consider a study of 5 individuals: One individual dies at \\(t=0.8\\) and \\(t=1.4\\) 5 more individuals join the study at \\(t=1\\) ; Left Truncated at \\(t=1\\) 3 individual leave the study at \\(t=0.5\\) ; Right Censored at \\(t=0.5\\) The information is more often concisely portrayed in a table format: \\(t\\) represents the time of death \\(d\\) represents the number of deaths \\(r\\) represents the risk set RIGHT BEFORE death; simply known as the sample size This is problematic as the sample size changes throughout the study. However, the Kaplan Meier method accounts for this by calculating the survival probability as the product of the survival probabilities for all previous intervals between deaths : \\[ {}_{t}\\hat{p}_x = \\prod \\left(1-\\frac{d_j}{r_j} \\right) \\] Given the above example, the resulting Kaplan Meier estimation would be: \\[ \\begin{aligned} {}_{1.4}\\hat{p}_0 &= P(t > 0.8) * P(t > 1.4 | x > 0.8) \\\\ &= \\left (1 - \\frac{1}{2} \\right) * \\left (1 - \\frac{1}{6} \\right) \\\\ &= \\left (\\frac{1}{2} \\right) * \\left (\\frac{5}{6} \\right) \\\\ &= \\frac{5}{12} \\end{aligned} \\] This results in a constant probability between deaths . Thus, the focus is on the times of death ; the entries and exits from the study are only used to change the sample size. Nelson Aelen Estimation \u00b6 Recall that the survival probability can be expressed as a function of the Force of Mortality: \\[ {}_{t}\\hat{p}_x = e^{- \\int^t_0 \\mu_{x+t}} \\] The Nelson Aelen method estimates the Cumulative Force of Mortality (the exponent of the expression), which can be then used to compute the survival probability. It is an extremely common mistake to take the result of the Nelson Aelen estimation as the survival probability. The nelson aelen estimator is computed as the sum of the death probabilities for all previous intervals between deaths : \\[ \\int^t_0 \\mu_{x+t} = \\sum \\frac{d_j}{r_j} \\] Given the previous example, the resulting Nelson Aelen estimation would be: \\[ \\begin{aligned} \\int^t_0 \\mu_{x+1.4} &= P(t < 0.8) + P(t < 1.4 | x > 0.8) \\\\ &= \\left (\\frac{1}{2} \\right) + \\left (\\frac{1}{6} \\right) \\\\ &= \\frac{2}{3} \\\\ \\\\ {}_{1.4}\\hat{p}_x &= e^{-\\frac{2}{3}} \\\\ &= 0.51171 \\end{aligned} \\] Notice that the two values are not too far off from one another. Kaplan Meier Nelson Aelen Estimates Survival Probability Estimates Cumulative Force of Mortality Uses Survival probabilities Uses Death probabilities Maximum Likelihood Estimation \u00b6 Unlike the Nelson Aelen method, the MLE DIRECTLY estimates the force of mortality , which can be then used to compute survival probabilities. One key concept is the idea of Central Exposed to Risk ( \\(E^c_x\\) ), which is a measure of how long the individual has been alive in the study at the time. Consider the following example to estimate the force of mortality at age 51 : Firstly, we need to determine if and when the individuals turn 51: The date they turn 51 is used as the starting point for the \\(E^c_x\\) . The ending point is based on the year that they turn 51 and is one of the following: Date of death - Died during the year Date of withdrawal - Left during the year Date they turn 52 - Survived the entire year From this, \\(E^c_x\\) is calculated in years : Using the CER, the force of mortality can be determined: \\[ \\hat{\\mu}_x = \\frac{d_x}{E^c_x} \\] Assuming that this force is constant , the resulting survival probability is calculated as: \\[ {}_{t}\\hat{p}_x = e^{-\\hat{\\mu}_x} \\] Statistical Inference \u00b6 Given that all the above methods are estimates of the survival probability, the estimated values will change depending on the underlying sample. Thus, it is important to consider the variance of the estimator as well across various different samples. Complete Variance \u00b6 Each individual is either dead or or alive - thus, the number of people who survive past the specified time can be modelled using a Bernoulli Random variable . Recall that the variance of the bernoulli distribution is: \\[ Var(\\text{# of individuals survive past t}) = n \\cdot {}_{t}p_x \\cdot (1-{}_{t}p_x) \\] Thus, the variance of the \\[ \\begin{aligned} Var({}_{t}\\hat{p}_x) &= Var \\left(\\frac{\\text{# of individuals survive past k}}{n} \\right) \\\\ &= \\frac{1}{n^2} Var(\\text{# of individuals survive past t}) \\\\ &= \\frac{1}{n^2} n \\cdot {}_{t}p_x \\cdot (1-{}_{t}p_x) \\\\ &= \\frac{{}_{t}p_x \\cdot (1-{}_{t}p_x)}{n} \\\\ \\end{aligned} \\] Since the actual survival probability is not known, it is often substituted with the estimate: \\[ Var({}_{t}\\hat{p}_x) = \\frac{{}_{t}\\hat{p}_x \\cdot (1-{}_{t}\\hat{p}_x)}{n} \\] Note that the variance of the estimated death probability is identical to that of the survival probability: \\[ \\begin{aligned} Var({}_{t}\\hat{q}_x) &= Var(1 - {}_{t}\\hat{p}_x) \\\\ &= Var({}_{t}\\hat{p}_x) \\\\ \\end{aligned} \\] Incomplete Variance \u00b6 Due to the complexity of calculating the variance under these methods, the formula for variances are provided in the formula sheets. Kaplan Meier Variance : \\[ Var({}_{t}\\hat{p}_x) = ({}_{t}\\hat{p}_x)^2 \\sum \\frac{d_j}{r_j (r_j - d_j)} \\] Nelson Aelen Variance : \\[ Var({}_{t}\\hat{p}_x) = ({}_{t}\\hat{p}_x)^2 \\sum \\frac{d_j (r_j - d_j)}{r^3_j} \\] Maximum Likelihood Variance : \\[ Var({}_{t}\\hat{p}_x) = ({}_{t}\\hat{p}_x)^2 \\cdot \\frac{d_x}{\\left( E^c_x \\right)^2} \\] Confidence Intervals \u00b6 Linear Log Transformed","title":"Model Estimation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#model-estimation","text":"","title":"Model Estimation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#overview","text":"Previously, mortality rates were obtained through some parametric distribution. However, this requires us to make an assumption about the population, which may not always hold true. Thus, a better method would be to study the underlying population directly to estimate the mortality rates.","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#complete-data","text":"Consider a study where a group of individuals of the same age are observed till they die. The number of years taken for each individual to die are recorded as observations in the study. Each study has a total of \\(n\\) individuals starting at age \\(x\\) , thus the observations are denoted as \\(t_1, t_2, ..., t_n\\) .","title":"Complete Data"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#individual-data","text":"If each indvidual has an observation tagged to them (similar to what was described earlier), then Individual Data is provided. Equal probability is assigned to each observation, thus the probability of an individual aged \\(x\\) surviving \\(k\\) years is simply the ratio of the number of observations larger than \\(k\\) to the total number of observations: \\[ {}_{t}\\hat{p}_x = \\frac{\\text{# of individuals survive past k}}{n} \\]","title":"Individual Data"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#grouped-data","text":"If the population of the study is large, then the data is usually Grouped into ranges for convenience. The number of observations within each range is also provided. Ranges are concisely expressed using the bracket notation - () denotes that the bound is exlcusive while [] denotes that it is inclusive. For instance, (15, 20] denoted that the time to death is strictly larger than 15 but less than or equal to 20; \\(15 \\lt 5 \\le 20\\) . If \\(k\\) is the boundary of any of the ranges , then the probability of surviving \\(k\\) years is calculated the same way as the individual data - it is the ratio of the number of observations to the total number: \\[ {}_{t}\\hat{p}_x = \\frac{\\text{# of observations larger than k}}{n} \\] However, if \\(k\\) is within the range, by assuming that values within the range are uniformly distributed , then the probability can be linearly interpolated from the probabilities at the boundary: Consider the range \\((a,b]\\) : \\[ {}_{t}\\hat{p}_x = \\frac{k-a}{b-a} \\cdot \\hat{S}_x(a) + \\frac{b-k}{b-a} \\cdot \\hat{S}_x(b) \\] The interpolation follows the same intuition as the fractional age assumption, where the uniform assumption results in a linear graph of probabilities:","title":"Grouped Data"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#incomplete-data","text":"In practice, due to various constraints, it is impossible to observe an entire group of individuals till they die. Thus, for some observations, the recorded time to death is not the exact time to death , known as Incomplete Data .","title":"Incomplete Data"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#censoring","text":"Censoring refers to when the value of the observation is only partially known . For instance, if the study stops prematurely, the time to deaths for the individuals alive will be recorded at the time the study ends. However, in reality, they die at some future time, resulting in a larger time to death than what was recorded . Right Censored - Actual value is larger than the recorded value Left Censored - Actual value is smaller than the recorded value Naturally, since time to death is an increasing function, the only Right Censored data is relevant in an actuarial context Generally speaking, there are two types for censored data: Type 1 Censoring - Study is stopped at a pre-determined time Type 2 Censoring - Study is stopped a certain number of deaths The remaining individuals still in the study at the point of stopping are right censored Censored data are usually denoted by a '+' superscript. Interval censoring is grouped data, random censoring","title":"Censoring"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#truncation","text":"Truncation refers to when the sample does not contain certain observations outside of some range. This is not to be confused with its mathematical definition of rounding. For instance, consider a study that wishes to study the mortality for people aged 50. However, only people aged 52 are able to be found. Naturally, then the times to death from age 50 for these individuals will always be at least 2 ; it will never be smaller than 2 . Right Truncated - No observations with values larger than cutoff Left Truncated - No observations with values smaller than cutoff Naturally, since time to death is an increasing function, the only Left Truncated data is relevant in an actuarial context Truncation and Censoring are often confused with one another as both result in observations within a given range. Truncation is when observations outside the range are not recorded at all while Censoring still records the observations , with the caveat that it is a floor/ceiling.","title":"Truncation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#kaplan-meier-estimation","text":"Consider a study of 5 individuals: One individual dies at \\(t=0.8\\) and \\(t=1.4\\) 5 more individuals join the study at \\(t=1\\) ; Left Truncated at \\(t=1\\) 3 individual leave the study at \\(t=0.5\\) ; Right Censored at \\(t=0.5\\) The information is more often concisely portrayed in a table format: \\(t\\) represents the time of death \\(d\\) represents the number of deaths \\(r\\) represents the risk set RIGHT BEFORE death; simply known as the sample size This is problematic as the sample size changes throughout the study. However, the Kaplan Meier method accounts for this by calculating the survival probability as the product of the survival probabilities for all previous intervals between deaths : \\[ {}_{t}\\hat{p}_x = \\prod \\left(1-\\frac{d_j}{r_j} \\right) \\] Given the above example, the resulting Kaplan Meier estimation would be: \\[ \\begin{aligned} {}_{1.4}\\hat{p}_0 &= P(t > 0.8) * P(t > 1.4 | x > 0.8) \\\\ &= \\left (1 - \\frac{1}{2} \\right) * \\left (1 - \\frac{1}{6} \\right) \\\\ &= \\left (\\frac{1}{2} \\right) * \\left (\\frac{5}{6} \\right) \\\\ &= \\frac{5}{12} \\end{aligned} \\] This results in a constant probability between deaths . Thus, the focus is on the times of death ; the entries and exits from the study are only used to change the sample size.","title":"Kaplan Meier Estimation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#nelson-aelen-estimation","text":"Recall that the survival probability can be expressed as a function of the Force of Mortality: \\[ {}_{t}\\hat{p}_x = e^{- \\int^t_0 \\mu_{x+t}} \\] The Nelson Aelen method estimates the Cumulative Force of Mortality (the exponent of the expression), which can be then used to compute the survival probability. It is an extremely common mistake to take the result of the Nelson Aelen estimation as the survival probability. The nelson aelen estimator is computed as the sum of the death probabilities for all previous intervals between deaths : \\[ \\int^t_0 \\mu_{x+t} = \\sum \\frac{d_j}{r_j} \\] Given the previous example, the resulting Nelson Aelen estimation would be: \\[ \\begin{aligned} \\int^t_0 \\mu_{x+1.4} &= P(t < 0.8) + P(t < 1.4 | x > 0.8) \\\\ &= \\left (\\frac{1}{2} \\right) + \\left (\\frac{1}{6} \\right) \\\\ &= \\frac{2}{3} \\\\ \\\\ {}_{1.4}\\hat{p}_x &= e^{-\\frac{2}{3}} \\\\ &= 0.51171 \\end{aligned} \\] Notice that the two values are not too far off from one another. Kaplan Meier Nelson Aelen Estimates Survival Probability Estimates Cumulative Force of Mortality Uses Survival probabilities Uses Death probabilities","title":"Nelson Aelen Estimation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#maximum-likelihood-estimation","text":"Unlike the Nelson Aelen method, the MLE DIRECTLY estimates the force of mortality , which can be then used to compute survival probabilities. One key concept is the idea of Central Exposed to Risk ( \\(E^c_x\\) ), which is a measure of how long the individual has been alive in the study at the time. Consider the following example to estimate the force of mortality at age 51 : Firstly, we need to determine if and when the individuals turn 51: The date they turn 51 is used as the starting point for the \\(E^c_x\\) . The ending point is based on the year that they turn 51 and is one of the following: Date of death - Died during the year Date of withdrawal - Left during the year Date they turn 52 - Survived the entire year From this, \\(E^c_x\\) is calculated in years : Using the CER, the force of mortality can be determined: \\[ \\hat{\\mu}_x = \\frac{d_x}{E^c_x} \\] Assuming that this force is constant , the resulting survival probability is calculated as: \\[ {}_{t}\\hat{p}_x = e^{-\\hat{\\mu}_x} \\]","title":"Maximum Likelihood Estimation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#statistical-inference","text":"Given that all the above methods are estimates of the survival probability, the estimated values will change depending on the underlying sample. Thus, it is important to consider the variance of the estimator as well across various different samples.","title":"Statistical Inference"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#complete-variance","text":"Each individual is either dead or or alive - thus, the number of people who survive past the specified time can be modelled using a Bernoulli Random variable . Recall that the variance of the bernoulli distribution is: \\[ Var(\\text{# of individuals survive past t}) = n \\cdot {}_{t}p_x \\cdot (1-{}_{t}p_x) \\] Thus, the variance of the \\[ \\begin{aligned} Var({}_{t}\\hat{p}_x) &= Var \\left(\\frac{\\text{# of individuals survive past k}}{n} \\right) \\\\ &= \\frac{1}{n^2} Var(\\text{# of individuals survive past t}) \\\\ &= \\frac{1}{n^2} n \\cdot {}_{t}p_x \\cdot (1-{}_{t}p_x) \\\\ &= \\frac{{}_{t}p_x \\cdot (1-{}_{t}p_x)}{n} \\\\ \\end{aligned} \\] Since the actual survival probability is not known, it is often substituted with the estimate: \\[ Var({}_{t}\\hat{p}_x) = \\frac{{}_{t}\\hat{p}_x \\cdot (1-{}_{t}\\hat{p}_x)}{n} \\] Note that the variance of the estimated death probability is identical to that of the survival probability: \\[ \\begin{aligned} Var({}_{t}\\hat{q}_x) &= Var(1 - {}_{t}\\hat{p}_x) \\\\ &= Var({}_{t}\\hat{p}_x) \\\\ \\end{aligned} \\]","title":"Complete Variance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#incomplete-variance","text":"Due to the complexity of calculating the variance under these methods, the formula for variances are provided in the formula sheets. Kaplan Meier Variance : \\[ Var({}_{t}\\hat{p}_x) = ({}_{t}\\hat{p}_x)^2 \\sum \\frac{d_j}{r_j (r_j - d_j)} \\] Nelson Aelen Variance : \\[ Var({}_{t}\\hat{p}_x) = ({}_{t}\\hat{p}_x)^2 \\sum \\frac{d_j (r_j - d_j)}{r^3_j} \\] Maximum Likelihood Variance : \\[ Var({}_{t}\\hat{p}_x) = ({}_{t}\\hat{p}_x)^2 \\cdot \\frac{d_x}{\\left( E^c_x \\right)^2} \\]","title":"Incomplete Variance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#confidence-intervals","text":"Linear Log Transformed","title":"Confidence Intervals"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/","text":"Review of Probability Theory \u00b6 Basic Probability \u00b6 Probability is the study of Experiments whose results cannot be predicted with certainty. The result of such an experiment is known as its Outcome . The Sample Space \\(\\left(\\Omega \\right)\\) is the set of ALL possible outcomes from an experiment. The Event Space \\((E)\\) is a subset of the sample space, representing only the outcomes that we are interested in studying. Conversely, its Complement \\((E^c)\\) is the set of all OTHER outcomes not inside \\(E\\) . The probability of the event occuring is the ratio of the number of elements in the event to the sample space. It is a measure of the chance that the outcome of the experiment is inside the event space. \\[ P(E) = \\frac{n(E)}{n\\left(\\Omega \\right)} \\] Consider the probability of rolling an odd number on a standard dice: Experiment - Rolling a dice Outcome - The number showed on the dice Sample Space - \\({1, 2, 3, 4, 5, 6}\\) Event Space - \\({1, 3, 5}\\) Complement - \\({2, 4, 6}\\) Probability of Event - \\(\\frac{3}{6}\\) Probability of Complement - \\(\\frac{3}{6}\\) Within the same experiment, there may be multiple events of interest. For any two events A and B, its Union \\((A \\cup B)\\) is the set with outcomes that are either in A or B while their Intersection \\((A \\cap B)\\) is the set with outcomes that are in BOTH A and B . If both A and B have no outcomes in common \\((A \\cap B = \\emptyset)\\) , then they are said to be Mutually Exclusive . Naturally, an event and its complement are always mutually exclusive. Warning The following seems intuitive , but is actually a common mistake: \\[ (A \\cap B)^c \\ne A^c \\cap B^c \\] This is properly explained through De-morgans Law : It can be easily remembered by applying the complement to all components of the expression, including the intersection/union symbol : \\[ \\begin{aligned} \\cap^c &= \\cup \\\\ \\cup^c &= \\cap \\end{aligned} \\] Probability Axioms \u00b6 Axiom 1 states that all probabilities must be non-negative : \\[ P(E) \\ge 0 \\] Axiom 2 states that probability of the Sample Space is exactly equal to 1: \\[ P(\\Omega) = 1 \\] Axiom 3 states the probability of a union of mutually exclusive events is equal to the sum of their probabilities, known as Countable Additivity . \\[ \\begin{aligned} A \\cap B &= \\emptyset \\\\ P(A \\cup B) &= P(A) + P(B) \\end{aligned} \\] Based on these axioms, several other important properties can also be deduced: Monoticity : \\(A \\subset B \\rightarrow P(A) \\le P(B)\\) Empty Set : \\(P(\\emptyset) = 0\\) Complement Rule : \\(P(E^c) = 1 - P(E)\\) Numeric Bound : \\(0 \\le P(E) \\le 1\\) Sum Rule : \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) Conditional Probability \u00b6 Conditional Probabilities are denoted by \\(P(A \\mid B)\\) , which is the probability of event A occuring given that event B has already occurred . The intuition is best understood by considering the following - Given that event B has already occured, what is the probability that event A also occurs? The event space is \\(A \\cap B\\) , as we are interested in the probability that both A and B occur. However, since event B has already occured, the sample space is no longer all possible outcomes but rather the event space for B! \\[ \\begin{aligned} P(A \\mid B) &= \\frac{n(A \\cap B)}{n(B)} \\\\ &= \\frac{\\frac{n(A \\cap B)}{n(\\Omega)}}{\\frac{n(B)}{n(\\Omega)}} \\\\ &= \\frac{P(A \\cap B)}{P(B)} \\end{aligned} \\] Following this expression, the probability of an intersection of two events is given by: \\[ P(A \\cap B) = P(A \\mid B) \\cdot P(B) = P(B \\mid A) \\cdot P(A) \\] Most experiments involving conditional probabilities are multi-staged experiments, which are best visualized using Probability Trees : Instead of calculating conditional probabilities from scratch, some questions provide the conditional probability \\(P(A \\mid B)\\) (or the components to do so!) and ask us to find the reverse - \\(P(B \\mid A)\\) . \\[ P(B \\mid A) = \\frac{P(B \\cap A)}{P(A)} \\] The formula is the same as before, but the issue is that the unconditional probability of event A is usually not given. This problem is accounted for in Bayes Theorem : \\[ \\begin{aligned} A &= (A \\cap B) + (A \\cap B^c) \\\\ P(A) &= P(A \\cap B) + P(A \\cap B^c) \\\\ P(A) &= P(A \\mid B) \\cdot P(B) + P(A \\mid B^c) \\cdot P(B^c) \\\\ \\\\ \\therefore P(B \\mid A) &= \\frac{P(A \\mid B) \\cdot P(B)}{P(A \\mid B) \\cdot P(B) + P(A \\mid B^c) \\cdot P(B^c)} \\end{aligned} \\] Note that if the Conditional Probability of A given B is the same as the unconditional probability of A, then events A and B are independent ; B has no effect on A. Thus, the probability of an intersection of two independent events is simply their product: \\[ \\begin{aligned} P(A \\mid B) &= \\frac{P(A \\cap B)}{P(B)} \\\\ P(A) &= \\frac{P(A \\cap B)}{P(B)} \\\\ P(A \\cap B) &= P(A) \\cdot P(B) \\end{aligned} \\] Random Variables \u00b6 Unlike rolling a dice, the outcome of most experiments are non-numeric , which makes them hard to work with. For instance, the outcomes of a coin toss are \"Heads\" and \"Tails\". A Random Variable is a many to one function that maps each outcome to a single real number. Each outcome must have only one corresponding number, but different outcomes can have the same value. Note Although the mapping is deterministic, the underlying experiment is still random which is why it is still a \"random\" variable. The range of possible values that the random variable can take is known as its Support . They are broadly categorized based on its support: Discrete Continuous Countable Support Uncountable Support 1, 2, 3, 4, ... 1, 1.1, 1.01, 1.001, ... Random variables are denoted using upper case letters (X, Y, Z) while their corresponding values are denoted using lower case letters (x, y, z) and their appropriate subscripts . The notation \\(X(s) = x_1\\) denotes that the random variable \\(X\\) maps the outcome \\(s\\) to the value of \\(x_1\\) . Thus, the corresponding probability is denoted by \\(P(X = x_1)\\) . Note If the subscript is omitted, then \\(P(X=x)\\) is a general expression that describes the entire distribution of \\(X\\) , not just a single probability. Probability Distributions \u00b6 Similar to how a random variable maps the outcomes to a real number, a Probability Distribution is a function that maps the outcomes to its probability of occurrence . For Discrete Random Variables , their distribution is described using a Probability Mass Function (PMF). The PMF provides the probability that the random variable is exactly equal to some value \\((X = x_1)\\) . It is typically denoted in lower case and sometimes includes a subscript of the random variable when working with multiple to distinguish them from one another. \\[ \\begin{aligned} P(X = a) &= p(a) \\\\ \\\\ P(X = a) &= p_X(a) \\\\ P(Y = a) &= p_Y(a) \\end{aligned} \\] Since it is a probability measure, the sum of the PMF over the support of the random variable must be equal to 1 (Probability Axiom). \\[ \\sum_{x \\in \\text{Support}} p(x) = 1 \\] PMFs can be represented in three main ways - Functions, Tables or Histograms. For Continuous Random Variables , their distribution is described using a Probability Density Function (PDF). The PDF is a non-negative function where the area under it provides the probability that the random variable takes on some range of values \\((a \\le X \\le b)\\) . Similarly, it is typically denoted in lower case and includes a subscript when working with multiple random variables: \\[ \\begin{aligned} P(a \\le X \\le b) = \\int^b_a f(x) \\\\ \\\\ P(a \\le X \\le b) = \\int^b_a f_X(x) \\\\ P(a \\le Y \\le b) = \\int^b_a f_Y(y) \\end{aligned} \\] Similarly, since the area is a probability measure, the total area under the graph must be equal to 1 : \\[ P(-\\infty \\le X \\le \\infty) = \\int^{\\infty}_{-\\infty} f(x) = 1 \\] Note \\(\\infty\\) is used as a catch all for the upper and lower bound of the random variable. If the actual bounds are known, then using them instead is more appropriate. Additionally, note that the probability of a specific value for a continuous RV is 0. This is because there is an infinite number of possible values , thus the probability of a specific value (EG. 1.45679383920) is infinitely small such that it is assumed to be 0. \\[ P(X = a) = \\int^{a}_{a} f(x) = 0 \\] The Cumulative Distribution Function (CDF) is the probability that the random variable is less than or equal to some value \\(X \\le t\\) . It is typically denoted in upper case to distinguish it from the PDF and includes subscripts as well when working with multiple random variables. \\[ \\begin{aligned} F(t) &= P(X < t) \\\\ \\\\ F_X(t) &= P(X < t) \\\\ F_Y(t) &= P(Y < t) \\end{aligned} \\] For discrete variables, the CDF is the sum of all probabilities before the specified value . Following this, the difference of consecutive CDFs allows us to obtain the PMFs at that value: \\[ \\begin{aligned} F(t) &= \\sum_{x \\le t} p(x) \\\\ p(x_i) &= F(x_i) - F(x_{i-1}) \\end{aligned} \\] For continuous variables, the CDF is the integral from the lower bound to the specified value . However, instead of integrating with respect to an actual value, it is better to integrate with respect to a dummy variable \\(t\\) to obtain a general expression for the CDF , allowing it to be easily calculated for any value. Although the CDF is very useful, it can only be used to calculate probabilities starting from the lower bound . When probabilities starting from other ranges are needed, the PDF can be obtained from the CDF by differentiating it and then re-integrating with different limits. \\[ \\begin{aligned} F(t) &= \\int^{t}_{-\\infty} f(x) dx \\\\ \\\\ F(t) &= \\int^{t}_{-\\infty} f(t) dt \\\\ f(t) &= F'(t) \\end{aligned} \\] Note The integral of the PDF can lead also lead to the Survival Function , as shown in the Survival Model Section . Moments \u00b6 The Moments of a distribution are quantities that describe characteristics of the distribution . Raw Moments are calculated with respect to the origin . The n-th raw moment is calculated as the following: \\[ \\begin{aligned} E(X^n) &= \\int x^n \\cdot f(x) dx \\\\ \\mu'_k &= \\sum x^n \\cdot p(x) \\end{aligned} \\] The first raw moment is known as the Mean , which is a measure of the Centrality of the distribution. It is commonly denoted as \\(\\mu\\) , without any super or subscripts. Note that the mean of a constant is the constant itself : \\[ E(c) = c \\] Central Moments are calculated with respect to the mean . The n-th central moment is calculated as the following: \\[ \\begin{aligned} E[(X - \\mu)^n] &= \\int (x - \\mu)^n \\cdot f(x) dx \\\\ \\mu_k&= \\sum (x - \\mu)^n \\cdot p(x) \\end{aligned} \\] The second central moment is known as the Variance , which is a measure of the Spread of the distribution about the mean. Since calculating central moments directly is complicated, it can be simplified to an expression involving raw moments: \\[ \\begin{aligned} Var(X) &= E[(X - \\mu)^2] \\\\ &= E(X^2 - 2\\mu X + \\mu^2) \\\\ &= E(X^2) - 2\\mu^2 + \\mu^2 \\\\ &= E(X^2) - \\mu^2 \\\\ &= E(X^2) - [E(X)]^2 \\end{aligned} \\] Note that the variance of a constant is 0 as a constant cannot change: \\[ Var(c) = 0 \\] However, one problem with variance is that it uses squared units, which makes it hard to intepret. Thus, the squareroot of the variance is used instead, known as the Standard Deviation . \\[ \\sigma = \\sqrt{Var(X)} \\] Similarly, standard deviation cannot be used to compare data with different units . Thus, the Coefficient of Variation is used instead, which is a unitless measure of the spread of the distribution. \\[ CV(X) = \\frac{\\sigma}{\\mu} \\] The third central moment is Skewness , which is a measure of the symmetry of distribution about the mean. Being left/right skewed means that the distribution has a \"longer tail\" on that side, which implies that values on the opposite side are more likely to occur . Tip Skewness is also sometimes referred to as being Positively or Negatively Skewed . An easy way to remember is that positive values occur to the right of the origin, hence is the same as being right skewed; vice-versa. \\[ \\begin{aligned} \\text{Skewness} &= \\frac{E[(X - \\mu)^3]}{\\sigma^3} \\\\ &= \\frac{E[(X - \\mu)^3]}{(\\sigma^2)^\\frac{3}{2}} \\\\ &= \\frac{E(X^3) - 3 E(X^2) \\cdot E(X) + 2 [E(X)]^3}{(E(X^2) - [E(X)]^2)^\\frac{3}{2}} \\end{aligned} \\] The fourth central moment is Kurtosis , which is a measure of the flatness of the distribution, typically with respect to the normal distribution. It is indicative of the likelihood of producing extreme values (outliers). The normal distribution has a kurtosis of 3. If a distribution has a kurtosis greater than 3, then it is flatter and hence more likely to produce outliers as compared to the normal distribution. \\[ \\begin{aligned} \\text{Kurtosis} &= \\frac{E[(X - \\mu)^4]}{\\sigma^4} \\\\ &= \\frac{E[(X - \\mu)^4]}{(\\sigma^2)^2} \\\\ &= \\frac{E(X^4) - 4 E(X^3) \\cdot E(X) + 6 E(X^2) \\cdot [E(X)]^2 - 3 [E(X)]^4}{(E(X^2) - [E(X)]^2)^2} \\end{aligned} \\] Statistical Metrics \u00b6 Apart from the moment of the distribution, there are some other Statistical Metrics that provide useful information. The Median is the value of the random variable that seperates the upper and lower half of the probability distribution. For discrete variables, the median \\(M\\) is the smallest value such that \\(P(X \\le M) \\ge 0.5\\) and \\(P(X \\ge M) \\ge 0.5\\) . For continuous variables, the median \\(M\\) is found by solving \\(F(M) = 0.5\\) . The key difference is that the continuous median is the value that exactly seperates the distribution while the discrete one approximately splits it, depending on the PMF. The Mode is the value of the random variable that maximises the PMF or PDF. It is the most likely outcome of the experiment (loosely speaking for continuous variables). The Percentile is the value of the random variable below which a certain percentage of observations fall . For instance, the 85 th percentile is the value below which 85% of the observations fall. Let \\(p\\) be the percentage of observations. The 100p-th percentile for a discrete variable is the smallest value \\(a\\) such that \\(P(X \\lt a) \\le p \\lt P(X \\le a)\\) . Similar as before, for continuous variables, the percentile is found by solving \\(F(a) = p\\) . Info 100p-th looks strange because \\(p\\) is a percentage . For instance, if \\(p\\) is 0.85, then 100p is 85, representing the 85-th percentile. Also, the methods for Percentiles and Medians look similar because the median is simply the 50 th percentile ! The 25 th , 50 th & 75 th percentile are known as the first, second & third Quartiles \\((q)\\) respectively. The difference between the 3 rd and 1 st quartile is known as the Inter Quartile Range . \\[ IQR = q_3 - q_1 \\] Shifting, Scaling & Transformation \u00b6 An existing random variable \\(X\\) can be adjusted in order to make a new random variable \\(Y\\) . If a constant \\(a\\) has been multiplied to the random variable, then it has been Scaled by \\(a\\) : \\[ \\begin{aligned} Y &= aX \\\\ \\\\ F_Y(y) &= P(Y \\le y) \\\\ &= P(aX \\le y) \\\\ &= P \\left(X \\le \\frac{y}{a} \\right) \\\\ &= F_X \\left(\\frac{y}{a} \\right) \\\\ \\\\ f_Y(y) &= \\frac{d}{dy} F_Y(y) \\\\ &= \\frac{1}{a} \\cdot f_X \\left(\\frac{y}{a} \\right) \\end{aligned} \\] If a constant \\(b\\) is added to the random variable instead, then it has been Shifted by \\(b\\) : \\[ \\begin{aligned} Y &= X + b \\\\ \\\\ F_Y(y) &= P(Y \\le y) \\\\ &= P(X + b \\le y) \\\\ &= P(X \\le y - b) \\\\ &= F_X (y - b) \\\\ \\\\ f_Y(y) &= \\frac{d}{dy} F_Y(y) \\\\ &= f_X (y - b) \\end{aligned} \\] For both scaling and transformation, the expectation and variance can be easily determined if that of the original is known as well: \\[ \\begin{aligned} E(cX) &= c \\cdot E(X) \\\\ E(X+c) &= E(X) + E(c) \\\\ &= E(X) + c \\\\ Var(cX) &= c^2 \\cdot Var(x) \\\\ Var(X+c) &= Var(X) + Var(c) \\\\ &= Var(X) \\end{aligned} \\] If the random variable has been raised by a power of \\(\\frac{1}{c}\\) where \\(c \\ne 1\\) , then it has been Power Transformed : \\[ \\begin{aligned} Y &= X^{\\frac{1}{c}} \\\\ \\\\ F_Y(y) &= P(Y \\le y) \\\\ &= P(X^{\\frac{1}{c}} \\le y) \\\\ &= P(X \\le y^c) \\\\ &= F_X (y^c) \\\\ \\\\ f_Y(y) &= \\frac{d}{dy} F_Y(y) \\\\ &= cy^{c-1}\\cdot f_X (y - b) \\end{aligned} \\] If the random variable has been exponentiated , then it has also been Exponential Transformed : \\[ \\begin{aligned} Y &= e^X \\\\ \\\\ F_Y(y) &= P(Y \\le y) \\\\ &= P(e^X \\le y) \\\\ &= P(X \\le \\ln y) \\\\ &= F_X (\\ln y) \\\\ \\\\ f_Y(y) &= \\frac{d}{dy} F_Y(y) \\\\ &= \\frac{1}{y} \\cdot f_X (\\ln y) \\end{aligned} \\] For both types of transformations, there is no simple method of determining the mean and variance. The various raw moments must be manually determined via integration . Joint Distributions \u00b6 Consider a scenario where there are two or more random variables \\((X_1, X_2, \\dots)\\) in the same probability space such that we are interested in some function of them \\(Y = g(x_1, x_2, \\dots)\\) . Note For the purposes of this exam, only Linear Combinations (sums) of random variables will be considered. For instance, the experiment could be studying the sum of rolling two dice . There would be two random variables \\((X_1, X_2)\\) , each denoting the value of their respective dice. Thus, the sum of the two dice would be \\(Y = X_1 + X_2\\) . Consider the probability of obtaining a sum of 3 - consider ALL combination of values of the two underlying variables will result in it: \\[ P(Y = 3) = P(X_1 = 1, X_2 = 2) + P(X_1 = 2, X_2 = 1) \\] This can be more generally expressed as the Joint Distribution of the two variables: \\[ \\begin{aligned} P(X = x, Y = y) &= P_{X,Y} (x,y) \\\\ P(a \\lt X \\lt b, c \\lt Y \\lt d) &= \\int^{d}_{c} \\int^{b}_{a} f_{X,Y} (x,y) \\ dx dy \\end{aligned} \\] The individual distributions within this shared probability space is known as the Marginal Distribution and is obtained by \"integrating out\" the other variable: \\[ \\begin{aligned} f_{X}(x) &= \\int^{d}_{c} f_{X,Y} (x,y) \\ dy \\\\ f_{Y}(y) &= \\int^{b}_{a} f_{X,Y} (x,y) \\ dx \\end{aligned} \\] Joint Domain \u00b6 One of the main concerns when dealing with joint distributions is the domain of the distribution. Each random variable is defined on their own domain, but the joint distribution is only defined in the domain in which two variables coincide . For a joint distribution of discrete variables , it is simply a matter of finding all possible combinations of the two variables. For a two variable distribution, this can be easily visualized via a Contingency Table : The table sometimes lists the frequency of observations while sometimes it lists their probability . In either case, by finding the cells of interest , the probability can be obtained. For a joint distribution of continuous variables , it is best visualized graphically . For a two variable distribution: \\[ 1 \\lt X_1 \\lt 5, \\ 2 \\lt X_2 \\lt 8 \\] Let \\(Y\\) be the sum of the two random variables. Consider the following probability: \\[ \\begin{aligned} Y &= X_1 + X_2 \\\\ P(Y \\lt 5) &= P(X_1 + X_2 \\lt 5) \\\\ &= P(X_2 \\lt 5 - X_1) \\end{aligned} \\] Thus, we need to find the domain of the above inequality WITHIN the joint domain . This can be done by plotting the graph of the upper/lower bound (simply change the inequality to an equal sign). The domain can be intepreted in one of two ways: \\(X_1\\) to line first: \\(1 \\lt X_1 \\lt 3, \\ 2 \\lt y \\lt 5-x\\) \\(X_2\\) to line first: \\(1 \\lt X_1 \\lt 5-y, \\ 2 \\lt y \\lt 4\\) Using these domains, the double integration can be performed to obtain the probability: \\[ \\begin{aligned} P(X_2 \\lt 5 - X_1) &= \\int^{1}_{3} \\int^{5-x}_{2} f_{X,Y}(x,y) \\ dy dx \\\\ &= \\int^{2}_{4} \\int^{5-y}_{1} f_{X,Y}(x,y) \\ dx dy \\end{aligned} \\] Tip Recall that when performing a double integration, the INNER integral is evaluated first . The inner and outer integral are interchangeable, thus set the inner integral such that it will ease the integration . In this case, the integral with an algebraic expression should always be evaluated first, ensuring that the final result is a probability and not an expression. If there is a Kink in the shape, then the area should be broken into two smaller areas without any kinks in their shape. The overall probability is the sum of the areas: \\[ P(X_2 \\lt 8 - X_1) = \\text{Orange Area} + \\text{Purple Area} \\] Joint Moments \u00b6 Given that \\(Y = X_1 + X_2\\) , its mean and variance is the following: \\[ \\begin{aligned} E(Y) &= E(X_1) + E(X_2) \\\\ \\\\ Var (Y) &= Var (X_1) + Var (X_2) + 2 \\cdot Cov (X,Y) \\end{aligned} \\] The last term is known as the Covariance , which is the first central moment about of the joint distribution. It measures the linear relationship between variables: Positive Covariance : Both variables move in the same direction Negative Covariance : Both variables move in opposite directions \\[ \\begin{aligned} Cov (X_1, X_2) &= E[(X_1 - E(X_1))(X_2 - E(X_2))] \\\\ &= E(X_1 \\cdot X_2) - E(X_1) \\cdot E(X_2) \\\\ \\\\ E(X_1 \\cdot X_2) &= \\int \\int (x_1 \\cdot x_2) \\cdot f_{X_1, X_2} (x_1, x_2) \\ dx_1 dx_2 \\end{aligned} \\] Note that it has some interesting properties: \\(Cov (X_1, c) = 0\\) \\(Cov (X_1, X_2) = Var (X_1)\\) \\(Cov (c \\cdot X_1, X_2) = c \\cdot Cov (X_1,X_2)\\) \\(Cov (X_1 + c, X_2) = Cov (X_1, X_2) + Cov (c, X_2)\\) However, covariance is both limitless and squared units , which makes hard to use as a metric for comparison. We can overcome these problems by scaling the covariance down by the individual standard deviations, obtaining the Correlation of the two variables: \\[ Corr (X_1, X_2) = \\frac{Cov (X_1, X_2)}{SD(X_1), SD(X_2)} \\] If \\(X_1\\) and \\(X_2\\) are Independent , then their distributions and moments become the following: \\[ \\begin{aligned} P(X_1 = x_1, X_2 = x_2) &= P(X_1 = x_1) \\cdot P(X_2 = x_2) \\\\ f_{X_1,X_2} (x_1,x_2) &= f_{X_1}(x_1) \\cdot f_{X_2}(x_2) \\\\ \\\\ E(X_1 \\cdot X_2) &= E(X_1) \\cdot E(X_2) \\\\ Var (X_1 + X_2) &= Var (X_1) + Var (X_2) \\end{aligned} \\] Warning If the domain of the random variables naturally contain other variables, then they are not independent, even if the above conditions are met. Note that having 0 covariance is a consequence of independence , but is NOT indicative of it. Variables may also have 0 covariance when they have a non-linear relationship . \\[ \\begin{aligned} Cov (X_1, X_2) &= E(X_1 \\cdot X_2) - E(X_1) \\cdot E(X_2) \\\\ &= E(X_1) \\cdot E(X_2) - E(X_1) \\cdot E(X_2) \\\\ &= 0 \\\\ \\\\ \\therefore Corr (X_1, X_2) &= 0 \\end{aligned} \\] Generating Functions \u00b6 A useful way to analyze the sum of independent random variables is to convert the PDF/PMF of the individual distributions into a Generating Function . The first kind is known as a Moment Generating Function (MGF): \\[ \\begin{aligned} M_{X}(t) &= E(e^{tX}) \\\\ &= \\sum e^{tx} \\cdot p_{X}(x) \\\\ &= \\int e^{tx} \\cdot f_{X}(x) \\end{aligned} \\] As its name suggests, the MGF can be used to calculate the raw moments of the distribution. To obtain the n-th raw moment , Differentiate the MGF k times Evaluate the expression at \\(t=0\\) \\[ E \\left(X^n \\right) = \\frac{d^n}{dt^n} \\cdot M_{X}(0) \\] However, the main benefit of MGFs is that they uniquely identify a distribution. If two random variables have the same MGF , then they have the same distribution . This becomes especially useful when dealing with sums of independent random variables . By determining the MGF of the combination, its exact distribution can be determined. \\[ \\begin{aligned} Y &= X_1 + X_2 + ... + X_n \\\\ \\\\ M_Y(t) &= E(e^{tY}) \\\\ &= E(e^{t(X_1 + X_2 + ... + X_n)}) \\\\ &= E(e^{tX_1} \\cdot e^{tX_2} \\cdot ... \\cdot e^{tX_n}) \\\\ &= \\prod E(e^{tx}) \\\\ &= \\prod M_{X}(t) \\end{aligned} \\] The other kind is known as a Probability Generating Function (PGF), which only applies for sums of discrete variables : \\[ \\begin{aligned} P_{X}(t) &= E(t^x) \\\\ &= \\sum t^X \\cdot p(x) \\end{aligned} \\] Note The PGF is denoted in upper case \\(P\\) while the PMF is denoted in lower case \\(p\\) . As its name suggests, the PGF can be used to calculate the individual probabilities of the distribution. To obtain the probability of the n-th value , Differentiate the PGF k times Divide the expression by n factorial Evaluate the expression at \\(t=0\\) \\[ P(X = k) = \\frac{d^n}{dt^n} \\cdot \\frac{P_{X}(0)}{n!} \\] Similarly, the PGF uniquely identifies the distribution and can be used in all the same ways as an MGF in this regard: \\[ \\begin{aligned} Y &= X_1 + X_2 + ... + X_n\\\\ \\\\ P_Y(t) &= E(t^Y) \\\\ &= E(t^{X_1 + X_2 + ... + X_n}) \\\\ &= E(t^{X_1} \\cdot t^{X_2} \\cdot ... \\cdot t^{X_n}) \\\\ &= \\prod E(t^{x}) \\\\ &= \\prod P_{X}(t) \\end{aligned} \\] Given how similar the two are, they can be easily converted to and from one another: \\[ \\begin{aligned} P_X(t) &= E(t^X) \\\\ &= E(e^{\\ln t^X}) \\\\ &= M_X(\\ln t^x) \\\\ \\\\ M_X(t) &= E(e^tX) \\\\ &= E[(e^t)^x] \\\\ &= P_X(e^t) \\end{aligned} \\] Conditional Distributions \u00b6 If a random variable \\(X\\) is conditioned on a range of values of itself , then it has the Conditional Distribution \\((X \\mid X>j)\\) : \\[ \\begin{aligned} P(X = x \\mid X > j) &= \\frac{P(X = x)}{P(X > j)} \\\\ f_{X \\mid X>j} (x) &= \\frac{f_X(x)}{P(X > j)} \\end{aligned} \\] Similarly, a random variable can be conditional on ANOTHER random variable , resulting in the Conditional Distribution \\((X \\mid Y)\\) . \\[ \\begin{aligned} P(X = x \\mid Y = y) &= \\frac{P(X = x, Y = y)}{P(Y = y)} \\\\ f_{X \\mid Y} (x,y) &= \\frac{f_{X,Y}(x,y)}{f_{Y}(y)} \\\\ \\text{Conditional} &= \\frac{\\text{Joint}}{\\text{Marginal}} \\end{aligned} \\] Any calculations involving the above distributions must be solved via first principles . However, most problems require us to find the Marginal Distribution given only the conditional distributions: \\(X\\) is the random variable denoting the test grades (A, B, C) \\(Y\\) is the random variable denoting the gender (M, F) The teacher would like to find the marginal distribution of test scores \\((X)\\) , but only has the conditional distribution of the scores of the students for each gender \\((X \\mid Y)\\) and the proportion of the Genders \\((Y)\\) . The Law of Total Probability can be used to determine the marginal probability of \\(X\\) : \\[ \\begin{aligned} P(X = a) &= E_Y[P(X = a \\mid Y)] \\\\ &= \\sum_{y} P(X = a \\mid y) \\cdot p(Y = y) \\end{aligned} \\] Note that this is equivalent to adding up the final probabilities from the relevant branches from a probability tree: Naturally, this also means that the marginal CDF can be obtained in similar fashion: \\[ \\begin{aligned} F_X(a) &= E_Y[F_{X \\mid Y}(a)] \\\\ &= \\sum_{y} F_{X \\mid Y}(a) \\cdot p(Y = y) \\end{aligned} \\] Following the same logic, the Law of Total Expectation can be used to determine the marginal expectation of \\(x\\) : \\[ \\begin{aligned} E(X) &= E_Y[E_X(X \\mid Y)] \\\\ &= \\sum_{y} E_X(X \\mid Y) \\cdot p(Y = y) \\end{aligned} \\] Warning Since \\(E(X)\\) is a constant, a common mistake is thinking that \\(E_X(X \\mid Y)\\) is a constant as well since they are both expectations. The issue is that it is conditional on \\(Y\\) , which is still random, which makes the conditional expectation still random . The Law of Total Variance can be used to determine the marginal variance of \\(x\\) . However, unlike the previous two, it is NOT simply the expectation of the conditional variance: \\[ \\begin{aligned} Var (X) &= E_Y [Var_X(X \\mid Y)] + Var_Y[E_X(X \\mid Y)] \\\\ \\\\ E_Y [Var_X(X \\mid Y)] &= \\sum_{y} Var_X(X \\mid Y) \\cdot p(Y = y) \\\\ \\\\ Var_Y[E_X(X \\mid Y)] &= E_Y[E_X(X \\mid Y)^2] - (E_Y[E_X(X \\mid Y)])^2 \\\\ &= \\sum_{y} E_X(X \\mid Y)^2 \\cdot p(Y = y) - \\sum_{y} E_X(X \\mid Y) \\cdot p(Y = y) \\end{aligned} \\] Alternatively, the Marginal Variance can be directly calculated using the typical formula of \\(E(X^2) - [E(X)]^2\\) , where the two marginal expectations are calculated using the law of total expectation. Alternatively once more, if the conditional distribution \\(Y\\) only has two outcomes, then the Bernoulli Shortcut (covered in a later section) can be used to quickly compute the value of \\(Var_Y[E_X(X \\mid Y)]\\) . Tip The discrete case was shown in this section due to its simplicity. All the same concepts apply to the continuous variables as well - simply replace the summation & PMFs with integrals and PDFs . Mixture Distributions \u00b6 A mixture distribution is a distribution whose values can be intepreted as being derived from an underlying set of other random variables . In an insurance context, a Homeowners Insurance claim could be from a fire, burglary or liability accident. To model it, we could use a mixture that is made up of the basic distributions used to individually model each type of accident. If the mixture contains a countable number of other distributions, then it is known as a Discrete Mixture . Otherwise, it is known as a Continuous Mixture . Warning It is a common mistake to think that a discrete mixture is only made up of discrete distributions, vice-versa. Any type of distribution can be included in a mixture; the classification is based on the number of distributions. The random variable \\(X\\) is a k-point mixture if its probability functions can be expressed as the weighted average of the probability functions of the \\(k\\) distributions \\(X_1, X_2, ... X_k\\) : \\[ \\begin{aligned} F_X(x) &= w_1 \\cdot F_{X_1}(x) + w_2 \\cdot F_{X_2}(x) + ... + w_k \\cdot F_{X_k}(x) \\\\ f_X(x) &= w_1 \\cdot f_{X_1}(x) + w_2 \\cdot f_{X_2}(x) + ... + w_k \\cdot f_{X_k}(x) \\end{aligned} \\] Note For this exam, questions will usually only use 2 or 3 point mixtures . \\(w\\) represents the mixing weights , such that \\(w_1 + w_2 + ... + w_k = 1\\) . It can be intepreted that \\(Y\\) follows the distribution of \\(X_1\\) \\(100w_1 \\%\\) of the time, follows the distribution of \\(X_2\\) \\(100w_2 \\%\\) of the time etc. Warning Another common mistake is confusing mixtures with Linear Combinations of random variables: \\[ X \\ne w_1 X_1 + w_2 X_2 + ... + w_k X_k \\] In a linear combination, \\(X\\) neither follows the distribution of any of the \\(X_k\\) . Furthermore, since \\(w_k\\) are not weights, they can be any real number and do not need to sum to 1 . The mixing weights can also be thought of as Discrete Probabilities that come from a random variable \\(Y\\) representing the \\(k\\) underlying distributions with the support \\(\\set{1, 2, ..., k}\\) where \\(P(Y = k) = w_k\\) . Thus, we can think of the overall mixture \\(X\\) as an unconditional distribution while each of the underlying distributions are conditional distributions \\(X \\mid Y\\) . This allows us to make use of the all the previous results from the conditional distributions: \\[ \\begin{aligned} P(X = a) &= E_Y[P(X = a \\mid Y)] \\\\ F_X(a) &= E_Y[F_{X \\mid Y}(a)] \\\\ E(X) &= E_Y[E_X(X \\mid Y)] \\\\ Var (X) &= E_Y [Var_X(X \\mid Y)] + Var_Y[E_X(X \\mid Y)] \\end{aligned} \\] In terms of the weights, for a simple two point mixture : \\[ \\begin{aligned} P(X = a) &= P(X = a \\mid Y=1) \\cdot w_1 + P(X = a \\mid Y=2) \\cdot w_2 \\\\ F_X(a) &= F_{X \\mid Y=1}(a) \\cdot w_1 + F_{X \\mid Y=2}(a) \\cdot w_2 \\\\ E(X) &= E_X(X \\mid Y=1) \\cdot w_1 + E_X(X \\mid Y=2) \\cdot w_2 \\end{aligned} \\]","title":"Review of Probability Theory"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#review-of-probability-theory","text":"","title":"Review of Probability Theory"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#basic-probability","text":"Probability is the study of Experiments whose results cannot be predicted with certainty. The result of such an experiment is known as its Outcome . The Sample Space \\(\\left(\\Omega \\right)\\) is the set of ALL possible outcomes from an experiment. The Event Space \\((E)\\) is a subset of the sample space, representing only the outcomes that we are interested in studying. Conversely, its Complement \\((E^c)\\) is the set of all OTHER outcomes not inside \\(E\\) . The probability of the event occuring is the ratio of the number of elements in the event to the sample space. It is a measure of the chance that the outcome of the experiment is inside the event space. \\[ P(E) = \\frac{n(E)}{n\\left(\\Omega \\right)} \\] Consider the probability of rolling an odd number on a standard dice: Experiment - Rolling a dice Outcome - The number showed on the dice Sample Space - \\({1, 2, 3, 4, 5, 6}\\) Event Space - \\({1, 3, 5}\\) Complement - \\({2, 4, 6}\\) Probability of Event - \\(\\frac{3}{6}\\) Probability of Complement - \\(\\frac{3}{6}\\) Within the same experiment, there may be multiple events of interest. For any two events A and B, its Union \\((A \\cup B)\\) is the set with outcomes that are either in A or B while their Intersection \\((A \\cap B)\\) is the set with outcomes that are in BOTH A and B . If both A and B have no outcomes in common \\((A \\cap B = \\emptyset)\\) , then they are said to be Mutually Exclusive . Naturally, an event and its complement are always mutually exclusive. Warning The following seems intuitive , but is actually a common mistake: \\[ (A \\cap B)^c \\ne A^c \\cap B^c \\] This is properly explained through De-morgans Law : It can be easily remembered by applying the complement to all components of the expression, including the intersection/union symbol : \\[ \\begin{aligned} \\cap^c &= \\cup \\\\ \\cup^c &= \\cap \\end{aligned} \\]","title":"Basic Probability"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#probability-axioms","text":"Axiom 1 states that all probabilities must be non-negative : \\[ P(E) \\ge 0 \\] Axiom 2 states that probability of the Sample Space is exactly equal to 1: \\[ P(\\Omega) = 1 \\] Axiom 3 states the probability of a union of mutually exclusive events is equal to the sum of their probabilities, known as Countable Additivity . \\[ \\begin{aligned} A \\cap B &= \\emptyset \\\\ P(A \\cup B) &= P(A) + P(B) \\end{aligned} \\] Based on these axioms, several other important properties can also be deduced: Monoticity : \\(A \\subset B \\rightarrow P(A) \\le P(B)\\) Empty Set : \\(P(\\emptyset) = 0\\) Complement Rule : \\(P(E^c) = 1 - P(E)\\) Numeric Bound : \\(0 \\le P(E) \\le 1\\) Sum Rule : \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)","title":"Probability Axioms"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#conditional-probability","text":"Conditional Probabilities are denoted by \\(P(A \\mid B)\\) , which is the probability of event A occuring given that event B has already occurred . The intuition is best understood by considering the following - Given that event B has already occured, what is the probability that event A also occurs? The event space is \\(A \\cap B\\) , as we are interested in the probability that both A and B occur. However, since event B has already occured, the sample space is no longer all possible outcomes but rather the event space for B! \\[ \\begin{aligned} P(A \\mid B) &= \\frac{n(A \\cap B)}{n(B)} \\\\ &= \\frac{\\frac{n(A \\cap B)}{n(\\Omega)}}{\\frac{n(B)}{n(\\Omega)}} \\\\ &= \\frac{P(A \\cap B)}{P(B)} \\end{aligned} \\] Following this expression, the probability of an intersection of two events is given by: \\[ P(A \\cap B) = P(A \\mid B) \\cdot P(B) = P(B \\mid A) \\cdot P(A) \\] Most experiments involving conditional probabilities are multi-staged experiments, which are best visualized using Probability Trees : Instead of calculating conditional probabilities from scratch, some questions provide the conditional probability \\(P(A \\mid B)\\) (or the components to do so!) and ask us to find the reverse - \\(P(B \\mid A)\\) . \\[ P(B \\mid A) = \\frac{P(B \\cap A)}{P(A)} \\] The formula is the same as before, but the issue is that the unconditional probability of event A is usually not given. This problem is accounted for in Bayes Theorem : \\[ \\begin{aligned} A &= (A \\cap B) + (A \\cap B^c) \\\\ P(A) &= P(A \\cap B) + P(A \\cap B^c) \\\\ P(A) &= P(A \\mid B) \\cdot P(B) + P(A \\mid B^c) \\cdot P(B^c) \\\\ \\\\ \\therefore P(B \\mid A) &= \\frac{P(A \\mid B) \\cdot P(B)}{P(A \\mid B) \\cdot P(B) + P(A \\mid B^c) \\cdot P(B^c)} \\end{aligned} \\] Note that if the Conditional Probability of A given B is the same as the unconditional probability of A, then events A and B are independent ; B has no effect on A. Thus, the probability of an intersection of two independent events is simply their product: \\[ \\begin{aligned} P(A \\mid B) &= \\frac{P(A \\cap B)}{P(B)} \\\\ P(A) &= \\frac{P(A \\cap B)}{P(B)} \\\\ P(A \\cap B) &= P(A) \\cdot P(B) \\end{aligned} \\]","title":"Conditional Probability"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#random-variables","text":"Unlike rolling a dice, the outcome of most experiments are non-numeric , which makes them hard to work with. For instance, the outcomes of a coin toss are \"Heads\" and \"Tails\". A Random Variable is a many to one function that maps each outcome to a single real number. Each outcome must have only one corresponding number, but different outcomes can have the same value. Note Although the mapping is deterministic, the underlying experiment is still random which is why it is still a \"random\" variable. The range of possible values that the random variable can take is known as its Support . They are broadly categorized based on its support: Discrete Continuous Countable Support Uncountable Support 1, 2, 3, 4, ... 1, 1.1, 1.01, 1.001, ... Random variables are denoted using upper case letters (X, Y, Z) while their corresponding values are denoted using lower case letters (x, y, z) and their appropriate subscripts . The notation \\(X(s) = x_1\\) denotes that the random variable \\(X\\) maps the outcome \\(s\\) to the value of \\(x_1\\) . Thus, the corresponding probability is denoted by \\(P(X = x_1)\\) . Note If the subscript is omitted, then \\(P(X=x)\\) is a general expression that describes the entire distribution of \\(X\\) , not just a single probability.","title":"Random Variables"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#probability-distributions","text":"Similar to how a random variable maps the outcomes to a real number, a Probability Distribution is a function that maps the outcomes to its probability of occurrence . For Discrete Random Variables , their distribution is described using a Probability Mass Function (PMF). The PMF provides the probability that the random variable is exactly equal to some value \\((X = x_1)\\) . It is typically denoted in lower case and sometimes includes a subscript of the random variable when working with multiple to distinguish them from one another. \\[ \\begin{aligned} P(X = a) &= p(a) \\\\ \\\\ P(X = a) &= p_X(a) \\\\ P(Y = a) &= p_Y(a) \\end{aligned} \\] Since it is a probability measure, the sum of the PMF over the support of the random variable must be equal to 1 (Probability Axiom). \\[ \\sum_{x \\in \\text{Support}} p(x) = 1 \\] PMFs can be represented in three main ways - Functions, Tables or Histograms. For Continuous Random Variables , their distribution is described using a Probability Density Function (PDF). The PDF is a non-negative function where the area under it provides the probability that the random variable takes on some range of values \\((a \\le X \\le b)\\) . Similarly, it is typically denoted in lower case and includes a subscript when working with multiple random variables: \\[ \\begin{aligned} P(a \\le X \\le b) = \\int^b_a f(x) \\\\ \\\\ P(a \\le X \\le b) = \\int^b_a f_X(x) \\\\ P(a \\le Y \\le b) = \\int^b_a f_Y(y) \\end{aligned} \\] Similarly, since the area is a probability measure, the total area under the graph must be equal to 1 : \\[ P(-\\infty \\le X \\le \\infty) = \\int^{\\infty}_{-\\infty} f(x) = 1 \\] Note \\(\\infty\\) is used as a catch all for the upper and lower bound of the random variable. If the actual bounds are known, then using them instead is more appropriate. Additionally, note that the probability of a specific value for a continuous RV is 0. This is because there is an infinite number of possible values , thus the probability of a specific value (EG. 1.45679383920) is infinitely small such that it is assumed to be 0. \\[ P(X = a) = \\int^{a}_{a} f(x) = 0 \\] The Cumulative Distribution Function (CDF) is the probability that the random variable is less than or equal to some value \\(X \\le t\\) . It is typically denoted in upper case to distinguish it from the PDF and includes subscripts as well when working with multiple random variables. \\[ \\begin{aligned} F(t) &= P(X < t) \\\\ \\\\ F_X(t) &= P(X < t) \\\\ F_Y(t) &= P(Y < t) \\end{aligned} \\] For discrete variables, the CDF is the sum of all probabilities before the specified value . Following this, the difference of consecutive CDFs allows us to obtain the PMFs at that value: \\[ \\begin{aligned} F(t) &= \\sum_{x \\le t} p(x) \\\\ p(x_i) &= F(x_i) - F(x_{i-1}) \\end{aligned} \\] For continuous variables, the CDF is the integral from the lower bound to the specified value . However, instead of integrating with respect to an actual value, it is better to integrate with respect to a dummy variable \\(t\\) to obtain a general expression for the CDF , allowing it to be easily calculated for any value. Although the CDF is very useful, it can only be used to calculate probabilities starting from the lower bound . When probabilities starting from other ranges are needed, the PDF can be obtained from the CDF by differentiating it and then re-integrating with different limits. \\[ \\begin{aligned} F(t) &= \\int^{t}_{-\\infty} f(x) dx \\\\ \\\\ F(t) &= \\int^{t}_{-\\infty} f(t) dt \\\\ f(t) &= F'(t) \\end{aligned} \\] Note The integral of the PDF can lead also lead to the Survival Function , as shown in the Survival Model Section .","title":"Probability Distributions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#moments","text":"The Moments of a distribution are quantities that describe characteristics of the distribution . Raw Moments are calculated with respect to the origin . The n-th raw moment is calculated as the following: \\[ \\begin{aligned} E(X^n) &= \\int x^n \\cdot f(x) dx \\\\ \\mu'_k &= \\sum x^n \\cdot p(x) \\end{aligned} \\] The first raw moment is known as the Mean , which is a measure of the Centrality of the distribution. It is commonly denoted as \\(\\mu\\) , without any super or subscripts. Note that the mean of a constant is the constant itself : \\[ E(c) = c \\] Central Moments are calculated with respect to the mean . The n-th central moment is calculated as the following: \\[ \\begin{aligned} E[(X - \\mu)^n] &= \\int (x - \\mu)^n \\cdot f(x) dx \\\\ \\mu_k&= \\sum (x - \\mu)^n \\cdot p(x) \\end{aligned} \\] The second central moment is known as the Variance , which is a measure of the Spread of the distribution about the mean. Since calculating central moments directly is complicated, it can be simplified to an expression involving raw moments: \\[ \\begin{aligned} Var(X) &= E[(X - \\mu)^2] \\\\ &= E(X^2 - 2\\mu X + \\mu^2) \\\\ &= E(X^2) - 2\\mu^2 + \\mu^2 \\\\ &= E(X^2) - \\mu^2 \\\\ &= E(X^2) - [E(X)]^2 \\end{aligned} \\] Note that the variance of a constant is 0 as a constant cannot change: \\[ Var(c) = 0 \\] However, one problem with variance is that it uses squared units, which makes it hard to intepret. Thus, the squareroot of the variance is used instead, known as the Standard Deviation . \\[ \\sigma = \\sqrt{Var(X)} \\] Similarly, standard deviation cannot be used to compare data with different units . Thus, the Coefficient of Variation is used instead, which is a unitless measure of the spread of the distribution. \\[ CV(X) = \\frac{\\sigma}{\\mu} \\] The third central moment is Skewness , which is a measure of the symmetry of distribution about the mean. Being left/right skewed means that the distribution has a \"longer tail\" on that side, which implies that values on the opposite side are more likely to occur . Tip Skewness is also sometimes referred to as being Positively or Negatively Skewed . An easy way to remember is that positive values occur to the right of the origin, hence is the same as being right skewed; vice-versa. \\[ \\begin{aligned} \\text{Skewness} &= \\frac{E[(X - \\mu)^3]}{\\sigma^3} \\\\ &= \\frac{E[(X - \\mu)^3]}{(\\sigma^2)^\\frac{3}{2}} \\\\ &= \\frac{E(X^3) - 3 E(X^2) \\cdot E(X) + 2 [E(X)]^3}{(E(X^2) - [E(X)]^2)^\\frac{3}{2}} \\end{aligned} \\] The fourth central moment is Kurtosis , which is a measure of the flatness of the distribution, typically with respect to the normal distribution. It is indicative of the likelihood of producing extreme values (outliers). The normal distribution has a kurtosis of 3. If a distribution has a kurtosis greater than 3, then it is flatter and hence more likely to produce outliers as compared to the normal distribution. \\[ \\begin{aligned} \\text{Kurtosis} &= \\frac{E[(X - \\mu)^4]}{\\sigma^4} \\\\ &= \\frac{E[(X - \\mu)^4]}{(\\sigma^2)^2} \\\\ &= \\frac{E(X^4) - 4 E(X^3) \\cdot E(X) + 6 E(X^2) \\cdot [E(X)]^2 - 3 [E(X)]^4}{(E(X^2) - [E(X)]^2)^2} \\end{aligned} \\]","title":"Moments"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#statistical-metrics","text":"Apart from the moment of the distribution, there are some other Statistical Metrics that provide useful information. The Median is the value of the random variable that seperates the upper and lower half of the probability distribution. For discrete variables, the median \\(M\\) is the smallest value such that \\(P(X \\le M) \\ge 0.5\\) and \\(P(X \\ge M) \\ge 0.5\\) . For continuous variables, the median \\(M\\) is found by solving \\(F(M) = 0.5\\) . The key difference is that the continuous median is the value that exactly seperates the distribution while the discrete one approximately splits it, depending on the PMF. The Mode is the value of the random variable that maximises the PMF or PDF. It is the most likely outcome of the experiment (loosely speaking for continuous variables). The Percentile is the value of the random variable below which a certain percentage of observations fall . For instance, the 85 th percentile is the value below which 85% of the observations fall. Let \\(p\\) be the percentage of observations. The 100p-th percentile for a discrete variable is the smallest value \\(a\\) such that \\(P(X \\lt a) \\le p \\lt P(X \\le a)\\) . Similar as before, for continuous variables, the percentile is found by solving \\(F(a) = p\\) . Info 100p-th looks strange because \\(p\\) is a percentage . For instance, if \\(p\\) is 0.85, then 100p is 85, representing the 85-th percentile. Also, the methods for Percentiles and Medians look similar because the median is simply the 50 th percentile ! The 25 th , 50 th & 75 th percentile are known as the first, second & third Quartiles \\((q)\\) respectively. The difference between the 3 rd and 1 st quartile is known as the Inter Quartile Range . \\[ IQR = q_3 - q_1 \\]","title":"Statistical Metrics"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#shifting-scaling-transformation","text":"An existing random variable \\(X\\) can be adjusted in order to make a new random variable \\(Y\\) . If a constant \\(a\\) has been multiplied to the random variable, then it has been Scaled by \\(a\\) : \\[ \\begin{aligned} Y &= aX \\\\ \\\\ F_Y(y) &= P(Y \\le y) \\\\ &= P(aX \\le y) \\\\ &= P \\left(X \\le \\frac{y}{a} \\right) \\\\ &= F_X \\left(\\frac{y}{a} \\right) \\\\ \\\\ f_Y(y) &= \\frac{d}{dy} F_Y(y) \\\\ &= \\frac{1}{a} \\cdot f_X \\left(\\frac{y}{a} \\right) \\end{aligned} \\] If a constant \\(b\\) is added to the random variable instead, then it has been Shifted by \\(b\\) : \\[ \\begin{aligned} Y &= X + b \\\\ \\\\ F_Y(y) &= P(Y \\le y) \\\\ &= P(X + b \\le y) \\\\ &= P(X \\le y - b) \\\\ &= F_X (y - b) \\\\ \\\\ f_Y(y) &= \\frac{d}{dy} F_Y(y) \\\\ &= f_X (y - b) \\end{aligned} \\] For both scaling and transformation, the expectation and variance can be easily determined if that of the original is known as well: \\[ \\begin{aligned} E(cX) &= c \\cdot E(X) \\\\ E(X+c) &= E(X) + E(c) \\\\ &= E(X) + c \\\\ Var(cX) &= c^2 \\cdot Var(x) \\\\ Var(X+c) &= Var(X) + Var(c) \\\\ &= Var(X) \\end{aligned} \\] If the random variable has been raised by a power of \\(\\frac{1}{c}\\) where \\(c \\ne 1\\) , then it has been Power Transformed : \\[ \\begin{aligned} Y &= X^{\\frac{1}{c}} \\\\ \\\\ F_Y(y) &= P(Y \\le y) \\\\ &= P(X^{\\frac{1}{c}} \\le y) \\\\ &= P(X \\le y^c) \\\\ &= F_X (y^c) \\\\ \\\\ f_Y(y) &= \\frac{d}{dy} F_Y(y) \\\\ &= cy^{c-1}\\cdot f_X (y - b) \\end{aligned} \\] If the random variable has been exponentiated , then it has also been Exponential Transformed : \\[ \\begin{aligned} Y &= e^X \\\\ \\\\ F_Y(y) &= P(Y \\le y) \\\\ &= P(e^X \\le y) \\\\ &= P(X \\le \\ln y) \\\\ &= F_X (\\ln y) \\\\ \\\\ f_Y(y) &= \\frac{d}{dy} F_Y(y) \\\\ &= \\frac{1}{y} \\cdot f_X (\\ln y) \\end{aligned} \\] For both types of transformations, there is no simple method of determining the mean and variance. The various raw moments must be manually determined via integration .","title":"Shifting, Scaling &amp; Transformation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#joint-distributions","text":"Consider a scenario where there are two or more random variables \\((X_1, X_2, \\dots)\\) in the same probability space such that we are interested in some function of them \\(Y = g(x_1, x_2, \\dots)\\) . Note For the purposes of this exam, only Linear Combinations (sums) of random variables will be considered. For instance, the experiment could be studying the sum of rolling two dice . There would be two random variables \\((X_1, X_2)\\) , each denoting the value of their respective dice. Thus, the sum of the two dice would be \\(Y = X_1 + X_2\\) . Consider the probability of obtaining a sum of 3 - consider ALL combination of values of the two underlying variables will result in it: \\[ P(Y = 3) = P(X_1 = 1, X_2 = 2) + P(X_1 = 2, X_2 = 1) \\] This can be more generally expressed as the Joint Distribution of the two variables: \\[ \\begin{aligned} P(X = x, Y = y) &= P_{X,Y} (x,y) \\\\ P(a \\lt X \\lt b, c \\lt Y \\lt d) &= \\int^{d}_{c} \\int^{b}_{a} f_{X,Y} (x,y) \\ dx dy \\end{aligned} \\] The individual distributions within this shared probability space is known as the Marginal Distribution and is obtained by \"integrating out\" the other variable: \\[ \\begin{aligned} f_{X}(x) &= \\int^{d}_{c} f_{X,Y} (x,y) \\ dy \\\\ f_{Y}(y) &= \\int^{b}_{a} f_{X,Y} (x,y) \\ dx \\end{aligned} \\]","title":"Joint Distributions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#joint-domain","text":"One of the main concerns when dealing with joint distributions is the domain of the distribution. Each random variable is defined on their own domain, but the joint distribution is only defined in the domain in which two variables coincide . For a joint distribution of discrete variables , it is simply a matter of finding all possible combinations of the two variables. For a two variable distribution, this can be easily visualized via a Contingency Table : The table sometimes lists the frequency of observations while sometimes it lists their probability . In either case, by finding the cells of interest , the probability can be obtained. For a joint distribution of continuous variables , it is best visualized graphically . For a two variable distribution: \\[ 1 \\lt X_1 \\lt 5, \\ 2 \\lt X_2 \\lt 8 \\] Let \\(Y\\) be the sum of the two random variables. Consider the following probability: \\[ \\begin{aligned} Y &= X_1 + X_2 \\\\ P(Y \\lt 5) &= P(X_1 + X_2 \\lt 5) \\\\ &= P(X_2 \\lt 5 - X_1) \\end{aligned} \\] Thus, we need to find the domain of the above inequality WITHIN the joint domain . This can be done by plotting the graph of the upper/lower bound (simply change the inequality to an equal sign). The domain can be intepreted in one of two ways: \\(X_1\\) to line first: \\(1 \\lt X_1 \\lt 3, \\ 2 \\lt y \\lt 5-x\\) \\(X_2\\) to line first: \\(1 \\lt X_1 \\lt 5-y, \\ 2 \\lt y \\lt 4\\) Using these domains, the double integration can be performed to obtain the probability: \\[ \\begin{aligned} P(X_2 \\lt 5 - X_1) &= \\int^{1}_{3} \\int^{5-x}_{2} f_{X,Y}(x,y) \\ dy dx \\\\ &= \\int^{2}_{4} \\int^{5-y}_{1} f_{X,Y}(x,y) \\ dx dy \\end{aligned} \\] Tip Recall that when performing a double integration, the INNER integral is evaluated first . The inner and outer integral are interchangeable, thus set the inner integral such that it will ease the integration . In this case, the integral with an algebraic expression should always be evaluated first, ensuring that the final result is a probability and not an expression. If there is a Kink in the shape, then the area should be broken into two smaller areas without any kinks in their shape. The overall probability is the sum of the areas: \\[ P(X_2 \\lt 8 - X_1) = \\text{Orange Area} + \\text{Purple Area} \\]","title":"Joint Domain"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#joint-moments","text":"Given that \\(Y = X_1 + X_2\\) , its mean and variance is the following: \\[ \\begin{aligned} E(Y) &= E(X_1) + E(X_2) \\\\ \\\\ Var (Y) &= Var (X_1) + Var (X_2) + 2 \\cdot Cov (X,Y) \\end{aligned} \\] The last term is known as the Covariance , which is the first central moment about of the joint distribution. It measures the linear relationship between variables: Positive Covariance : Both variables move in the same direction Negative Covariance : Both variables move in opposite directions \\[ \\begin{aligned} Cov (X_1, X_2) &= E[(X_1 - E(X_1))(X_2 - E(X_2))] \\\\ &= E(X_1 \\cdot X_2) - E(X_1) \\cdot E(X_2) \\\\ \\\\ E(X_1 \\cdot X_2) &= \\int \\int (x_1 \\cdot x_2) \\cdot f_{X_1, X_2} (x_1, x_2) \\ dx_1 dx_2 \\end{aligned} \\] Note that it has some interesting properties: \\(Cov (X_1, c) = 0\\) \\(Cov (X_1, X_2) = Var (X_1)\\) \\(Cov (c \\cdot X_1, X_2) = c \\cdot Cov (X_1,X_2)\\) \\(Cov (X_1 + c, X_2) = Cov (X_1, X_2) + Cov (c, X_2)\\) However, covariance is both limitless and squared units , which makes hard to use as a metric for comparison. We can overcome these problems by scaling the covariance down by the individual standard deviations, obtaining the Correlation of the two variables: \\[ Corr (X_1, X_2) = \\frac{Cov (X_1, X_2)}{SD(X_1), SD(X_2)} \\] If \\(X_1\\) and \\(X_2\\) are Independent , then their distributions and moments become the following: \\[ \\begin{aligned} P(X_1 = x_1, X_2 = x_2) &= P(X_1 = x_1) \\cdot P(X_2 = x_2) \\\\ f_{X_1,X_2} (x_1,x_2) &= f_{X_1}(x_1) \\cdot f_{X_2}(x_2) \\\\ \\\\ E(X_1 \\cdot X_2) &= E(X_1) \\cdot E(X_2) \\\\ Var (X_1 + X_2) &= Var (X_1) + Var (X_2) \\end{aligned} \\] Warning If the domain of the random variables naturally contain other variables, then they are not independent, even if the above conditions are met. Note that having 0 covariance is a consequence of independence , but is NOT indicative of it. Variables may also have 0 covariance when they have a non-linear relationship . \\[ \\begin{aligned} Cov (X_1, X_2) &= E(X_1 \\cdot X_2) - E(X_1) \\cdot E(X_2) \\\\ &= E(X_1) \\cdot E(X_2) - E(X_1) \\cdot E(X_2) \\\\ &= 0 \\\\ \\\\ \\therefore Corr (X_1, X_2) &= 0 \\end{aligned} \\]","title":"Joint Moments"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#generating-functions","text":"A useful way to analyze the sum of independent random variables is to convert the PDF/PMF of the individual distributions into a Generating Function . The first kind is known as a Moment Generating Function (MGF): \\[ \\begin{aligned} M_{X}(t) &= E(e^{tX}) \\\\ &= \\sum e^{tx} \\cdot p_{X}(x) \\\\ &= \\int e^{tx} \\cdot f_{X}(x) \\end{aligned} \\] As its name suggests, the MGF can be used to calculate the raw moments of the distribution. To obtain the n-th raw moment , Differentiate the MGF k times Evaluate the expression at \\(t=0\\) \\[ E \\left(X^n \\right) = \\frac{d^n}{dt^n} \\cdot M_{X}(0) \\] However, the main benefit of MGFs is that they uniquely identify a distribution. If two random variables have the same MGF , then they have the same distribution . This becomes especially useful when dealing with sums of independent random variables . By determining the MGF of the combination, its exact distribution can be determined. \\[ \\begin{aligned} Y &= X_1 + X_2 + ... + X_n \\\\ \\\\ M_Y(t) &= E(e^{tY}) \\\\ &= E(e^{t(X_1 + X_2 + ... + X_n)}) \\\\ &= E(e^{tX_1} \\cdot e^{tX_2} \\cdot ... \\cdot e^{tX_n}) \\\\ &= \\prod E(e^{tx}) \\\\ &= \\prod M_{X}(t) \\end{aligned} \\] The other kind is known as a Probability Generating Function (PGF), which only applies for sums of discrete variables : \\[ \\begin{aligned} P_{X}(t) &= E(t^x) \\\\ &= \\sum t^X \\cdot p(x) \\end{aligned} \\] Note The PGF is denoted in upper case \\(P\\) while the PMF is denoted in lower case \\(p\\) . As its name suggests, the PGF can be used to calculate the individual probabilities of the distribution. To obtain the probability of the n-th value , Differentiate the PGF k times Divide the expression by n factorial Evaluate the expression at \\(t=0\\) \\[ P(X = k) = \\frac{d^n}{dt^n} \\cdot \\frac{P_{X}(0)}{n!} \\] Similarly, the PGF uniquely identifies the distribution and can be used in all the same ways as an MGF in this regard: \\[ \\begin{aligned} Y &= X_1 + X_2 + ... + X_n\\\\ \\\\ P_Y(t) &= E(t^Y) \\\\ &= E(t^{X_1 + X_2 + ... + X_n}) \\\\ &= E(t^{X_1} \\cdot t^{X_2} \\cdot ... \\cdot t^{X_n}) \\\\ &= \\prod E(t^{x}) \\\\ &= \\prod P_{X}(t) \\end{aligned} \\] Given how similar the two are, they can be easily converted to and from one another: \\[ \\begin{aligned} P_X(t) &= E(t^X) \\\\ &= E(e^{\\ln t^X}) \\\\ &= M_X(\\ln t^x) \\\\ \\\\ M_X(t) &= E(e^tX) \\\\ &= E[(e^t)^x] \\\\ &= P_X(e^t) \\end{aligned} \\]","title":"Generating Functions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#conditional-distributions","text":"If a random variable \\(X\\) is conditioned on a range of values of itself , then it has the Conditional Distribution \\((X \\mid X>j)\\) : \\[ \\begin{aligned} P(X = x \\mid X > j) &= \\frac{P(X = x)}{P(X > j)} \\\\ f_{X \\mid X>j} (x) &= \\frac{f_X(x)}{P(X > j)} \\end{aligned} \\] Similarly, a random variable can be conditional on ANOTHER random variable , resulting in the Conditional Distribution \\((X \\mid Y)\\) . \\[ \\begin{aligned} P(X = x \\mid Y = y) &= \\frac{P(X = x, Y = y)}{P(Y = y)} \\\\ f_{X \\mid Y} (x,y) &= \\frac{f_{X,Y}(x,y)}{f_{Y}(y)} \\\\ \\text{Conditional} &= \\frac{\\text{Joint}}{\\text{Marginal}} \\end{aligned} \\] Any calculations involving the above distributions must be solved via first principles . However, most problems require us to find the Marginal Distribution given only the conditional distributions: \\(X\\) is the random variable denoting the test grades (A, B, C) \\(Y\\) is the random variable denoting the gender (M, F) The teacher would like to find the marginal distribution of test scores \\((X)\\) , but only has the conditional distribution of the scores of the students for each gender \\((X \\mid Y)\\) and the proportion of the Genders \\((Y)\\) . The Law of Total Probability can be used to determine the marginal probability of \\(X\\) : \\[ \\begin{aligned} P(X = a) &= E_Y[P(X = a \\mid Y)] \\\\ &= \\sum_{y} P(X = a \\mid y) \\cdot p(Y = y) \\end{aligned} \\] Note that this is equivalent to adding up the final probabilities from the relevant branches from a probability tree: Naturally, this also means that the marginal CDF can be obtained in similar fashion: \\[ \\begin{aligned} F_X(a) &= E_Y[F_{X \\mid Y}(a)] \\\\ &= \\sum_{y} F_{X \\mid Y}(a) \\cdot p(Y = y) \\end{aligned} \\] Following the same logic, the Law of Total Expectation can be used to determine the marginal expectation of \\(x\\) : \\[ \\begin{aligned} E(X) &= E_Y[E_X(X \\mid Y)] \\\\ &= \\sum_{y} E_X(X \\mid Y) \\cdot p(Y = y) \\end{aligned} \\] Warning Since \\(E(X)\\) is a constant, a common mistake is thinking that \\(E_X(X \\mid Y)\\) is a constant as well since they are both expectations. The issue is that it is conditional on \\(Y\\) , which is still random, which makes the conditional expectation still random . The Law of Total Variance can be used to determine the marginal variance of \\(x\\) . However, unlike the previous two, it is NOT simply the expectation of the conditional variance: \\[ \\begin{aligned} Var (X) &= E_Y [Var_X(X \\mid Y)] + Var_Y[E_X(X \\mid Y)] \\\\ \\\\ E_Y [Var_X(X \\mid Y)] &= \\sum_{y} Var_X(X \\mid Y) \\cdot p(Y = y) \\\\ \\\\ Var_Y[E_X(X \\mid Y)] &= E_Y[E_X(X \\mid Y)^2] - (E_Y[E_X(X \\mid Y)])^2 \\\\ &= \\sum_{y} E_X(X \\mid Y)^2 \\cdot p(Y = y) - \\sum_{y} E_X(X \\mid Y) \\cdot p(Y = y) \\end{aligned} \\] Alternatively, the Marginal Variance can be directly calculated using the typical formula of \\(E(X^2) - [E(X)]^2\\) , where the two marginal expectations are calculated using the law of total expectation. Alternatively once more, if the conditional distribution \\(Y\\) only has two outcomes, then the Bernoulli Shortcut (covered in a later section) can be used to quickly compute the value of \\(Var_Y[E_X(X \\mid Y)]\\) . Tip The discrete case was shown in this section due to its simplicity. All the same concepts apply to the continuous variables as well - simply replace the summation & PMFs with integrals and PDFs .","title":"Conditional Distributions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#mixture-distributions","text":"A mixture distribution is a distribution whose values can be intepreted as being derived from an underlying set of other random variables . In an insurance context, a Homeowners Insurance claim could be from a fire, burglary or liability accident. To model it, we could use a mixture that is made up of the basic distributions used to individually model each type of accident. If the mixture contains a countable number of other distributions, then it is known as a Discrete Mixture . Otherwise, it is known as a Continuous Mixture . Warning It is a common mistake to think that a discrete mixture is only made up of discrete distributions, vice-versa. Any type of distribution can be included in a mixture; the classification is based on the number of distributions. The random variable \\(X\\) is a k-point mixture if its probability functions can be expressed as the weighted average of the probability functions of the \\(k\\) distributions \\(X_1, X_2, ... X_k\\) : \\[ \\begin{aligned} F_X(x) &= w_1 \\cdot F_{X_1}(x) + w_2 \\cdot F_{X_2}(x) + ... + w_k \\cdot F_{X_k}(x) \\\\ f_X(x) &= w_1 \\cdot f_{X_1}(x) + w_2 \\cdot f_{X_2}(x) + ... + w_k \\cdot f_{X_k}(x) \\end{aligned} \\] Note For this exam, questions will usually only use 2 or 3 point mixtures . \\(w\\) represents the mixing weights , such that \\(w_1 + w_2 + ... + w_k = 1\\) . It can be intepreted that \\(Y\\) follows the distribution of \\(X_1\\) \\(100w_1 \\%\\) of the time, follows the distribution of \\(X_2\\) \\(100w_2 \\%\\) of the time etc. Warning Another common mistake is confusing mixtures with Linear Combinations of random variables: \\[ X \\ne w_1 X_1 + w_2 X_2 + ... + w_k X_k \\] In a linear combination, \\(X\\) neither follows the distribution of any of the \\(X_k\\) . Furthermore, since \\(w_k\\) are not weights, they can be any real number and do not need to sum to 1 . The mixing weights can also be thought of as Discrete Probabilities that come from a random variable \\(Y\\) representing the \\(k\\) underlying distributions with the support \\(\\set{1, 2, ..., k}\\) where \\(P(Y = k) = w_k\\) . Thus, we can think of the overall mixture \\(X\\) as an unconditional distribution while each of the underlying distributions are conditional distributions \\(X \\mid Y\\) . This allows us to make use of the all the previous results from the conditional distributions: \\[ \\begin{aligned} P(X = a) &= E_Y[P(X = a \\mid Y)] \\\\ F_X(a) &= E_Y[F_{X \\mid Y}(a)] \\\\ E(X) &= E_Y[E_X(X \\mid Y)] \\\\ Var (X) &= E_Y [Var_X(X \\mid Y)] + Var_Y[E_X(X \\mid Y)] \\end{aligned} \\] In terms of the weights, for a simple two point mixture : \\[ \\begin{aligned} P(X = a) &= P(X = a \\mid Y=1) \\cdot w_1 + P(X = a \\mid Y=2) \\cdot w_2 \\\\ F_X(a) &= F_{X \\mid Y=1}(a) \\cdot w_1 + F_{X \\mid Y=2}(a) \\cdot w_2 \\\\ E(X) &= E_X(X \\mid Y=1) \\cdot w_1 + E_X(X \\mid Y=2) \\cdot w_2 \\end{aligned} \\]","title":"Mixture Distributions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/","text":"Frequency Models \u00b6 Frequency Distributions \u00b6 The number of claims is usually known as the Loss Frequency and is typically modelled with a discrete distribution. Let \\(N\\) be the random variable denoting the number of claims. This section will cover all the relevant frequency distributions, but note that there is no need to memorize anything as they are all provided in the formula sheet. Poisson Distribution \u00b6 The Poisson Distribution is used to count the number of random events in a specified unit of space or time . It only has one parameter \\(\\mu\\) , the mean number of occurrences in the specified unit of space or time. The key property is that its mean and variance are both equal to \\(\\mu\\) . \\[ \\begin{aligned} N &\\sim \\text{Poisson}(\\mu) \\\\ \\\\ p_n &= \\frac{e^{-\\mu} \\cdot \\mu^k}{k!} \\\\ \\\\ E(N) &= \\mu \\\\ Var (N) &= \\mu \\end{aligned} \\] The sum of \\(k\\) independent poisson random variables is still poisson : \\[ \\begin{aligned} N_k &\\sim \\text{Poisson}(\\mu_k) \\\\ N &= N_1 + N_2 + ... + N_k \\\\ \\therefore N &\\sim \\text{Poisson} (\\mu_1 + \\mu_2 + ... + \\mu_k) \\end{aligned} \\] Bernoulli Distribution \u00b6 The Bernoulli Distribution is used to determine the outcome of a Bernoulli Trial . They are experiments with only two possible outcomes . For a Standard Bernoulli Distribution , the two outcomes are Successes and Failures , denoted by 1 and 0 respectively: \\[ \\begin{aligned} X &\\sim \\text{Bernoulli} (q) \\\\ \\\\ p_x &= \\begin{cases} \\text{Success } (1),& \\text{Probability} = q, \\\\ \\text{Failure } (0),& \\text{Probability} = 1-q \\end{cases} \\\\ \\\\ E(X) &= q \\\\ Var (X) &= q(1-q) \\end{aligned} \\] However, the Bernoulli Shortcut generalizes this for any two mutually exclusive outcomes , denoted by \\(a\\) and \\(b\\) respectively: \\[ \\begin{aligned} Y &= (a-b)X + b \\\\ \\\\ Y &= \\begin{cases} \\text{Outcome 1 } (a),& \\text{Probability} = q, \\\\ \\text{Outcome 2 } (b),& \\text{Probability} = 1-q \\end{cases} \\\\ \\\\ E(Y) &= E[(a-b)X + b] \\\\ &= (a-b)q + b \\\\ \\\\ Var (Y) &= Var[(a-b)X + b] \\\\ &= (a-b)^2 \\cdot q(1-q) \\end{aligned} \\] Note that there is no need to memorize the mean and variance for the bernoulli shortcut - they can be easily derived from the standard distribution. Binomial Distribution \u00b6 The Binomial Distribution is used to count the number of successes out of a fixed number of independent Standard Bernoulli Trials . It has two parameters : The number of independent trials, \\(m\\) The probability of success for each trial, \\(q\\) \\[ \\begin{aligned} N &\\sim \\text{Binomial} (m, q) \\\\ \\\\ p_n &= {m \\choose k} q^k (1-q)^{1-k} \\\\ \\\\ E(N) &= nq \\\\ Var (N) &= nq(1-q) \\end{aligned} \\] The binomial distribution is actually the sum of \\(m\\) independent standard bernoulli variables with the same probability : \\[ \\begin{aligned} X_k &\\sim \\text{Bernoulli} (q) \\\\ N &= X_1 + X_2 + ... + X_m \\\\ \\therefore N &\\sim \\text{Binomial} (m, q) \\end{aligned} \\] Tip A bernoulli distribution is simply a binomial distribution with \\(m=1\\) . Thus, the sum of \\(k\\) independent binomial variables with the same probability will still binomial : \\[ \\begin{aligned} N_k &\\sim \\text{Binomial}(m, q) \\\\ N &= N_1 + N_2 + ... + N_k \\\\ \\therefore N &\\sim \\text{Binomial} (m_1 + m_2 + ... + m_k, q) \\end{aligned} \\] This is intuitive, because the sum of binomial variables is actually the sum of even more bernoulli variables , which as shown previously, will follow the binomial distribution. Geometric Distribution \u00b6 The Geometric Distribution can be understood in one of two ways, with respect to a sequence of independent bernoulli trials : Number of trials needed to get the first success , \\(N \\in \\set{1, 2, 3, ...}\\) Number of failures needed to get the first success , \\(F \\in \\set{0, 1, 2, ...}\\) If not explicitly stated, the two intepretations can be distinguished from their supports - there must be at least 1 trial but there can be 0 failures. It is useful to remember just one intepretation and understand how they are related: \\[ F = N-1 \\] The second intepretation is preferred , since that is what is provided in the formula sheet. Regardless, the Geometric distribution only has one parameter , the probability of success: \\[ \\begin{aligned} N &\\sim \\text{Geom} (q) \\\\ \\\\ p_k &= (1-q)^{k} \\cdot q \\\\ \\\\ E(N) &= \\frac{1-p}{p} \\\\ Var (N) &= \\frac{1-p}{p^2} \\end{aligned} \\] The key property is that it is Memoryless . Intuitively, the geometric distribution is \"waiting\" for the first success to occur. However, it does not matter how many failures have occurred , the probability of the first success occuring in another \\(k\\) failures is independent of the history of the process ; it remains constant over time. The distribution \"forgets\" (memoryless) the current state that the process is in. \\[ \\begin{aligned} P(N > m + n \\mid N \\ge m) &= P(N > n) \\\\ E(N > m + n \\mid N \\ge m) &= E(N) \\\\ Var(N > m + n \\mid N \\ge m) &= Var(N) \\end{aligned} \\] Negative Binomial Distribution \u00b6 The Negative Binomial Distribution is used to count the number of failures to get a fixed number of successes , with respect to a sequence of independent bernoulli trials. It has two parameters: The number of successes, \\(r\\) The probability of success, \\(q\\) \\[ \\begin{aligned} N &\\sim \\text{Negative Binomial} (r, q) \\\\ \\\\ p_n &= {{k+r-1} \\choose {k}} (1-p)^k p^{r} \\\\ \\\\ E(N) &= \\frac{r(1-p)}{p} \\\\ Var (N) &= \\frac{r(1-p)}{p^2} \\end{aligned} \\] Info The distribution is also referred to as Pascals Distribution , but is most commonly called the \"negative\" binomial because it uses a negative integer in the binomial coefficient : \\[ \\begin{aligned} {n \\choose k} &= \\frac{n!}{k! \\cdot (n-k)!} \\\\ \\\\ {{k+r-1} \\choose {k}} &= \\frac{(k+r-1)!}{k! \\cdot (r-1)!} \\\\ &= \\frac{(k+r-1) \\cdot (k+r-2) \\cdot \\dots \\cdot (r)}{k!} \\\\ &= (-1)^k \\cdot \\frac{(-r) \\cdot (-r-1) \\cdot \\dots \\cdot (-r-k+1)}{k!} \\\\ &= (-1)^k {-r \\choose k} \\end{aligned} \\] The negative binomial distribution is actually the sum of \\(r\\) i.i.d. geometric variables with the same probability : \\[ \\begin{aligned} X_k &\\sim \\text{Geom} (q) \\\\ N &= X_1 + X_2 + ... + X_r \\\\ \\therefore N &\\sim \\text{Negative Binomial} (r, q) \\end{aligned} \\] Tip A geometric distribution is simply a negative binomial distribution with \\(r=1\\) . Thus, following the same logic as before, the sum of \\(k\\) independent negative binomial variables with the same probability will still be negative binomial : \\[ \\begin{aligned} N_k &\\sim \\text{Negative Binomial}(r, q) \\\\ N &= N_1 + N_2 + ... + N_k \\\\ \\therefore N &\\sim \\text{Negative Binomial} (r_1 + r_2 + ... + r_k, q) \\end{aligned} \\] Alternative Parameterization \u00b6 Re-parameterized, refer to wikipedia Odds vs Probabiliyu Poisson Gamma Mixture \u00b6 Consider a poisson distribution with mean \\(\\mu\\) where \\(\\mu\\) follows a gamma distribution. The unconditional distribution actually follows a Negative Binomial Distribution with the same parameters as the gamma distribution. \\[ \\begin{aligned} N \\mid \\mu &\\sim \\text{Poisson} (\\mu) \\\\ \\mu &\\sim \\text{Gamma} (\\alpha, \\theta) \\\\ \\therefore N &\\sim \\text{Negative Binomial}(r = \\alpha, \\beta = \\theta) \\end{aligned} \\] The above result is a Continuous Mixture - it is the result of a mixture of an infinite number of poisson distributions . Continuous Mixtures are beyond the scope of this exam, which is why the proof is not shown. However, this specific result is important to know. (a, b, 0) Class \u00b6 ONLY the distributions discussed above fall into the (a, b, 0) class of distributions, as their PMFs follow the same recursion : \\[ \\begin{aligned} \\frac{p_{n}}{p_{n-1}} &= a + \\frac{b}{n} \\\\ p_n &= \\left(a + \\frac{b}{n} \\right) p_{n-1} \\end{aligned} \\] \\(a\\) and \\(b\\) are constants that are unique to each distribution while the \"0\" comes from the fact that the recursion starts from \\(n-1=0\\) . Tip The parameters for each distribution can also be determined from the values of \\(a\\) and \\(b\\) . Notice that each distribution has a different signs for \\(a\\) . Thus, given the recursive equation, the underlying distribution can be determined based on the sign of \\(a\\) . Choosing Distributions \u00b6 Given a sample of data, we need to decide which distribution best represents it. The general idea is that each distribution has some unique characteristics - if the dataset matches those characteristics, then the distribution should be used. The first method involves the Mean and Variance of the distribution. Notice that for all three distributions, the mean and variance have a different relative size to one another . Thus, by comparing the Sample Mean and Unbiased Sample Variance of the dataset, it can be matched to the corresponding distribution. Warning It is almost impossible (whether in practice or theory) to find a sample mean and variance that is exactly equal to one another. Generally speaking, if they are sufficiently close together, then they can be assumed to be equal. However, being \"sufficiently close together\" is arbitrary. Thus, a better way would be to compare the ratio of the two values - if the ratio is around 1.00 , then it can be concluded that the two values are equal . The second method involves the sign of \\(a\\) in the recursion process. Since all three distributions have different signs for \\(a\\) , their probabilities will move differently as \\(n\\) increases. Thus, by observing how the probability of the sample changes, it can be matched to the corresponding distribution. However, we must first multiply the ratio of the probabilities by \\(n\\) in order to obtain a smooth linear line that clearly exhibits the relationships: \\[ \\begin{aligned} \\frac{p_{n}}{p_{n-1}} &= a + \\frac{b}{n} \\\\ \\frac{n \\cdot p_{n}}{p_{n-1}} &= an + b \\end{aligned} \\] (a, b, 1) Class \u00b6 When using the (a, b, 0) class of distributions to model experiments, one common problem is that the probability at \\(n=0\\) does not match experience . Thus, there is a need to modify the distribution such that the probability at 0 is at the desired level . This modification is known as the Zero-Modification . Note If the desired level is 0, then the modification is known as a Zero-Truncation , which is a special case of the zero-modification. This is because this truncates (removes) the possibility of 0 from the random variable. This is done by directly changing the probability at 0 to the desired level. However, this modification alone would cause the sum of probabilities to deviate from 1, thus the probabilities at all other levels of the support must be scaled such that they sum to 1 . Let \\(N^M\\) be the zero-modified distribution and \\(p^{M}_{n}\\) be its PMF. \\[ p^{M}_{n} = \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_n \\] If \\(N\\) is a poisson random variable, then \\(N^M\\) is known as a Zero-Modified Poisson Random Variable . However, it is important to note that \\(N^M\\) does NOT follow a poisson distribution - it has its own unique distribution. Thus, by applying the previous result, the moments of the new distribution can be calculated: \\[ \\begin{aligned} E \\left(N^M \\right) &= 0 \\cdot p^{M}_{0} + 1 \\cdot p^{M}_{1} + 2 \\cdot p^{M}_{2} + ... \\\\ &= 0 \\cdot \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_0 + 1 \\cdot \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_1 + 2 \\cdot \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_2 + ... \\\\ &= \\frac{1-p^{M}_{0}}{1-p_{0}} (0 \\cdot p_0 + 1 \\cdot p_1 + 2 \\cdot p_2 + ...) \\\\ &= \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot E(N) \\\\ \\\\ \\therefore E \\left[\\left(N^M \\right)^k \\right] &= \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot E \\left[N^k \\right] \\end{aligned} \\] Note that this result ONLY applies to raw moments . In order to calculate the central moments, express them in terms of raw moments and then calculate them from the above conversion. Even after modification, the PMFs can still follow the same recursion: \\[ \\begin{aligned} \\frac{p^{M}_{n}}{p^{M}_{n-1}} &= \\frac{\\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_{n}}{\\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_{n-1}} \\\\ &= \\frac{p_{n}}{p_{n-1}} \\\\ &= a + \\frac{b}{n} \\end{aligned} \\] However, since the distribution is zero-modified, the recursion can only start from \\(k-1=1\\) , which is why they are known as the (a, b, 1) class of distributions. The domain of the recursion is what allows us to distinguish the two classes of distributions. Warning There are only 4 distributions under the (a, b, 0) class: Poisson, Binomial, Geometric & Negative Binomial. It is seemingly intuitive to think that their 0-modified versions are the only members of the (a, b, 1) class. However, the Logarithmic Distribution is also a member of the (a, b, 1).","title":"Frequency Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/#frequency-models","text":"","title":"Frequency Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/#frequency-distributions","text":"The number of claims is usually known as the Loss Frequency and is typically modelled with a discrete distribution. Let \\(N\\) be the random variable denoting the number of claims. This section will cover all the relevant frequency distributions, but note that there is no need to memorize anything as they are all provided in the formula sheet.","title":"Frequency Distributions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/#poisson-distribution","text":"The Poisson Distribution is used to count the number of random events in a specified unit of space or time . It only has one parameter \\(\\mu\\) , the mean number of occurrences in the specified unit of space or time. The key property is that its mean and variance are both equal to \\(\\mu\\) . \\[ \\begin{aligned} N &\\sim \\text{Poisson}(\\mu) \\\\ \\\\ p_n &= \\frac{e^{-\\mu} \\cdot \\mu^k}{k!} \\\\ \\\\ E(N) &= \\mu \\\\ Var (N) &= \\mu \\end{aligned} \\] The sum of \\(k\\) independent poisson random variables is still poisson : \\[ \\begin{aligned} N_k &\\sim \\text{Poisson}(\\mu_k) \\\\ N &= N_1 + N_2 + ... + N_k \\\\ \\therefore N &\\sim \\text{Poisson} (\\mu_1 + \\mu_2 + ... + \\mu_k) \\end{aligned} \\]","title":"Poisson Distribution"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/#bernoulli-distribution","text":"The Bernoulli Distribution is used to determine the outcome of a Bernoulli Trial . They are experiments with only two possible outcomes . For a Standard Bernoulli Distribution , the two outcomes are Successes and Failures , denoted by 1 and 0 respectively: \\[ \\begin{aligned} X &\\sim \\text{Bernoulli} (q) \\\\ \\\\ p_x &= \\begin{cases} \\text{Success } (1),& \\text{Probability} = q, \\\\ \\text{Failure } (0),& \\text{Probability} = 1-q \\end{cases} \\\\ \\\\ E(X) &= q \\\\ Var (X) &= q(1-q) \\end{aligned} \\] However, the Bernoulli Shortcut generalizes this for any two mutually exclusive outcomes , denoted by \\(a\\) and \\(b\\) respectively: \\[ \\begin{aligned} Y &= (a-b)X + b \\\\ \\\\ Y &= \\begin{cases} \\text{Outcome 1 } (a),& \\text{Probability} = q, \\\\ \\text{Outcome 2 } (b),& \\text{Probability} = 1-q \\end{cases} \\\\ \\\\ E(Y) &= E[(a-b)X + b] \\\\ &= (a-b)q + b \\\\ \\\\ Var (Y) &= Var[(a-b)X + b] \\\\ &= (a-b)^2 \\cdot q(1-q) \\end{aligned} \\] Note that there is no need to memorize the mean and variance for the bernoulli shortcut - they can be easily derived from the standard distribution.","title":"Bernoulli Distribution"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/#binomial-distribution","text":"The Binomial Distribution is used to count the number of successes out of a fixed number of independent Standard Bernoulli Trials . It has two parameters : The number of independent trials, \\(m\\) The probability of success for each trial, \\(q\\) \\[ \\begin{aligned} N &\\sim \\text{Binomial} (m, q) \\\\ \\\\ p_n &= {m \\choose k} q^k (1-q)^{1-k} \\\\ \\\\ E(N) &= nq \\\\ Var (N) &= nq(1-q) \\end{aligned} \\] The binomial distribution is actually the sum of \\(m\\) independent standard bernoulli variables with the same probability : \\[ \\begin{aligned} X_k &\\sim \\text{Bernoulli} (q) \\\\ N &= X_1 + X_2 + ... + X_m \\\\ \\therefore N &\\sim \\text{Binomial} (m, q) \\end{aligned} \\] Tip A bernoulli distribution is simply a binomial distribution with \\(m=1\\) . Thus, the sum of \\(k\\) independent binomial variables with the same probability will still binomial : \\[ \\begin{aligned} N_k &\\sim \\text{Binomial}(m, q) \\\\ N &= N_1 + N_2 + ... + N_k \\\\ \\therefore N &\\sim \\text{Binomial} (m_1 + m_2 + ... + m_k, q) \\end{aligned} \\] This is intuitive, because the sum of binomial variables is actually the sum of even more bernoulli variables , which as shown previously, will follow the binomial distribution.","title":"Binomial Distribution"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/#geometric-distribution","text":"The Geometric Distribution can be understood in one of two ways, with respect to a sequence of independent bernoulli trials : Number of trials needed to get the first success , \\(N \\in \\set{1, 2, 3, ...}\\) Number of failures needed to get the first success , \\(F \\in \\set{0, 1, 2, ...}\\) If not explicitly stated, the two intepretations can be distinguished from their supports - there must be at least 1 trial but there can be 0 failures. It is useful to remember just one intepretation and understand how they are related: \\[ F = N-1 \\] The second intepretation is preferred , since that is what is provided in the formula sheet. Regardless, the Geometric distribution only has one parameter , the probability of success: \\[ \\begin{aligned} N &\\sim \\text{Geom} (q) \\\\ \\\\ p_k &= (1-q)^{k} \\cdot q \\\\ \\\\ E(N) &= \\frac{1-p}{p} \\\\ Var (N) &= \\frac{1-p}{p^2} \\end{aligned} \\] The key property is that it is Memoryless . Intuitively, the geometric distribution is \"waiting\" for the first success to occur. However, it does not matter how many failures have occurred , the probability of the first success occuring in another \\(k\\) failures is independent of the history of the process ; it remains constant over time. The distribution \"forgets\" (memoryless) the current state that the process is in. \\[ \\begin{aligned} P(N > m + n \\mid N \\ge m) &= P(N > n) \\\\ E(N > m + n \\mid N \\ge m) &= E(N) \\\\ Var(N > m + n \\mid N \\ge m) &= Var(N) \\end{aligned} \\]","title":"Geometric Distribution"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/#negative-binomial-distribution","text":"The Negative Binomial Distribution is used to count the number of failures to get a fixed number of successes , with respect to a sequence of independent bernoulli trials. It has two parameters: The number of successes, \\(r\\) The probability of success, \\(q\\) \\[ \\begin{aligned} N &\\sim \\text{Negative Binomial} (r, q) \\\\ \\\\ p_n &= {{k+r-1} \\choose {k}} (1-p)^k p^{r} \\\\ \\\\ E(N) &= \\frac{r(1-p)}{p} \\\\ Var (N) &= \\frac{r(1-p)}{p^2} \\end{aligned} \\] Info The distribution is also referred to as Pascals Distribution , but is most commonly called the \"negative\" binomial because it uses a negative integer in the binomial coefficient : \\[ \\begin{aligned} {n \\choose k} &= \\frac{n!}{k! \\cdot (n-k)!} \\\\ \\\\ {{k+r-1} \\choose {k}} &= \\frac{(k+r-1)!}{k! \\cdot (r-1)!} \\\\ &= \\frac{(k+r-1) \\cdot (k+r-2) \\cdot \\dots \\cdot (r)}{k!} \\\\ &= (-1)^k \\cdot \\frac{(-r) \\cdot (-r-1) \\cdot \\dots \\cdot (-r-k+1)}{k!} \\\\ &= (-1)^k {-r \\choose k} \\end{aligned} \\] The negative binomial distribution is actually the sum of \\(r\\) i.i.d. geometric variables with the same probability : \\[ \\begin{aligned} X_k &\\sim \\text{Geom} (q) \\\\ N &= X_1 + X_2 + ... + X_r \\\\ \\therefore N &\\sim \\text{Negative Binomial} (r, q) \\end{aligned} \\] Tip A geometric distribution is simply a negative binomial distribution with \\(r=1\\) . Thus, following the same logic as before, the sum of \\(k\\) independent negative binomial variables with the same probability will still be negative binomial : \\[ \\begin{aligned} N_k &\\sim \\text{Negative Binomial}(r, q) \\\\ N &= N_1 + N_2 + ... + N_k \\\\ \\therefore N &\\sim \\text{Negative Binomial} (r_1 + r_2 + ... + r_k, q) \\end{aligned} \\]","title":"Negative Binomial Distribution"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/#alternative-parameterization","text":"Re-parameterized, refer to wikipedia Odds vs Probabiliyu","title":"Alternative Parameterization"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/#poisson-gamma-mixture","text":"Consider a poisson distribution with mean \\(\\mu\\) where \\(\\mu\\) follows a gamma distribution. The unconditional distribution actually follows a Negative Binomial Distribution with the same parameters as the gamma distribution. \\[ \\begin{aligned} N \\mid \\mu &\\sim \\text{Poisson} (\\mu) \\\\ \\mu &\\sim \\text{Gamma} (\\alpha, \\theta) \\\\ \\therefore N &\\sim \\text{Negative Binomial}(r = \\alpha, \\beta = \\theta) \\end{aligned} \\] The above result is a Continuous Mixture - it is the result of a mixture of an infinite number of poisson distributions . Continuous Mixtures are beyond the scope of this exam, which is why the proof is not shown. However, this specific result is important to know.","title":"Poisson Gamma Mixture"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/#a-b-0-class","text":"ONLY the distributions discussed above fall into the (a, b, 0) class of distributions, as their PMFs follow the same recursion : \\[ \\begin{aligned} \\frac{p_{n}}{p_{n-1}} &= a + \\frac{b}{n} \\\\ p_n &= \\left(a + \\frac{b}{n} \\right) p_{n-1} \\end{aligned} \\] \\(a\\) and \\(b\\) are constants that are unique to each distribution while the \"0\" comes from the fact that the recursion starts from \\(n-1=0\\) . Tip The parameters for each distribution can also be determined from the values of \\(a\\) and \\(b\\) . Notice that each distribution has a different signs for \\(a\\) . Thus, given the recursive equation, the underlying distribution can be determined based on the sign of \\(a\\) .","title":"(a, b, 0) Class"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/#choosing-distributions","text":"Given a sample of data, we need to decide which distribution best represents it. The general idea is that each distribution has some unique characteristics - if the dataset matches those characteristics, then the distribution should be used. The first method involves the Mean and Variance of the distribution. Notice that for all three distributions, the mean and variance have a different relative size to one another . Thus, by comparing the Sample Mean and Unbiased Sample Variance of the dataset, it can be matched to the corresponding distribution. Warning It is almost impossible (whether in practice or theory) to find a sample mean and variance that is exactly equal to one another. Generally speaking, if they are sufficiently close together, then they can be assumed to be equal. However, being \"sufficiently close together\" is arbitrary. Thus, a better way would be to compare the ratio of the two values - if the ratio is around 1.00 , then it can be concluded that the two values are equal . The second method involves the sign of \\(a\\) in the recursion process. Since all three distributions have different signs for \\(a\\) , their probabilities will move differently as \\(n\\) increases. Thus, by observing how the probability of the sample changes, it can be matched to the corresponding distribution. However, we must first multiply the ratio of the probabilities by \\(n\\) in order to obtain a smooth linear line that clearly exhibits the relationships: \\[ \\begin{aligned} \\frac{p_{n}}{p_{n-1}} &= a + \\frac{b}{n} \\\\ \\frac{n \\cdot p_{n}}{p_{n-1}} &= an + b \\end{aligned} \\]","title":"Choosing Distributions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/2.%20Frequency%20Models/#a-b-1-class","text":"When using the (a, b, 0) class of distributions to model experiments, one common problem is that the probability at \\(n=0\\) does not match experience . Thus, there is a need to modify the distribution such that the probability at 0 is at the desired level . This modification is known as the Zero-Modification . Note If the desired level is 0, then the modification is known as a Zero-Truncation , which is a special case of the zero-modification. This is because this truncates (removes) the possibility of 0 from the random variable. This is done by directly changing the probability at 0 to the desired level. However, this modification alone would cause the sum of probabilities to deviate from 1, thus the probabilities at all other levels of the support must be scaled such that they sum to 1 . Let \\(N^M\\) be the zero-modified distribution and \\(p^{M}_{n}\\) be its PMF. \\[ p^{M}_{n} = \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_n \\] If \\(N\\) is a poisson random variable, then \\(N^M\\) is known as a Zero-Modified Poisson Random Variable . However, it is important to note that \\(N^M\\) does NOT follow a poisson distribution - it has its own unique distribution. Thus, by applying the previous result, the moments of the new distribution can be calculated: \\[ \\begin{aligned} E \\left(N^M \\right) &= 0 \\cdot p^{M}_{0} + 1 \\cdot p^{M}_{1} + 2 \\cdot p^{M}_{2} + ... \\\\ &= 0 \\cdot \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_0 + 1 \\cdot \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_1 + 2 \\cdot \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_2 + ... \\\\ &= \\frac{1-p^{M}_{0}}{1-p_{0}} (0 \\cdot p_0 + 1 \\cdot p_1 + 2 \\cdot p_2 + ...) \\\\ &= \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot E(N) \\\\ \\\\ \\therefore E \\left[\\left(N^M \\right)^k \\right] &= \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot E \\left[N^k \\right] \\end{aligned} \\] Note that this result ONLY applies to raw moments . In order to calculate the central moments, express them in terms of raw moments and then calculate them from the above conversion. Even after modification, the PMFs can still follow the same recursion: \\[ \\begin{aligned} \\frac{p^{M}_{n}}{p^{M}_{n-1}} &= \\frac{\\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_{n}}{\\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_{n-1}} \\\\ &= \\frac{p_{n}}{p_{n-1}} \\\\ &= a + \\frac{b}{n} \\end{aligned} \\] However, since the distribution is zero-modified, the recursion can only start from \\(k-1=1\\) , which is why they are known as the (a, b, 1) class of distributions. The domain of the recursion is what allows us to distinguish the two classes of distributions. Warning There are only 4 distributions under the (a, b, 0) class: Poisson, Binomial, Geometric & Negative Binomial. It is seemingly intuitive to think that their 0-modified versions are the only members of the (a, b, 1) class. However, the Logarithmic Distribution is also a member of the (a, b, 1).","title":"(a, b, 1) Class"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/","text":"Severity Models \u00b6 Severity Distributions \u00b6 The size of the loss is usually known as Loss Severity and is typically modelled with a continuous distribution. Let \\(X\\) be the random variable denoting the size of the loss. This section will cover most of the relevant severity distributions, but similarly note that there is no need to memorize anything as they are all provided in the formula sheet. Normal Distribution \u00b6 The Normal Distribution is used to model processes where outcomes close to the mean are common and become increasingly less common as it strays further from it. The unique property of the normal distribution in that it has two parameters , which are already its Mean \\(\\mu\\) and Variance \\(\\sigma^2\\) : \\[ \\begin{aligned} X &\\sim N(\\mu, \\sigma^2) \\\\ \\\\ f(x) &= \\frac{1}{\\sigma \\sqrt{2\\pi}} \\cdot e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\\\ \\\\ E(X) &= \\mu \\\\ Var (X) &= \\sigma^2 \\\\ \\end{aligned} \\] Another interesting property of the normal distribution is that: Roughly 68.3% of the data is within 1 SD of the average ( \\(\\mu - 1\\sigma \\lt x \\lt \\mu + 1\\sigma\\) ) Roughly 95.5% of the data is within 2 SD of the average ( \\(\\mu - 2\\sigma \\lt x \\lt \\mu + 2\\sigma\\) ) Roughly 99.7% of the data is within 3 SD of the average ( \\(\\mu - 3\\sigma \\lt x \\lt \\mu + 3\\sigma\\) ) The sum of \\(k\\) independent normal random variables is still normal: \\[ \\begin{aligned} X_k &\\sim N(\\mu, \\sigma^2) \\\\ X &= X_1 + X_2 + \\dots + X_k \\\\ \\therefore X &\\sim N(\\mu_1 + \\mu_2 + ... + \\mu_k, \\sigma^2_1 + \\sigma^2_2 + ... + \\sigma_k) \\end{aligned} \\] Another key property is that they are symmetrical about their mean : \\[ \\begin{aligned} P(Z \\lt -a) &= P(Z \\gt a) \\\\ &= 1 - P(Z \\lt a) \\end{aligned} \\] Standard Normal Distribution \u00b6 If the parameters of the normal distribution are 0 and 1 respectively, then it is known as the Standard Normal Distribution , specially denoted by \\(Z\\) . \\[ Z \\sim N(0, 1) \\] Any normal distribution can be converted into a standard normal through Standardization : \\[ Z = \\frac{X - E(X)}{\\sqrt{Var (X)}} \\] Note An easy way to remember is that the above transformation sets the mean to 0 \\(E(X) - E(X) = 0\\) and sets the variance to 1 \\(\\frac{Var(X)}{Var(X)} = 1\\) . The CDF of a standard normal distribution is denoted by \\(\\Phi\\) : \\[ P(Z \\le z) = \\Phi(z) \\] Since the exam provides values for the CDF of Z , most calculations involving a normal distribution will have first have to be standardized and then calculated using its CDF: \\[ \\begin{aligned} P(X \\le x) &= P \\left(\\frac{X - E(X)}{\\sqrt{Var (X)}} \\le \\frac{x - E(X)}{\\sqrt{Var (X)}} \\right) \\\\ &= P \\left(Z \\le \\frac{x - E(X)}{\\sqrt{Var (X)}} \\right) \\\\ &= \\Phi \\left(\\frac{x - E(X)}{\\sqrt{Var (X)}} \\right) \\end{aligned} \\] The reverse is also common whereby the probability is provided to solve for the value of Z and hence the value of the underlying variable. The issue is that most of the time, the probability provided will not be exactly equal to the values on the table. In such a case, the midpoint between two values of Z will be taken: Given: \\(P = 0.95\\) Table Value: \\(Z = 1.64, P = 0.9495\\) Table Value: \\(Z = 1.65, P = 0.9505\\) Midpoint: \\(Z = 1.645\\) Lognormal Distribution \u00b6 A exponential transformed normal distribution is known as the Lognormal Distribution as it has a logarithm applied to it. It shares the same two parameters as a regular normal distribution , but they are NOT the mean and variance of the lognormal distribution. \\[ \\begin{aligned} Y &= e^X \\\\ Y &\\sim \\text{Lognormal} (\\mu, \\sigma^2) \\\\ \\\\ f(y) &= \\frac{1}{x\\sigma \\sqrt{2\\pi}} \\cdot e^{-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}} \\\\ \\\\ E(Y) &= e^{\\mu + 0.5 \\sigma^2} \\\\ Var (Y) &= E(Y)^2 \\cdot (e^{\\sigma^2} - 1) \\end{aligned} \\] Warning Given how similar the two distributions are, it is easy to mix them up. It is a common mistake to think that the lognormal parameters are its mean and variance. Note that since it is transformed from a normal distribution, it must use its parameters , but they are NOT its mean and variance. Another common mistake is thinking that the PDFs are identical, given how similar they look. However, there are two key differences : The denominator has an extra \\(x\\) multiplied to it The \\(x\\) in the exponent has become \\(\\ln x\\) The CDF of the log distribution can also be calculated through the standard normal distribution : \\[ \\begin{aligned} P(Y \\le y) &= P(e^X \\le y) \\\\ &= P(X \\le \\ln y) \\\\ &= P \\left(\\frac{X - \\mu}{\\sigma} \\le \\frac{\\ln y - \\mu}{\\sigma} \\right) \\\\ &= P \\left(Z \\le \\frac{\\ln y - \\mu}{\\sigma} \\right) \\\\ &= \\Phi \\left(\\frac{\\ln y - \\mu}{\\sigma} \\right) \\end{aligned} \\] Note that since the sum of independent normal variables is still normal, then the product of indepdent lognormal variables is still lognormal : \\[ \\begin{aligned} X &= X_1 + X_2 + \\dots + X_k \\\\ X &\\sim N(\\mu_1 + \\mu_2 + ... + \\mu_k, \\sigma^2_1 + \\sigma^2_2 + ... + \\sigma_k) \\\\ \\\\ Y &= e^X \\\\ &= e^{X_1 + X_2 + \\dots + X_k} \\\\ &= e^{X_1} \\cdot e^{X_2} \\cdot (\\dots) \\cdot e^{X_k} \\\\ \\therefore Y &\\sim \\text{Lognormal}(\\mu_1 + \\mu_2 + ... + \\mu_k, \\sigma^2_1 + \\sigma^2_2 + ... + \\sigma_k) \\end{aligned} \\] Exponential Distribution \u00b6 The Exponential Distribution is used to model the time taken for the first event to occur in a Poisson Process . It has a single parameter \\(\\lambda\\) , which represents the rate at which events occur, which can be LOOSELY thought of as the \"probability\" of the event occuring per unit time: \\[ \\begin{aligned} X &\\sim \\text{Exponential} (\\lambda) \\\\ \\\\ f(x) &= 1 - \\lambda \\cdot e^{-\\lambda x} \\\\ F(x) &= 1 - e^{-\\lambda x} \\\\ \\\\ E(x) &= \\frac{1}{\\lambda} \\\\ Var (x) &= \\frac{1}{\\lambda^2} \\end{aligned} \\] Similar to the geometric distribution, the exponential distribution is Memoryless . Recall that the intuition is that the rate that the event occurs remains constant over time . \\[ \\begin{aligned} P(X > m + x \\mid X \\ge m) &= P(X > x) \\\\ E(X > m + x \\mid X \\ge m) &= E(X) \\\\ Var(X > m + x \\mid X \\ge m) &= Var(X) \\end{aligned} \\] Alternative Parameterization \u00b6 Given that the Exponential and Poisson Distributions are related, the exponential distribution can also be parameterized in terms the poisson parameter \\(\\mu\\) . In fact, the rate is simply the inverse of the mean : \\[ \\begin{aligned} \\lambda &= \\frac{1}{\\mu} \\\\ \\therefore X &\\sim \\text{Exponential} \\left(\\frac{1}{\\mu} \\right) \\\\ \\\\ f(x) &= 1 - \\lambda \\cdot e^{-\\frac{x}{\\mu}} \\\\ F(x) &= 1 - e^{-\\frac{x}{\\mu}} \\\\ \\\\ E(x) &= \\mu \\\\ Var (x) &= \\mu^2 \\end{aligned} \\] This version of the exponential distribution will be more useful for the exam as it is provided on the formula sheet. Note that the formula sheet uses \\(\\theta\\) instead of \\(\\mu\\) , which is what will be used for all other exponential related distributions. Gamma Distribution \u00b6 The Gamma Distribution is used to model the total time till the \\(\\alpha\\) -th event occurs in a Poisson Process. It has two parameters: \\(\\alpha\\) : The number of events \\(\\theta\\) : Mean \\[ \\begin{aligned} X &\\sim \\text{Gamma}(\\alpha, \\theta) \\\\ \\\\ f(x) &= \\frac{\\theta e^{-\\theta x} \\cdot (\\theta x)^{\\alpha - 1}}{\\Gamma(\\alpha)} \\\\ \\Gamma(\\alpha) &= (\\alpha - 1)! \\\\ \\\\ E(X) &= \\frac{\\alpha}{\\theta} \\\\ Var (X) &= \\frac{\\alpha}{\\theta^2} \\end{aligned} \\] The CDF is hard to evaluate, thus we can determine it intuitively using the poisson process and poisson distribution. If \\(x\\) is the time taken for the \\(\\alpha\\) -th event to occur, then it means that \\(\\alpha-1\\) events occurred in the timespan of \\(x\\) . Given that events occur at a rate of \\(\\theta\\) , it means that on average only \\(\\theta \\cdot x\\) occur in that period of time. Thus, the probability of waiting \\(x\\) for the \\(\\alpha\\) -th event to occur is equivalent to the probability that only \\(\\alpha-1\\) events occur in that time : \\[ \\begin{aligned} X &\\sim \\text{Gamma} (\\alpha, \\theta) \\\\ N &\\sim \\text{Poisson} (\\frac{x}{\\theta}) \\\\ \\\\ \\therefore P(X \\le x) &= P(N \\le \\alpha - 1) \\end{aligned} \\] The gamma distribution is actually the sum of \\(\\alpha\\) independent exponential variables with the same rate ( \\(\\theta\\) ): \\[ \\begin{aligned} Y_k &\\sim \\text{Exponential} (\\theta) \\\\ X &= Y_1 + Y_2 + \\dots + Y_k \\\\ X &\\sim \\text{Gamma} (\\alpha, \\theta) \\end{aligned} \\] Tip An exponential distribution is a special case of a gamma distribution with \\(\\alpha = 1\\) . Thus, the sum of independent gamma distributions is still gamma: \\[ \\begin{aligned} X_k &\\sim \\text{Gamma} (\\alpha_k, \\theta) \\\\ X &= X_1 + X_2 + \\dots + X_k \\\\ X &\\sim \\text{Gamma} (\\alpha_1 + \\alpha_2 + \\dots + \\alpha_k, \\theta) \\end{aligned} \\] Weibull Distribution \u00b6 Similar to the Exponential Distribution, the Weibull Distribution is used to the model the time taken for the first event to occur in a poisson process. However, the difference is that unlike the exponential distribution, the rate of the event occuring changes over time . Info It is commonly used to model machine survival , where the event of interest is the failure of the machine. In other words, it models the probability of the machine failing. It has two parameters: \\(\\theta' = \\frac{1}{\\mu^{\\frac{1}{k}}}\\) : The rate at which events occur \\(k\\) : The behaviour of the rate over time The rate changes depending on the value of \\(k\\) : Larger than 1 \\((k \\gt 1)\\) : Rate increases over time Smaller than 1 \\((k \\lt 1)\\) Rate decreases over time Note that if \\(k=1\\) , it means that the rate that events occur is constant over time, which is simply equivalent to an exponential distribution . Thus, the weibull distribution is simply a transformed exponential distribution: \\[ Y = X^{\\frac{1}{k}} \\] Tip The exponential distribution is a special case of the weibull distribution where \\(k=1\\) . \\[ \\begin{aligned} Y &\\sim \\text{Weibull} \\left(\\theta', k \\right) \\\\ Y &\\sim \\text{Weibull} \\left(\\frac{1}{\\mu^{\\frac{1}{k}}}, k \\right) \\\\ \\\\ f(y) &= \\frac{k \\left(\\frac{y}{\\theta'} \\right)^k \\cdot e^{- \\left(\\frac{y}{\\theta'} \\right)^k}}{y} \\\\ F(y) &= 1 - e^{\\left(\\frac{y}{\\theta'} \\right)^k} \\\\ \\\\ E(Y) &= \\\\ Var (Y) &= \\\\ \\end{aligned} \\] Beta Distribution \u00b6 The Beta Distribution is a probability distribution of probabilities . In other words, it represents the distribution of all possible probabilities when we are unsure of the actual probability. Although out of scope for this exam, in Bayesian Inference, the Beta distribution is the Conjugate Prior to the Binomial Distribution. In other words, the beta distribution models the probability parameter in the binomial distribution . It has only two parameters, which represent the number of prior successes and failures: \\(a\\) : There are \\(a-1\\) prior successes before the experiment \\(b\\) : There are \\(b-1\\) prior failures before the experiment \\[ \\begin{aligned} X &\\sim \\text{Beta} (a, b) \\\\ \\\\ f(x) &= f(x) = c \\cdot x^{a-1} \\cdot (1-x)^{b-1} \\\\ \\\\ E(X) &= \\frac{a}{a+b} \\\\ Var (X) &= \\frac{ab}{(a+b)^2 \\cdot (a+b+1)} \\end{aligned} \\] Although the above is more common, the beta distribution can also be expressed more generally with a third parameter \\(\\theta\\) : \\[ \\begin{aligned} X &\\sim \\text{Beta} (a, b, \\theta) \\\\ \\\\ f(x) &= \\frac{\\Gamma (a+b)}{\\Gamma (a) \\Gamma (b)} \\left(\\frac{x}{\\theta} \\right)^a \\left(1 - \\frac{x}{\\theta} \\right)^{b-1} \\cdot \\frac{1}{x} \\\\ \\\\ E(X) &= \\frac{a}{a+b} \\cdot \\theta \\\\ Var (X) &= \\frac{ab}{(a+b)^2 \\cdot (a+b+1)} \\cdot \\theta^2 \\end{aligned} \\] Tip The \"normal\" beta distribution is a special case of the Generalized Beta Distribution with \\(\\theta = 1\\) . Uniform Distribution \u00b6 The Uniform Distribution is used to model events where every outcome has an equal probability of occuring . It has two parameters, which respectively represent the lower and upper bound of the possible outcomes: \\[ \\begin{aligned} X &\\sim \\text{Uniform} (a, b) \\\\ \\\\ f(x) &= \\frac{1}{b-a} \\\\ F(x) &= \\frac{x - a}{b - a} \\\\ \\\\ E(X) &= \\frac{a+b}{2} \\\\ Var (X) &= \\frac{(a-b)^2}{12} \\end{aligned} \\] The key property of the uniform distribution is that most of the common transformations can be found by simply adjusting the range: \\[ \\begin{aligned} X &\\sim \\text{Uniform} (a, b) \\\\ X \\mid X \\gt d &\\sim \\text{Uniform} (d, b) \\\\ X-d &\\sim \\text{Uniform} (a-d, b-d) \\\\ \\therefore X-d \\mid X \\gt d &\\sim \\text{Uniform} (0, b-d) \\end{aligned} \\] Warning This looks similar to the memoryless property, but it is a seperate concept. Note that the \\((0,\\theta)\\) uniform distribution can also be thought of as a Generalized Beta Distribution with parameters \\(a = b = 1\\) . In other words, there are no prior successes or failures , thus each outcome has an equal probability . Pareto Distribution \u00b6 The Pareto Distribution is used to model phenomena where few items account for a lot of it and a lot of items account for little of it. For instance, the Pareto Principle is used to describe wealth inequality: A small 20% of people account for a large 80% of wealth while a large 80% of people account for 20% of wealth. In statistical terms, it means that large outcomes are rare (20% of people, 80% of wealth) while small outcomes are common (80% of people, 20% of wealth). It has two parameters: \\(\\alpha\\) : \\(\\theta\\) : Minimum value \\[ \\begin{aligned} X &\\sim \\text{Pareto} (\\alpha, \\theta) \\\\ \\\\ f(x) &= \\frac{\\alpha \\theta^{\\alpha}}{(x+\\theta)^{\\alpha+1}} \\\\ \\\\ E(X) &= \\frac{\\theta}{\\alpha - 1} \\\\ Var(X) &= E(X)^2 \\cdot \\frac{\\alpha}{\\alpha - 2} \\end{aligned} \\] One of its key properties is that the k-th moment (raw & central) only exists if \\(k \\lt \\alpha\\) . Another special property is that... \\[ Y \\sim \\text{Pareto} (\\alpha, \\theta + d) \\] Single Parameter Pareto \u00b6 Parametric Distributions \u00b6 All the above distributions are those that are more commonly tested on the exam. However, many other distributions are also provided in the formula sheet but are much less likely to be tested due to their complexity. Parametric Distributions are distributions that are completely determined by a set of quantities known as Parameters. By assigning all possible numerical values to the parameters, the resulting set of distributions is known as a Parametric Distribution Family . An important parametric distribution family is the Linear Exponential Family (LEF). A distribution \\(f(x, \\theta)\\) is part of the LEF if it fulfils the following criteria: Its support does not depend on \\(\\theta\\) Its distribution function is in following form, where \\(p\\) , \\(q\\) and \\(r\\) are generic functions \\[ \\begin{aligned} f(x, \\theta) &= \\frac{p(x) \\cdot e^{r(\\theta) \\cdot x}}{q(\\theta)} \\\\ \\\\ E(X) &= \\frac{q'(\\theta)}{r'(\\theta) \\cdot q(\\theta)} \\\\ Var (X) &= \\frac{E'(X)}{r'(\\theta)} \\end{aligned} \\] Some examples of distributions belonging to the LEF: Discrete: Poisson, Binomial & Negative Binomial Gamma & Normal Consider adding example, only p(x) can contain x, every other function cannot Scale Distribution \u00b6 If a parametric distribution, when scaled by a constant is still within the same family of distributions , then is it known as a Scale Distribution . Scale Distributions typically have a special Scale Parameter , which fulfils the following conditions: Scale parameter is multiplied by the same constant All other parameters remain unchanged All other parameters are typically known as Shape Parameters . For the purposes of the exam, \\(\\theta\\) is used to specially denote Scale Parameters . However, note that there are multiple ways to parameterize a distribution and multiple ways to denote them, thus not all questions will follow the exam tables. Thus, it is important to practice how to identify them. For any of the above scale distributions, if scaled by a constant \\(c\\) , their parameters get changed in the following manner: \\[ \\begin{aligned} X &\\sim \\text{Distribution} (\\text{Shape Parameters}, \\theta) \\\\ cX &\\sim \\text{Distribution} (\\text{Shape Parameters}, c\\theta) \\\\ \\\\ X &\\sim \\text{Normal} (\\mu, \\sigma^2) \\\\ cX &\\sim \\text{Normal} (c\\mu, (c\\sigma)^2) \\\\ \\\\ X &\\sim \\text{Lognormal} (\\mu, \\sigma^2) \\\\ cX &\\sim \\text{Lognormal} (\\mu + \\ln c, \\sigma^2) \\end{aligned} \\]","title":"Severity Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#severity-models","text":"","title":"Severity Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#severity-distributions","text":"The size of the loss is usually known as Loss Severity and is typically modelled with a continuous distribution. Let \\(X\\) be the random variable denoting the size of the loss. This section will cover most of the relevant severity distributions, but similarly note that there is no need to memorize anything as they are all provided in the formula sheet.","title":"Severity Distributions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#normal-distribution","text":"The Normal Distribution is used to model processes where outcomes close to the mean are common and become increasingly less common as it strays further from it. The unique property of the normal distribution in that it has two parameters , which are already its Mean \\(\\mu\\) and Variance \\(\\sigma^2\\) : \\[ \\begin{aligned} X &\\sim N(\\mu, \\sigma^2) \\\\ \\\\ f(x) &= \\frac{1}{\\sigma \\sqrt{2\\pi}} \\cdot e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\\\ \\\\ E(X) &= \\mu \\\\ Var (X) &= \\sigma^2 \\\\ \\end{aligned} \\] Another interesting property of the normal distribution is that: Roughly 68.3% of the data is within 1 SD of the average ( \\(\\mu - 1\\sigma \\lt x \\lt \\mu + 1\\sigma\\) ) Roughly 95.5% of the data is within 2 SD of the average ( \\(\\mu - 2\\sigma \\lt x \\lt \\mu + 2\\sigma\\) ) Roughly 99.7% of the data is within 3 SD of the average ( \\(\\mu - 3\\sigma \\lt x \\lt \\mu + 3\\sigma\\) ) The sum of \\(k\\) independent normal random variables is still normal: \\[ \\begin{aligned} X_k &\\sim N(\\mu, \\sigma^2) \\\\ X &= X_1 + X_2 + \\dots + X_k \\\\ \\therefore X &\\sim N(\\mu_1 + \\mu_2 + ... + \\mu_k, \\sigma^2_1 + \\sigma^2_2 + ... + \\sigma_k) \\end{aligned} \\] Another key property is that they are symmetrical about their mean : \\[ \\begin{aligned} P(Z \\lt -a) &= P(Z \\gt a) \\\\ &= 1 - P(Z \\lt a) \\end{aligned} \\]","title":"Normal Distribution"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#standard-normal-distribution","text":"If the parameters of the normal distribution are 0 and 1 respectively, then it is known as the Standard Normal Distribution , specially denoted by \\(Z\\) . \\[ Z \\sim N(0, 1) \\] Any normal distribution can be converted into a standard normal through Standardization : \\[ Z = \\frac{X - E(X)}{\\sqrt{Var (X)}} \\] Note An easy way to remember is that the above transformation sets the mean to 0 \\(E(X) - E(X) = 0\\) and sets the variance to 1 \\(\\frac{Var(X)}{Var(X)} = 1\\) . The CDF of a standard normal distribution is denoted by \\(\\Phi\\) : \\[ P(Z \\le z) = \\Phi(z) \\] Since the exam provides values for the CDF of Z , most calculations involving a normal distribution will have first have to be standardized and then calculated using its CDF: \\[ \\begin{aligned} P(X \\le x) &= P \\left(\\frac{X - E(X)}{\\sqrt{Var (X)}} \\le \\frac{x - E(X)}{\\sqrt{Var (X)}} \\right) \\\\ &= P \\left(Z \\le \\frac{x - E(X)}{\\sqrt{Var (X)}} \\right) \\\\ &= \\Phi \\left(\\frac{x - E(X)}{\\sqrt{Var (X)}} \\right) \\end{aligned} \\] The reverse is also common whereby the probability is provided to solve for the value of Z and hence the value of the underlying variable. The issue is that most of the time, the probability provided will not be exactly equal to the values on the table. In such a case, the midpoint between two values of Z will be taken: Given: \\(P = 0.95\\) Table Value: \\(Z = 1.64, P = 0.9495\\) Table Value: \\(Z = 1.65, P = 0.9505\\) Midpoint: \\(Z = 1.645\\)","title":"Standard Normal Distribution"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#lognormal-distribution","text":"A exponential transformed normal distribution is known as the Lognormal Distribution as it has a logarithm applied to it. It shares the same two parameters as a regular normal distribution , but they are NOT the mean and variance of the lognormal distribution. \\[ \\begin{aligned} Y &= e^X \\\\ Y &\\sim \\text{Lognormal} (\\mu, \\sigma^2) \\\\ \\\\ f(y) &= \\frac{1}{x\\sigma \\sqrt{2\\pi}} \\cdot e^{-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}} \\\\ \\\\ E(Y) &= e^{\\mu + 0.5 \\sigma^2} \\\\ Var (Y) &= E(Y)^2 \\cdot (e^{\\sigma^2} - 1) \\end{aligned} \\] Warning Given how similar the two distributions are, it is easy to mix them up. It is a common mistake to think that the lognormal parameters are its mean and variance. Note that since it is transformed from a normal distribution, it must use its parameters , but they are NOT its mean and variance. Another common mistake is thinking that the PDFs are identical, given how similar they look. However, there are two key differences : The denominator has an extra \\(x\\) multiplied to it The \\(x\\) in the exponent has become \\(\\ln x\\) The CDF of the log distribution can also be calculated through the standard normal distribution : \\[ \\begin{aligned} P(Y \\le y) &= P(e^X \\le y) \\\\ &= P(X \\le \\ln y) \\\\ &= P \\left(\\frac{X - \\mu}{\\sigma} \\le \\frac{\\ln y - \\mu}{\\sigma} \\right) \\\\ &= P \\left(Z \\le \\frac{\\ln y - \\mu}{\\sigma} \\right) \\\\ &= \\Phi \\left(\\frac{\\ln y - \\mu}{\\sigma} \\right) \\end{aligned} \\] Note that since the sum of independent normal variables is still normal, then the product of indepdent lognormal variables is still lognormal : \\[ \\begin{aligned} X &= X_1 + X_2 + \\dots + X_k \\\\ X &\\sim N(\\mu_1 + \\mu_2 + ... + \\mu_k, \\sigma^2_1 + \\sigma^2_2 + ... + \\sigma_k) \\\\ \\\\ Y &= e^X \\\\ &= e^{X_1 + X_2 + \\dots + X_k} \\\\ &= e^{X_1} \\cdot e^{X_2} \\cdot (\\dots) \\cdot e^{X_k} \\\\ \\therefore Y &\\sim \\text{Lognormal}(\\mu_1 + \\mu_2 + ... + \\mu_k, \\sigma^2_1 + \\sigma^2_2 + ... + \\sigma_k) \\end{aligned} \\]","title":"Lognormal Distribution"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#exponential-distribution","text":"The Exponential Distribution is used to model the time taken for the first event to occur in a Poisson Process . It has a single parameter \\(\\lambda\\) , which represents the rate at which events occur, which can be LOOSELY thought of as the \"probability\" of the event occuring per unit time: \\[ \\begin{aligned} X &\\sim \\text{Exponential} (\\lambda) \\\\ \\\\ f(x) &= 1 - \\lambda \\cdot e^{-\\lambda x} \\\\ F(x) &= 1 - e^{-\\lambda x} \\\\ \\\\ E(x) &= \\frac{1}{\\lambda} \\\\ Var (x) &= \\frac{1}{\\lambda^2} \\end{aligned} \\] Similar to the geometric distribution, the exponential distribution is Memoryless . Recall that the intuition is that the rate that the event occurs remains constant over time . \\[ \\begin{aligned} P(X > m + x \\mid X \\ge m) &= P(X > x) \\\\ E(X > m + x \\mid X \\ge m) &= E(X) \\\\ Var(X > m + x \\mid X \\ge m) &= Var(X) \\end{aligned} \\]","title":"Exponential Distribution"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#alternative-parameterization","text":"Given that the Exponential and Poisson Distributions are related, the exponential distribution can also be parameterized in terms the poisson parameter \\(\\mu\\) . In fact, the rate is simply the inverse of the mean : \\[ \\begin{aligned} \\lambda &= \\frac{1}{\\mu} \\\\ \\therefore X &\\sim \\text{Exponential} \\left(\\frac{1}{\\mu} \\right) \\\\ \\\\ f(x) &= 1 - \\lambda \\cdot e^{-\\frac{x}{\\mu}} \\\\ F(x) &= 1 - e^{-\\frac{x}{\\mu}} \\\\ \\\\ E(x) &= \\mu \\\\ Var (x) &= \\mu^2 \\end{aligned} \\] This version of the exponential distribution will be more useful for the exam as it is provided on the formula sheet. Note that the formula sheet uses \\(\\theta\\) instead of \\(\\mu\\) , which is what will be used for all other exponential related distributions.","title":"Alternative Parameterization"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#gamma-distribution","text":"The Gamma Distribution is used to model the total time till the \\(\\alpha\\) -th event occurs in a Poisson Process. It has two parameters: \\(\\alpha\\) : The number of events \\(\\theta\\) : Mean \\[ \\begin{aligned} X &\\sim \\text{Gamma}(\\alpha, \\theta) \\\\ \\\\ f(x) &= \\frac{\\theta e^{-\\theta x} \\cdot (\\theta x)^{\\alpha - 1}}{\\Gamma(\\alpha)} \\\\ \\Gamma(\\alpha) &= (\\alpha - 1)! \\\\ \\\\ E(X) &= \\frac{\\alpha}{\\theta} \\\\ Var (X) &= \\frac{\\alpha}{\\theta^2} \\end{aligned} \\] The CDF is hard to evaluate, thus we can determine it intuitively using the poisson process and poisson distribution. If \\(x\\) is the time taken for the \\(\\alpha\\) -th event to occur, then it means that \\(\\alpha-1\\) events occurred in the timespan of \\(x\\) . Given that events occur at a rate of \\(\\theta\\) , it means that on average only \\(\\theta \\cdot x\\) occur in that period of time. Thus, the probability of waiting \\(x\\) for the \\(\\alpha\\) -th event to occur is equivalent to the probability that only \\(\\alpha-1\\) events occur in that time : \\[ \\begin{aligned} X &\\sim \\text{Gamma} (\\alpha, \\theta) \\\\ N &\\sim \\text{Poisson} (\\frac{x}{\\theta}) \\\\ \\\\ \\therefore P(X \\le x) &= P(N \\le \\alpha - 1) \\end{aligned} \\] The gamma distribution is actually the sum of \\(\\alpha\\) independent exponential variables with the same rate ( \\(\\theta\\) ): \\[ \\begin{aligned} Y_k &\\sim \\text{Exponential} (\\theta) \\\\ X &= Y_1 + Y_2 + \\dots + Y_k \\\\ X &\\sim \\text{Gamma} (\\alpha, \\theta) \\end{aligned} \\] Tip An exponential distribution is a special case of a gamma distribution with \\(\\alpha = 1\\) . Thus, the sum of independent gamma distributions is still gamma: \\[ \\begin{aligned} X_k &\\sim \\text{Gamma} (\\alpha_k, \\theta) \\\\ X &= X_1 + X_2 + \\dots + X_k \\\\ X &\\sim \\text{Gamma} (\\alpha_1 + \\alpha_2 + \\dots + \\alpha_k, \\theta) \\end{aligned} \\]","title":"Gamma Distribution"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#weibull-distribution","text":"Similar to the Exponential Distribution, the Weibull Distribution is used to the model the time taken for the first event to occur in a poisson process. However, the difference is that unlike the exponential distribution, the rate of the event occuring changes over time . Info It is commonly used to model machine survival , where the event of interest is the failure of the machine. In other words, it models the probability of the machine failing. It has two parameters: \\(\\theta' = \\frac{1}{\\mu^{\\frac{1}{k}}}\\) : The rate at which events occur \\(k\\) : The behaviour of the rate over time The rate changes depending on the value of \\(k\\) : Larger than 1 \\((k \\gt 1)\\) : Rate increases over time Smaller than 1 \\((k \\lt 1)\\) Rate decreases over time Note that if \\(k=1\\) , it means that the rate that events occur is constant over time, which is simply equivalent to an exponential distribution . Thus, the weibull distribution is simply a transformed exponential distribution: \\[ Y = X^{\\frac{1}{k}} \\] Tip The exponential distribution is a special case of the weibull distribution where \\(k=1\\) . \\[ \\begin{aligned} Y &\\sim \\text{Weibull} \\left(\\theta', k \\right) \\\\ Y &\\sim \\text{Weibull} \\left(\\frac{1}{\\mu^{\\frac{1}{k}}}, k \\right) \\\\ \\\\ f(y) &= \\frac{k \\left(\\frac{y}{\\theta'} \\right)^k \\cdot e^{- \\left(\\frac{y}{\\theta'} \\right)^k}}{y} \\\\ F(y) &= 1 - e^{\\left(\\frac{y}{\\theta'} \\right)^k} \\\\ \\\\ E(Y) &= \\\\ Var (Y) &= \\\\ \\end{aligned} \\]","title":"Weibull Distribution"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#beta-distribution","text":"The Beta Distribution is a probability distribution of probabilities . In other words, it represents the distribution of all possible probabilities when we are unsure of the actual probability. Although out of scope for this exam, in Bayesian Inference, the Beta distribution is the Conjugate Prior to the Binomial Distribution. In other words, the beta distribution models the probability parameter in the binomial distribution . It has only two parameters, which represent the number of prior successes and failures: \\(a\\) : There are \\(a-1\\) prior successes before the experiment \\(b\\) : There are \\(b-1\\) prior failures before the experiment \\[ \\begin{aligned} X &\\sim \\text{Beta} (a, b) \\\\ \\\\ f(x) &= f(x) = c \\cdot x^{a-1} \\cdot (1-x)^{b-1} \\\\ \\\\ E(X) &= \\frac{a}{a+b} \\\\ Var (X) &= \\frac{ab}{(a+b)^2 \\cdot (a+b+1)} \\end{aligned} \\] Although the above is more common, the beta distribution can also be expressed more generally with a third parameter \\(\\theta\\) : \\[ \\begin{aligned} X &\\sim \\text{Beta} (a, b, \\theta) \\\\ \\\\ f(x) &= \\frac{\\Gamma (a+b)}{\\Gamma (a) \\Gamma (b)} \\left(\\frac{x}{\\theta} \\right)^a \\left(1 - \\frac{x}{\\theta} \\right)^{b-1} \\cdot \\frac{1}{x} \\\\ \\\\ E(X) &= \\frac{a}{a+b} \\cdot \\theta \\\\ Var (X) &= \\frac{ab}{(a+b)^2 \\cdot (a+b+1)} \\cdot \\theta^2 \\end{aligned} \\] Tip The \"normal\" beta distribution is a special case of the Generalized Beta Distribution with \\(\\theta = 1\\) .","title":"Beta Distribution"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#uniform-distribution","text":"The Uniform Distribution is used to model events where every outcome has an equal probability of occuring . It has two parameters, which respectively represent the lower and upper bound of the possible outcomes: \\[ \\begin{aligned} X &\\sim \\text{Uniform} (a, b) \\\\ \\\\ f(x) &= \\frac{1}{b-a} \\\\ F(x) &= \\frac{x - a}{b - a} \\\\ \\\\ E(X) &= \\frac{a+b}{2} \\\\ Var (X) &= \\frac{(a-b)^2}{12} \\end{aligned} \\] The key property of the uniform distribution is that most of the common transformations can be found by simply adjusting the range: \\[ \\begin{aligned} X &\\sim \\text{Uniform} (a, b) \\\\ X \\mid X \\gt d &\\sim \\text{Uniform} (d, b) \\\\ X-d &\\sim \\text{Uniform} (a-d, b-d) \\\\ \\therefore X-d \\mid X \\gt d &\\sim \\text{Uniform} (0, b-d) \\end{aligned} \\] Warning This looks similar to the memoryless property, but it is a seperate concept. Note that the \\((0,\\theta)\\) uniform distribution can also be thought of as a Generalized Beta Distribution with parameters \\(a = b = 1\\) . In other words, there are no prior successes or failures , thus each outcome has an equal probability .","title":"Uniform Distribution"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#pareto-distribution","text":"The Pareto Distribution is used to model phenomena where few items account for a lot of it and a lot of items account for little of it. For instance, the Pareto Principle is used to describe wealth inequality: A small 20% of people account for a large 80% of wealth while a large 80% of people account for 20% of wealth. In statistical terms, it means that large outcomes are rare (20% of people, 80% of wealth) while small outcomes are common (80% of people, 20% of wealth). It has two parameters: \\(\\alpha\\) : \\(\\theta\\) : Minimum value \\[ \\begin{aligned} X &\\sim \\text{Pareto} (\\alpha, \\theta) \\\\ \\\\ f(x) &= \\frac{\\alpha \\theta^{\\alpha}}{(x+\\theta)^{\\alpha+1}} \\\\ \\\\ E(X) &= \\frac{\\theta}{\\alpha - 1} \\\\ Var(X) &= E(X)^2 \\cdot \\frac{\\alpha}{\\alpha - 2} \\end{aligned} \\] One of its key properties is that the k-th moment (raw & central) only exists if \\(k \\lt \\alpha\\) . Another special property is that... \\[ Y \\sim \\text{Pareto} (\\alpha, \\theta + d) \\]","title":"Pareto Distribution"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#single-parameter-pareto","text":"","title":"Single Parameter Pareto"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#parametric-distributions","text":"All the above distributions are those that are more commonly tested on the exam. However, many other distributions are also provided in the formula sheet but are much less likely to be tested due to their complexity. Parametric Distributions are distributions that are completely determined by a set of quantities known as Parameters. By assigning all possible numerical values to the parameters, the resulting set of distributions is known as a Parametric Distribution Family . An important parametric distribution family is the Linear Exponential Family (LEF). A distribution \\(f(x, \\theta)\\) is part of the LEF if it fulfils the following criteria: Its support does not depend on \\(\\theta\\) Its distribution function is in following form, where \\(p\\) , \\(q\\) and \\(r\\) are generic functions \\[ \\begin{aligned} f(x, \\theta) &= \\frac{p(x) \\cdot e^{r(\\theta) \\cdot x}}{q(\\theta)} \\\\ \\\\ E(X) &= \\frac{q'(\\theta)}{r'(\\theta) \\cdot q(\\theta)} \\\\ Var (X) &= \\frac{E'(X)}{r'(\\theta)} \\end{aligned} \\] Some examples of distributions belonging to the LEF: Discrete: Poisson, Binomial & Negative Binomial Gamma & Normal Consider adding example, only p(x) can contain x, every other function cannot","title":"Parametric Distributions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/3.%20Severity%20Models/#scale-distribution","text":"If a parametric distribution, when scaled by a constant is still within the same family of distributions , then is it known as a Scale Distribution . Scale Distributions typically have a special Scale Parameter , which fulfils the following conditions: Scale parameter is multiplied by the same constant All other parameters remain unchanged All other parameters are typically known as Shape Parameters . For the purposes of the exam, \\(\\theta\\) is used to specially denote Scale Parameters . However, note that there are multiple ways to parameterize a distribution and multiple ways to denote them, thus not all questions will follow the exam tables. Thus, it is important to practice how to identify them. For any of the above scale distributions, if scaled by a constant \\(c\\) , their parameters get changed in the following manner: \\[ \\begin{aligned} X &\\sim \\text{Distribution} (\\text{Shape Parameters}, \\theta) \\\\ cX &\\sim \\text{Distribution} (\\text{Shape Parameters}, c\\theta) \\\\ \\\\ X &\\sim \\text{Normal} (\\mu, \\sigma^2) \\\\ cX &\\sim \\text{Normal} (c\\mu, (c\\sigma)^2) \\\\ \\\\ X &\\sim \\text{Lognormal} (\\mu, \\sigma^2) \\\\ cX &\\sim \\text{Lognormal} (\\mu + \\ln c, \\sigma^2) \\end{aligned} \\]","title":"Scale Distribution"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/4.%20Policy%20Modifications/","text":"Policy Modifications \u00b6 Key Concepts \u00b6 Policy Modifications are the parts of an insurance policy that defines how much the insurer is required to pay for each loss incurred by the policyholder. Before continuing, there are two key notations ( \\(\\land, {}_{+}\\) )that will be used throughout this subsection: \\[ \\begin{aligned} \\min (a, b) &= \\begin{cases} a,& a \\lt b \\\\ b,& a \\ge b \\end{cases} \\\\ &= a \\land b \\\\ \\\\ \\max (a-b, 0) &= \\begin{cases} 0,& a \\lt b \\\\ a-b,& a \\ge b \\end{cases} \\\\ &= (a-b)_{+} \\\\ &= \\text{Non-negative part; floored at 0} \\end{aligned} \\] The two are related through the following expression: \\[ \\begin{aligned} (a-b)_{+} &= a - (a \\land b) \\\\ \\\\ \\text{If } a &\\gt b \\text{,} \\\\ (a-b)_{+} &= a-b \\\\ a - (a \\land b) &= a-b \\\\ \\therefore (a-b)_{+} &= a - (a \\land b) \\\\ \\\\ \\text{If } a &\\lt b \\text{,} \\\\ (a-b)_{+} &= 0 \\\\ a - (a \\land b) &= a-a = 0 \\\\ \\therefore (a-b)_{+} &= a - (a \\land b) \\\\ \\end{aligned} \\] There are three key random variables regarding claims and losses: \\(X\\) is the Ground Up Loss variable, representing the total loss incurred by the policyholder \\(Y^L\\) is the Payment per Loss variable, representing the amount paid by the insurer \\(Y^P\\) is the Payment per Payment variable, representing the amount paid by the insurer given that there is a positive payment To properly understand the difference between the three, consider an example of a policy with a deductible of 800 , but in two different situations: \\(X\\) or Loss \\(Y^L\\) \\(Y^P\\) 2000 1200 1200 600 0 NOT Observed There are two main takeaways: If \\(Y^L > 0\\) , then \\(Y^P = Y^L\\) If \\(Y^L = 0\\) , then \\(Y^P\\) is NOT Observed \\(\\therefore Y^P = Y^L \\mid Y^L > 0\\) Warning Do NOT mistakenly think that \\(Y^P\\) being unobserved is equivalent to \\(Y^P = 0\\) . They are completely DIFFERENT. \\(Y^L\\) can be thought of as a two point mixture distribution of \\(Y^P\\) and a not-so-random-variable that always takes 0 . Thus, it can also be thought of as a conditional distribution dependent on whether \\(Y^L \\gt 0\\) : \\[ \\begin{aligned} F_{Y^L} (t) &= F_{Y_L | Y^L > 0} (t) \\cdot P(Y^L > 0) + F_{Y_L | Y^L = 0} (t) \\cdot P(Y^L = 0) \\\\ &= F_{Y^P} (t) \\cdot P(Y^L > 0) + F_{0} (t) \\cdot P(Y^L = 0) \\\\ \\end{aligned} \\] Let \\(\\pi = P(Y^L > 0)\\) , which becomes the weight of the mixture: \\[ \\begin{aligned} F_{Y^L} (t) &= F_{Y_P} (t) \\cdot \\pi + F_{0}(t) \\cdot (1-\\pi) \\\\ 1 - F_{Y^L} (t) &= [1 - F_{Y_P} (t)] \\cdot \\pi + [1-F_{0}(t)] \\cdot (1-\\pi) \\\\ S_{Y^L} (t) &= S_{Y_P} (t) \\cdot \\pi \\\\ \\\\ E \\left(Y^L \\right) &= E \\left(Y^P \\right) \\cdot \\pi + E(0) \\cdot (1-\\pi) \\\\ E \\left(Y^L \\right) &= E \\left(Y^L \\right) \\cdot \\pi \\end{aligned} \\] Thus, the values for \\(Y^L\\) and \\(Y^P\\) can be easily converted to one another by using the weight of the mixture (probability of a positive payment). Additionally, since \\(Y^P\\) is essentially \\(Y^L\\) being conditional on itself, we can easily obtain its conditional distribution : \\[ \\begin{aligned} f_{Y_P} (y) &= f_{Y^L \\mid Y^L \\gt 0} (y) \\\\ &= \\frac{f_{Y^L} (y)}{P(Y^L \\gt 0)} \\end{aligned} \\] Tip If \\(Y^L\\) follows the exponential or geometric distribution which are memoryless, then the distribution of \\(Y^P\\) is the same as \\(Y^L\\) . \\[ \\begin{aligned} Y^L &\\sim \\text{Exponential/Geometric} \\\\ Y^P &= X \\mid X \\gt d \\\\ Y^P &\\sim \\text{Exponential/Geometric} \\end{aligned} \\] Similarly, if \\(Y^L\\) follows the uniform distribution, then the distribution of \\(Y^P\\) can be easily calculated: \\[ \\begin{aligned} X &\\sim uni (a,b) \\\\ Y^P &= X - d \\mid X \\gt d \\\\ \\therefore Y^P \\sim \\text{Uniform} (a, b-d) \\end{aligned} \\] Note that for all losses without deductibles , a loss will always result in a claim. Thus, \\(Y^P = Y^L\\) . Ordinary Deductibles \u00b6 The first type of policy modification is known as a Deductible , denoted by \\(d\\) . It is the amount that the policyholder is responsible for paying before the insurer pays a claim: If the loss amount does not exceed the deductible, then the insurer will pay out nothing If it does exceed the deductible , then the insurer will pay out the excess For each loss \\(X\\) , the policyholder is responsible for paying losses up to \\(d\\) . It is denoted by \\(X \\land d\\) , which is known as the Limited Loss Variable : \\[ \\begin{aligned} X \\land d &= \\begin{cases} X,& X \\le d\\\\ d,& X \\gt d \\end{cases} \\end{aligned} \\] Tip If \\(X\\) follows a known distribution , then \\(E(X \\land d)\\) can be easily found inside the formula sheet. However, if \\(X\\) does NOT follow a known distribution , then \\(E(X \\land d)\\) must be found through first principles. Most of the time, such questions assume that losses are discrete (EG. 40, 80, 120 & 160) with some deductible not exactly equal to any of the loss amounts (EG. 100). The tricky part is remembering that once the loss is larger the deductible, only the deductible will be paid: \\[ \\begin{aligned} X \\land d &= \\begin{cases} 40,& X = 40 \\\\ 80,& X = 80 \\\\ 100,& X \\ge 120 \\end{cases} \\\\ \\\\ \\therefore E(X \\land d) &= 40 \\cdot P(X = 40) + 80 \\cdot P(X = 80) + 100 \\cdot [1 - P(X = 40) - P(X = 80)] \\end{aligned} \\] The key is understanding that the final case is simply the deductible itself, with its probability being the complement of all other cases below the deductible . The insurer will then cover the excess amount, represented by the Payment Per Loss variable: \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& X \\le d\\\\ X - d,& X \\gt d \\end{cases} \\\\ &= (X-d)_{+} \\\\ &= X - (X \\land d) \\\\ \\\\ \\therefore E \\left(Y^L \\right) &= E(X) - E(X \\land d) \\end{aligned} \\] Tip An intuitive way to remember is that \\(\\text{Claim} = \\text{Full Loss} - \\text{Limited Loss}\\) . In practice, policyholders would not report losses below the deductible . Thus, the insurers payment is better represented by the Payment Per Payment variable, also known as the Excess Loss Variable : \\[ \\begin{aligned} Y^P &= Y^L \\mid Y^L \\gt 0 \\\\ &= X - d \\mid X \\gt d \\\\ &= \\begin{cases} \\text{Undefined},& X \\le d\\\\ X - d,& X \\gt d \\end{cases} \\\\ \\\\ f_{Y_P} (y) &= \\frac{f_{Y^L} (y)}{P(Y^L \\gt 0)} \\\\ &= \\frac{f_{Y^L} (y)}{P(X \\gt d)} \\\\ &= \\frac{f_{Y^L} (y)}{1 - F_{X}(d)} \\\\ \\\\ E \\left(Y^P \\right) &= \\int^{\\infty}_{d} y^P \\cdot f_{Y_P} (y) \\\\ &= \\int^{\\infty}_{d} (x-d) \\cdot \\frac{f_{Y^L} (y)}{1 - F_{X}(d)} \\\\ &= \\frac{1}{1 - F_{X}(d)} \\cdot \\int^{\\infty}_{d} (x-d) \\cdot f_{Y^L} (y) \\\\ &= \\frac{E \\left(Y^L \\right)}{1 - F_{X}(d)} \\\\ \\end{aligned} \\] The key metric insurers look out for is the Loss Elimintation Ratio (LER), which is a measure of how much the insurer saves by imposing the deductible; how much less they end up paying. Formally, it is defined as the ratio of a decrease in expected payment when a deductible is imposed to the expected payment without a deductible: \\[ \\begin{aligned} \\text{LER} &= \\frac{E(X) - E \\left(Y^L \\right)}{E(X)} \\\\ &= \\frac{E(X) - E [(X-d)_{+}]}{E(X)} \\\\ &= \\frac{E \\left[X-(X-d)_{+} \\right]}{E(X)} \\\\ &= \\frac{E(X \\land d)}{E(X)} \\end{aligned} \\] Variance of Payments \u00b6 Unfortunately, the formula for expectation ONLY holds for the first raw moment . Thus, the second moment can only be calculated via first principles . If severity is continuous, then the survival function method (FAM-L) can be used to calculate the second moment: \\[ \\begin{aligned} E \\left(Y^L \\right) &= E \\left[(X-d)_{+} \\right] \\\\ &= \\int^{d}_{0} 0 \\cdot f(x) + \\int^{\\infty}_{d} (x-d) \\cdot f(x) \\\\ &= \\int^{\\infty}_{d} (x-d) \\cdot f(x) \\\\ \\\\ E \\left[ \\left(Y^L \\right) \\right]^2 &= \\int^{\\infty}_{d} (x-d)^2 \\cdot f(x) \\\\ &= [S_X(x) \\cdot (x-d)^2]^{\\infty}_{d} - \\int^{\\infty}_{d} -S(x) \\cdot 2(x-d) \\\\ &= \\int^{\\infty}_{d} S(x) \\cdot 2(x-d) \\end{aligned} \\] If severity is discrete, then determine all possible values for \\(Y^L\\) and then manually calculate the second moment as the sum product. If the variance of the payment per payment variable is required, then the same conversion applies for the second moment as well: \\[ E \\left[ \\left(Y^P \\right) \\right]^2 = \\frac{E \\left[ \\left(Y^L \\right) \\right]^2}{P(X \\gt d)} \\] Franchise Deductibles \u00b6 A special kind of deductible is known as Franchise Deductible . Unlike a regular deductible that will only pay the excess, it will pay the full amount . \\[ \\begin{aligned} Y^L_{\\text{Franchise}} &= \\begin{cases} 0,& X \\le d\\\\ X,& X \\gt d \\end{cases} \\\\ \\\\ E \\left(Y^L_{\\text{Franchise}} \\right) &= \\int^{d}_{0} 0 \\cdot f(x) + \\int^{\\infty}_{d} x \\cdot f(x) \\\\ &= \\int^{\\infty}_{d} x \\cdot f(x) \\\\ &= \\int^{\\infty}_{d} (x-d+d) \\cdot f(x) \\\\ &= \\int^{\\infty}_{d} (x-d) \\cdot f(x) + d \\cdot \\int^{\\infty}_{d} f(x) \\\\ &= E[Y^L] + d \\cdot [1-F_{X}(d)] \\end{aligned} \\] The same relationship as before can be applied for the payment per payment variable: \\[ \\begin{aligned} Y^P &= \\begin{cases} \\text{Undefined},& X \\le d\\\\ X,& X \\gt d \\end{cases} \\\\ \\\\ E \\left(Y^P_{\\text{Franchise}} \\right) &= \\frac{E \\left(Y^L_{\\text{Franchise}} \\right)}{1 - F_{X}(d)} \\\\ &= \\frac{E[Y^L] + d \\cdot [1-F_{X}(d)]}{1 - F_{X}(d)} \\\\ &= \\frac{E[Y^L]}{1 - F_{X}(d)} + \\frac{d \\cdot [1-F_{X}(d)]}{1 - F_{X}(d)} \\\\ &= E \\left(Y^P \\right) + d \\end{aligned} \\] Coinsurance \u00b6 Another type of policy modification is known as Coinsurance , denoted by \\(\\alpha\\) . As its name suggests, the key idea is that both the insurer and policyholder insure the loss . The insurer covers \\(\\alpha\\) proportion of the loss while the poliyholder covers the remaining \\(1-\\alpha\\) proportion. \\[ \\begin{aligned} Y^L &= \\alpha \\cdot X \\\\ E \\left(Y^L \\right) &= \\alpha \\cdot E(X) \\\\ \\end{aligned} \\] Warning Co-insurance usually refers to the portion of the loss that the policyholder is responsible for. However, for this exam, we are taking the insurers perspective, thus it refers to the amount that the insurer is responsible for. Do not confuse the two! Coinsurance & Deductibles \u00b6 Coinsurance on its own is a simple concept - the difficulty comes about when it is mixed with a Deductible . If the coinsurance is applied after the deductible : Entire loss \\(X\\) is applied against the deductible; poliycholder pays first \\(d\\) Coinsurance is applied against the remaining loss ; insurer pays remaining \\(\\alpha(X-d)\\) Unless stated otherwise, it is usually assumed that the coinsurance is applied after the deductible \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& X \\le d \\\\ \\alpha \\cdot (X - d),& X \\gt d \\end{cases} \\\\ &= \\begin{cases} \\alpha \\cdot 0,& X \\le d \\\\ \\alpha \\cdot (X - d),& X \\gt d \\end{cases} \\\\ &= \\alpha \\cdot \\begin{cases} 0,& X \\le d \\\\ (X - d),& X \\gt d \\end{cases} \\\\ &= \\alpha \\cdot (X-d)_{+} \\\\ &= \\alpha \\cdot [X - (X \\land d)] \\\\ \\\\ E \\left(Y^L \\right) &= \\alpha \\cdot E [X - (X \\land d)] \\\\ &= \\alpha \\cdot [E(X) - E(X \\land d)] \\\\ \\\\ E \\left(Y^P \\right) &= \\frac{E \\left(Y^L \\right)}{1 - F_{X}(d)} \\end{aligned} \\] Note The key is remembering that any value multiplied by 0 is 0, thus the coinsurance factor can be factored out. If the coinsurance is applied before the deductible , Coinsurance is applied against the full loss ; insurer covers \\(\\alpha X\\) Deductible is applied against the coinsured amount ; insurer pays \\(\\alpha X - d\\) The deductible effectively increases to \\(\\frac{d}{a}\\) \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& \\alpha X \\le d \\\\ \\alpha X - d,& \\alpha X \\gt d \\end{cases} \\\\ &= \\begin{cases} 0,& X \\le \\frac{d}{a} \\\\ \\alpha \\cdot \\left(X - \\frac{d}{a} \\right),& X \\gt \\frac{d}{a} \\end{cases} \\\\ &= \\alpha \\cdot \\left(X - \\frac{d}{a} \\right)_{+} \\\\ &= \\alpha \\cdot \\left[X - \\left(X \\land \\frac{d}{a} \\right) \\right] \\\\ \\\\ E \\left(Y^L \\right) &= \\alpha \\cdot E \\left[X - \\left(X \\land \\frac{d}{a} \\right) \\right] \\\\ &= \\alpha \\cdot \\left[E(X) - E \\left(X \\land \\frac{d}{a} \\right) \\right] \\\\ \\\\ E \\left(Y^P \\right) &= \\frac{E \\left(Y^L \\right)}{1 - F_{X} \\left(\\frac{d}{a} \\right)} \\end{aligned} \\] Warning It is a common mistake to forget to update the deductible to the new one when using the complement of the CDF for the conversion between \\(Y^L\\) and \\(Y^P\\) . Bonus Incentive \u00b6 A special combination of a Deductible and Coinsurance is known as a Bonus Incentive . Insurers usually provide incentives to their agents if the total losses for the year are below some threshold , where the incentive is some percentage of the losses below this amount . Thus, the situation is similar to coinsurance applied after a deductible: \\[ \\begin{aligned} B &= \\begin{cases} \\alpha \\cdot (d - X),& X \\lt d \\\\ 0,& X \\ge d \\end{cases} \\\\ &= \\alpha \\cdot (d - X)_{+} \\\\ \\\\ \\therefore E(B) &= E(d-X)_{+} \\\\ &= E(d) - E(d \\land X) \\\\ &= E(d) - E(X \\land d) \\end{aligned} \\] Policy Limits \u00b6 The second type of policy modification is known as Policy Limit , denoted by \\(u\\) . It is the maximum amount that the insurer will payout. If \\(X\\) is below the limit, then the insurer will pay the loss If \\(X\\) is above the limit, then the insurer will only pay the limit \\[ \\begin{aligned} Y^L &= \\begin{cases} X,& X \\lt u \\\\ u,& X \\ge u \\end{cases} \\\\ &= X \\land u \\\\ \\\\ E \\left(Y^L \\right) &= \\int^{u}_{0} x \\cdot f(x) + \\int^{\\infty}_{u} u \\cdot f(x) \\\\ \\end{aligned} \\] Warning Under a deductible, \\(X \\land d\\) represents the limited loss of the policyholder . Under a policy limit, \\(x \\land u\\) represents the limited payment of the insurer . Note that the expectation can be simplified using the Survival Function Method (FAM-L): \\[ \\begin{aligned} E \\left(Y^L \\right) &= \\int^{u}_{0} x \\cdot f(x) + \\int^{\\infty}_{u} u \\cdot f(x) \\\\ &= \\left(\\left[x \\cdot (-S(x)) \\right]^u_0 - \\int^{u}_{0} (-S(x)) \\cdot 1 \\right) + u \\cdot S(u) \\\\ &= -u \\cdot S(u) - 0 + \\int^{u}_{0} S(x) + u \\cdot S(u) \\\\ &= \\int^{u}_{0} S(x) dx \\end{aligned} \\] Similar to the LER, insurers would like to find out how much more they have to pay if they were to increase the policy limit, measured through the Increased Limit Factor (ILF). It is calculated as the ratio of the expected payment with the new limit \\(b\\) to that of the current limit \\(u\\) : \\[ \\text{ILF} = \\frac{E(X \\land b)}{E(X \\land u)} \\] Limits & Deductibles \u00b6 Policy Limits on their own are a simple concept - the difficulty comes about when they are mixed with deductibles . Consider the possible values of the payment variable: \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& X \\lt d \\\\ X-d,& d \\lt X \\lt d + u \\\\ u,& X \\gt d + u \\end{cases} \\end{aligned} \\] The key is understanding that only what the insurer pays is counted towards the limit - thus, the effective limit of the policy is \\(d + u\\) . This is known as the Maximum Covered Loss \\((m)\\) which is the smallest loss amount at or above which the insurer will pay the policy limit. In other words, if \\(X \\ge m\\) then \\(Y^L = u\\) . We can find an expression for payment by decomposing the piecewise function: \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& X \\lt d \\\\ X-d,& d \\lt X \\lt d + u \\\\ m - d,& X \\gt d + u \\end{cases} \\\\ &= \\begin{cases} X - X,& X \\lt d \\\\ X-d,& d \\lt X \\lt d + u \\\\ m - d,& X \\gt d + u \\end{cases} \\\\ &= \\begin{cases} X,& X \\lt d \\\\ X,& d \\lt X \\lt d + u \\\\ m,& X \\gt d + u \\end{cases} - \\begin{cases} X,& X \\lt d \\\\ d,& d \\lt X \\lt d + u \\\\ d,& X \\gt d + u \\end{cases} \\\\ &= (X \\land m) - (X \\land d) \\\\ \\\\ \\therefore E(Y^L) &= E(X \\land m) - (X \\land d) \\end{aligned} \\] Maximum Covered Loss \u00b6 The concept of maximum covered loss can be extended to all combinations of modifications involving a limit: Policy Limit ONLY : \\(m=u\\) With Ordinary Deductible : \\(m = u + d\\) With Coinsurance : \\(m = \\frac{u}{\\alpha}\\) With BOTH Ordinary Deductible and Coinsurance : \\(m = \\frac{u}{\\alpha} + d\\) Note For a franchise deductible, since the full loss is paid if the deductible has been met, \\(m=u\\) . Thus, policy limits are more generally expressed as: \\[ \\begin{aligned} Y^L_{\\text{Policy Limit}} &= \\begin{cases} X,& X \\lt m \\\\ u,& X \\ge m \\end{cases} \\\\ &= X \\land m \\\\ \\end{aligned} \\] Inflation \u00b6 Although not technically a policy modification, Inflation is another factor that affects how the payments are calculated. If the losses in the current year are \\(X\\) , then losses in the FOLLOWING year are expected to inflate by a factor of \\(r\\) , resulting in \\((1+r)X\\) . Since coinsurance is already a scaling of the loss random variable, adding an additional term for inflation does not impact much: \\[ \\begin{aligned} Y^L_{\\text{Coinsurance}} &= \\alpha \\cdot (1+r) \\cdot X \\\\ \\therefore E \\left(Y^L_{\\text{Coinsurance}} \\right) &= \\alpha \\cdot (1+r) \\cdot E(X) \\end{aligned} \\] However, policy limits involve a minimum function , which is more complicated to manipulate: \\[ \\begin{aligned} Y^L_{\\text{Policy Limit}} &= (1+r) \\cdot X \\land u \\\\ &= (1+r) \\left(X \\land \\frac{u}{1+r} \\right) \\\\ \\therefore E \\left(Y^L_{\\text{Policy Limit}} \\right) &= (1+r) \\cdot E \\left(X \\land \\frac{u}{1+r} \\right) \\\\ \\end{aligned} \\] Thus, the same logic can be applied for deductibles with inflation: \\[ \\begin{aligned} Y^L_{\\text{Deductible}} &= (1+r) \\cdot X - (1+r) \\cdot X \\land d \\\\ &= (1+r) \\cdot X - (1+r) \\cdot \\left(X \\land \\frac{d}{1+r} \\right) \\\\ &= (1+r) \\left[X - \\left(X \\land \\frac{d}{1+r} \\right) \\right] \\\\ \\therefore E(Y^L_{\\text{Deductible}}) &= (1+r) \\cdot E \\left[X - \\left(X \\land \\frac{d}{1+r} \\right) \\right] \\end{aligned} \\] Tip This is the exact same result as when coinsurance is applied before the deductible, as both involve scaling of \\(X\\) without the deductible. Thus, remember to use the updated deductible when converting between \\(Y^L\\) and \\(Y^P\\) . All Modifications \u00b6 Consider a policy with all three policy modifications (with the coinsurance applied after the deductible): Below the deductible , nothing will be paid Above the maximum covered loss , the policy limit will be paid Between these two values, the coinsured portion of the excess loss is paid \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& x \\lt d \\\\ \\alpha (X-d),& d \\lt X \\lt m \\\\ u,& X \\ge m \\end{cases} \\end{aligned} \\] This can then be simplified to yield the following results: \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& X \\le d \\\\ \\alpha(X-d),& d \\lt X \\lt m \\\\ u,& X \\ge m \\end{cases} \\\\ &= \\begin{cases} \\alpha (X-X),& X \\le d \\\\ \\alpha \\cdot (X-d),& d \\lt X \\lt m \\\\ \\alpha (m-d),& X \\ge m \\end{cases} \\\\ &= \\alpha \\cdot \\begin{cases} X-X,& X \\le d \\\\ X-d,& d \\lt X \\lt m \\\\ m-d,& X \\ge m \\end{cases} \\\\ &= \\alpha \\cdot \\left [ \\begin{cases} X,& X \\le d \\\\ X,& d \\lt X \\lt m \\\\ m,& X \\ge m \\end{cases} - \\begin{cases} X,& X \\le d \\\\ d,& d \\lt X \\lt m \\\\ d,& X \\ge m \\end{cases} \\right] \\\\ &= \\alpha \\cdot \\left[(X \\land m) - (X \\land d) \\right] \\\\ \\\\ E \\left(Y^L \\right) &= \\alpha \\cdot E \\left[(X \\land m) - (X \\land d) \\right] \\\\ &= \\alpha \\cdot \\left (E \\left[(X \\land m) \\right] - E \\left[(X \\land d) \\right] \\right) \\\\ \\\\ E \\left(Y^P \\right) &= \\frac{E \\left(Y^L \\right)}{1 - F_{X}(d)} \\end{aligned} \\] If inflation is considered, then the above can be easily adjusted for the following year: \\[ \\begin{aligned} E \\left(Y^L \\right) &= \\alpha (1+r) \\cdot \\left (E \\left[\\left(X \\land \\frac{m}{1+r}\\right) \\right] - E \\left[\\left(X \\land \\frac{d}{1+r} \\right) \\right] \\right) \\\\ \\\\ E \\left(Y^P \\right) &= \\frac{E \\left(Y^L \\right)}{1 - F_{X} \\left(\\frac{d}{1+r} \\right)} \\end{aligned} \\] Hybrid Modification \u00b6 There may be policies that contain an unusual combination of features. The only way to solve them is by first principles . Consider the overall piecewise function and then split them into more recognisable ones, using the \"tricks\" shown above. Consider a two deductible example from SOA Sample Question #60 : \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& X \\lt 1000 \\\\ 0.8 \\cdot (X - 1000),& 1000 \\lt X \\lt 6000 \\\\ 0.8 \\cdot (6000 - 1000),& 6000 \\lt X \\lt 14000 \\\\ 0.8 \\cdot (6000 - 1000) + 0.9 \\cdot (X - 14000),& X \\gt 14000 \\end{cases} \\end{aligned} \\] The tricky part about forming this function is the third level: Since the policyholder bears the full cost of the repairs, the insurer does not pay anything new ; they pay the maximum amount of the previous level . The policyholder has paid 1000 from the deductible and 1000 from the coinsurance , thus has to bear 8000 of losses themselves, resulting in an upper limit of 14000 . Using the methods shown earlier, it can be split into the following components:","title":"Policy Modifications"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/4.%20Policy%20Modifications/#policy-modifications","text":"","title":"Policy Modifications"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/4.%20Policy%20Modifications/#key-concepts","text":"Policy Modifications are the parts of an insurance policy that defines how much the insurer is required to pay for each loss incurred by the policyholder. Before continuing, there are two key notations ( \\(\\land, {}_{+}\\) )that will be used throughout this subsection: \\[ \\begin{aligned} \\min (a, b) &= \\begin{cases} a,& a \\lt b \\\\ b,& a \\ge b \\end{cases} \\\\ &= a \\land b \\\\ \\\\ \\max (a-b, 0) &= \\begin{cases} 0,& a \\lt b \\\\ a-b,& a \\ge b \\end{cases} \\\\ &= (a-b)_{+} \\\\ &= \\text{Non-negative part; floored at 0} \\end{aligned} \\] The two are related through the following expression: \\[ \\begin{aligned} (a-b)_{+} &= a - (a \\land b) \\\\ \\\\ \\text{If } a &\\gt b \\text{,} \\\\ (a-b)_{+} &= a-b \\\\ a - (a \\land b) &= a-b \\\\ \\therefore (a-b)_{+} &= a - (a \\land b) \\\\ \\\\ \\text{If } a &\\lt b \\text{,} \\\\ (a-b)_{+} &= 0 \\\\ a - (a \\land b) &= a-a = 0 \\\\ \\therefore (a-b)_{+} &= a - (a \\land b) \\\\ \\end{aligned} \\] There are three key random variables regarding claims and losses: \\(X\\) is the Ground Up Loss variable, representing the total loss incurred by the policyholder \\(Y^L\\) is the Payment per Loss variable, representing the amount paid by the insurer \\(Y^P\\) is the Payment per Payment variable, representing the amount paid by the insurer given that there is a positive payment To properly understand the difference between the three, consider an example of a policy with a deductible of 800 , but in two different situations: \\(X\\) or Loss \\(Y^L\\) \\(Y^P\\) 2000 1200 1200 600 0 NOT Observed There are two main takeaways: If \\(Y^L > 0\\) , then \\(Y^P = Y^L\\) If \\(Y^L = 0\\) , then \\(Y^P\\) is NOT Observed \\(\\therefore Y^P = Y^L \\mid Y^L > 0\\) Warning Do NOT mistakenly think that \\(Y^P\\) being unobserved is equivalent to \\(Y^P = 0\\) . They are completely DIFFERENT. \\(Y^L\\) can be thought of as a two point mixture distribution of \\(Y^P\\) and a not-so-random-variable that always takes 0 . Thus, it can also be thought of as a conditional distribution dependent on whether \\(Y^L \\gt 0\\) : \\[ \\begin{aligned} F_{Y^L} (t) &= F_{Y_L | Y^L > 0} (t) \\cdot P(Y^L > 0) + F_{Y_L | Y^L = 0} (t) \\cdot P(Y^L = 0) \\\\ &= F_{Y^P} (t) \\cdot P(Y^L > 0) + F_{0} (t) \\cdot P(Y^L = 0) \\\\ \\end{aligned} \\] Let \\(\\pi = P(Y^L > 0)\\) , which becomes the weight of the mixture: \\[ \\begin{aligned} F_{Y^L} (t) &= F_{Y_P} (t) \\cdot \\pi + F_{0}(t) \\cdot (1-\\pi) \\\\ 1 - F_{Y^L} (t) &= [1 - F_{Y_P} (t)] \\cdot \\pi + [1-F_{0}(t)] \\cdot (1-\\pi) \\\\ S_{Y^L} (t) &= S_{Y_P} (t) \\cdot \\pi \\\\ \\\\ E \\left(Y^L \\right) &= E \\left(Y^P \\right) \\cdot \\pi + E(0) \\cdot (1-\\pi) \\\\ E \\left(Y^L \\right) &= E \\left(Y^L \\right) \\cdot \\pi \\end{aligned} \\] Thus, the values for \\(Y^L\\) and \\(Y^P\\) can be easily converted to one another by using the weight of the mixture (probability of a positive payment). Additionally, since \\(Y^P\\) is essentially \\(Y^L\\) being conditional on itself, we can easily obtain its conditional distribution : \\[ \\begin{aligned} f_{Y_P} (y) &= f_{Y^L \\mid Y^L \\gt 0} (y) \\\\ &= \\frac{f_{Y^L} (y)}{P(Y^L \\gt 0)} \\end{aligned} \\] Tip If \\(Y^L\\) follows the exponential or geometric distribution which are memoryless, then the distribution of \\(Y^P\\) is the same as \\(Y^L\\) . \\[ \\begin{aligned} Y^L &\\sim \\text{Exponential/Geometric} \\\\ Y^P &= X \\mid X \\gt d \\\\ Y^P &\\sim \\text{Exponential/Geometric} \\end{aligned} \\] Similarly, if \\(Y^L\\) follows the uniform distribution, then the distribution of \\(Y^P\\) can be easily calculated: \\[ \\begin{aligned} X &\\sim uni (a,b) \\\\ Y^P &= X - d \\mid X \\gt d \\\\ \\therefore Y^P \\sim \\text{Uniform} (a, b-d) \\end{aligned} \\] Note that for all losses without deductibles , a loss will always result in a claim. Thus, \\(Y^P = Y^L\\) .","title":"Key Concepts"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/4.%20Policy%20Modifications/#ordinary-deductibles","text":"The first type of policy modification is known as a Deductible , denoted by \\(d\\) . It is the amount that the policyholder is responsible for paying before the insurer pays a claim: If the loss amount does not exceed the deductible, then the insurer will pay out nothing If it does exceed the deductible , then the insurer will pay out the excess For each loss \\(X\\) , the policyholder is responsible for paying losses up to \\(d\\) . It is denoted by \\(X \\land d\\) , which is known as the Limited Loss Variable : \\[ \\begin{aligned} X \\land d &= \\begin{cases} X,& X \\le d\\\\ d,& X \\gt d \\end{cases} \\end{aligned} \\] Tip If \\(X\\) follows a known distribution , then \\(E(X \\land d)\\) can be easily found inside the formula sheet. However, if \\(X\\) does NOT follow a known distribution , then \\(E(X \\land d)\\) must be found through first principles. Most of the time, such questions assume that losses are discrete (EG. 40, 80, 120 & 160) with some deductible not exactly equal to any of the loss amounts (EG. 100). The tricky part is remembering that once the loss is larger the deductible, only the deductible will be paid: \\[ \\begin{aligned} X \\land d &= \\begin{cases} 40,& X = 40 \\\\ 80,& X = 80 \\\\ 100,& X \\ge 120 \\end{cases} \\\\ \\\\ \\therefore E(X \\land d) &= 40 \\cdot P(X = 40) + 80 \\cdot P(X = 80) + 100 \\cdot [1 - P(X = 40) - P(X = 80)] \\end{aligned} \\] The key is understanding that the final case is simply the deductible itself, with its probability being the complement of all other cases below the deductible . The insurer will then cover the excess amount, represented by the Payment Per Loss variable: \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& X \\le d\\\\ X - d,& X \\gt d \\end{cases} \\\\ &= (X-d)_{+} \\\\ &= X - (X \\land d) \\\\ \\\\ \\therefore E \\left(Y^L \\right) &= E(X) - E(X \\land d) \\end{aligned} \\] Tip An intuitive way to remember is that \\(\\text{Claim} = \\text{Full Loss} - \\text{Limited Loss}\\) . In practice, policyholders would not report losses below the deductible . Thus, the insurers payment is better represented by the Payment Per Payment variable, also known as the Excess Loss Variable : \\[ \\begin{aligned} Y^P &= Y^L \\mid Y^L \\gt 0 \\\\ &= X - d \\mid X \\gt d \\\\ &= \\begin{cases} \\text{Undefined},& X \\le d\\\\ X - d,& X \\gt d \\end{cases} \\\\ \\\\ f_{Y_P} (y) &= \\frac{f_{Y^L} (y)}{P(Y^L \\gt 0)} \\\\ &= \\frac{f_{Y^L} (y)}{P(X \\gt d)} \\\\ &= \\frac{f_{Y^L} (y)}{1 - F_{X}(d)} \\\\ \\\\ E \\left(Y^P \\right) &= \\int^{\\infty}_{d} y^P \\cdot f_{Y_P} (y) \\\\ &= \\int^{\\infty}_{d} (x-d) \\cdot \\frac{f_{Y^L} (y)}{1 - F_{X}(d)} \\\\ &= \\frac{1}{1 - F_{X}(d)} \\cdot \\int^{\\infty}_{d} (x-d) \\cdot f_{Y^L} (y) \\\\ &= \\frac{E \\left(Y^L \\right)}{1 - F_{X}(d)} \\\\ \\end{aligned} \\] The key metric insurers look out for is the Loss Elimintation Ratio (LER), which is a measure of how much the insurer saves by imposing the deductible; how much less they end up paying. Formally, it is defined as the ratio of a decrease in expected payment when a deductible is imposed to the expected payment without a deductible: \\[ \\begin{aligned} \\text{LER} &= \\frac{E(X) - E \\left(Y^L \\right)}{E(X)} \\\\ &= \\frac{E(X) - E [(X-d)_{+}]}{E(X)} \\\\ &= \\frac{E \\left[X-(X-d)_{+} \\right]}{E(X)} \\\\ &= \\frac{E(X \\land d)}{E(X)} \\end{aligned} \\]","title":"Ordinary Deductibles"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/4.%20Policy%20Modifications/#variance-of-payments","text":"Unfortunately, the formula for expectation ONLY holds for the first raw moment . Thus, the second moment can only be calculated via first principles . If severity is continuous, then the survival function method (FAM-L) can be used to calculate the second moment: \\[ \\begin{aligned} E \\left(Y^L \\right) &= E \\left[(X-d)_{+} \\right] \\\\ &= \\int^{d}_{0} 0 \\cdot f(x) + \\int^{\\infty}_{d} (x-d) \\cdot f(x) \\\\ &= \\int^{\\infty}_{d} (x-d) \\cdot f(x) \\\\ \\\\ E \\left[ \\left(Y^L \\right) \\right]^2 &= \\int^{\\infty}_{d} (x-d)^2 \\cdot f(x) \\\\ &= [S_X(x) \\cdot (x-d)^2]^{\\infty}_{d} - \\int^{\\infty}_{d} -S(x) \\cdot 2(x-d) \\\\ &= \\int^{\\infty}_{d} S(x) \\cdot 2(x-d) \\end{aligned} \\] If severity is discrete, then determine all possible values for \\(Y^L\\) and then manually calculate the second moment as the sum product. If the variance of the payment per payment variable is required, then the same conversion applies for the second moment as well: \\[ E \\left[ \\left(Y^P \\right) \\right]^2 = \\frac{E \\left[ \\left(Y^L \\right) \\right]^2}{P(X \\gt d)} \\]","title":"Variance of Payments"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/4.%20Policy%20Modifications/#franchise-deductibles","text":"A special kind of deductible is known as Franchise Deductible . Unlike a regular deductible that will only pay the excess, it will pay the full amount . \\[ \\begin{aligned} Y^L_{\\text{Franchise}} &= \\begin{cases} 0,& X \\le d\\\\ X,& X \\gt d \\end{cases} \\\\ \\\\ E \\left(Y^L_{\\text{Franchise}} \\right) &= \\int^{d}_{0} 0 \\cdot f(x) + \\int^{\\infty}_{d} x \\cdot f(x) \\\\ &= \\int^{\\infty}_{d} x \\cdot f(x) \\\\ &= \\int^{\\infty}_{d} (x-d+d) \\cdot f(x) \\\\ &= \\int^{\\infty}_{d} (x-d) \\cdot f(x) + d \\cdot \\int^{\\infty}_{d} f(x) \\\\ &= E[Y^L] + d \\cdot [1-F_{X}(d)] \\end{aligned} \\] The same relationship as before can be applied for the payment per payment variable: \\[ \\begin{aligned} Y^P &= \\begin{cases} \\text{Undefined},& X \\le d\\\\ X,& X \\gt d \\end{cases} \\\\ \\\\ E \\left(Y^P_{\\text{Franchise}} \\right) &= \\frac{E \\left(Y^L_{\\text{Franchise}} \\right)}{1 - F_{X}(d)} \\\\ &= \\frac{E[Y^L] + d \\cdot [1-F_{X}(d)]}{1 - F_{X}(d)} \\\\ &= \\frac{E[Y^L]}{1 - F_{X}(d)} + \\frac{d \\cdot [1-F_{X}(d)]}{1 - F_{X}(d)} \\\\ &= E \\left(Y^P \\right) + d \\end{aligned} \\]","title":"Franchise Deductibles"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/4.%20Policy%20Modifications/#coinsurance","text":"Another type of policy modification is known as Coinsurance , denoted by \\(\\alpha\\) . As its name suggests, the key idea is that both the insurer and policyholder insure the loss . The insurer covers \\(\\alpha\\) proportion of the loss while the poliyholder covers the remaining \\(1-\\alpha\\) proportion. \\[ \\begin{aligned} Y^L &= \\alpha \\cdot X \\\\ E \\left(Y^L \\right) &= \\alpha \\cdot E(X) \\\\ \\end{aligned} \\] Warning Co-insurance usually refers to the portion of the loss that the policyholder is responsible for. However, for this exam, we are taking the insurers perspective, thus it refers to the amount that the insurer is responsible for. Do not confuse the two!","title":"Coinsurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/4.%20Policy%20Modifications/#coinsurance-deductibles","text":"Coinsurance on its own is a simple concept - the difficulty comes about when it is mixed with a Deductible . If the coinsurance is applied after the deductible : Entire loss \\(X\\) is applied against the deductible; poliycholder pays first \\(d\\) Coinsurance is applied against the remaining loss ; insurer pays remaining \\(\\alpha(X-d)\\) Unless stated otherwise, it is usually assumed that the coinsurance is applied after the deductible \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& X \\le d \\\\ \\alpha \\cdot (X - d),& X \\gt d \\end{cases} \\\\ &= \\begin{cases} \\alpha \\cdot 0,& X \\le d \\\\ \\alpha \\cdot (X - d),& X \\gt d \\end{cases} \\\\ &= \\alpha \\cdot \\begin{cases} 0,& X \\le d \\\\ (X - d),& X \\gt d \\end{cases} \\\\ &= \\alpha \\cdot (X-d)_{+} \\\\ &= \\alpha \\cdot [X - (X \\land d)] \\\\ \\\\ E \\left(Y^L \\right) &= \\alpha \\cdot E [X - (X \\land d)] \\\\ &= \\alpha \\cdot [E(X) - E(X \\land d)] \\\\ \\\\ E \\left(Y^P \\right) &= \\frac{E \\left(Y^L \\right)}{1 - F_{X}(d)} \\end{aligned} \\] Note The key is remembering that any value multiplied by 0 is 0, thus the coinsurance factor can be factored out. If the coinsurance is applied before the deductible , Coinsurance is applied against the full loss ; insurer covers \\(\\alpha X\\) Deductible is applied against the coinsured amount ; insurer pays \\(\\alpha X - d\\) The deductible effectively increases to \\(\\frac{d}{a}\\) \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& \\alpha X \\le d \\\\ \\alpha X - d,& \\alpha X \\gt d \\end{cases} \\\\ &= \\begin{cases} 0,& X \\le \\frac{d}{a} \\\\ \\alpha \\cdot \\left(X - \\frac{d}{a} \\right),& X \\gt \\frac{d}{a} \\end{cases} \\\\ &= \\alpha \\cdot \\left(X - \\frac{d}{a} \\right)_{+} \\\\ &= \\alpha \\cdot \\left[X - \\left(X \\land \\frac{d}{a} \\right) \\right] \\\\ \\\\ E \\left(Y^L \\right) &= \\alpha \\cdot E \\left[X - \\left(X \\land \\frac{d}{a} \\right) \\right] \\\\ &= \\alpha \\cdot \\left[E(X) - E \\left(X \\land \\frac{d}{a} \\right) \\right] \\\\ \\\\ E \\left(Y^P \\right) &= \\frac{E \\left(Y^L \\right)}{1 - F_{X} \\left(\\frac{d}{a} \\right)} \\end{aligned} \\] Warning It is a common mistake to forget to update the deductible to the new one when using the complement of the CDF for the conversion between \\(Y^L\\) and \\(Y^P\\) .","title":"Coinsurance &amp; Deductibles"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/4.%20Policy%20Modifications/#bonus-incentive","text":"A special combination of a Deductible and Coinsurance is known as a Bonus Incentive . Insurers usually provide incentives to their agents if the total losses for the year are below some threshold , where the incentive is some percentage of the losses below this amount . Thus, the situation is similar to coinsurance applied after a deductible: \\[ \\begin{aligned} B &= \\begin{cases} \\alpha \\cdot (d - X),& X \\lt d \\\\ 0,& X \\ge d \\end{cases} \\\\ &= \\alpha \\cdot (d - X)_{+} \\\\ \\\\ \\therefore E(B) &= E(d-X)_{+} \\\\ &= E(d) - E(d \\land X) \\\\ &= E(d) - E(X \\land d) \\end{aligned} \\]","title":"Bonus Incentive"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/4.%20Policy%20Modifications/#policy-limits","text":"The second type of policy modification is known as Policy Limit , denoted by \\(u\\) . It is the maximum amount that the insurer will payout. If \\(X\\) is below the limit, then the insurer will pay the loss If \\(X\\) is above the limit, then the insurer will only pay the limit \\[ \\begin{aligned} Y^L &= \\begin{cases} X,& X \\lt u \\\\ u,& X \\ge u \\end{cases} \\\\ &= X \\land u \\\\ \\\\ E \\left(Y^L \\right) &= \\int^{u}_{0} x \\cdot f(x) + \\int^{\\infty}_{u} u \\cdot f(x) \\\\ \\end{aligned} \\] Warning Under a deductible, \\(X \\land d\\) represents the limited loss of the policyholder . Under a policy limit, \\(x \\land u\\) represents the limited payment of the insurer . Note that the expectation can be simplified using the Survival Function Method (FAM-L): \\[ \\begin{aligned} E \\left(Y^L \\right) &= \\int^{u}_{0} x \\cdot f(x) + \\int^{\\infty}_{u} u \\cdot f(x) \\\\ &= \\left(\\left[x \\cdot (-S(x)) \\right]^u_0 - \\int^{u}_{0} (-S(x)) \\cdot 1 \\right) + u \\cdot S(u) \\\\ &= -u \\cdot S(u) - 0 + \\int^{u}_{0} S(x) + u \\cdot S(u) \\\\ &= \\int^{u}_{0} S(x) dx \\end{aligned} \\] Similar to the LER, insurers would like to find out how much more they have to pay if they were to increase the policy limit, measured through the Increased Limit Factor (ILF). It is calculated as the ratio of the expected payment with the new limit \\(b\\) to that of the current limit \\(u\\) : \\[ \\text{ILF} = \\frac{E(X \\land b)}{E(X \\land u)} \\]","title":"Policy Limits"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/4.%20Policy%20Modifications/#limits-deductibles","text":"Policy Limits on their own are a simple concept - the difficulty comes about when they are mixed with deductibles . Consider the possible values of the payment variable: \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& X \\lt d \\\\ X-d,& d \\lt X \\lt d + u \\\\ u,& X \\gt d + u \\end{cases} \\end{aligned} \\] The key is understanding that only what the insurer pays is counted towards the limit - thus, the effective limit of the policy is \\(d + u\\) . This is known as the Maximum Covered Loss \\((m)\\) which is the smallest loss amount at or above which the insurer will pay the policy limit. In other words, if \\(X \\ge m\\) then \\(Y^L = u\\) . We can find an expression for payment by decomposing the piecewise function: \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& X \\lt d \\\\ X-d,& d \\lt X \\lt d + u \\\\ m - d,& X \\gt d + u \\end{cases} \\\\ &= \\begin{cases} X - X,& X \\lt d \\\\ X-d,& d \\lt X \\lt d + u \\\\ m - d,& X \\gt d + u \\end{cases} \\\\ &= \\begin{cases} X,& X \\lt d \\\\ X,& d \\lt X \\lt d + u \\\\ m,& X \\gt d + u \\end{cases} - \\begin{cases} X,& X \\lt d \\\\ d,& d \\lt X \\lt d + u \\\\ d,& X \\gt d + u \\end{cases} \\\\ &= (X \\land m) - (X \\land d) \\\\ \\\\ \\therefore E(Y^L) &= E(X \\land m) - (X \\land d) \\end{aligned} \\]","title":"Limits &amp; Deductibles"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/4.%20Policy%20Modifications/#maximum-covered-loss","text":"The concept of maximum covered loss can be extended to all combinations of modifications involving a limit: Policy Limit ONLY : \\(m=u\\) With Ordinary Deductible : \\(m = u + d\\) With Coinsurance : \\(m = \\frac{u}{\\alpha}\\) With BOTH Ordinary Deductible and Coinsurance : \\(m = \\frac{u}{\\alpha} + d\\) Note For a franchise deductible, since the full loss is paid if the deductible has been met, \\(m=u\\) . Thus, policy limits are more generally expressed as: \\[ \\begin{aligned} Y^L_{\\text{Policy Limit}} &= \\begin{cases} X,& X \\lt m \\\\ u,& X \\ge m \\end{cases} \\\\ &= X \\land m \\\\ \\end{aligned} \\]","title":"Maximum Covered Loss"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/4.%20Policy%20Modifications/#inflation","text":"Although not technically a policy modification, Inflation is another factor that affects how the payments are calculated. If the losses in the current year are \\(X\\) , then losses in the FOLLOWING year are expected to inflate by a factor of \\(r\\) , resulting in \\((1+r)X\\) . Since coinsurance is already a scaling of the loss random variable, adding an additional term for inflation does not impact much: \\[ \\begin{aligned} Y^L_{\\text{Coinsurance}} &= \\alpha \\cdot (1+r) \\cdot X \\\\ \\therefore E \\left(Y^L_{\\text{Coinsurance}} \\right) &= \\alpha \\cdot (1+r) \\cdot E(X) \\end{aligned} \\] However, policy limits involve a minimum function , which is more complicated to manipulate: \\[ \\begin{aligned} Y^L_{\\text{Policy Limit}} &= (1+r) \\cdot X \\land u \\\\ &= (1+r) \\left(X \\land \\frac{u}{1+r} \\right) \\\\ \\therefore E \\left(Y^L_{\\text{Policy Limit}} \\right) &= (1+r) \\cdot E \\left(X \\land \\frac{u}{1+r} \\right) \\\\ \\end{aligned} \\] Thus, the same logic can be applied for deductibles with inflation: \\[ \\begin{aligned} Y^L_{\\text{Deductible}} &= (1+r) \\cdot X - (1+r) \\cdot X \\land d \\\\ &= (1+r) \\cdot X - (1+r) \\cdot \\left(X \\land \\frac{d}{1+r} \\right) \\\\ &= (1+r) \\left[X - \\left(X \\land \\frac{d}{1+r} \\right) \\right] \\\\ \\therefore E(Y^L_{\\text{Deductible}}) &= (1+r) \\cdot E \\left[X - \\left(X \\land \\frac{d}{1+r} \\right) \\right] \\end{aligned} \\] Tip This is the exact same result as when coinsurance is applied before the deductible, as both involve scaling of \\(X\\) without the deductible. Thus, remember to use the updated deductible when converting between \\(Y^L\\) and \\(Y^P\\) .","title":"Inflation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/4.%20Policy%20Modifications/#all-modifications","text":"Consider a policy with all three policy modifications (with the coinsurance applied after the deductible): Below the deductible , nothing will be paid Above the maximum covered loss , the policy limit will be paid Between these two values, the coinsured portion of the excess loss is paid \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& x \\lt d \\\\ \\alpha (X-d),& d \\lt X \\lt m \\\\ u,& X \\ge m \\end{cases} \\end{aligned} \\] This can then be simplified to yield the following results: \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& X \\le d \\\\ \\alpha(X-d),& d \\lt X \\lt m \\\\ u,& X \\ge m \\end{cases} \\\\ &= \\begin{cases} \\alpha (X-X),& X \\le d \\\\ \\alpha \\cdot (X-d),& d \\lt X \\lt m \\\\ \\alpha (m-d),& X \\ge m \\end{cases} \\\\ &= \\alpha \\cdot \\begin{cases} X-X,& X \\le d \\\\ X-d,& d \\lt X \\lt m \\\\ m-d,& X \\ge m \\end{cases} \\\\ &= \\alpha \\cdot \\left [ \\begin{cases} X,& X \\le d \\\\ X,& d \\lt X \\lt m \\\\ m,& X \\ge m \\end{cases} - \\begin{cases} X,& X \\le d \\\\ d,& d \\lt X \\lt m \\\\ d,& X \\ge m \\end{cases} \\right] \\\\ &= \\alpha \\cdot \\left[(X \\land m) - (X \\land d) \\right] \\\\ \\\\ E \\left(Y^L \\right) &= \\alpha \\cdot E \\left[(X \\land m) - (X \\land d) \\right] \\\\ &= \\alpha \\cdot \\left (E \\left[(X \\land m) \\right] - E \\left[(X \\land d) \\right] \\right) \\\\ \\\\ E \\left(Y^P \\right) &= \\frac{E \\left(Y^L \\right)}{1 - F_{X}(d)} \\end{aligned} \\] If inflation is considered, then the above can be easily adjusted for the following year: \\[ \\begin{aligned} E \\left(Y^L \\right) &= \\alpha (1+r) \\cdot \\left (E \\left[\\left(X \\land \\frac{m}{1+r}\\right) \\right] - E \\left[\\left(X \\land \\frac{d}{1+r} \\right) \\right] \\right) \\\\ \\\\ E \\left(Y^P \\right) &= \\frac{E \\left(Y^L \\right)}{1 - F_{X} \\left(\\frac{d}{1+r} \\right)} \\end{aligned} \\]","title":"All Modifications"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/4.%20Policy%20Modifications/#hybrid-modification","text":"There may be policies that contain an unusual combination of features. The only way to solve them is by first principles . Consider the overall piecewise function and then split them into more recognisable ones, using the \"tricks\" shown above. Consider a two deductible example from SOA Sample Question #60 : \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& X \\lt 1000 \\\\ 0.8 \\cdot (X - 1000),& 1000 \\lt X \\lt 6000 \\\\ 0.8 \\cdot (6000 - 1000),& 6000 \\lt X \\lt 14000 \\\\ 0.8 \\cdot (6000 - 1000) + 0.9 \\cdot (X - 14000),& X \\gt 14000 \\end{cases} \\end{aligned} \\] The tricky part about forming this function is the third level: Since the policyholder bears the full cost of the repairs, the insurer does not pay anything new ; they pay the maximum amount of the previous level . The policyholder has paid 1000 from the deductible and 1000 from the coinsurance , thus has to bear 8000 of losses themselves, resulting in an upper limit of 14000 . Using the methods shown earlier, it can be split into the following components:","title":"Hybrid Modification"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/5.%20Aggregate%20Models/","text":"Aggregate Models \u00b6 Overview \u00b6 The combination of the Frequency Models \\((N)\\) and the Severity Models \\((X)\\) discussed previously results in Aggregate Model \\((S)\\) , which represents the total losses experienced by an insurance company. \\[ \\begin{aligned} S &= X_1 + X_2 + ... + X_N \\end{aligned} \\] There are two main types of aggregate models: The first type of aggregate model is known as the Collective Risk Model , which makes assumes the following assumptions: Losses are iid : \\(X_1, X_2, ..., X_N\\) are iid Frequency and Severity are independent : \\(X\\) and \\(N\\) are independent The second type is known as the Individual Risk Model , which makes similar assumptions: Losses are independent but not necessarily identical : \\(X_1, X_2, \\dots, X_N\\) are independent but MAY have different distributions Frequency is a constant : \\(N = n\\) Typically used for Group Policies where there is a fixed number of poliycholders with varying levels of coverage Generally speaking, it is much better to model frequency and severity seperately and then combine into an aggregate loss rather than directly modelling it as it more accurate and flexible . Collective Risk Model \u00b6 The collective risk model is a sum of an unknown number of random variables . In probability theory, this is known as a Compound Distribution where \\(N\\) is known as the Primary Distribution and \\(X\\) is known as the Secondary Distribution . Another perspective is that the aggregate model follows the conditional distribution \\(S \\mid N\\) . Thus, we can compute the probability, expectation and variance using previously found results. Note It can also be thought of as a mixture distribution where the mixing weights are the probability that the there that number of losses: \\[ w_k = P(N=K) \\] Thus, Compound Distributions and Mixture Distributions are essentially the same concept . Starting with the Law of Total Expectation , \\[ \\begin{aligned} E(S) &= E_{N} [E_{S}(S \\mid N)] \\\\ \\\\ E_{S}(S \\mid N) &= E(X_1 + X_2 + \\dots + X_N \\mid N) \\\\ &= E(X_1 \\mid N) + E(X_2 \\mid N) + \\dots + E(X_N \\mid N) \\\\ &= E(X_1) + E(X_2) + \\dots + E(X_N), \\ \\text{Independence} \\\\ &= E(X) + E(X)+ \\dots + E(X), \\ \\text{Identical} \\\\ &= N \\cdot \\underbrace{E(X)}_{\\text{Constant}} \\\\ \\\\ \\therefore E(S) &= E_{N} [E_{S}(S \\mid N)] \\\\ &= E_{N} [N \\cdot E(X)] \\\\ &= E(X)\\cdot E(N) \\end{aligned} \\] Note This result is highly intuitive as \\(\\text{Expected Risk} = \\text{Expected Claims} \\cdot \\text{Expected Severity}\\) . Moving on to the Law of Total Variance : \\[ \\begin{aligned} Var (S) &= E_N [Var (S \\mid N)] + Var_{N} [E(S \\mid N)] \\\\ \\\\ Var (S \\mid N) &= Var (X_1 + X_2 + ... + X_N \\mid N) \\\\ &= N \\cdot \\underbrace{Var(X)}_\\text{Constant}, \\ \\text{IID} \\\\ \\\\ Var (S) &= E_N [Var (S \\mid N)] + Var_{N} [E(S \\mid N)] \\\\ &= E [N \\cdot Var(X)] + Var_{N} [N \\cdot E(X)] \\\\ &= Var(X) \\cdot E(N) + [E(X)]^2 \\cdot Var(N) \\end{aligned} \\] Tip If the m**ean and variance of the frequency distribution are equal** (EG. Poisson Distribution), then the Variance can be simplified: \\[ \\begin{aligned} Var (S) &= Var(X) \\cdot E(N) + [E(X)]^2 \\cdot Var(N) \\\\ &= E(N) \\cdot [Var(X) + [E(X)]^2] \\\\ &= E(N) \\cdot E(X^2) \\end{aligned} \\] The probability of \\(s\\) is difficult to derive . Thus, for simplicity, we will assume a Discrete Severity for this portion. By the Law of Total Probability : \\[ \\begin{aligned} P(S = s) &= E_{N} [P(S = s \\mid N = n)] \\\\ &= \\sum P(S = s \\mid N = n) \\cdot P(N = n) \\end{aligned} \\] Note If both the Frequency and Severity models are discrete, then the Aggrgegate Loss is discrete as well. Convolutions \u00b6 The term within the above summation is known as a Convolution , as it is the probability of a sum of random variables. Since there are \\(n\\) random variables, it is known as the n-fold Convolution . \\[ P(S = s \\mid N = n) = P(X_1 + X_2 + \\dots + X_n = s) \\] For simplicity, consider a two fold convolution: \\[ \\begin{aligned} P(S = s \\mid n = 2) &= \\sum P(X_1 = x, X_2 = s - x_1) \\\\ &= \\sum P(X_1 = x) \\cdot P(X_2 = s - x_1) \\end{aligned} \\] A convolution is simply the probability of all possible combinations of the random variables that result in the specified value (Recall the two dice example). However, one implicit assumption for this to work is that the individual severities cannot be 0 . The reason is because it would lead to endless possibilities for each case. Consider \\(S=1\\) : \\(1 + 0 + 0 + \\dots + 0 = 1\\) \\(0 + 1 + 0 + \\dots + 0 = 1\\) \\(0 + 0 + 1 + \\dots + 0 = 1\\) \\(0 + 0 + 0 + \\dots + 1 = 1\\) However, then how is \\(S=0\\) determined, since there are no sums that result in 0 ? There is now only ONE way for aggregate losses to be 0 - if there are no claims at all . \\[ P(S = 0) = P(n = 0) \\] Generating Functions \u00b6 An alternative method to calculate the moments would be to use the MGF : \\[ M_{S}(t) = M_{N} [\\ln M_{X}(t)] \\] Similarly, if severity is discrete, then an alternative method to calculate its probability would be to use its PGF : \\[ P_{S}(t) = P_{N} [P_{X}(t)] \\] These methods tend to be more time consuming , thus they are preferred only when calculating higher moments or large values of \\(s\\) . Approximation \u00b6 Another method of determining the probability would be to simply approximate it. This is especially useful when the individual losses are continuous, such that the previous methods would not work. By the Central Limit Theorem , as the number of random variables increase, their sum is approximately normal distributed : \\[ S \\sim (E[S], Var[S]) \\] Instead of using the normal distribution, the Lognormal Distribution can be used as well. However, we need to first solve for its parameters : \\[ \\begin{aligned} e^{\\mu + 0.5\\sigma^2} &= E(S) \\\\ \\left(e^{\\mu + 0.5\\sigma^2} \\right)^2 \\cdot (e^{\\sigma^2}-1) &= Var (S) \\\\ \\\\ \\therefore S \\sim \\text{Log Normal} (\\mu, \\sigma^2) \\end{aligned} \\] Warning Although the notation is the same, do NOT confuse the lognormal parameters with the normal parameters. Continuity Correction \u00b6 We may still want to use the approximation even when losses are discrete as they may still be too complicated to compute. However, this may lead to some inaccuracies as we are using a continuous distribution to approximate a discrete distribution. Under a discrete distribution, the following are complements: \\[ P(X \\le n) + P(X \\gt n+1) = 1 \\] However, the same cannot be said for a continuous distribution, as there is some density between \\(n\\) and \\(n+1\\) that is not accounted for: \\[ P(S \\le n) + P(S \\gt n+1) \\ne 1 \\] Thus, Continuity Correction attempts to account for this by reducing the missing density. The general idea is to adjust the range such that the midpoint of the two values : \\[ \\begin{aligned} P(S \\le n) &= P \\left(S \\le \\frac{n+(n+1)}{2} \\right) \\\\ P(S \\ge n) &= P \\left(S \\ge \\frac{n+(n-1)}{2} \\right) \\end{aligned} \\] Another concept to remember when going from Continuous to Discrete is that a continuous \\(S \\lt n\\) is a discrete \\(S \\le (n-1)\\) since there are a countable number of values, vice versa. If required to approximate the probability at a specified value , then the above method will not work as a continuous distribution cannot find the probability at a single point. Thus, it will be converted to a range around the specified value instead, with the midpoint value used on both sides of the specified value: Tip If the discrete distribution does not support consecutive integers \\(\\set{0, 1, 2, \\dots}\\) , then simply take the midpoint of whatever consecutive values in its support \\(\\set{100, 200, 300}\\) . Aggregate Deductibles \u00b6 Insurers can also purchase insurance for themselves, known as Reinsurance . Insurers may want to limit their losses to a certain amount and getting the excess loss insured by a reinsurer. This is known as Stop Loss Insurance . Intuitively, it can be thought of as a deductible being applied to Aggregate Losses , as the insurer (policyholder) only pays for losses up the deductible while the reinsurer pays for the rest. As with regular deductibles, the main quantity of interest is the reinsurer's expected loss , as it represents the Net Premium for the Stop Loss insurance . Info Recall from FAM-L that Net Premiums refer to premiums that are charged without consideration of expenses or profits, purely based on the insurers loss. \\[ \\begin{aligned} E(S-d)_{+} &= E(S) - E(S \\land d) \\end{aligned} \\] \\(E(S)\\) can be found using the methods described earlier. However, unlike a normal deductible, the distribution of \\(S\\) is not known, thus \\(E(S \\land d)\\) can only be found via first principles . In most cases, the underlying severity distribution is discrete for simplicity. What makes this much more challenging than its regular deductible counterpart is that it is tricky to obtain all possible values of S and their corresponding probabilities. It is recommended to consider a probability tree to easily see all combinations of X and N that results in \\(S \\le d\\) and its corresponding probability. Tip There may be multiple combinations that may lead to the same aggregate loss: \\(X = 2, \\ N = 1, \\ \\therefore S = 2\\) \\(X = 1, \\ N = 2, \\ \\therefore S = 2\\) The probability of \\(S=2\\) must reflect both these combinations . Aggregate Payments \u00b6 The discussion so far has solely been on Aggregate Losses . However, if each policy has a deductible, then there is a need to consider Aggregate Payments instead. Both types of aggregate payments (per loss and per payment) can be used to determine aggregate payments: If considering Payments per Losses \\((Y^L)\\) , then as its name suggests, the appropriate frequency to use is the Loss Frequency \\((N)\\) . \\[ \\begin{aligned} S &= Y^L_{1} + Y^L_{2} + \\dots + Y^L_{N} \\\\ \\\\ E(S) &= E(N) \\cdot E(Y^L) \\\\ Var (S) &= Var(Y^L) \\cdot E(N) + \\left[E \\left(Y^L \\right) \\right]^2 \\cdot Var (N) \\end{aligned} \\] If considering Payments per Payments \\((Y^P)\\) , then as its name suggests, the appropriate frequency to use is the Payment Frequency \\((N^{*})\\) . \\[ \\begin{aligned} S &= Y^P_{1} + Y^P_{2} + \\dots + Y^P_{N^{*}} \\\\ \\\\ E(S) &= E(N^{*}) \\cdot E(Y^P) \\\\ Var (S) &= Var(Y^P) \\cdot E(N^{*}) + \\left[E \\left(Y^P \\right) \\right]^2 \\cdot Var (N^{*}) \\end{aligned} \\] The key is to match the frequency distribution with the severity : Loss & Loss Frequency; Payments & Payment Frequency. Payment Frequency \u00b6 Imposing a deductible naturally results in fewer payments than losses as not all losses result in a payment. The Payment Frequency \\((N^{*})\\) is thus an adjusted version of the loss distribution that only takes into account the losses that result in a payment. Consider a Bernoulli Indicator variable that takes on the value 1 if a payment will be made and 0 if it will not: \\[ \\begin{aligned} I &= \\begin{cases} 1 \\ (\\text{Paid}),& X \\gt d \\\\ 0 \\ (\\text{Not Paid}),& X \\le d \\end{cases} \\\\ \\\\ v &= P(X \\gt d) \\\\ P_I(t) &= 1 + v(t-1) \\end{aligned} \\] \\(N^{*}\\) is the sum of \\(N\\) idendepent indicator variables : \\[ N^{*} = I_1 + I_2 + \\dots + I_N \\] It is essentially a Compound Distribution with \\(N\\) being the primary distribution and \\(I\\) being the secondary distribution. Thus, the PGF of \\(N^{*}\\) can be determined: \\[ \\begin{aligned} P_{N^*} &= P_{N}[P_{I}(t)] \\\\ &= P_{N}[1 + p(t-1)] \\end{aligned} \\] By expanding simplfying the expression, the distribution of \\(N^{*}\\) can be determined: \\[ \\begin{aligned} N &\\sim \\text{Poisson}(\\mu) \\\\ \\\\ P_{N}[1 + p(t-1)] &= e^{\\mu(1 + v(z-1) -1)} \\\\ &= e^{\\mu (v(z-1))} \\\\ &= e^{v\\mu (z-1)} \\\\ \\\\ \\therefore N^{*} &\\sim \\text{Poisson}(v \\cdot \\mu) \\end{aligned} \\] Repeating the process for the other two members of the (a, b, 0) class, \\[ \\begin{aligned} N &\\sim \\text{Binomial} (n, p) \\\\ \\therefore N^{*} &\\sim \\text{Binomial} (n, vp) \\\\ \\\\ N &\\sim \\text{Negative Binomial} (r, \\theta) \\\\ \\therefore N^{*} &\\sim \\text{Negative Binomial} (r, v \\theta) \\end{aligned} \\] Individual Risk Model \u00b6","title":"Aggregate Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/5.%20Aggregate%20Models/#aggregate-models","text":"","title":"Aggregate Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/5.%20Aggregate%20Models/#overview","text":"The combination of the Frequency Models \\((N)\\) and the Severity Models \\((X)\\) discussed previously results in Aggregate Model \\((S)\\) , which represents the total losses experienced by an insurance company. \\[ \\begin{aligned} S &= X_1 + X_2 + ... + X_N \\end{aligned} \\] There are two main types of aggregate models: The first type of aggregate model is known as the Collective Risk Model , which makes assumes the following assumptions: Losses are iid : \\(X_1, X_2, ..., X_N\\) are iid Frequency and Severity are independent : \\(X\\) and \\(N\\) are independent The second type is known as the Individual Risk Model , which makes similar assumptions: Losses are independent but not necessarily identical : \\(X_1, X_2, \\dots, X_N\\) are independent but MAY have different distributions Frequency is a constant : \\(N = n\\) Typically used for Group Policies where there is a fixed number of poliycholders with varying levels of coverage Generally speaking, it is much better to model frequency and severity seperately and then combine into an aggregate loss rather than directly modelling it as it more accurate and flexible .","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/5.%20Aggregate%20Models/#collective-risk-model","text":"The collective risk model is a sum of an unknown number of random variables . In probability theory, this is known as a Compound Distribution where \\(N\\) is known as the Primary Distribution and \\(X\\) is known as the Secondary Distribution . Another perspective is that the aggregate model follows the conditional distribution \\(S \\mid N\\) . Thus, we can compute the probability, expectation and variance using previously found results. Note It can also be thought of as a mixture distribution where the mixing weights are the probability that the there that number of losses: \\[ w_k = P(N=K) \\] Thus, Compound Distributions and Mixture Distributions are essentially the same concept . Starting with the Law of Total Expectation , \\[ \\begin{aligned} E(S) &= E_{N} [E_{S}(S \\mid N)] \\\\ \\\\ E_{S}(S \\mid N) &= E(X_1 + X_2 + \\dots + X_N \\mid N) \\\\ &= E(X_1 \\mid N) + E(X_2 \\mid N) + \\dots + E(X_N \\mid N) \\\\ &= E(X_1) + E(X_2) + \\dots + E(X_N), \\ \\text{Independence} \\\\ &= E(X) + E(X)+ \\dots + E(X), \\ \\text{Identical} \\\\ &= N \\cdot \\underbrace{E(X)}_{\\text{Constant}} \\\\ \\\\ \\therefore E(S) &= E_{N} [E_{S}(S \\mid N)] \\\\ &= E_{N} [N \\cdot E(X)] \\\\ &= E(X)\\cdot E(N) \\end{aligned} \\] Note This result is highly intuitive as \\(\\text{Expected Risk} = \\text{Expected Claims} \\cdot \\text{Expected Severity}\\) . Moving on to the Law of Total Variance : \\[ \\begin{aligned} Var (S) &= E_N [Var (S \\mid N)] + Var_{N} [E(S \\mid N)] \\\\ \\\\ Var (S \\mid N) &= Var (X_1 + X_2 + ... + X_N \\mid N) \\\\ &= N \\cdot \\underbrace{Var(X)}_\\text{Constant}, \\ \\text{IID} \\\\ \\\\ Var (S) &= E_N [Var (S \\mid N)] + Var_{N} [E(S \\mid N)] \\\\ &= E [N \\cdot Var(X)] + Var_{N} [N \\cdot E(X)] \\\\ &= Var(X) \\cdot E(N) + [E(X)]^2 \\cdot Var(N) \\end{aligned} \\] Tip If the m**ean and variance of the frequency distribution are equal** (EG. Poisson Distribution), then the Variance can be simplified: \\[ \\begin{aligned} Var (S) &= Var(X) \\cdot E(N) + [E(X)]^2 \\cdot Var(N) \\\\ &= E(N) \\cdot [Var(X) + [E(X)]^2] \\\\ &= E(N) \\cdot E(X^2) \\end{aligned} \\] The probability of \\(s\\) is difficult to derive . Thus, for simplicity, we will assume a Discrete Severity for this portion. By the Law of Total Probability : \\[ \\begin{aligned} P(S = s) &= E_{N} [P(S = s \\mid N = n)] \\\\ &= \\sum P(S = s \\mid N = n) \\cdot P(N = n) \\end{aligned} \\] Note If both the Frequency and Severity models are discrete, then the Aggrgegate Loss is discrete as well.","title":"Collective Risk Model"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/5.%20Aggregate%20Models/#convolutions","text":"The term within the above summation is known as a Convolution , as it is the probability of a sum of random variables. Since there are \\(n\\) random variables, it is known as the n-fold Convolution . \\[ P(S = s \\mid N = n) = P(X_1 + X_2 + \\dots + X_n = s) \\] For simplicity, consider a two fold convolution: \\[ \\begin{aligned} P(S = s \\mid n = 2) &= \\sum P(X_1 = x, X_2 = s - x_1) \\\\ &= \\sum P(X_1 = x) \\cdot P(X_2 = s - x_1) \\end{aligned} \\] A convolution is simply the probability of all possible combinations of the random variables that result in the specified value (Recall the two dice example). However, one implicit assumption for this to work is that the individual severities cannot be 0 . The reason is because it would lead to endless possibilities for each case. Consider \\(S=1\\) : \\(1 + 0 + 0 + \\dots + 0 = 1\\) \\(0 + 1 + 0 + \\dots + 0 = 1\\) \\(0 + 0 + 1 + \\dots + 0 = 1\\) \\(0 + 0 + 0 + \\dots + 1 = 1\\) However, then how is \\(S=0\\) determined, since there are no sums that result in 0 ? There is now only ONE way for aggregate losses to be 0 - if there are no claims at all . \\[ P(S = 0) = P(n = 0) \\]","title":"Convolutions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/5.%20Aggregate%20Models/#generating-functions","text":"An alternative method to calculate the moments would be to use the MGF : \\[ M_{S}(t) = M_{N} [\\ln M_{X}(t)] \\] Similarly, if severity is discrete, then an alternative method to calculate its probability would be to use its PGF : \\[ P_{S}(t) = P_{N} [P_{X}(t)] \\] These methods tend to be more time consuming , thus they are preferred only when calculating higher moments or large values of \\(s\\) .","title":"Generating Functions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/5.%20Aggregate%20Models/#approximation","text":"Another method of determining the probability would be to simply approximate it. This is especially useful when the individual losses are continuous, such that the previous methods would not work. By the Central Limit Theorem , as the number of random variables increase, their sum is approximately normal distributed : \\[ S \\sim (E[S], Var[S]) \\] Instead of using the normal distribution, the Lognormal Distribution can be used as well. However, we need to first solve for its parameters : \\[ \\begin{aligned} e^{\\mu + 0.5\\sigma^2} &= E(S) \\\\ \\left(e^{\\mu + 0.5\\sigma^2} \\right)^2 \\cdot (e^{\\sigma^2}-1) &= Var (S) \\\\ \\\\ \\therefore S \\sim \\text{Log Normal} (\\mu, \\sigma^2) \\end{aligned} \\] Warning Although the notation is the same, do NOT confuse the lognormal parameters with the normal parameters.","title":"Approximation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/5.%20Aggregate%20Models/#continuity-correction","text":"We may still want to use the approximation even when losses are discrete as they may still be too complicated to compute. However, this may lead to some inaccuracies as we are using a continuous distribution to approximate a discrete distribution. Under a discrete distribution, the following are complements: \\[ P(X \\le n) + P(X \\gt n+1) = 1 \\] However, the same cannot be said for a continuous distribution, as there is some density between \\(n\\) and \\(n+1\\) that is not accounted for: \\[ P(S \\le n) + P(S \\gt n+1) \\ne 1 \\] Thus, Continuity Correction attempts to account for this by reducing the missing density. The general idea is to adjust the range such that the midpoint of the two values : \\[ \\begin{aligned} P(S \\le n) &= P \\left(S \\le \\frac{n+(n+1)}{2} \\right) \\\\ P(S \\ge n) &= P \\left(S \\ge \\frac{n+(n-1)}{2} \\right) \\end{aligned} \\] Another concept to remember when going from Continuous to Discrete is that a continuous \\(S \\lt n\\) is a discrete \\(S \\le (n-1)\\) since there are a countable number of values, vice versa. If required to approximate the probability at a specified value , then the above method will not work as a continuous distribution cannot find the probability at a single point. Thus, it will be converted to a range around the specified value instead, with the midpoint value used on both sides of the specified value: Tip If the discrete distribution does not support consecutive integers \\(\\set{0, 1, 2, \\dots}\\) , then simply take the midpoint of whatever consecutive values in its support \\(\\set{100, 200, 300}\\) .","title":"Continuity Correction"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/5.%20Aggregate%20Models/#aggregate-deductibles","text":"Insurers can also purchase insurance for themselves, known as Reinsurance . Insurers may want to limit their losses to a certain amount and getting the excess loss insured by a reinsurer. This is known as Stop Loss Insurance . Intuitively, it can be thought of as a deductible being applied to Aggregate Losses , as the insurer (policyholder) only pays for losses up the deductible while the reinsurer pays for the rest. As with regular deductibles, the main quantity of interest is the reinsurer's expected loss , as it represents the Net Premium for the Stop Loss insurance . Info Recall from FAM-L that Net Premiums refer to premiums that are charged without consideration of expenses or profits, purely based on the insurers loss. \\[ \\begin{aligned} E(S-d)_{+} &= E(S) - E(S \\land d) \\end{aligned} \\] \\(E(S)\\) can be found using the methods described earlier. However, unlike a normal deductible, the distribution of \\(S\\) is not known, thus \\(E(S \\land d)\\) can only be found via first principles . In most cases, the underlying severity distribution is discrete for simplicity. What makes this much more challenging than its regular deductible counterpart is that it is tricky to obtain all possible values of S and their corresponding probabilities. It is recommended to consider a probability tree to easily see all combinations of X and N that results in \\(S \\le d\\) and its corresponding probability. Tip There may be multiple combinations that may lead to the same aggregate loss: \\(X = 2, \\ N = 1, \\ \\therefore S = 2\\) \\(X = 1, \\ N = 2, \\ \\therefore S = 2\\) The probability of \\(S=2\\) must reflect both these combinations .","title":"Aggregate Deductibles"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/5.%20Aggregate%20Models/#aggregate-payments","text":"The discussion so far has solely been on Aggregate Losses . However, if each policy has a deductible, then there is a need to consider Aggregate Payments instead. Both types of aggregate payments (per loss and per payment) can be used to determine aggregate payments: If considering Payments per Losses \\((Y^L)\\) , then as its name suggests, the appropriate frequency to use is the Loss Frequency \\((N)\\) . \\[ \\begin{aligned} S &= Y^L_{1} + Y^L_{2} + \\dots + Y^L_{N} \\\\ \\\\ E(S) &= E(N) \\cdot E(Y^L) \\\\ Var (S) &= Var(Y^L) \\cdot E(N) + \\left[E \\left(Y^L \\right) \\right]^2 \\cdot Var (N) \\end{aligned} \\] If considering Payments per Payments \\((Y^P)\\) , then as its name suggests, the appropriate frequency to use is the Payment Frequency \\((N^{*})\\) . \\[ \\begin{aligned} S &= Y^P_{1} + Y^P_{2} + \\dots + Y^P_{N^{*}} \\\\ \\\\ E(S) &= E(N^{*}) \\cdot E(Y^P) \\\\ Var (S) &= Var(Y^P) \\cdot E(N^{*}) + \\left[E \\left(Y^P \\right) \\right]^2 \\cdot Var (N^{*}) \\end{aligned} \\] The key is to match the frequency distribution with the severity : Loss & Loss Frequency; Payments & Payment Frequency.","title":"Aggregate Payments"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/5.%20Aggregate%20Models/#payment-frequency","text":"Imposing a deductible naturally results in fewer payments than losses as not all losses result in a payment. The Payment Frequency \\((N^{*})\\) is thus an adjusted version of the loss distribution that only takes into account the losses that result in a payment. Consider a Bernoulli Indicator variable that takes on the value 1 if a payment will be made and 0 if it will not: \\[ \\begin{aligned} I &= \\begin{cases} 1 \\ (\\text{Paid}),& X \\gt d \\\\ 0 \\ (\\text{Not Paid}),& X \\le d \\end{cases} \\\\ \\\\ v &= P(X \\gt d) \\\\ P_I(t) &= 1 + v(t-1) \\end{aligned} \\] \\(N^{*}\\) is the sum of \\(N\\) idendepent indicator variables : \\[ N^{*} = I_1 + I_2 + \\dots + I_N \\] It is essentially a Compound Distribution with \\(N\\) being the primary distribution and \\(I\\) being the secondary distribution. Thus, the PGF of \\(N^{*}\\) can be determined: \\[ \\begin{aligned} P_{N^*} &= P_{N}[P_{I}(t)] \\\\ &= P_{N}[1 + p(t-1)] \\end{aligned} \\] By expanding simplfying the expression, the distribution of \\(N^{*}\\) can be determined: \\[ \\begin{aligned} N &\\sim \\text{Poisson}(\\mu) \\\\ \\\\ P_{N}[1 + p(t-1)] &= e^{\\mu(1 + v(z-1) -1)} \\\\ &= e^{\\mu (v(z-1))} \\\\ &= e^{v\\mu (z-1)} \\\\ \\\\ \\therefore N^{*} &\\sim \\text{Poisson}(v \\cdot \\mu) \\end{aligned} \\] Repeating the process for the other two members of the (a, b, 0) class, \\[ \\begin{aligned} N &\\sim \\text{Binomial} (n, p) \\\\ \\therefore N^{*} &\\sim \\text{Binomial} (n, vp) \\\\ \\\\ N &\\sim \\text{Negative Binomial} (r, \\theta) \\\\ \\therefore N^{*} &\\sim \\text{Negative Binomial} (r, v \\theta) \\end{aligned} \\]","title":"Payment Frequency"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/5.%20Aggregate%20Models/#individual-risk-model","text":"","title":"Individual Risk Model"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/6.%20Loss%20Reserving/","text":"Loss Reserving \u00b6 Overview \u00b6 General Insurance claims may be filed long after the loss event occurs, thus they must set up a Loss Reserve to ensure that they have enough capital to pay out these claims well into the future. Generally speaking, this process involves estimating how the loss will develop over time , until it reaches some final amount. The loss reserve is thus difference between the final amount that they expect to pay out and what has already been paid out: \\[ \\text{Loss Reserve} = \\text{Final Loss} - \\text{Paid Loss} \\] Loss Terminology \u00b6 This subsection will properly introduce the proper terminology regarding losses There are three basic building blocks of a reported claim: Incurred Losses : Reported loss amount Paid Losses : Loss amounts that have been reported and paid out Case Reserve : Loss amounts that have been reported but not yet paid out \\[ \\text{Incurred Loss} = \\text{Paid Loss} + \\text{Case Reserve} \\] As mentioned previously, claims can be reported any time after an accident has occured. Thus, there will always be claims for losses that have already been incurred in the past but have not been reported to the insurer, appropriately named as Incurred But Not Reported (IBNR) losses. This is the main quantity that the insurer must set up a reserve for, known as the IBNR Reserve . The loss reserve is thus the combination of the two aforementioned reserves: \\[ \\text{Loss Reserve} = \\text{Case Reserve} + \\text{IBNR Reserve} \\] The estimated total loss amount that the insurer will pay to finally the close the claim is known as the Ultimate Loss . Thus, the first equation can be rewritten more formally: \\[ \\begin{aligned} \\text{Loss Reserve} &= \\text{Ultimate Loss} - \\text{Paid Loss} \\\\ \\\\ \\text{Loss Reserve} &= \\text{Ultimate Loss} - \\text{Paid Loss} \\\\ \\text{Case Reserve} + \\text{IBNR Reserve} &= \\text{Ultimate Loss} - \\text{Paid Loss} \\\\ \\text{IBNR Reserve} &= \\text{Ultimate Loss} - \\text{Paid Loss} - \\text{Case Reserve} \\\\ \\text{IBNR Reserve} &= \\text{Ultimate Loss} - (\\text{Paid Loss} + \\text{Case Reserve}) \\\\ \\text{IBNR Reserve} &= \\text{Ultimate Loss} - \\text{Incurred Loss} \\end{aligned} \\] Expected Loss Ratio Method \u00b6 The simplest method for estimating ultimate losses is known as the Expected Loss Ratio Method . Loss Ratios (LR) are the ratio of losses to premiums , which is a measure of the percentage of premiums that have been used to pay losses. Ideally, the insurer would like to have a loss ratio lesser than 100% . \\[ \\text{LR} = \\frac{\\text{Loss}}{\\text{Premium}} \\] The Expected Loss Ratio (ELR) is the final loss ratio ; the ratio of the ultimate loss to the total premiums collected. Thus, since total premiums are known, ultimate losses can be estimated by estimating the ELR: \\[ \\begin{aligned} \\text{ELR} &= \\frac{\\text{Ultimate Loss}}{\\text{Total Premium}} \\\\ \\\\ \\therefore \\text{Ultimate Loss} &= \\text{Earned Premium} \\cdot \\text{ELR} \\end{aligned} \\] The above is calculated per Block of Business . It refers to a group of similar policies under the same line of business over the same policy period . For most questions, the data provided is typically aggregated by accident year , thus different accident years represent different blocks of business . The total reserve is thus the sum of the reserve for all blocks of business (AY): \\[ \\text{Total Reserve} = \\sum \\text{Reserve}_\\text{AY} \\] Chain Ladder Method \u00b6 Another method of estimating ultimate losses is known as the Chain Ladder (CL) Method. This methods works by finding patterns in historical loss development and assumes that future losses will continue to develop in a similar manner . In order to efficiently find patterns in historical losses, they are recorded in a convenient format known as a Development Table : Each Cell represents the loss that occurred in that year Each Row shows losses that occurred in the same Accident Year (AY) Each Column shows losses that occurred in the same Development Year (DY) Each Diagonal shows losses that occurred in the same Calendar Year (CY) Note The AY that the diagonal originates from is the CY for that calendar year. This can be seen by the colouring of the cells in the table below. DYs are also known as the age of the claim as they indicate how long after the AY when the claims are paid - DY3 represents that the claim was paid 3 years after the AY. The next step is to find the Cumulative Losses per AY. This is done by changing the values of each cell to the sum of all previous cells in the same row . \\[ \\begin{aligned} \\text{Cumulative Loss} &= \\text{Sum of Losses in the same AY} \\\\ \\text{Cumulative Cell}_{AY, DY} &= \\sum_{DY} \\text{Incremental Cell}_{AY, DY} \\end{aligned} \\] Note Some questions may directly provide the cumulative loss in the table rather than incremental losses, saving you the time of having to sum everything up. The number of columns indicates how long the insurer expects the losses to develop for. If the last column is DY6, then all claims must be settled and closed by DY6 . Thus, the cumulative loss at DY6 is the ultimate loss . Warning Some questions will require us to construct our own table. Thus, it is important to fully understand the intuition behind each of the components. The fundamental assumption behind the method is that losses take the same duration to mature to their ultimate values. Note that this DOES NOT mean that losses all reach thier ultimate values on the same year. Losses originating from different AYs will always reach ultimate on a different year; same duration NOT same date . Thus, the DYs can be thought of as the age of the losses . Losses from later AYs are \"younger\" and hence need more time to mature . The goal is now to find the h**istorical patterns** in the losses and then use those patterns to fill up the blank half of the development table. The historical patterns are known as Loss Development Factors (LDF), which is the ratio of the current DY's cumulative loss to the previous DY's cumulative loss : \\[ \\text{LDF}_{AY, DY} = \\frac{\\text{Cumulative Cell}_{AY, DY}}{\\text{Cumulative Cell}_{AY, DY-1}} \\] The next step would be to develop each years using the development factors , until it reaches the ultimate loss: \\[ \\text{Cumulative Cells}_{AY, DY+1} = \\text{Cumulative Cells}_{AY, DY} \\cdot \\text{LDF}_{AY, DY} \\] However, the problem is that for all years other than the final year, there are multiple LDFs for each DY. There are a few solutions to this: Simple Average, Recent Average or Weighted Average, usually specified by the question . Assuming that the simple average is used, we will arrive at the following LDFs and thus the corresponding final development table : Using this table, the reserve for each AY can be calculated: Ultimate Loss : Cell in the last column Paid Loss : Cell in the last originally filled column (Highlighted in yellow) \\[ \\begin{aligned} \\text{Reserve}_{AY} &= \\text{Ultimate Loss} - \\text{Paid Loss} \\\\ &= \\text{Cumulative Cell}_{AY, DY Final} - \\text{Cumulative Cell}_{AY, DY Original} \\\\ \\\\ \\therefore \\text{Total Reserve} &= \\sum_{\\text{AY}} \\text{Reserve}_{AY} \\end{aligned} \\] Technically speaking, it is a waste of time find an estimate for all the blank cells in the table, since we are only interested in the final column. Thus, instead of slowly multiplying the LDFs each year, we can instead find an Age to Ultimate LDF, which as its name suggests, when multiplied provides the ultimate loss for that AY. \\[ \\begin{aligned} \\text{LDF}_{AY, Ultimate} &= \\prod^{\\text{DY Final}}_{DY Original} \\text{LDF}_{AY, DY} \\\\ \\\\ \\text{Ultimate Loss} &= \\text{Paid Loss} \\cdot \\text{LDF}_{AY, Ultimate} \\end{aligned} \\] Bornhautter Ferguson Method \u00b6 The Bornhautter Ferguson (BF) method balances both the above methods to produce an estimate. Using the CL method, the following result is obtained: \\[ \\begin{aligned} \\text{Ultimate Loss} &= \\text{Paid Loss} \\cdot \\text{LDF}_{\\text{Ultimate}} \\\\ \\frac{\\text{Paid Loss}}{\\text{Ultimate Loss}} &= \\frac{1}{\\text{LDF}_{\\text{Ultimate}}} \\\\ \\text{Proportion of Paid Losses} &= \\frac{1}{\\text{LDF}_{\\text{Ultimate}}} \\\\ \\text{Proportion of Unpaid Losses} &= 1 - \\frac{1}{\\text{LDF}_{\\text{Ultimate}}} \\\\ \\\\ \\therefore \\text{Loss Reserve} &= \\text{Ultimate Loss} \\cdot \\left(1 - \\frac{1}{\\text{LDF}_{\\text{Ultimate}}} \\right) \\end{aligned} \\] The BF method combines the ELR and CL method together by calculating the Ultimate Loss in the above expression using the ELR method, resulting in the BF Reserves : \\[ \\text{Loss Reserve}_{BF} = \\text{Ultimate Loss}_{ELR} \\cdot \\left(1 - \\frac{1}{\\text{LDF}_{\\text{CL, Ultimate}}} \\right) \\] Warning One common misunderstanding is that the BF Ultimate Loss is the ELR Ultimate Loss, because it was what is used in the equation. Notice that unlike the previous two methods, the BF method directly estimates the reserves , not the Ultimate Losses. Thus, to obtain the ultimate losses, simply add the estimated reserves to the paid losses : \\[ \\text{Ultimate Loss}_{\\text{BF}} = \\text{Loss Reserve}_{\\text{BF}} + \\text{Paid Loss} \\] Another way of expressing the above is that the BF method uses the weighted average of the two methods : \\[ \\begin{aligned} \\text{Loss Reserve}_{BF} &= w \\cdot \\text{Loss Reserve}_{CL} + (1-w) \\cdot \\text{Loss Reserve}_{ELR} \\\\ w &= \\frac{1}{\\text{LDF}_{\\text{CL, Ultimate}}} \\end{aligned} \\] Warning The above applies for ONLY THE OVERALL RESERVE, not the Case or IBNR reserves. Note As the claims approach maturity, more the LDF factors decrease, causing the weights to the CL method to increase. This is intuitive, as more data is available for those claims, thus more weight is given to the method that uses actual claims experience (CL). As with both the other methods, the BF reserves is calculated per AY , thus the total reserve is the sum of the reserves for each AY: \\[ \\text{Total Reserve}_{BF} = \\sum \\text{Reserve}_\\text{BF, AY} \\]","title":"Loss Reserving"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/6.%20Loss%20Reserving/#loss-reserving","text":"","title":"Loss Reserving"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/6.%20Loss%20Reserving/#overview","text":"General Insurance claims may be filed long after the loss event occurs, thus they must set up a Loss Reserve to ensure that they have enough capital to pay out these claims well into the future. Generally speaking, this process involves estimating how the loss will develop over time , until it reaches some final amount. The loss reserve is thus difference between the final amount that they expect to pay out and what has already been paid out: \\[ \\text{Loss Reserve} = \\text{Final Loss} - \\text{Paid Loss} \\]","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/6.%20Loss%20Reserving/#loss-terminology","text":"This subsection will properly introduce the proper terminology regarding losses There are three basic building blocks of a reported claim: Incurred Losses : Reported loss amount Paid Losses : Loss amounts that have been reported and paid out Case Reserve : Loss amounts that have been reported but not yet paid out \\[ \\text{Incurred Loss} = \\text{Paid Loss} + \\text{Case Reserve} \\] As mentioned previously, claims can be reported any time after an accident has occured. Thus, there will always be claims for losses that have already been incurred in the past but have not been reported to the insurer, appropriately named as Incurred But Not Reported (IBNR) losses. This is the main quantity that the insurer must set up a reserve for, known as the IBNR Reserve . The loss reserve is thus the combination of the two aforementioned reserves: \\[ \\text{Loss Reserve} = \\text{Case Reserve} + \\text{IBNR Reserve} \\] The estimated total loss amount that the insurer will pay to finally the close the claim is known as the Ultimate Loss . Thus, the first equation can be rewritten more formally: \\[ \\begin{aligned} \\text{Loss Reserve} &= \\text{Ultimate Loss} - \\text{Paid Loss} \\\\ \\\\ \\text{Loss Reserve} &= \\text{Ultimate Loss} - \\text{Paid Loss} \\\\ \\text{Case Reserve} + \\text{IBNR Reserve} &= \\text{Ultimate Loss} - \\text{Paid Loss} \\\\ \\text{IBNR Reserve} &= \\text{Ultimate Loss} - \\text{Paid Loss} - \\text{Case Reserve} \\\\ \\text{IBNR Reserve} &= \\text{Ultimate Loss} - (\\text{Paid Loss} + \\text{Case Reserve}) \\\\ \\text{IBNR Reserve} &= \\text{Ultimate Loss} - \\text{Incurred Loss} \\end{aligned} \\]","title":"Loss Terminology"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/6.%20Loss%20Reserving/#expected-loss-ratio-method","text":"The simplest method for estimating ultimate losses is known as the Expected Loss Ratio Method . Loss Ratios (LR) are the ratio of losses to premiums , which is a measure of the percentage of premiums that have been used to pay losses. Ideally, the insurer would like to have a loss ratio lesser than 100% . \\[ \\text{LR} = \\frac{\\text{Loss}}{\\text{Premium}} \\] The Expected Loss Ratio (ELR) is the final loss ratio ; the ratio of the ultimate loss to the total premiums collected. Thus, since total premiums are known, ultimate losses can be estimated by estimating the ELR: \\[ \\begin{aligned} \\text{ELR} &= \\frac{\\text{Ultimate Loss}}{\\text{Total Premium}} \\\\ \\\\ \\therefore \\text{Ultimate Loss} &= \\text{Earned Premium} \\cdot \\text{ELR} \\end{aligned} \\] The above is calculated per Block of Business . It refers to a group of similar policies under the same line of business over the same policy period . For most questions, the data provided is typically aggregated by accident year , thus different accident years represent different blocks of business . The total reserve is thus the sum of the reserve for all blocks of business (AY): \\[ \\text{Total Reserve} = \\sum \\text{Reserve}_\\text{AY} \\]","title":"Expected Loss Ratio Method"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/6.%20Loss%20Reserving/#chain-ladder-method","text":"Another method of estimating ultimate losses is known as the Chain Ladder (CL) Method. This methods works by finding patterns in historical loss development and assumes that future losses will continue to develop in a similar manner . In order to efficiently find patterns in historical losses, they are recorded in a convenient format known as a Development Table : Each Cell represents the loss that occurred in that year Each Row shows losses that occurred in the same Accident Year (AY) Each Column shows losses that occurred in the same Development Year (DY) Each Diagonal shows losses that occurred in the same Calendar Year (CY) Note The AY that the diagonal originates from is the CY for that calendar year. This can be seen by the colouring of the cells in the table below. DYs are also known as the age of the claim as they indicate how long after the AY when the claims are paid - DY3 represents that the claim was paid 3 years after the AY. The next step is to find the Cumulative Losses per AY. This is done by changing the values of each cell to the sum of all previous cells in the same row . \\[ \\begin{aligned} \\text{Cumulative Loss} &= \\text{Sum of Losses in the same AY} \\\\ \\text{Cumulative Cell}_{AY, DY} &= \\sum_{DY} \\text{Incremental Cell}_{AY, DY} \\end{aligned} \\] Note Some questions may directly provide the cumulative loss in the table rather than incremental losses, saving you the time of having to sum everything up. The number of columns indicates how long the insurer expects the losses to develop for. If the last column is DY6, then all claims must be settled and closed by DY6 . Thus, the cumulative loss at DY6 is the ultimate loss . Warning Some questions will require us to construct our own table. Thus, it is important to fully understand the intuition behind each of the components. The fundamental assumption behind the method is that losses take the same duration to mature to their ultimate values. Note that this DOES NOT mean that losses all reach thier ultimate values on the same year. Losses originating from different AYs will always reach ultimate on a different year; same duration NOT same date . Thus, the DYs can be thought of as the age of the losses . Losses from later AYs are \"younger\" and hence need more time to mature . The goal is now to find the h**istorical patterns** in the losses and then use those patterns to fill up the blank half of the development table. The historical patterns are known as Loss Development Factors (LDF), which is the ratio of the current DY's cumulative loss to the previous DY's cumulative loss : \\[ \\text{LDF}_{AY, DY} = \\frac{\\text{Cumulative Cell}_{AY, DY}}{\\text{Cumulative Cell}_{AY, DY-1}} \\] The next step would be to develop each years using the development factors , until it reaches the ultimate loss: \\[ \\text{Cumulative Cells}_{AY, DY+1} = \\text{Cumulative Cells}_{AY, DY} \\cdot \\text{LDF}_{AY, DY} \\] However, the problem is that for all years other than the final year, there are multiple LDFs for each DY. There are a few solutions to this: Simple Average, Recent Average or Weighted Average, usually specified by the question . Assuming that the simple average is used, we will arrive at the following LDFs and thus the corresponding final development table : Using this table, the reserve for each AY can be calculated: Ultimate Loss : Cell in the last column Paid Loss : Cell in the last originally filled column (Highlighted in yellow) \\[ \\begin{aligned} \\text{Reserve}_{AY} &= \\text{Ultimate Loss} - \\text{Paid Loss} \\\\ &= \\text{Cumulative Cell}_{AY, DY Final} - \\text{Cumulative Cell}_{AY, DY Original} \\\\ \\\\ \\therefore \\text{Total Reserve} &= \\sum_{\\text{AY}} \\text{Reserve}_{AY} \\end{aligned} \\] Technically speaking, it is a waste of time find an estimate for all the blank cells in the table, since we are only interested in the final column. Thus, instead of slowly multiplying the LDFs each year, we can instead find an Age to Ultimate LDF, which as its name suggests, when multiplied provides the ultimate loss for that AY. \\[ \\begin{aligned} \\text{LDF}_{AY, Ultimate} &= \\prod^{\\text{DY Final}}_{DY Original} \\text{LDF}_{AY, DY} \\\\ \\\\ \\text{Ultimate Loss} &= \\text{Paid Loss} \\cdot \\text{LDF}_{AY, Ultimate} \\end{aligned} \\]","title":"Chain Ladder Method"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/6.%20Loss%20Reserving/#bornhautter-ferguson-method","text":"The Bornhautter Ferguson (BF) method balances both the above methods to produce an estimate. Using the CL method, the following result is obtained: \\[ \\begin{aligned} \\text{Ultimate Loss} &= \\text{Paid Loss} \\cdot \\text{LDF}_{\\text{Ultimate}} \\\\ \\frac{\\text{Paid Loss}}{\\text{Ultimate Loss}} &= \\frac{1}{\\text{LDF}_{\\text{Ultimate}}} \\\\ \\text{Proportion of Paid Losses} &= \\frac{1}{\\text{LDF}_{\\text{Ultimate}}} \\\\ \\text{Proportion of Unpaid Losses} &= 1 - \\frac{1}{\\text{LDF}_{\\text{Ultimate}}} \\\\ \\\\ \\therefore \\text{Loss Reserve} &= \\text{Ultimate Loss} \\cdot \\left(1 - \\frac{1}{\\text{LDF}_{\\text{Ultimate}}} \\right) \\end{aligned} \\] The BF method combines the ELR and CL method together by calculating the Ultimate Loss in the above expression using the ELR method, resulting in the BF Reserves : \\[ \\text{Loss Reserve}_{BF} = \\text{Ultimate Loss}_{ELR} \\cdot \\left(1 - \\frac{1}{\\text{LDF}_{\\text{CL, Ultimate}}} \\right) \\] Warning One common misunderstanding is that the BF Ultimate Loss is the ELR Ultimate Loss, because it was what is used in the equation. Notice that unlike the previous two methods, the BF method directly estimates the reserves , not the Ultimate Losses. Thus, to obtain the ultimate losses, simply add the estimated reserves to the paid losses : \\[ \\text{Ultimate Loss}_{\\text{BF}} = \\text{Loss Reserve}_{\\text{BF}} + \\text{Paid Loss} \\] Another way of expressing the above is that the BF method uses the weighted average of the two methods : \\[ \\begin{aligned} \\text{Loss Reserve}_{BF} &= w \\cdot \\text{Loss Reserve}_{CL} + (1-w) \\cdot \\text{Loss Reserve}_{ELR} \\\\ w &= \\frac{1}{\\text{LDF}_{\\text{CL, Ultimate}}} \\end{aligned} \\] Warning The above applies for ONLY THE OVERALL RESERVE, not the Case or IBNR reserves. Note As the claims approach maturity, more the LDF factors decrease, causing the weights to the CL method to increase. This is intuitive, as more data is available for those claims, thus more weight is given to the method that uses actual claims experience (CL). As with both the other methods, the BF reserves is calculated per AY , thus the total reserve is the sum of the reserves for each AY: \\[ \\text{Total Reserve}_{BF} = \\sum \\text{Reserve}_\\text{BF, AY} \\]","title":"Bornhautter Ferguson Method"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/7.%20Ratemaking/","text":"Ratemaking \u00b6 Fundamental Equation \u00b6 Ratemaking is the process of determining how much premium to charge a policyholder. It is important to set rates such that there is enough premium collected to cover losses and expenses while still generating a profit for the insurer. This is illustrated through the Fundamental Insurance Equation : \\[ \\text{Premium} = \\text{Loss} + \\text{Expense} + \\text{Profit} \\] The remaining subsections will go in-depth into each component of this equation. Losses \u00b6 Losses are the main component of determining premiums. However, before historical losses can be used, they must be Aggregated , Developed & Trended . Loss Aggregation \u00b6 Losses can be aggregated according to the following categories: Calendar Year (CY): Losses that only occur from 1 Jan to 31 Dec of that year Accident Year (AY): ALL losses from an accident that occurred in a specified 12 month period Policy Year (PY): ALL losses from a policy that was issued in a specified 12 month period Unless otherwise stated, the specified 12 month period follows the calendar year . In otherwise, Policy Year 2015 refers to a policy that was issued from 1 Jan 2015 to 31 Dec 2015. Since AY and PY consider ALL losses, they usually have a cut off date known as the Valuation Date , such that they only consider the losses occuring at or before that date . The valuation date is usually referred to as the losses \"As at\" or \"As of\". They refer to the cumulative losses at that time. \\[ \\begin{aligned} \\text{CY Paid} &= \\text{Sum of all paid losses in CY} \\\\ \\text{CY Incurred} &= \\text{CY Paid} + \\text{Change in Case Reserves} \\\\ \\\\ \\text{AY/PY Paid} &= \\text{Sum of all paid losses at Valuation Date} \\\\ \\text{AY/PY Incurred} &= \\text{AY/PY Paid} + \\text{Case Reserves at Valuation Date} \\end{aligned} \\] Since the default is to assume that PY and AY follow CY, unless stated otherwise as well, the valuation date is the end of the year . Most questions already provide the aggregated losses. However, if given raw data , we are expected to know how to aggregate them ourselves. Loss Development \u00b6 Using the above aggregations, the losses can be developed to their ultimate values using the chain ladder method described in the previous reserving section. Loss Trending \u00b6 Lastly, losses must be trended to account for factors that drive losses such as inflation, increased medical costs or technological advances. Warning Loss Trending and Development are often confused with one another as both involve adjusting the loss amount. Some people also believe that only one of them should be applied to a loss because applying both would double count certain factors , known as the Overlap Fallacy . However, note that they are extremely different concepts: Loss Development is about bringing an immature loss to its ultimate amount , as losses take time to be discovered and reported to the insurer. Loss Trending is about bringing the ultimate loss today to what it would be at a future time , as losses are also subject to external factors such as inflation . Similar to development, trending involves using historical data to find trends and then project them into the future: \\[ \\text{Trended Ultimate Loss} = \\text{Ultimate Loss} \\cdot (1 + \\text{Trend Factor})^\\text{Trend Period} \\] However, finding trend factors is out of scope for this exam. Thus, the main focus will be on determining the trend period . Formally speaking, it is the difference between the midpoint of the Experience Period and the midpoint of the Forecast Period : Experience Period : Period in which losses can occur for policies with old rates Forecast Period : Period in which losses can occur for policies with new rates The calculation for the duration of each period depends on what was used to aggregate the loss data: AY Aggregation : Period is the AY PY Aggregation : Period is the PY plus the policy term Assume that rates are in effect for exactly one year , from the start to the end of the PY. Assuming that policies are written uniformly throughout the year , it means that there will be policies written on the first and last day of the PY : Written on First Day : Expires on the last day of PY Written on Last Day : Expires on last day of PY plus policy term Both of the above policies are written using the same set of rates. Thus, the maximum possible timespan that policies written with the same set of rates can exists are: \\[ \\text{PY Timespan} = \\text{Rates Duration} + \\text{Policy Term} \\] Warning The above example assumed that rates are in effect for one year, exactly at the start and end of the PY. In practice, rates can start and end at any time . The trend period is thus the difference between the midpoint of the two periods: \\[ \\text{Trend Period} = \\text{Forecast Period Midpoint} - \\text{Experience Period Midpoint} \\] Tip For all date related computations, assume that the base unit is 1 year . Most questions provides dates that are usually on the same day of the month (EG. 1 st ). The calculation mainly involves a difference in months : \\[ \\text{1 Jan} - \\text{1 Oct} = \\frac{3}{12} = \\frac{1}{4} \\] However, questions can also provide dates that are on different days of the month (EG. 15 th vs 1 st ). The difference in days must also be accounted for: \\[ \\text{1 Jan} - \\text{15 Oct} = \\frac{2.5}{12} = \\frac{5}{24} \\] The most common mistake when calculating dates is getting a quarter and third mixed up: 3 Months difference is a quarter \\(\\frac{1}{4}\\) 4 Months difference is a third \\(\\frac{1}{3}\\) The 3 and 4 get swapped in the denominator! Other Trending Methods \u00b6 Rather than using just one trending factor, insurers might sometimes use multiple trending factors to obtain a trended ultimate and then take a weighted average of all of them instead. Similarly, they could also use regression to project the trended ultimate. Based on the regression slope \\((m)\\) , the trended ultimate can be calculated: \\[ \\begin{aligned} L^T &= L^U + mt \\\\ \\\\ \\ln L^T &= \\ln L^U + mt \\\\ \\ln \\frac{L^T}{L^U} &= mt \\\\ \\frac{L^T}{L^U} &= e^{mt} \\\\ L^T &= L^U \\cdot e^{mt} \\end{aligned} \\] Expenses \u00b6 There are two main types of expenses - Loss Adjustment Expenses (LAE) and Other Expenses . LAE are expenses that are incurred during the claim settlement process and are typically included as part of the incurred losses . They can be further split into two categories: Allocated LAE (ALAE): Expenses associated with a specific claim (EG. Lawyer's Fee) Unallocated LAE (ULAE): Expenses that are NOT associated with a specific claim (EG. Salaries) As its name suggests, other expenses is a catch all for non-LAE expenses. They are generally split into two categories: Variable Expenses (VE): Expenses that vary with premium (EG. Commissions) Fixed Expenses (FE): Expenses that do NOT vary with premium (EG. Salaries) \\[ \\text{Expenses} = \\text{VE} \\cdot P + \\text{FE} \\] Profit \u00b6 Profits can be included in ratemaking either implicitly or explicitly. By using more conservative estimates than usual, actual experience should be better than expected, which implicitly results in profit for the insurer. Alternatively, they can explicitly include a margin for the target profit (T) in the ratemaking process, as seen in the fundamental insurance equation. \\[ \\text{Profit} = T \\cdot P \\] Based on this, we can find the target loss ratio such that the premiums are sufficient to cover both variable expenses and meet the profitability target, known as the Permissible Loss Ratio (PLR): \\[ \\begin{aligned} P &= L + FE + VE \\cdot P + T \\cdot P \\\\ P - VE \\cdot P - T \\cdot P &= L + FE \\\\ P \\cdot (1 - VE - T) &= L + FE \\\\ 1 - VE - T &= \\frac{L + FE}{P} \\\\ \\\\ \\therefore \\text{PLR} &= 1 - VE - T \\end{aligned} \\] Premiums \u00b6 Similar to losses, historical premiums must be first aggregated and then brought to their current level (\"trended\"). Premium Aggregation \u00b6 Since premiums cannot be linked to specific accidents, they can only be aggregated by CY or PY. Premiums can also be categorized in three ways, based on their accounting definitions: Written Premium (WP): Total premium collected Earned Premium (EP): Portion of premium used to provide insurance Unearned Premium (UP): Portion of premium that has yet to be used to provide insurance The above are all calculated at a specific point in time, which is the end of year for CY and the valuation date for PY. \\[ \\text{WP} = \\text{EP} + \\text{UP} \\] As with loss aggregation, most questions already provide the aggregated premiums. However, if given raw data, we are expected to know how to aggregate them ourselves. Premium Trending \u00b6 Premiums then need to be brought to their current level, similar to how ultimate losses were trended. Extension of Exposures \u00b6 The first method is known as the Extension of Exposures method, which adjusts premiums for each individual policyholder by accounting for rate changes in every single exposure. For some background, premiums are calculated as the product of a rate and an some base: Exposure Base : Rate per unit of Exposure : \\[ \\text{Premium} = \\text{Exposure Base} \\cdot \\text{Rate per Exposure} \\] However, exposures need not be continuous (EG. Gender - Male, Female). Possible exposures are known as Risk Classifications . For simplicity, this section will consider only discrete exposures . These premiums are then calculated relatively - in other words, it assumes some base risk classification will be charged a certain premium and then adjusts it accordingly if the risk classification is different . Note The concept is similar to dummy variable regression where \\(x=0\\) is assumed as the base case and then changes accordingly. Formally speaking, the base risk classification is known as the Base Cell , which is usually the most common risk class . The premium for this risk class is known as the Base Rate . In order to adjust the premiums, each risk class has its own rate , collectively known as the Rate Differentials for the rating variable. Naturally, the rates for the base cell are 1 to reflect that it is the base. \\[ \\text{Premium} = \\text{Base Rate} \\cdot \\text{Rate Relativities} \\] Thus, old premiums are brought to the current level by simply recalculating them at the current rates instead: \\[ \\begin{aligned} \\text{Old Premium} &= 300 \\cdot 1.2 \\cdot 0.8 = 288 \\\\ \\text{On Level Premium} &= 330 \\cdot 1.30 \\cdot 0.70 = 300.30 \\end{aligned} \\] Parallelogram Method \u00b6 Although the above method is more \"correct\", the problem is that it requires highly granular data for every policyholder and the individual rates. However, insurers may only have access to aggregated data for both premiums and rates, in which case the Parallelogram Method must be used. Instead of being provided with the exact rates, yearly aggregate rate changes are provided: This information can be reorganized into a series of parallelograms : Each square represents a calendar year while diagonal lines represent policies . Each diagonal has a horizontal length equal to the policy term in years. In particular, diagonals start from the first day of a new rate . Thus, the area between any two diagonals forms a Parellogram , which represent all policies written under a particular set of rates. Note Recall from Loss Trending that the timespan is from the first day of rates to the last day of rates plus the policy term; the parallelograms are a visualization of the timespan . Since the length of each square is exactly one calendar year, it is equal to 1. Thus, the area of the square is 1 . Due to all the diagonals, the square can be \"broken up\" into different shapes, whose area must all still sum to 1 : Each shape within the square corresponds to policies written under different set of rates . Assuming that premiums are aggregated by CY , the premiums collected in the year is the sum of the premiums from all these policies. Since the area of the square is 1, the area of these different shapes that represent different rates can be intepreted as weights . Thus, the aggregate premium collected for that year is based on the weighted average rate of all rates within the square: \\[ \\text{Average Rate} = \\sum \\text{Area} \\cdot \\text{Yearly Rate} \\] Note The rate for each year is the cumultive rate changes up till that time. Using the given rate changes in the table above, \\[ \\begin{aligned} \\text{CY 2014 Rates} &= 1 \\\\ \\text{CY 2015 Rates} &= 1 \\cdot (1.05) \\\\ \\text{CY 2016 Rates} &= 1 \\cdot (1.05)(1.1) \\\\ \\text{CY 2017 Rates} &= 1 \\cdot (1.05)(1.1)(0.98) \\\\ \\end{aligned} \\] Recall from the extension method that the Historical and Current premiums are essentially the same exposure base multiplied by different rates. Using that logic, we can come up with an adjustment factor that transforms historical premiums to current premiums, known as the On Level Factor : \\[ \\begin{aligned} \\text{Historical Premium} &= \\text{Exposure Base} \\cdot \\text{Average Rate} \\\\ \\\\ \\text{Current Premium} &= \\text{Exposure Base} \\cdot \\text{Current Rate} \\\\ &= \\text{Exposure Base} \\cdot \\text{Current Rate} \\cdot \\frac{\\text{Average Rate}}{\\text{Average Rate}} \\\\ &= \\text{Exposure Base} \\cdot \\text{Average Rate} \\cdot \\frac{\\text{Current Rate}}{\\text{Average Rate}} \\\\ &= \\text{Historical Premium} \\cdot \\text{On Level Factor} \\\\ \\\\ \\therefore \\text{On Level Factor} &= \\frac{\\text{Current Rate}}{\\text{Average Rate}} \\end{aligned} \\] Paralellogram Geometry \u00b6 The difficulty comes from being able to correctly calculate the lengths of various components , which requires us to understand the geometry behind a parallelogram. Every parallelogram has the following basic layout: Warning If the policy term is lesser than one year, then depending on the start date, the policy may not even cross even to the next year. For instance, a 6 month policy bought on 1 Jun will end on 1 Jan, not crossing the CY boundary. Thus, always perform a sense check when dealing with a policy of under a year. Given the start date of the rates, the time to the start of the next year \\((x)\\) can be calculated. Assuming a policy term of 1 year , the length of the small triangle is simply the complement \\((1-x)\\) . In order to calculate the height of the small triangle, geometric rules must be used. Since the two triangles share the same angle at the tip and a right angle, the two triangles thus have the same angle at every vertice and thus are considered to be similar triangles . For two similar triangles, the ratio of their lengths and heights must be equal: \\[ \\begin{aligned} \\frac{L_1}{H_1} &= \\frac{L_2}{H_2} \\\\ \\\\ \\frac{1}{1} &= \\frac{x}{H_2} \\\\ \\therefore H_2 &= x \\end{aligned} \\] Using the above method, the dimensions of any triangles can be found and hence used to calculate the area. Generally speaking, it is better to calculate the area of the two triangles and then take the area of the trapezium as their complement . Warning Although 1 year policy terms are the most common, do NOT mistakenly believe that the triangles are always isoceles. Repeating the same process for a policy term of 6 months will result in a non-isoceles triangle : Thus, it is best to apply first principles to determine the dimensions of the triangles. Rate Changes \u00b6 Ratemaking is a prospective process to determine how much premiums should be charged based on our estimates of the future. The objective is to find out how much the current premiums should be adjusted to reach the adequate level, known as a Rate Change . The premiums currently being charged is known as the Current Premium \\((P^c)\\) while the Premiums that should be charged are known as the Indicative Premium \\((P^I)\\) . The indicative premium can be determined using the fundamental insurance equation: \\[ \\begin{aligned} P^I &= L + FE + VE \\cdot P^I + T \\cdot P \\\\ P^I - VE \\cdot P^I - T \\cdot P^I &= L + FE \\\\ P^I \\cdot (1 - VE - T) &= L + FE \\\\ P^I &= \\frac{L + FE}{1 - VE - T} \\\\ \\end{aligned} \\] The rate change can determined in one of the two following methods: The Loss Ratio Method divides both sides by the Current Premium \\((P^c)\\) , turning the loss on the RHS into a Loss Ratio , which earns the method its name. \\[ \\begin{aligned} P^I &= \\frac{L + FE}{1 - VE - T} \\\\ \\frac{P^I}{P^c} &= \\frac{\\frac{L}{P^c} + \\frac{FE}{P^c}}{1 - VE - T} \\\\ \\frac{P^I}{P^c} - 1 &= \\frac{LR + \\frac{FE}{P^c}}{1 - VE - T} - 1 \\\\ \\\\ \\therefore \\text{Rate Change} &= \\frac{LR + \\frac{FE}{P^c}}{1 - VE - T} - 1 \\end{aligned} \\] Similarly, the Pure Premium Method divides both sides by the Number of Exposures \\((n^e)\\) , turning the loss on the RHS into a Pure Premium, which earns the method its name. \\[ \\begin{aligned} P^I &= \\frac{L + FE}{1 - VE - T} \\\\ \\frac{P^I}{n^e} &= \\frac{\\frac{L}{n^e} + \\frac{FE}{n^e}}{1 - VE - T} \\\\ \\\\ \\therefore \\text{Rate Change} &= \\frac{\\frac{L}{n^e} + \\frac{FE}{P^c}}{1 - VE - T} \\end{aligned} \\] Tip If multiple years of data is given, then take the average of the Losses/Current Premium before plugging them back into the equation.","title":"Ratemaking"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/7.%20Ratemaking/#ratemaking","text":"","title":"Ratemaking"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/7.%20Ratemaking/#fundamental-equation","text":"Ratemaking is the process of determining how much premium to charge a policyholder. It is important to set rates such that there is enough premium collected to cover losses and expenses while still generating a profit for the insurer. This is illustrated through the Fundamental Insurance Equation : \\[ \\text{Premium} = \\text{Loss} + \\text{Expense} + \\text{Profit} \\] The remaining subsections will go in-depth into each component of this equation.","title":"Fundamental Equation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/7.%20Ratemaking/#losses","text":"Losses are the main component of determining premiums. However, before historical losses can be used, they must be Aggregated , Developed & Trended .","title":"Losses"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/7.%20Ratemaking/#loss-aggregation","text":"Losses can be aggregated according to the following categories: Calendar Year (CY): Losses that only occur from 1 Jan to 31 Dec of that year Accident Year (AY): ALL losses from an accident that occurred in a specified 12 month period Policy Year (PY): ALL losses from a policy that was issued in a specified 12 month period Unless otherwise stated, the specified 12 month period follows the calendar year . In otherwise, Policy Year 2015 refers to a policy that was issued from 1 Jan 2015 to 31 Dec 2015. Since AY and PY consider ALL losses, they usually have a cut off date known as the Valuation Date , such that they only consider the losses occuring at or before that date . The valuation date is usually referred to as the losses \"As at\" or \"As of\". They refer to the cumulative losses at that time. \\[ \\begin{aligned} \\text{CY Paid} &= \\text{Sum of all paid losses in CY} \\\\ \\text{CY Incurred} &= \\text{CY Paid} + \\text{Change in Case Reserves} \\\\ \\\\ \\text{AY/PY Paid} &= \\text{Sum of all paid losses at Valuation Date} \\\\ \\text{AY/PY Incurred} &= \\text{AY/PY Paid} + \\text{Case Reserves at Valuation Date} \\end{aligned} \\] Since the default is to assume that PY and AY follow CY, unless stated otherwise as well, the valuation date is the end of the year . Most questions already provide the aggregated losses. However, if given raw data , we are expected to know how to aggregate them ourselves.","title":"Loss Aggregation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/7.%20Ratemaking/#loss-development","text":"Using the above aggregations, the losses can be developed to their ultimate values using the chain ladder method described in the previous reserving section.","title":"Loss Development"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/7.%20Ratemaking/#loss-trending","text":"Lastly, losses must be trended to account for factors that drive losses such as inflation, increased medical costs or technological advances. Warning Loss Trending and Development are often confused with one another as both involve adjusting the loss amount. Some people also believe that only one of them should be applied to a loss because applying both would double count certain factors , known as the Overlap Fallacy . However, note that they are extremely different concepts: Loss Development is about bringing an immature loss to its ultimate amount , as losses take time to be discovered and reported to the insurer. Loss Trending is about bringing the ultimate loss today to what it would be at a future time , as losses are also subject to external factors such as inflation . Similar to development, trending involves using historical data to find trends and then project them into the future: \\[ \\text{Trended Ultimate Loss} = \\text{Ultimate Loss} \\cdot (1 + \\text{Trend Factor})^\\text{Trend Period} \\] However, finding trend factors is out of scope for this exam. Thus, the main focus will be on determining the trend period . Formally speaking, it is the difference between the midpoint of the Experience Period and the midpoint of the Forecast Period : Experience Period : Period in which losses can occur for policies with old rates Forecast Period : Period in which losses can occur for policies with new rates The calculation for the duration of each period depends on what was used to aggregate the loss data: AY Aggregation : Period is the AY PY Aggregation : Period is the PY plus the policy term Assume that rates are in effect for exactly one year , from the start to the end of the PY. Assuming that policies are written uniformly throughout the year , it means that there will be policies written on the first and last day of the PY : Written on First Day : Expires on the last day of PY Written on Last Day : Expires on last day of PY plus policy term Both of the above policies are written using the same set of rates. Thus, the maximum possible timespan that policies written with the same set of rates can exists are: \\[ \\text{PY Timespan} = \\text{Rates Duration} + \\text{Policy Term} \\] Warning The above example assumed that rates are in effect for one year, exactly at the start and end of the PY. In practice, rates can start and end at any time . The trend period is thus the difference between the midpoint of the two periods: \\[ \\text{Trend Period} = \\text{Forecast Period Midpoint} - \\text{Experience Period Midpoint} \\] Tip For all date related computations, assume that the base unit is 1 year . Most questions provides dates that are usually on the same day of the month (EG. 1 st ). The calculation mainly involves a difference in months : \\[ \\text{1 Jan} - \\text{1 Oct} = \\frac{3}{12} = \\frac{1}{4} \\] However, questions can also provide dates that are on different days of the month (EG. 15 th vs 1 st ). The difference in days must also be accounted for: \\[ \\text{1 Jan} - \\text{15 Oct} = \\frac{2.5}{12} = \\frac{5}{24} \\] The most common mistake when calculating dates is getting a quarter and third mixed up: 3 Months difference is a quarter \\(\\frac{1}{4}\\) 4 Months difference is a third \\(\\frac{1}{3}\\) The 3 and 4 get swapped in the denominator!","title":"Loss Trending"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/7.%20Ratemaking/#other-trending-methods","text":"Rather than using just one trending factor, insurers might sometimes use multiple trending factors to obtain a trended ultimate and then take a weighted average of all of them instead. Similarly, they could also use regression to project the trended ultimate. Based on the regression slope \\((m)\\) , the trended ultimate can be calculated: \\[ \\begin{aligned} L^T &= L^U + mt \\\\ \\\\ \\ln L^T &= \\ln L^U + mt \\\\ \\ln \\frac{L^T}{L^U} &= mt \\\\ \\frac{L^T}{L^U} &= e^{mt} \\\\ L^T &= L^U \\cdot e^{mt} \\end{aligned} \\]","title":"Other Trending Methods"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/7.%20Ratemaking/#expenses","text":"There are two main types of expenses - Loss Adjustment Expenses (LAE) and Other Expenses . LAE are expenses that are incurred during the claim settlement process and are typically included as part of the incurred losses . They can be further split into two categories: Allocated LAE (ALAE): Expenses associated with a specific claim (EG. Lawyer's Fee) Unallocated LAE (ULAE): Expenses that are NOT associated with a specific claim (EG. Salaries) As its name suggests, other expenses is a catch all for non-LAE expenses. They are generally split into two categories: Variable Expenses (VE): Expenses that vary with premium (EG. Commissions) Fixed Expenses (FE): Expenses that do NOT vary with premium (EG. Salaries) \\[ \\text{Expenses} = \\text{VE} \\cdot P + \\text{FE} \\]","title":"Expenses"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/7.%20Ratemaking/#profit","text":"Profits can be included in ratemaking either implicitly or explicitly. By using more conservative estimates than usual, actual experience should be better than expected, which implicitly results in profit for the insurer. Alternatively, they can explicitly include a margin for the target profit (T) in the ratemaking process, as seen in the fundamental insurance equation. \\[ \\text{Profit} = T \\cdot P \\] Based on this, we can find the target loss ratio such that the premiums are sufficient to cover both variable expenses and meet the profitability target, known as the Permissible Loss Ratio (PLR): \\[ \\begin{aligned} P &= L + FE + VE \\cdot P + T \\cdot P \\\\ P - VE \\cdot P - T \\cdot P &= L + FE \\\\ P \\cdot (1 - VE - T) &= L + FE \\\\ 1 - VE - T &= \\frac{L + FE}{P} \\\\ \\\\ \\therefore \\text{PLR} &= 1 - VE - T \\end{aligned} \\]","title":"Profit"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/7.%20Ratemaking/#premiums","text":"Similar to losses, historical premiums must be first aggregated and then brought to their current level (\"trended\").","title":"Premiums"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/7.%20Ratemaking/#premium-aggregation","text":"Since premiums cannot be linked to specific accidents, they can only be aggregated by CY or PY. Premiums can also be categorized in three ways, based on their accounting definitions: Written Premium (WP): Total premium collected Earned Premium (EP): Portion of premium used to provide insurance Unearned Premium (UP): Portion of premium that has yet to be used to provide insurance The above are all calculated at a specific point in time, which is the end of year for CY and the valuation date for PY. \\[ \\text{WP} = \\text{EP} + \\text{UP} \\] As with loss aggregation, most questions already provide the aggregated premiums. However, if given raw data, we are expected to know how to aggregate them ourselves.","title":"Premium Aggregation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/7.%20Ratemaking/#premium-trending","text":"Premiums then need to be brought to their current level, similar to how ultimate losses were trended.","title":"Premium Trending"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/7.%20Ratemaking/#extension-of-exposures","text":"The first method is known as the Extension of Exposures method, which adjusts premiums for each individual policyholder by accounting for rate changes in every single exposure. For some background, premiums are calculated as the product of a rate and an some base: Exposure Base : Rate per unit of Exposure : \\[ \\text{Premium} = \\text{Exposure Base} \\cdot \\text{Rate per Exposure} \\] However, exposures need not be continuous (EG. Gender - Male, Female). Possible exposures are known as Risk Classifications . For simplicity, this section will consider only discrete exposures . These premiums are then calculated relatively - in other words, it assumes some base risk classification will be charged a certain premium and then adjusts it accordingly if the risk classification is different . Note The concept is similar to dummy variable regression where \\(x=0\\) is assumed as the base case and then changes accordingly. Formally speaking, the base risk classification is known as the Base Cell , which is usually the most common risk class . The premium for this risk class is known as the Base Rate . In order to adjust the premiums, each risk class has its own rate , collectively known as the Rate Differentials for the rating variable. Naturally, the rates for the base cell are 1 to reflect that it is the base. \\[ \\text{Premium} = \\text{Base Rate} \\cdot \\text{Rate Relativities} \\] Thus, old premiums are brought to the current level by simply recalculating them at the current rates instead: \\[ \\begin{aligned} \\text{Old Premium} &= 300 \\cdot 1.2 \\cdot 0.8 = 288 \\\\ \\text{On Level Premium} &= 330 \\cdot 1.30 \\cdot 0.70 = 300.30 \\end{aligned} \\]","title":"Extension of Exposures"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/7.%20Ratemaking/#parallelogram-method","text":"Although the above method is more \"correct\", the problem is that it requires highly granular data for every policyholder and the individual rates. However, insurers may only have access to aggregated data for both premiums and rates, in which case the Parallelogram Method must be used. Instead of being provided with the exact rates, yearly aggregate rate changes are provided: This information can be reorganized into a series of parallelograms : Each square represents a calendar year while diagonal lines represent policies . Each diagonal has a horizontal length equal to the policy term in years. In particular, diagonals start from the first day of a new rate . Thus, the area between any two diagonals forms a Parellogram , which represent all policies written under a particular set of rates. Note Recall from Loss Trending that the timespan is from the first day of rates to the last day of rates plus the policy term; the parallelograms are a visualization of the timespan . Since the length of each square is exactly one calendar year, it is equal to 1. Thus, the area of the square is 1 . Due to all the diagonals, the square can be \"broken up\" into different shapes, whose area must all still sum to 1 : Each shape within the square corresponds to policies written under different set of rates . Assuming that premiums are aggregated by CY , the premiums collected in the year is the sum of the premiums from all these policies. Since the area of the square is 1, the area of these different shapes that represent different rates can be intepreted as weights . Thus, the aggregate premium collected for that year is based on the weighted average rate of all rates within the square: \\[ \\text{Average Rate} = \\sum \\text{Area} \\cdot \\text{Yearly Rate} \\] Note The rate for each year is the cumultive rate changes up till that time. Using the given rate changes in the table above, \\[ \\begin{aligned} \\text{CY 2014 Rates} &= 1 \\\\ \\text{CY 2015 Rates} &= 1 \\cdot (1.05) \\\\ \\text{CY 2016 Rates} &= 1 \\cdot (1.05)(1.1) \\\\ \\text{CY 2017 Rates} &= 1 \\cdot (1.05)(1.1)(0.98) \\\\ \\end{aligned} \\] Recall from the extension method that the Historical and Current premiums are essentially the same exposure base multiplied by different rates. Using that logic, we can come up with an adjustment factor that transforms historical premiums to current premiums, known as the On Level Factor : \\[ \\begin{aligned} \\text{Historical Premium} &= \\text{Exposure Base} \\cdot \\text{Average Rate} \\\\ \\\\ \\text{Current Premium} &= \\text{Exposure Base} \\cdot \\text{Current Rate} \\\\ &= \\text{Exposure Base} \\cdot \\text{Current Rate} \\cdot \\frac{\\text{Average Rate}}{\\text{Average Rate}} \\\\ &= \\text{Exposure Base} \\cdot \\text{Average Rate} \\cdot \\frac{\\text{Current Rate}}{\\text{Average Rate}} \\\\ &= \\text{Historical Premium} \\cdot \\text{On Level Factor} \\\\ \\\\ \\therefore \\text{On Level Factor} &= \\frac{\\text{Current Rate}}{\\text{Average Rate}} \\end{aligned} \\]","title":"Parallelogram Method"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/7.%20Ratemaking/#paralellogram-geometry","text":"The difficulty comes from being able to correctly calculate the lengths of various components , which requires us to understand the geometry behind a parallelogram. Every parallelogram has the following basic layout: Warning If the policy term is lesser than one year, then depending on the start date, the policy may not even cross even to the next year. For instance, a 6 month policy bought on 1 Jun will end on 1 Jan, not crossing the CY boundary. Thus, always perform a sense check when dealing with a policy of under a year. Given the start date of the rates, the time to the start of the next year \\((x)\\) can be calculated. Assuming a policy term of 1 year , the length of the small triangle is simply the complement \\((1-x)\\) . In order to calculate the height of the small triangle, geometric rules must be used. Since the two triangles share the same angle at the tip and a right angle, the two triangles thus have the same angle at every vertice and thus are considered to be similar triangles . For two similar triangles, the ratio of their lengths and heights must be equal: \\[ \\begin{aligned} \\frac{L_1}{H_1} &= \\frac{L_2}{H_2} \\\\ \\\\ \\frac{1}{1} &= \\frac{x}{H_2} \\\\ \\therefore H_2 &= x \\end{aligned} \\] Using the above method, the dimensions of any triangles can be found and hence used to calculate the area. Generally speaking, it is better to calculate the area of the two triangles and then take the area of the trapezium as their complement . Warning Although 1 year policy terms are the most common, do NOT mistakenly believe that the triangles are always isoceles. Repeating the same process for a policy term of 6 months will result in a non-isoceles triangle : Thus, it is best to apply first principles to determine the dimensions of the triangles.","title":"Paralellogram Geometry"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/7.%20Ratemaking/#rate-changes","text":"Ratemaking is a prospective process to determine how much premiums should be charged based on our estimates of the future. The objective is to find out how much the current premiums should be adjusted to reach the adequate level, known as a Rate Change . The premiums currently being charged is known as the Current Premium \\((P^c)\\) while the Premiums that should be charged are known as the Indicative Premium \\((P^I)\\) . The indicative premium can be determined using the fundamental insurance equation: \\[ \\begin{aligned} P^I &= L + FE + VE \\cdot P^I + T \\cdot P \\\\ P^I - VE \\cdot P^I - T \\cdot P^I &= L + FE \\\\ P^I \\cdot (1 - VE - T) &= L + FE \\\\ P^I &= \\frac{L + FE}{1 - VE - T} \\\\ \\end{aligned} \\] The rate change can determined in one of the two following methods: The Loss Ratio Method divides both sides by the Current Premium \\((P^c)\\) , turning the loss on the RHS into a Loss Ratio , which earns the method its name. \\[ \\begin{aligned} P^I &= \\frac{L + FE}{1 - VE - T} \\\\ \\frac{P^I}{P^c} &= \\frac{\\frac{L}{P^c} + \\frac{FE}{P^c}}{1 - VE - T} \\\\ \\frac{P^I}{P^c} - 1 &= \\frac{LR + \\frac{FE}{P^c}}{1 - VE - T} - 1 \\\\ \\\\ \\therefore \\text{Rate Change} &= \\frac{LR + \\frac{FE}{P^c}}{1 - VE - T} - 1 \\end{aligned} \\] Similarly, the Pure Premium Method divides both sides by the Number of Exposures \\((n^e)\\) , turning the loss on the RHS into a Pure Premium, which earns the method its name. \\[ \\begin{aligned} P^I &= \\frac{L + FE}{1 - VE - T} \\\\ \\frac{P^I}{n^e} &= \\frac{\\frac{L}{n^e} + \\frac{FE}{n^e}}{1 - VE - T} \\\\ \\\\ \\therefore \\text{Rate Change} &= \\frac{\\frac{L}{n^e} + \\frac{FE}{P^c}}{1 - VE - T} \\end{aligned} \\] Tip If multiple years of data is given, then take the average of the Losses/Current Premium before plugging them back into the equation.","title":"Rate Changes"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/8.%20Model%20Estimation/","text":"Model Estimation \u00b6 This section assumes some basic knowledge on Maximum Likelihood Estimation (MLE), which can be found under another set of notes covering a Review of Statistical Theory . Complete Data \u00b6 If given a full dataset where the value of each observation is known, then it is a complete dataset. Distribution parameters can be estimated normally through MLE. For complete data only, the MOM shortcut can be used for the following distributions: Gamma & Exponential Normal & Lognormal Poisson Binomial & Bernoulli Negative Binomial & Geometric Notice that all of the (a, b, 0) distributions are listed. Thus, their zero-modified counterparts can also be found using the shortcut: \\[ \\begin{aligned} E \\left(X^M \\right) &= \\frac{1 - p^{m}_{0}}{1 - p_{0}} \\cdot E(X) \\\\ &= \\frac{1 - p^{m}_{0}}{1 - p_{0}} \\cdot \\bar{x} \\end{aligned} \\] Incomplete Data \u00b6 In practice, insurers often work with incomplete data , where they do not have the exact values of observations or only have a partial dataset . Truncated \u00b6 The first type of incomplete data is known as Truncated Data . It means that the dataset does not include observations past a specified threshold. Left Truncation Right Truncation Truncated from below Truncated from above No values below \\(d\\) No values above \\(u\\) The most common case would be Left Truncated data, which occurs when the policy has a Deductible . This is because when the loss amount does not need meet the deductible, policyholders would not bother to report it. Thus, the insurer only receives loss amounts that exceed the deductible . Warning The notation for the threshold is \\(d\\) to denote deductible. HOWEVER, the notation \\(u\\) here DOES NOT represent policy limits! In terms of MLE, this means that the ENTIRE distribution is conditioned on having values above \\(d\\) or below \\(u\\) . Thus, the likelihood functions should be constructed using conditional PMFs/PDFs of every observation : \\[ \\begin{aligned} P_{\\theta} (X = x \\mid X \\gt d) &= \\frac{P_{\\theta}(X = x)}{P(X \\gt d)} \\\\ \\therefore L^{LT}(\\theta) &= \\prod P_{\\theta} (X = x_i \\mid X \\gt d) \\\\ \\\\ P_{\\theta} (X = x \\mid X \\lt u) &= \\frac{P_{\\theta}(X = x)}{P(X \\lt u)} \\\\ \\therefore L^{RT}(\\theta) &= \\prod P_{\\theta} (X = x_i \\mid X \\lt u) \\end{aligned} \\] Censored \u00b6 The second type of incomplete data is known as Censored Data . It means that exact values of certain observations are not known. Left Censored Right Censored Censored from below Censored from above Values at most \\(d\\) ; record as \\(d\\) Values at least \\(u\\) ; record as \\(u\\) The most common case would be Right Censored data, which occurs when the policy has a Policy Limit . This is because when the policyholder submits a claim, they will only submit up to the amount that the limit covers , even when the actual loss exceeds that amount. Thus, insurers will only receive loss amounts at the limit . Warning Similar to the warning from before, the notation for the threshold is \\(u\\) to denote policy limit. HOWEVER, the notation \\(d\\) here DOES NOT represent deductibles! \\(u\\) and \\(d\\) are still used in order to be consistent and ease understanding. In terms of MLE, this means that the likelihoods of censored observations are incorrect . Instead of the PMF/PDF, they should be represented by the probability that they are larger/smaller than the threshold . Thus, for censored observations at the threshold, the updated likelihoods must be used: \\[ \\begin{aligned} L^{LC}(\\theta) &= \\prod P_{\\theta} (X = x_{\\text{Not Censored}}) \\cdot \\prod P(X_{\\text{Censored}} \\le d) \\\\ L^{RC}(\\theta) &= \\prod P_{\\theta} (X = x_{\\text{Not Censored}}) \\cdot \\prod P(X_{\\text{Censored}} \\ge u) \\end{aligned} \\] Naturally, truncation and censoring can occur at the same time. The likelihood for the censored observation in this case is thus: \\[ P_{\\theta}(X \\gt d + u \\mid x \\gt d) = \\frac{P_{\\theta}(X \\gt d + u)}{P_{\\theta}(X \\gt d)} \\] Grouped \u00b6 The last type of incomplete data is known as Grouped Data . Similar to censoring, the exact values of ALL observations are not known. Instead, they are presented as the number of observations in a distinct interval . In terms of MLE, the likelihood of an observation within this range is simply the difference of the CDFs at the end and beginning of the interval: \\[ P_{\\theta}(a \\le X \\le b) = P_{\\theta}(X \\le b) - P_{\\theta}(X \\le a) \\] For a discrete distribution, this can be simplified to the sum of PMFs at each value: \\[ P_{\\theta}(a \\le X \\le b) = P_{\\theta}(X = a) + \\dots + P_{\\theta}(X = b) \\]","title":"Model Estimation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/8.%20Model%20Estimation/#model-estimation","text":"This section assumes some basic knowledge on Maximum Likelihood Estimation (MLE), which can be found under another set of notes covering a Review of Statistical Theory .","title":"Model Estimation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/8.%20Model%20Estimation/#complete-data","text":"If given a full dataset where the value of each observation is known, then it is a complete dataset. Distribution parameters can be estimated normally through MLE. For complete data only, the MOM shortcut can be used for the following distributions: Gamma & Exponential Normal & Lognormal Poisson Binomial & Bernoulli Negative Binomial & Geometric Notice that all of the (a, b, 0) distributions are listed. Thus, their zero-modified counterparts can also be found using the shortcut: \\[ \\begin{aligned} E \\left(X^M \\right) &= \\frac{1 - p^{m}_{0}}{1 - p_{0}} \\cdot E(X) \\\\ &= \\frac{1 - p^{m}_{0}}{1 - p_{0}} \\cdot \\bar{x} \\end{aligned} \\]","title":"Complete Data"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/8.%20Model%20Estimation/#incomplete-data","text":"In practice, insurers often work with incomplete data , where they do not have the exact values of observations or only have a partial dataset .","title":"Incomplete Data"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/8.%20Model%20Estimation/#truncated","text":"The first type of incomplete data is known as Truncated Data . It means that the dataset does not include observations past a specified threshold. Left Truncation Right Truncation Truncated from below Truncated from above No values below \\(d\\) No values above \\(u\\) The most common case would be Left Truncated data, which occurs when the policy has a Deductible . This is because when the loss amount does not need meet the deductible, policyholders would not bother to report it. Thus, the insurer only receives loss amounts that exceed the deductible . Warning The notation for the threshold is \\(d\\) to denote deductible. HOWEVER, the notation \\(u\\) here DOES NOT represent policy limits! In terms of MLE, this means that the ENTIRE distribution is conditioned on having values above \\(d\\) or below \\(u\\) . Thus, the likelihood functions should be constructed using conditional PMFs/PDFs of every observation : \\[ \\begin{aligned} P_{\\theta} (X = x \\mid X \\gt d) &= \\frac{P_{\\theta}(X = x)}{P(X \\gt d)} \\\\ \\therefore L^{LT}(\\theta) &= \\prod P_{\\theta} (X = x_i \\mid X \\gt d) \\\\ \\\\ P_{\\theta} (X = x \\mid X \\lt u) &= \\frac{P_{\\theta}(X = x)}{P(X \\lt u)} \\\\ \\therefore L^{RT}(\\theta) &= \\prod P_{\\theta} (X = x_i \\mid X \\lt u) \\end{aligned} \\]","title":"Truncated"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/8.%20Model%20Estimation/#censored","text":"The second type of incomplete data is known as Censored Data . It means that exact values of certain observations are not known. Left Censored Right Censored Censored from below Censored from above Values at most \\(d\\) ; record as \\(d\\) Values at least \\(u\\) ; record as \\(u\\) The most common case would be Right Censored data, which occurs when the policy has a Policy Limit . This is because when the policyholder submits a claim, they will only submit up to the amount that the limit covers , even when the actual loss exceeds that amount. Thus, insurers will only receive loss amounts at the limit . Warning Similar to the warning from before, the notation for the threshold is \\(u\\) to denote policy limit. HOWEVER, the notation \\(d\\) here DOES NOT represent deductibles! \\(u\\) and \\(d\\) are still used in order to be consistent and ease understanding. In terms of MLE, this means that the likelihoods of censored observations are incorrect . Instead of the PMF/PDF, they should be represented by the probability that they are larger/smaller than the threshold . Thus, for censored observations at the threshold, the updated likelihoods must be used: \\[ \\begin{aligned} L^{LC}(\\theta) &= \\prod P_{\\theta} (X = x_{\\text{Not Censored}}) \\cdot \\prod P(X_{\\text{Censored}} \\le d) \\\\ L^{RC}(\\theta) &= \\prod P_{\\theta} (X = x_{\\text{Not Censored}}) \\cdot \\prod P(X_{\\text{Censored}} \\ge u) \\end{aligned} \\] Naturally, truncation and censoring can occur at the same time. The likelihood for the censored observation in this case is thus: \\[ P_{\\theta}(X \\gt d + u \\mid x \\gt d) = \\frac{P_{\\theta}(X \\gt d + u)}{P_{\\theta}(X \\gt d)} \\]","title":"Censored"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/8.%20Model%20Estimation/#grouped","text":"The last type of incomplete data is known as Grouped Data . Similar to censoring, the exact values of ALL observations are not known. Instead, they are presented as the number of observations in a distinct interval . In terms of MLE, the likelihood of an observation within this range is simply the difference of the CDFs at the end and beginning of the interval: \\[ P_{\\theta}(a \\le X \\le b) = P_{\\theta}(X \\le b) - P_{\\theta}(X \\le a) \\] For a discrete distribution, this can be simplified to the sum of PMFs at each value: \\[ P_{\\theta}(a \\le X \\le b) = P_{\\theta}(X = a) + \\dots + P_{\\theta}(X = b) \\]","title":"Grouped"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/9.%20Credibility%20Theory/","text":"Credibility Theory \u00b6 Overview \u00b6 Credibility Theory is a formal method of determining how much weight we should place on different sources of data when estimating future outcomes. Consider two sources of data: Internal Data : More specific but smaller sample size External Data : Too general but larger sample size Credibility \\((Z)\\) is the weight placed on our own internally sourced data . The overall estimate is the weighted average of the two sources: \\[ \\text{Overall Estimate} = Z \\cdot \\text{Internal Estimate} + (1-Z) \\cdot \\text{External Estimate} \\] There are two unique properties of the credibility factor: As the number of internal observations increases, the credibility factor increases As the number of internal observations increases, the rate of increase decreases \\[ \\begin{aligned} \\frac{dZ}{dn} \\gt 0 \\\\ \\frac{d^2Z}{dn^2} \\lt 0 \\end{aligned} \\] Full Credibility \u00b6 The internal observations have Full Credibility when \\(Z=1\\) . This occurs when there are a large number of observations. The goal is to quantify what constitutes a \"large\" sample size. Generally speaking, a source is credible if its sample mean is equal to its theoretical mean . The smallest number of observations where this occurs is what constitutes a sufficiently large sample size. In practice, it is unreasonable to expect that the sample mean would be exactly equal to the theoretical mean. Thus, we instead expect that they will be close to one another, within some small margin of error , equivalent to some percentage of the theoretical mean : \\[ |\\bar{x} - E(X)| \\le k \\cdot E(X) \\] Additionally, since \\(\\bar{x}\\) is a random variable, it is unreasonable to expect that it will always be within this margin of error. Thus, we instead expect that they will be close together most of the time, with some minimum probability : \\[ P(|\\bar{x} - E(X)| \\le k \\cdot E(X)) \\ge p \\] Thus, full credibility is dictated by two parameters: \\(k\\) : Permitted fluctuation; usually small & close to 0 \\(p\\) : Minimum probability; usually large & close to 1 Given that there are a large number of observations, the distribution of the sample mean is approximately normal via the Central Limit Theorem : Using the normal distribution, we can set up an equation to solve for the minimum number of observations \\(n\\) : \\[ \\begin{aligned} \\bar{x} \\sim N(\\mu_{\\bar{x}}, \\sigma^2_{\\bar{x}}) \\\\ \\\\ P(|\\bar{x} - E(X)| \\le kE(X)) &= p \\\\ P(-kE(X) \\le \\bar{x} - E(X) \\le kE(X)) &= p \\\\ P \\left(\\frac{-kE(X)}{\\sqrt {Var(\\bar{x})}} \\le \\frac{\\bar{x} - E(\\bar{x})}{Var (\\bar{x})} \\le \\frac{kE(X)}{\\sqrt {Var(\\bar{x})}} \\right) &= p \\\\ P \\left(Z \\le \\frac{kE(X)}{\\sqrt {Var(\\bar{x})}} \\right) &= 1 - \\frac{1-p}{2} \\\\ \\Phi \\left(\\frac{kE(X)}{\\sqrt {Var(\\bar{x})}} \\right) &= \\frac{1+p}{2} \\end{aligned} \\] Note Since we are solving for the minimum number of observations, we consider the minimum case to have full credibility - where the probability is EXACTLY \\(p\\) . This means that each of the areas in the two tails are \\(\\frac{1-p}{2}\\) . We then solve the equation to obtain the following expression: \\[ \\begin{aligned} \\Phi \\left(\\frac{kE(X)}{\\sqrt {Var(\\bar{x})}} \\right) &= \\frac{1+p}{2} \\\\ \\frac{kE(X)}{\\sqrt {Var(\\bar{x})}} &= z_{(1+p)/2} \\\\ \\frac{kE(X)}{\\frac{\\sqrt {Var(X)}}{\\sqrt{n}}} &= z_{(1+p)/2} \\\\ \\sqrt{n} &= z_{(1+p)/2} \\cdot \\frac{1}{k} \\cdot \\frac{\\sqrt {Var(X)}}{E(X)} \\\\ \\sqrt{n} &= \\frac{z_{(1+p)/2}}{k} \\cdot CV_{X} \\\\ n &= \\left(\\frac{z_{(1+p)/2}}{k} \\cdot CV_{X} \\right)^2 \\end{aligned} \\] Tip Although it is not hard to derive this expression on the spot, it is a rather long and tedious proof. Thus, it is recommended to memorize this formula to save time. Number of Exposures \u00b6 In an actuarial context, credibility is often used when analyzing aggregate claims data ( \\(S\\) ). Each observation is called an Exposure , thus we are solving for the minimum Number of Exposures . Note Exposures are calculated based on a per unit, per year basis. For instance, two exposures could refer to either of the following scenarios: Two Insureds for one year One Insureds for two years If using the collective risk model, the mean and variance of the aggregate model can be decomposed into that of the underlying Frequency and Severity models : \\[ \\begin{aligned} n_{e} &= \\left(\\frac{z_{(1+p)/2}}{k} \\cdot \\frac{\\sqrt{Var(S)}}{E(S)} \\right)^2 \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\frac{Var (S)}{E(S)^2} \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\frac{Var(X) \\cdot E(N) + [E(X)]^2 \\cdot Var (N)}{E(N)^2 \\cdot E(X)^2} \\\\ \\end{aligned} \\] Number of Claims \u00b6 Alternatively, the number of exposures can also be expressed in terms of number of claims . If each exposure is a policyholder and the average number of claims per policyholder is given, then the number of claims is simply: \\[ \\begin{aligned} n_c &= n_e \\cdot E(N) \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\frac{Var(X) \\cdot E(N) + [E(X)]^2 \\cdot Var (N)}{E(N)^2 \\cdot E(X)^2} \\cdot E(N) \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\frac{Var(X) \\cdot E(N) + [E(X)]^2 \\cdot Var (N)}{E(N) \\cdot E(X)^2} \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\left(\\frac{Var(X)}{E(X)^2} + \\frac{Var (N)}{E(N)} \\right) \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\left( \\left(\\frac{SD(X)}{E(X)} \\right)^2 + \\frac{Var (N)}{E(N)} \\right) \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\left( CV_{X}^2 + \\frac{Var (N)}{E(N)} \\right) \\end{aligned} \\] Notice that the number of claims nicely cleanly seperates out the Severity and Frequency components. Thus, this allows credibility to be determined based on ONLY frequency or severity data by setting the other component to 0 : \\[ \\begin{aligned} n^{\\text{Frequency}}_c &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\left(0 + \\frac{Var (N)}{E(N)} \\right) \\\\ n^{\\text{Severity}}_c &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\left(CV_{X}^2 + 0 \\right) \\end{aligned} \\] We can then express them in terms of the number of exposures: \\[ \\begin{aligned} n_c &= n_e \\cdot E(N) \\\\ n_e &= \\frac{n_c}{E(N)} \\end{aligned} \\] Tip Given that the number of claims method can be used for Aggregate, Frequency & Severity data, it is much better to remember to always start with the number of claims and then convert to the number of exposures if needed. Partial Credibility \u00b6 If there are insufficient observations , then the data is said to have Partial Credibility \\((Z \\lt 1)\\) . The appropriate factor to use can be determined via the square root rule , where \\(n'\\) denotes the actual number of exposures or claims in the data: \\[ Z = \\sqrt{\\frac{n'_{e}}{n_e}} = \\sqrt{\\frac{n'_{c}}{n_c}} \\]","title":"Credibility Theory"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/9.%20Credibility%20Theory/#credibility-theory","text":"","title":"Credibility Theory"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/9.%20Credibility%20Theory/#overview","text":"Credibility Theory is a formal method of determining how much weight we should place on different sources of data when estimating future outcomes. Consider two sources of data: Internal Data : More specific but smaller sample size External Data : Too general but larger sample size Credibility \\((Z)\\) is the weight placed on our own internally sourced data . The overall estimate is the weighted average of the two sources: \\[ \\text{Overall Estimate} = Z \\cdot \\text{Internal Estimate} + (1-Z) \\cdot \\text{External Estimate} \\] There are two unique properties of the credibility factor: As the number of internal observations increases, the credibility factor increases As the number of internal observations increases, the rate of increase decreases \\[ \\begin{aligned} \\frac{dZ}{dn} \\gt 0 \\\\ \\frac{d^2Z}{dn^2} \\lt 0 \\end{aligned} \\]","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/9.%20Credibility%20Theory/#full-credibility","text":"The internal observations have Full Credibility when \\(Z=1\\) . This occurs when there are a large number of observations. The goal is to quantify what constitutes a \"large\" sample size. Generally speaking, a source is credible if its sample mean is equal to its theoretical mean . The smallest number of observations where this occurs is what constitutes a sufficiently large sample size. In practice, it is unreasonable to expect that the sample mean would be exactly equal to the theoretical mean. Thus, we instead expect that they will be close to one another, within some small margin of error , equivalent to some percentage of the theoretical mean : \\[ |\\bar{x} - E(X)| \\le k \\cdot E(X) \\] Additionally, since \\(\\bar{x}\\) is a random variable, it is unreasonable to expect that it will always be within this margin of error. Thus, we instead expect that they will be close together most of the time, with some minimum probability : \\[ P(|\\bar{x} - E(X)| \\le k \\cdot E(X)) \\ge p \\] Thus, full credibility is dictated by two parameters: \\(k\\) : Permitted fluctuation; usually small & close to 0 \\(p\\) : Minimum probability; usually large & close to 1 Given that there are a large number of observations, the distribution of the sample mean is approximately normal via the Central Limit Theorem : Using the normal distribution, we can set up an equation to solve for the minimum number of observations \\(n\\) : \\[ \\begin{aligned} \\bar{x} \\sim N(\\mu_{\\bar{x}}, \\sigma^2_{\\bar{x}}) \\\\ \\\\ P(|\\bar{x} - E(X)| \\le kE(X)) &= p \\\\ P(-kE(X) \\le \\bar{x} - E(X) \\le kE(X)) &= p \\\\ P \\left(\\frac{-kE(X)}{\\sqrt {Var(\\bar{x})}} \\le \\frac{\\bar{x} - E(\\bar{x})}{Var (\\bar{x})} \\le \\frac{kE(X)}{\\sqrt {Var(\\bar{x})}} \\right) &= p \\\\ P \\left(Z \\le \\frac{kE(X)}{\\sqrt {Var(\\bar{x})}} \\right) &= 1 - \\frac{1-p}{2} \\\\ \\Phi \\left(\\frac{kE(X)}{\\sqrt {Var(\\bar{x})}} \\right) &= \\frac{1+p}{2} \\end{aligned} \\] Note Since we are solving for the minimum number of observations, we consider the minimum case to have full credibility - where the probability is EXACTLY \\(p\\) . This means that each of the areas in the two tails are \\(\\frac{1-p}{2}\\) . We then solve the equation to obtain the following expression: \\[ \\begin{aligned} \\Phi \\left(\\frac{kE(X)}{\\sqrt {Var(\\bar{x})}} \\right) &= \\frac{1+p}{2} \\\\ \\frac{kE(X)}{\\sqrt {Var(\\bar{x})}} &= z_{(1+p)/2} \\\\ \\frac{kE(X)}{\\frac{\\sqrt {Var(X)}}{\\sqrt{n}}} &= z_{(1+p)/2} \\\\ \\sqrt{n} &= z_{(1+p)/2} \\cdot \\frac{1}{k} \\cdot \\frac{\\sqrt {Var(X)}}{E(X)} \\\\ \\sqrt{n} &= \\frac{z_{(1+p)/2}}{k} \\cdot CV_{X} \\\\ n &= \\left(\\frac{z_{(1+p)/2}}{k} \\cdot CV_{X} \\right)^2 \\end{aligned} \\] Tip Although it is not hard to derive this expression on the spot, it is a rather long and tedious proof. Thus, it is recommended to memorize this formula to save time.","title":"Full Credibility"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/9.%20Credibility%20Theory/#number-of-exposures","text":"In an actuarial context, credibility is often used when analyzing aggregate claims data ( \\(S\\) ). Each observation is called an Exposure , thus we are solving for the minimum Number of Exposures . Note Exposures are calculated based on a per unit, per year basis. For instance, two exposures could refer to either of the following scenarios: Two Insureds for one year One Insureds for two years If using the collective risk model, the mean and variance of the aggregate model can be decomposed into that of the underlying Frequency and Severity models : \\[ \\begin{aligned} n_{e} &= \\left(\\frac{z_{(1+p)/2}}{k} \\cdot \\frac{\\sqrt{Var(S)}}{E(S)} \\right)^2 \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\frac{Var (S)}{E(S)^2} \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\frac{Var(X) \\cdot E(N) + [E(X)]^2 \\cdot Var (N)}{E(N)^2 \\cdot E(X)^2} \\\\ \\end{aligned} \\]","title":"Number of Exposures"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/9.%20Credibility%20Theory/#number-of-claims","text":"Alternatively, the number of exposures can also be expressed in terms of number of claims . If each exposure is a policyholder and the average number of claims per policyholder is given, then the number of claims is simply: \\[ \\begin{aligned} n_c &= n_e \\cdot E(N) \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\frac{Var(X) \\cdot E(N) + [E(X)]^2 \\cdot Var (N)}{E(N)^2 \\cdot E(X)^2} \\cdot E(N) \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\frac{Var(X) \\cdot E(N) + [E(X)]^2 \\cdot Var (N)}{E(N) \\cdot E(X)^2} \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\left(\\frac{Var(X)}{E(X)^2} + \\frac{Var (N)}{E(N)} \\right) \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\left( \\left(\\frac{SD(X)}{E(X)} \\right)^2 + \\frac{Var (N)}{E(N)} \\right) \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\left( CV_{X}^2 + \\frac{Var (N)}{E(N)} \\right) \\end{aligned} \\] Notice that the number of claims nicely cleanly seperates out the Severity and Frequency components. Thus, this allows credibility to be determined based on ONLY frequency or severity data by setting the other component to 0 : \\[ \\begin{aligned} n^{\\text{Frequency}}_c &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\left(0 + \\frac{Var (N)}{E(N)} \\right) \\\\ n^{\\text{Severity}}_c &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\left(CV_{X}^2 + 0 \\right) \\end{aligned} \\] We can then express them in terms of the number of exposures: \\[ \\begin{aligned} n_c &= n_e \\cdot E(N) \\\\ n_e &= \\frac{n_c}{E(N)} \\end{aligned} \\] Tip Given that the number of claims method can be used for Aggregate, Frequency & Severity data, it is much better to remember to always start with the number of claims and then convert to the number of exposures if needed.","title":"Number of Claims"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAMS/9.%20Credibility%20Theory/#partial-credibility","text":"If there are insufficient observations , then the data is said to have Partial Credibility \\((Z \\lt 1)\\) . The appropriate factor to use can be determined via the square root rule , where \\(n'\\) denotes the actual number of exposures or claims in the data: \\[ Z = \\sqrt{\\frac{n'_{e}}{n_e}} = \\sqrt{\\frac{n'_{c}}{n_c}} \\]","title":"Partial Credibility"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/","text":"Linear Regression \u00b6 Population Regression Model \u00b6 Regression is a statistical model that relates a Dependent Variable (DV) to one or more Independent Variables (IV). The dependent variable is regressed on to the independent variable. They are fundamentally a function of the independent variables and several Regression Parameters , \\(\\beta\\) . The functional form of the regression is based on the relationship between the variables. The goal is to use a model that best captures the relationship between the variables. \\[ Y = f(X, \\beta) \\] Independent Variable(s) Dependent Variables Variable used to make predictions Variable being predicted Free to change the value Depends on the value of indepenent variable Deterministic Random Variable Denoted as \\(X\\) Denoted as \\(Y\\) To be precise, for every set of IVs, the DV has a Conditional Distribution dependent on the given IV. For instance, the the \\(Y\\) could take any possible value (Marginal Distribution), but given these set of \\(X\\) , the possible values can be narrowed down to a certain range (conditional distribution). \\[ \\displaylines{ Y \\sim Distribution \\\\ Y|X \\sim Conditional~Distribution} \\] Thus, the output of the regression model is actually the Expected Value of the conditional distribution, \\(E(Y|X)\\) , for every possible \\(X\\) . \\[ E(Y|X) = f(X, \\beta) \\] The actual observations are unlikely to be exactly equal to its expectation, thus there is a difference between an observation and the corresponding regression output. It known as the Random Error Term which accounts for all other factors that affect the DV that are not captured in the regression. This means that the relationship between the \\(Y\\) and \\(X\\) is only approximate , as the true relationship is probably different due to the possibility of unaccounted variables. Note that the sign of the errors are significant - positive implies the actual value lies above the regression output while negative implies it lies below. \\[ \\varepsilon_i = y_i - f(x_i, \\beta) \\] This means that \\(Y\\) (not its expectation!) can be expressed as a sum of the regression model and the error terms: \\[ y_i = f(x_i,\\beta) + \\varepsilon_i \\] The Regression is known as the Systematic component as it is shared among all observations The Error is known as the Non-Systematic component as it is unique to each observation Sample Regression Model \u00b6 In practice, the population is unobservable hence it is impossible to construct the population regression model. Instead, a regression model is constructed from a sample instead, which aims to estimate the population model. \\[ \\hat{y} = f(X,\\hat{\\beta}) \\] Similarly, the output of this model can be compared to the actual observations. However, the resulting difference is known as the Residual of the model, which like all the other components, is an estimate for the Error term. \\[ \\hat{\\varepsilon_i} = y_i - \\hat{y_i} \\] There are several different methods to estimate the regression parameters, but they usually involve minimizing the residuals of the model, such that the resulting model best fits the given sample, which is why it is also known as the Fitted Regression Model . Hypothesis Testing \u00b6 Once a regression model has been fit, the next step is to determine if the relationship found in the sample is indicative of a relationship in the population. This can be determined through the following two-sided hypothesis test : Null Hypothesis: \\(\\beta_1 = 0\\) Alternative Hypothesis: \\(\\beta_1 \\ne 0\\) Under the null, the regression parameters are assumed to be 0, implying that there is no relationship between \\(Y\\) and \\(X\\) . The test should reject the null , proving that there IS a relationship between the DV and IVs. Prediction \u00b6 Once the best model has been determined, it can be used to make Predictions about future unobserved values. Let these future values be denoted by the subscript \\(*\\) . These unobserved DVs come from the population, thus can be expressed as a function of the population model: \\[ y_* = f(x_*, \\beta) + \\varepsilon_* \\] The corresponding values from the sample regression model is an estimate for this unobserved value: \\[ \\hat{y}_* = f(x_*, \\hat{\\beta}) \\] Like before, the predicted value is unlikely to be exactly equal to the actual value. Thus, the difference between both values can be measured as the Prediction Error : \\[ y_* - \\hat{y_*} = \\varepsilon_* + [f(x_*, \\beta) - f(x_*, \\hat{\\beta})] \\] The prediction error is thus made up of two components : Inherent error present in the DV ( \\(\\varepsilon_*\\) ) Error in estimating the population model ( \\(f(x_*, \\beta) - f(x_*, \\hat{\\beta})\\) ) Based on the distribution of the prediction error, a Prediction Interval at a given confidence level can be calculated to accompany the regression estimate, which is essentially a confidence interval for the predicted value . Note that the prediction intervals will always be wider than confidence intervals . This is because CIs only takes into the account the error in estimating the population model/parameters while PIs take into account the inherent error of the DV as well.","title":"Regression Overview"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#linear-regression","text":"","title":"Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#population-regression-model","text":"Regression is a statistical model that relates a Dependent Variable (DV) to one or more Independent Variables (IV). The dependent variable is regressed on to the independent variable. They are fundamentally a function of the independent variables and several Regression Parameters , \\(\\beta\\) . The functional form of the regression is based on the relationship between the variables. The goal is to use a model that best captures the relationship between the variables. \\[ Y = f(X, \\beta) \\] Independent Variable(s) Dependent Variables Variable used to make predictions Variable being predicted Free to change the value Depends on the value of indepenent variable Deterministic Random Variable Denoted as \\(X\\) Denoted as \\(Y\\) To be precise, for every set of IVs, the DV has a Conditional Distribution dependent on the given IV. For instance, the the \\(Y\\) could take any possible value (Marginal Distribution), but given these set of \\(X\\) , the possible values can be narrowed down to a certain range (conditional distribution). \\[ \\displaylines{ Y \\sim Distribution \\\\ Y|X \\sim Conditional~Distribution} \\] Thus, the output of the regression model is actually the Expected Value of the conditional distribution, \\(E(Y|X)\\) , for every possible \\(X\\) . \\[ E(Y|X) = f(X, \\beta) \\] The actual observations are unlikely to be exactly equal to its expectation, thus there is a difference between an observation and the corresponding regression output. It known as the Random Error Term which accounts for all other factors that affect the DV that are not captured in the regression. This means that the relationship between the \\(Y\\) and \\(X\\) is only approximate , as the true relationship is probably different due to the possibility of unaccounted variables. Note that the sign of the errors are significant - positive implies the actual value lies above the regression output while negative implies it lies below. \\[ \\varepsilon_i = y_i - f(x_i, \\beta) \\] This means that \\(Y\\) (not its expectation!) can be expressed as a sum of the regression model and the error terms: \\[ y_i = f(x_i,\\beta) + \\varepsilon_i \\] The Regression is known as the Systematic component as it is shared among all observations The Error is known as the Non-Systematic component as it is unique to each observation","title":"Population Regression Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#sample-regression-model","text":"In practice, the population is unobservable hence it is impossible to construct the population regression model. Instead, a regression model is constructed from a sample instead, which aims to estimate the population model. \\[ \\hat{y} = f(X,\\hat{\\beta}) \\] Similarly, the output of this model can be compared to the actual observations. However, the resulting difference is known as the Residual of the model, which like all the other components, is an estimate for the Error term. \\[ \\hat{\\varepsilon_i} = y_i - \\hat{y_i} \\] There are several different methods to estimate the regression parameters, but they usually involve minimizing the residuals of the model, such that the resulting model best fits the given sample, which is why it is also known as the Fitted Regression Model .","title":"Sample Regression Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#hypothesis-testing","text":"Once a regression model has been fit, the next step is to determine if the relationship found in the sample is indicative of a relationship in the population. This can be determined through the following two-sided hypothesis test : Null Hypothesis: \\(\\beta_1 = 0\\) Alternative Hypothesis: \\(\\beta_1 \\ne 0\\) Under the null, the regression parameters are assumed to be 0, implying that there is no relationship between \\(Y\\) and \\(X\\) . The test should reject the null , proving that there IS a relationship between the DV and IVs.","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#prediction","text":"Once the best model has been determined, it can be used to make Predictions about future unobserved values. Let these future values be denoted by the subscript \\(*\\) . These unobserved DVs come from the population, thus can be expressed as a function of the population model: \\[ y_* = f(x_*, \\beta) + \\varepsilon_* \\] The corresponding values from the sample regression model is an estimate for this unobserved value: \\[ \\hat{y}_* = f(x_*, \\hat{\\beta}) \\] Like before, the predicted value is unlikely to be exactly equal to the actual value. Thus, the difference between both values can be measured as the Prediction Error : \\[ y_* - \\hat{y_*} = \\varepsilon_* + [f(x_*, \\beta) - f(x_*, \\hat{\\beta})] \\] The prediction error is thus made up of two components : Inherent error present in the DV ( \\(\\varepsilon_*\\) ) Error in estimating the population model ( \\(f(x_*, \\beta) - f(x_*, \\hat{\\beta})\\) ) Based on the distribution of the prediction error, a Prediction Interval at a given confidence level can be calculated to accompany the regression estimate, which is essentially a confidence interval for the predicted value . Note that the prediction intervals will always be wider than confidence intervals . This is because CIs only takes into the account the error in estimating the population model/parameters while PIs take into account the inherent error of the DV as well.","title":"Prediction"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/10.%20Clustering/","text":"","title":"10. Clustering"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/","text":"Simple Linear Regression \u00b6 Simple Linear Regression (SLR) assumes a Linear Relationship between a Numeric DV and single continuous quantitative IV. The model is considered to be simple because it only contains a single independent variable. \\[ E(Y|X) = \\beta_0 + \\beta_1 X \\] \\(\\beta_0\\) \\(\\beta_1\\) Expected value of \\(Y\\) when \\(X = 0\\) Change in the expected value of \\(Y\\) given a one unit increase in \\(X\\) Intercept Parameter Slope Parameter Each observation can also be expressed as sum of the regression and its error term: \\[ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\] Ordinary Least Squares \u00b6 SLR parameters are estimated using the Ordinary Least Squares method, which minimizes the Sum of Squared Residuals of the fitted model. It is commonly referred to as the Residual Sum Squared (RSS). \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 \\] The minimization is solved through calculus by setting the partial derivatives of the RSS to 0: For the intercept parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_0} RSS &= 0 \\\\ -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum y_i - \\sum \\hat{\\beta}_0 - \\sum \\hat{\\beta}_1 x &= 0 \\\\ n\\bar{y} -n\\hat{\\beta}_0 - n\\hat{\\beta}_1 \\bar{x} &= 0 \\\\ \\end{aligned} \\] \\[\\therefore \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\] For the slope parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_1} RSS &= 0 \\\\ -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum (y_i x_i) - \\hat{\\beta}_0 \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} + n\\hat{\\beta}_1 \\bar{x}^2 - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} &= \\hat{\\beta}_1 \\sum (x^2_i) - n\\bar{x}^2 \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 = \\frac{\\sum (x_i y_i) - n\\bar{x}\\bar{y}}{\\sum (x_i^2) - n\\bar{x}^2} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} = r * \\frac{s_y}{s_x} \\] This results in the following fitted regression model, which can be graphically expressed as a Regression Line : \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] Note that it should \\(\\hat{\\varepsilon}\\) in the above image, not \\(e_i\\) . OLS Properties \u00b6 By re-arranging the formula for \\(\\hat{\\beta}_0\\) , we can show that \\((\\bar{x}, \\bar{y})\\) always lies on the fitted regression model: \\[ \\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x} \\] Additionally, since the parameters are estimated through minimization, the resulting model must always fulfil the two first order conditions . The model thus has \\(n-2\\) degrees of freedom to reflect these \"constraints\". \\(\\beta_0\\) FOC \\(\\beta_1\\) FOC \\(\\frac{\\partial}{\\partial \\beta_0} = -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\frac{\\partial}{\\partial \\beta_1} = -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\sum \\hat{\\varepsilon_i} = 0\\) \\(\\sum x_i \\hat{\\varepsilon_i} = 0\\) Residuals are negatively correlated Residuals and Independendent variables are uncorrelated Using the above results, we can also show the following that the mean of the regression outputs is equal to the mean of the population: \\[ \\begin{aligned} \\hat{\\varepsilon_i} &= y_i - \\hat{y_i} \\\\ \\sum \\hat{\\varepsilon_i} &= \\sum y_i - \\hat{y_i} \\\\ 0 &= n\\bar{y} - n\\bar{\\hat{y}} \\\\ \\bar{\\hat{y}} &= \\bar{y} \\\\ \\end{aligned} \\] Goodness of Fit \u00b6 Ideally, the regression should fit the sample closely, having as small as residuals as possible . The size of all the residuals in the model can be summarized through the RSS. The lower the RSS, the better the fit of the model. Recall that the residuals naturally sum to 0 under OLS - the residuals are thus squared to remove the sign so that they can be summed together. \\[ RSS = \\sum (y_i - \\hat{y})^2 \\] However, the SSR on its own is hard to intepret as there is no indication of how low or high it actually is. Thus, the Total Sum of Squares (TSS) can be used as a benchmark for the RSS as it is at least equal to or higher than the RSS. The TSS represents the RSS for a Null Regression - a model with containing only the intercept parameter . The output of this regression is always the sample mean \\(\\bar{y}\\) , which is used for the computation of its residuals. It represents the worst possible model which thus has the highest possible RSS . The lower the RSS compared to the TSS, the better the fit of the model. \\[ TSS = \\sum (y_i - \\bar{y}) \\] Null Model \u00b6 Consider a regression with only the intercept; \\(\\beta_1 = 0\\) . It is known as the Null Model as there are no independent variables used. \\[ y = \\beta_0 \\] We can estimate \\(\\hat{\\beta_0}\\) using OLS, which results in the following result: \\[ \\begin{aligned} -2 \\sum (y_i - \\hat{\\beta_0}) &= 0 \\\\ n \\bar{y} - n \\hat{\\beta_0} &= 0 \\\\ \\hat{\\beta_0} &= \\bar{y} \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{y} = \\bar{y} \\] Sum of Squares \u00b6 The TSS can be further decomposed into two more parts for analysis: \\[ \\begin{aligned} TSS &= \\sum (y_i - \\bar{y})^2 \\\\ TSS &= \\sum[(y_i - \\hat{y}) + (\\hat{y}-\\bar{y})]^2 \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 2 \\sum((y_i - \\hat{y})(\\hat{y}-\\bar{y})) \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 0 \\\\ TSS &= RSS + RegSS \\end{aligned} \\] Residual SS (RSS) Regression SS (RegSS) \\(\\sum(\\hat{y}-\\bar{y})^2\\) \\(\\sum(y_i - \\hat{y})^2\\) Variation of the observed values about the regression Variation of the regression output about the sample mean Variation explained by the regression Variation unexplained by the regression Note that it can also be expressed in terms of the Slope Parameter : \\[ \\begin{align} RegSS &= \\sum(\\hat{y}-\\bar{y})^2 \\\\ &= \\sum(\\hat{\\beta}_0 + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum(\\bar{y} - \\beta_1 \\bar{x} + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum[\\hat{\\beta}_1 (x_i - \\bar{x})]^2 \\\\ &= \\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2 \\\\ \\end{align} \\] The Coefficient of Determination \\(R^2\\) can also be used to demonstrate goodness of fit. It measures the proportion of variation explained by the regression model: \\[ R^2 = \\frac{RegSS}{TSS} = 1 - \\frac{RSS}{TSS} \\] Building off the above expression, it can also be expressed in terms of the Sample Correlation : \\[ \\begin{align} R^2 &= \\frac{\\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\\right)^2 \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^4} \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x}) \\sum (y_i - \\bar{y})}\\right)^2 \\\\ &= r_{y,x}^2 \\end{align} \\] Degrees of Freedom \u00b6 The TSS is based on the naive model with only the intercept parameter, thus, it is subject to the single contraint of all residuals summing to 0. The TSS thus has \\(n-1\\) degrees of freedom . The RSS is based on the SLR with both the intercept and slope parameter, thus it is subject to an additional constraint of the sumproduct of all residuals and independent variables being 0. The RSS thus has \\(n-2\\) degrees of freedom. The sum of the RSS and RegSS is equal to the TSS, thus the sum of their degrees of freedom must also be equal to that of the TSS. By working backwards, the RegSS thus has only \\(1\\) degree of freedom . Mean Squared \u00b6 The division of any Sum of Square (TSS, RSS, RegSS) by its Degrees of Freedom is known as the Mean Squared (MS), which is a measure of its average variance . The MS of the TSS is the Unbiased Estimator for Population Variance , which is why this process is known as the Analysis of Variance , as it decomposes the variance of \\(Y\\) into its constituent components: \\[ s = \\frac{TSS}{n-1} \\] The MS of the RSS is known the Mean Squared Residuals , often also referred to as the Mean Squared Error , as it is an estimate for the population variance of the error \\(\\sigma^2\\) : \\[ MS_{\\text{Residuals}} = \\frac{RSS}{n-2} \\] The MS of the RegSS is known as the Mean Squared Regression , which represents the proportion of variance explained per \\(X\\) used. \\[ MS_{\\text{Regression}} = \\frac{RegSS}{1} \\] F Statistic \u00b6 The ANOVA parameters can be used to conduct a hypothesis test to determine if there is in fact a relationship between \\(X\\) and \\(Y\\) : \\(H_0\\) : \\(\\beta_1 = 0\\) \\(H_1\\) : \\(\\beta_1 \\ne 0\\) Under the null hypothesis, there should be no difference between the assumed model and a null model as both only contain the intercept parameter, thus \\(TSS = RSS\\) , where \\(RegSS = 0\\) . Thus, the F-statistic is testing for the equality of variance between the TSS and RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_1 \\ne 0\\) . The F-statistic can be constructed using the sum of squares: \\[ \\begin{aligned} F &= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &= \\frac{RegSS/1}{RSS/(n-2)} \\\\ &= \\frac{(TSS - RSS)/1}{RSS/(n-2)} \\\\ &= (n-2) * \\frac{R^2}{1-R^2}, \\text{divide both by TSS} \\end{aligned} \\] Similar to the variance test, it can be shown that under the null, this test-statistic follows an F distribution with \\(1\\) and \\(n-2\\) degrees of freedom: \\[ \\begin{aligned} T &= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &= \\frac{\\sigma^2_{RegSS} \\frac{MS_{Reg}}{\\sigma^2_{RegSS}}}{\\sigma^2_{RSS} \\frac{MS_{RSS}}{\\sigma^2_{RSS}}} \\\\ &= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\frac{1 * MS_{Reg}}{\\sigma^2_{RegSS}} * \\frac{1}{1}}{\\frac{(n-2) * MS_{RegSS}}{\\sigma^2_{RegSS}}* \\frac{1}{n-2}} \\\\ &= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\chi_1}{\\chi_{n-2}} * (n-2) \\\\ &= 1 * F_{1, n-2} * (n-2) \\\\ &= F_{1, n-2} * (n-2) \\end{aligned} \\] \\[ \\therefore F \\sim F_{1, n-2} \\] ANOVA Table \u00b6 All the above information is then summarized in a table for convenience, known as the ANOVA Table : Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(1\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-2\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) - Statistical Inference \u00b6 Sampling Distributions \u00b6 Since the errors are assumed to be normally distributed, then \\(Y\\) is assumed to be normally distributed as well. Since \\(Y\\) is a linear combination of the regression parameters, then the parameters (& their estimates) are normally distributed as well. Both estimates can be expressed in another form that makes it more convenient to find their expectation & variances. \\[ \\begin{aligned} \\hat{\\beta}_1 &= \\frac{\\sum [(x_i - \\bar{x})(y_i - \\bar{y})]}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})y_i}{\\sum (x^2_i - \\bar{x})} - \\frac{\\bar{y} \\sum (x_i - \\bar{x})}{\\sum (x^2_i - \\bar{x})} \\\\ &= \\sum \\frac{(x_i - \\bar{x})}{(x^2_i - \\bar{x})}* y_i - 0 \\\\ &= \\sum w_i * y_i \\\\ \\\\ \\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\ &= \\frac{1}{n} \\sum y_i - \\bar{x} \\sum w_i * y_i \\\\ &= \\sum y_i (\\frac{1}{n} - \\bar{x}w_i) \\end{aligned} \\] \\(w_i\\) is a sort of \"weight\" parameter of the sum of squares. It has three interesting properties that makes it useful: \\[ \\begin{aligned} \\sum w_i &= \\frac{\\sum (x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{n\\bar{x}-n\\bar{x}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{0}{\\sum (x^2_i - \\bar{x})} \\\\ &= 0 \\\\ \\\\ \\sum w_i x_i &= \\frac{\\sum x_i(x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x^2_i - \\bar{x} \\sum x_i)}{\\sum x^2_i - 2\\bar{x}\\sum x_i + \\sum \\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - \\bar{x}(n \\bar{x})}{\\sum x^2_i - 2\\bar{x}(n\\bar{x}) + n\\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - 2n\\bar{x}^2 + n\\bar{x}^2} \\\\ &=\\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - n\\bar{x}^2} \\\\ &= 1 \\\\ \\\\ \\sum w_i^2 &= \\frac{\\sum (x_i - \\bar{x})^2}{\\sum (x_i - \\bar{x})^4} \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i) &= n(\\frac{1}{n}) - \\bar{x} \\sum w_i \\\\ &= 1 - 0 \\\\ &= 1 \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i &= \\frac{1}{n} \\sum x_i - \\bar{x} \\sum w_i x_i \\\\ &= \\frac{1}{n} (n\\bar{x}) - \\bar{x} (1) \\\\ &= \\bar{x} - \\bar{x} \\\\ &= 0 \\end{aligned} \\] Using this, the Expectation & Variance can be determined: \\[ \\begin{aligned} E(\\hat{\\beta}_1) &= \\sum w_i E(y_i) \\\\ &= \\sum w_i E(\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum w_i + \\beta_1 \\sum w_i x_i \\\\ &= \\beta_0 (0) + \\beta_1 (1) \\\\ &= \\beta_1\\\\ \\\\ Var(\\hat{\\beta}_1) &= Var(\\sum w_i y_i) \\\\ &= \\sum w_i^2 Var (y_i) \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2} \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 \\sim N(\\beta_1, \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2}) \\] \\[ \\begin{aligned} E(\\hat{\\beta}_0) &= \\sum (\\frac{1}{n} - \\bar{x}w_i) E(y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i) (\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum (\\frac{1}{n} - \\bar{x}w_i) + \\beta_1 \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i \\\\ &= \\beta_0 (1) + \\beta_1 (0) \\\\ &= \\beta_0 \\\\ \\\\ Var(\\hat{\\beta}_0) &= Var(\\sum (\\frac{1}{n} - \\bar{x}w_i)y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i)^2 Var (y_i) \\\\ &= \\sigma^2 \\sum (\\frac{1}{n^2} -\\frac{2\\bar{x}w_i}{n} + \\bar{x}^2 w_i^2) \\\\ &= \\sigma^2 (\\sum \\frac{1}{n^2} - \\frac{2\\bar{x}}{n} \\sum w_i + \\bar{x}^2 \\sum w_i^2) \\\\ &= \\sigma^2 [n(\\frac{1}{n^2}) - \\frac{2\\bar{x}}{n} (0) + \\bar{x}^2 (\\frac{1}{\\sum (x_i - \\bar{x})^2})] \\\\ &= \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2}) \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_0 \\sim N(\\beta_0, \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2})) \\] Hypothesis Testing \u00b6 Since the regression parameters are normally distributed, a z-statistic can also be used to conduct the tests. However, since the population variance is not known, a t-statistic is used instead: \\[ \\begin{aligned} t &= \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\end{aligned} \\] Since the population variance is estimated by the MSE which has \\(n-2\\) degrees of freedom, the corresponding chi-squared and hence t-distribution has \\(n-2\\) degrees of freedom as well. \\[ \\begin{aligned} \\hat{Var}(\\hat{\\beta_1}) &= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} * \\frac{\\frac{1}{n-1}}{\\frac{1}{n-1}} \\\\ &= \\frac{MS_{RSS}}{(n-1) s^2} \\end{aligned} \\] \\[ t \\sim t_{n-2} \\] Since the square of the t-statistic is the F-statistic, both are equivalent ways of doing so and will always lead to the same conclusions. \\[ t^2 \\sim F_{1, n-2} \\] Confidence Intervals \u00b6 Since the distribution of t-statistic is known, it can be used to determine the confidence interval of the estimate: \\[ P\\left(\\text{Margin of Error} < \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} < \\text{Margin of Error}\\right) = 1 - \\alpha \\] \\[ \\text{Confidence Interval} = \\hat{\\beta}_1 \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_1}} \\] Prediction Intervals \u00b6 Consider the Prediction Error of the SLR model: \\[ y_* - \\hat{y_*} = \\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\] Since both \\(y_*\\) and \\(\\hat{y_*}\\) are normally distributed, the prediction errors are normally distributed as well: \\[ \\begin{aligned} E(y_* - \\hat{y_*}) &= E[\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)]] \\\\ &= 0 + \\beta_0 + \\beta_1 E(x_*) - \\beta_0 - \\beta_1 E(x_*) \\\\ &= 0 \\\\ \\\\ Var(y_* - \\hat{y_*}) &= Var(\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= Var(\\varepsilon_*) + Var[(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= ... \\\\ &= \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}] \\end{aligned} \\] \\[ \\therefore y_* - \\hat{y_*} \\sim N\\left(0, \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}]\\right) \\] Similar to before, since the population variance is unknown, it can be approximated using the MSE. Thus, a t-statistic constructed from this sample will have a t-distribution , allowing the following prediction interval to be calculated: \\[ \\text{Prediction Interval} = \\hat{y}_* \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\] Notice that the standard error of the prediction interval increases as \\(x_*\\) moves further away \\(\\bar{x}\\) , indicating that the predictions become less accurate for those values.","title":"Simple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#simple-linear-regression","text":"Simple Linear Regression (SLR) assumes a Linear Relationship between a Numeric DV and single continuous quantitative IV. The model is considered to be simple because it only contains a single independent variable. \\[ E(Y|X) = \\beta_0 + \\beta_1 X \\] \\(\\beta_0\\) \\(\\beta_1\\) Expected value of \\(Y\\) when \\(X = 0\\) Change in the expected value of \\(Y\\) given a one unit increase in \\(X\\) Intercept Parameter Slope Parameter Each observation can also be expressed as sum of the regression and its error term: \\[ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\]","title":"Simple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#ordinary-least-squares","text":"SLR parameters are estimated using the Ordinary Least Squares method, which minimizes the Sum of Squared Residuals of the fitted model. It is commonly referred to as the Residual Sum Squared (RSS). \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 \\] The minimization is solved through calculus by setting the partial derivatives of the RSS to 0: For the intercept parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_0} RSS &= 0 \\\\ -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum y_i - \\sum \\hat{\\beta}_0 - \\sum \\hat{\\beta}_1 x &= 0 \\\\ n\\bar{y} -n\\hat{\\beta}_0 - n\\hat{\\beta}_1 \\bar{x} &= 0 \\\\ \\end{aligned} \\] \\[\\therefore \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\] For the slope parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_1} RSS &= 0 \\\\ -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum (y_i x_i) - \\hat{\\beta}_0 \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} + n\\hat{\\beta}_1 \\bar{x}^2 - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} &= \\hat{\\beta}_1 \\sum (x^2_i) - n\\bar{x}^2 \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 = \\frac{\\sum (x_i y_i) - n\\bar{x}\\bar{y}}{\\sum (x_i^2) - n\\bar{x}^2} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} = r * \\frac{s_y}{s_x} \\] This results in the following fitted regression model, which can be graphically expressed as a Regression Line : \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] Note that it should \\(\\hat{\\varepsilon}\\) in the above image, not \\(e_i\\) .","title":"Ordinary Least Squares"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#ols-properties","text":"By re-arranging the formula for \\(\\hat{\\beta}_0\\) , we can show that \\((\\bar{x}, \\bar{y})\\) always lies on the fitted regression model: \\[ \\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x} \\] Additionally, since the parameters are estimated through minimization, the resulting model must always fulfil the two first order conditions . The model thus has \\(n-2\\) degrees of freedom to reflect these \"constraints\". \\(\\beta_0\\) FOC \\(\\beta_1\\) FOC \\(\\frac{\\partial}{\\partial \\beta_0} = -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\frac{\\partial}{\\partial \\beta_1} = -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\sum \\hat{\\varepsilon_i} = 0\\) \\(\\sum x_i \\hat{\\varepsilon_i} = 0\\) Residuals are negatively correlated Residuals and Independendent variables are uncorrelated Using the above results, we can also show the following that the mean of the regression outputs is equal to the mean of the population: \\[ \\begin{aligned} \\hat{\\varepsilon_i} &= y_i - \\hat{y_i} \\\\ \\sum \\hat{\\varepsilon_i} &= \\sum y_i - \\hat{y_i} \\\\ 0 &= n\\bar{y} - n\\bar{\\hat{y}} \\\\ \\bar{\\hat{y}} &= \\bar{y} \\\\ \\end{aligned} \\]","title":"OLS Properties"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#goodness-of-fit","text":"Ideally, the regression should fit the sample closely, having as small as residuals as possible . The size of all the residuals in the model can be summarized through the RSS. The lower the RSS, the better the fit of the model. Recall that the residuals naturally sum to 0 under OLS - the residuals are thus squared to remove the sign so that they can be summed together. \\[ RSS = \\sum (y_i - \\hat{y})^2 \\] However, the SSR on its own is hard to intepret as there is no indication of how low or high it actually is. Thus, the Total Sum of Squares (TSS) can be used as a benchmark for the RSS as it is at least equal to or higher than the RSS. The TSS represents the RSS for a Null Regression - a model with containing only the intercept parameter . The output of this regression is always the sample mean \\(\\bar{y}\\) , which is used for the computation of its residuals. It represents the worst possible model which thus has the highest possible RSS . The lower the RSS compared to the TSS, the better the fit of the model. \\[ TSS = \\sum (y_i - \\bar{y}) \\]","title":"Goodness of Fit"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#null-model","text":"Consider a regression with only the intercept; \\(\\beta_1 = 0\\) . It is known as the Null Model as there are no independent variables used. \\[ y = \\beta_0 \\] We can estimate \\(\\hat{\\beta_0}\\) using OLS, which results in the following result: \\[ \\begin{aligned} -2 \\sum (y_i - \\hat{\\beta_0}) &= 0 \\\\ n \\bar{y} - n \\hat{\\beta_0} &= 0 \\\\ \\hat{\\beta_0} &= \\bar{y} \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{y} = \\bar{y} \\]","title":"Null Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#sum-of-squares","text":"The TSS can be further decomposed into two more parts for analysis: \\[ \\begin{aligned} TSS &= \\sum (y_i - \\bar{y})^2 \\\\ TSS &= \\sum[(y_i - \\hat{y}) + (\\hat{y}-\\bar{y})]^2 \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 2 \\sum((y_i - \\hat{y})(\\hat{y}-\\bar{y})) \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 0 \\\\ TSS &= RSS + RegSS \\end{aligned} \\] Residual SS (RSS) Regression SS (RegSS) \\(\\sum(\\hat{y}-\\bar{y})^2\\) \\(\\sum(y_i - \\hat{y})^2\\) Variation of the observed values about the regression Variation of the regression output about the sample mean Variation explained by the regression Variation unexplained by the regression Note that it can also be expressed in terms of the Slope Parameter : \\[ \\begin{align} RegSS &= \\sum(\\hat{y}-\\bar{y})^2 \\\\ &= \\sum(\\hat{\\beta}_0 + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum(\\bar{y} - \\beta_1 \\bar{x} + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum[\\hat{\\beta}_1 (x_i - \\bar{x})]^2 \\\\ &= \\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2 \\\\ \\end{align} \\] The Coefficient of Determination \\(R^2\\) can also be used to demonstrate goodness of fit. It measures the proportion of variation explained by the regression model: \\[ R^2 = \\frac{RegSS}{TSS} = 1 - \\frac{RSS}{TSS} \\] Building off the above expression, it can also be expressed in terms of the Sample Correlation : \\[ \\begin{align} R^2 &= \\frac{\\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\\right)^2 \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^4} \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x}) \\sum (y_i - \\bar{y})}\\right)^2 \\\\ &= r_{y,x}^2 \\end{align} \\]","title":"Sum of Squares"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#degrees-of-freedom","text":"The TSS is based on the naive model with only the intercept parameter, thus, it is subject to the single contraint of all residuals summing to 0. The TSS thus has \\(n-1\\) degrees of freedom . The RSS is based on the SLR with both the intercept and slope parameter, thus it is subject to an additional constraint of the sumproduct of all residuals and independent variables being 0. The RSS thus has \\(n-2\\) degrees of freedom. The sum of the RSS and RegSS is equal to the TSS, thus the sum of their degrees of freedom must also be equal to that of the TSS. By working backwards, the RegSS thus has only \\(1\\) degree of freedom .","title":"Degrees of Freedom"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#mean-squared","text":"The division of any Sum of Square (TSS, RSS, RegSS) by its Degrees of Freedom is known as the Mean Squared (MS), which is a measure of its average variance . The MS of the TSS is the Unbiased Estimator for Population Variance , which is why this process is known as the Analysis of Variance , as it decomposes the variance of \\(Y\\) into its constituent components: \\[ s = \\frac{TSS}{n-1} \\] The MS of the RSS is known the Mean Squared Residuals , often also referred to as the Mean Squared Error , as it is an estimate for the population variance of the error \\(\\sigma^2\\) : \\[ MS_{\\text{Residuals}} = \\frac{RSS}{n-2} \\] The MS of the RegSS is known as the Mean Squared Regression , which represents the proportion of variance explained per \\(X\\) used. \\[ MS_{\\text{Regression}} = \\frac{RegSS}{1} \\]","title":"Mean Squared"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#f-statistic","text":"The ANOVA parameters can be used to conduct a hypothesis test to determine if there is in fact a relationship between \\(X\\) and \\(Y\\) : \\(H_0\\) : \\(\\beta_1 = 0\\) \\(H_1\\) : \\(\\beta_1 \\ne 0\\) Under the null hypothesis, there should be no difference between the assumed model and a null model as both only contain the intercept parameter, thus \\(TSS = RSS\\) , where \\(RegSS = 0\\) . Thus, the F-statistic is testing for the equality of variance between the TSS and RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_1 \\ne 0\\) . The F-statistic can be constructed using the sum of squares: \\[ \\begin{aligned} F &= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &= \\frac{RegSS/1}{RSS/(n-2)} \\\\ &= \\frac{(TSS - RSS)/1}{RSS/(n-2)} \\\\ &= (n-2) * \\frac{R^2}{1-R^2}, \\text{divide both by TSS} \\end{aligned} \\] Similar to the variance test, it can be shown that under the null, this test-statistic follows an F distribution with \\(1\\) and \\(n-2\\) degrees of freedom: \\[ \\begin{aligned} T &= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &= \\frac{\\sigma^2_{RegSS} \\frac{MS_{Reg}}{\\sigma^2_{RegSS}}}{\\sigma^2_{RSS} \\frac{MS_{RSS}}{\\sigma^2_{RSS}}} \\\\ &= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\frac{1 * MS_{Reg}}{\\sigma^2_{RegSS}} * \\frac{1}{1}}{\\frac{(n-2) * MS_{RegSS}}{\\sigma^2_{RegSS}}* \\frac{1}{n-2}} \\\\ &= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\chi_1}{\\chi_{n-2}} * (n-2) \\\\ &= 1 * F_{1, n-2} * (n-2) \\\\ &= F_{1, n-2} * (n-2) \\end{aligned} \\] \\[ \\therefore F \\sim F_{1, n-2} \\]","title":"F Statistic"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#anova-table","text":"All the above information is then summarized in a table for convenience, known as the ANOVA Table : Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(1\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-2\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) -","title":"ANOVA Table"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#statistical-inference","text":"","title":"Statistical Inference"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#sampling-distributions","text":"Since the errors are assumed to be normally distributed, then \\(Y\\) is assumed to be normally distributed as well. Since \\(Y\\) is a linear combination of the regression parameters, then the parameters (& their estimates) are normally distributed as well. Both estimates can be expressed in another form that makes it more convenient to find their expectation & variances. \\[ \\begin{aligned} \\hat{\\beta}_1 &= \\frac{\\sum [(x_i - \\bar{x})(y_i - \\bar{y})]}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})y_i}{\\sum (x^2_i - \\bar{x})} - \\frac{\\bar{y} \\sum (x_i - \\bar{x})}{\\sum (x^2_i - \\bar{x})} \\\\ &= \\sum \\frac{(x_i - \\bar{x})}{(x^2_i - \\bar{x})}* y_i - 0 \\\\ &= \\sum w_i * y_i \\\\ \\\\ \\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\ &= \\frac{1}{n} \\sum y_i - \\bar{x} \\sum w_i * y_i \\\\ &= \\sum y_i (\\frac{1}{n} - \\bar{x}w_i) \\end{aligned} \\] \\(w_i\\) is a sort of \"weight\" parameter of the sum of squares. It has three interesting properties that makes it useful: \\[ \\begin{aligned} \\sum w_i &= \\frac{\\sum (x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{n\\bar{x}-n\\bar{x}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{0}{\\sum (x^2_i - \\bar{x})} \\\\ &= 0 \\\\ \\\\ \\sum w_i x_i &= \\frac{\\sum x_i(x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x^2_i - \\bar{x} \\sum x_i)}{\\sum x^2_i - 2\\bar{x}\\sum x_i + \\sum \\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - \\bar{x}(n \\bar{x})}{\\sum x^2_i - 2\\bar{x}(n\\bar{x}) + n\\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - 2n\\bar{x}^2 + n\\bar{x}^2} \\\\ &=\\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - n\\bar{x}^2} \\\\ &= 1 \\\\ \\\\ \\sum w_i^2 &= \\frac{\\sum (x_i - \\bar{x})^2}{\\sum (x_i - \\bar{x})^4} \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i) &= n(\\frac{1}{n}) - \\bar{x} \\sum w_i \\\\ &= 1 - 0 \\\\ &= 1 \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i &= \\frac{1}{n} \\sum x_i - \\bar{x} \\sum w_i x_i \\\\ &= \\frac{1}{n} (n\\bar{x}) - \\bar{x} (1) \\\\ &= \\bar{x} - \\bar{x} \\\\ &= 0 \\end{aligned} \\] Using this, the Expectation & Variance can be determined: \\[ \\begin{aligned} E(\\hat{\\beta}_1) &= \\sum w_i E(y_i) \\\\ &= \\sum w_i E(\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum w_i + \\beta_1 \\sum w_i x_i \\\\ &= \\beta_0 (0) + \\beta_1 (1) \\\\ &= \\beta_1\\\\ \\\\ Var(\\hat{\\beta}_1) &= Var(\\sum w_i y_i) \\\\ &= \\sum w_i^2 Var (y_i) \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2} \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 \\sim N(\\beta_1, \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2}) \\] \\[ \\begin{aligned} E(\\hat{\\beta}_0) &= \\sum (\\frac{1}{n} - \\bar{x}w_i) E(y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i) (\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum (\\frac{1}{n} - \\bar{x}w_i) + \\beta_1 \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i \\\\ &= \\beta_0 (1) + \\beta_1 (0) \\\\ &= \\beta_0 \\\\ \\\\ Var(\\hat{\\beta}_0) &= Var(\\sum (\\frac{1}{n} - \\bar{x}w_i)y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i)^2 Var (y_i) \\\\ &= \\sigma^2 \\sum (\\frac{1}{n^2} -\\frac{2\\bar{x}w_i}{n} + \\bar{x}^2 w_i^2) \\\\ &= \\sigma^2 (\\sum \\frac{1}{n^2} - \\frac{2\\bar{x}}{n} \\sum w_i + \\bar{x}^2 \\sum w_i^2) \\\\ &= \\sigma^2 [n(\\frac{1}{n^2}) - \\frac{2\\bar{x}}{n} (0) + \\bar{x}^2 (\\frac{1}{\\sum (x_i - \\bar{x})^2})] \\\\ &= \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2}) \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_0 \\sim N(\\beta_0, \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2})) \\]","title":"Sampling Distributions"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#hypothesis-testing","text":"Since the regression parameters are normally distributed, a z-statistic can also be used to conduct the tests. However, since the population variance is not known, a t-statistic is used instead: \\[ \\begin{aligned} t &= \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\end{aligned} \\] Since the population variance is estimated by the MSE which has \\(n-2\\) degrees of freedom, the corresponding chi-squared and hence t-distribution has \\(n-2\\) degrees of freedom as well. \\[ \\begin{aligned} \\hat{Var}(\\hat{\\beta_1}) &= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} * \\frac{\\frac{1}{n-1}}{\\frac{1}{n-1}} \\\\ &= \\frac{MS_{RSS}}{(n-1) s^2} \\end{aligned} \\] \\[ t \\sim t_{n-2} \\] Since the square of the t-statistic is the F-statistic, both are equivalent ways of doing so and will always lead to the same conclusions. \\[ t^2 \\sim F_{1, n-2} \\]","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#confidence-intervals","text":"Since the distribution of t-statistic is known, it can be used to determine the confidence interval of the estimate: \\[ P\\left(\\text{Margin of Error} < \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} < \\text{Margin of Error}\\right) = 1 - \\alpha \\] \\[ \\text{Confidence Interval} = \\hat{\\beta}_1 \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_1}} \\]","title":"Confidence Intervals"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#prediction-intervals","text":"Consider the Prediction Error of the SLR model: \\[ y_* - \\hat{y_*} = \\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\] Since both \\(y_*\\) and \\(\\hat{y_*}\\) are normally distributed, the prediction errors are normally distributed as well: \\[ \\begin{aligned} E(y_* - \\hat{y_*}) &= E[\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)]] \\\\ &= 0 + \\beta_0 + \\beta_1 E(x_*) - \\beta_0 - \\beta_1 E(x_*) \\\\ &= 0 \\\\ \\\\ Var(y_* - \\hat{y_*}) &= Var(\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= Var(\\varepsilon_*) + Var[(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= ... \\\\ &= \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}] \\end{aligned} \\] \\[ \\therefore y_* - \\hat{y_*} \\sim N\\left(0, \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}]\\right) \\] Similar to before, since the population variance is unknown, it can be approximated using the MSE. Thus, a t-statistic constructed from this sample will have a t-distribution , allowing the following prediction interval to be calculated: \\[ \\text{Prediction Interval} = \\hat{y}_* \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\] Notice that the standard error of the prediction interval increases as \\(x_*\\) moves further away \\(\\bar{x}\\) , indicating that the predictions become less accurate for those values.","title":"Prediction Intervals"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/","text":"Multiple Linear Regression \u00b6 The natural extension of the SLR model is to include more than one independent variable , which thus results in the more generalized Multiple Linear Regression (MLR) model. \\[ E(Y|X_1, ... X_p) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_{p}X_{p} \\] Unlike the SLR which studies how each individual IV influences the DV, the goal of the MLR model is to study how all the IVs operate together to influence the DV. \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) when \\(X_1 = X_2 = ... = 0\\) Change in \\(E(Y)\\) given a one unit increase in \\(X_j\\) , holding all other \\(X\\) 's constant Intercept Parameter \"Slope\" Parameter For avoidance of doubt, the subscript \\(i\\) will be used to denote observations while \\(j\\) will be used to denote independent variables. Every observation can also be expressed as the sum of the regression and the error term. However, due to the multi-dimensional nature of the model, it is commonly expressed in matrix notation: \\[ \\begin{aligned} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} &= \\begin{pmatrix} 1 & x_{11} & x_{12} & ... & x_{1p} \\\\ 1 & x_{21} & x_{22} & ... & x_{2p} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & ... & x_{np} \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix} + \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix} \\\\ \\boldsymbol{y} &= \\boldsymbol{X\\beta + \\varepsilon} \\end{aligned} \\] Ordinary Least Squares \u00b6 Similar to the SLR model, the regression parameters can be found by minimizing the sum of squared residuals: \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_p x_ip)]^2 \\] There are \\(p+1\\) FOC equations to solve through the minimization, with the additional one reflecting the intercept parameter. It is difficult to algebraically solve this system of equations, thus there is no closed form tsolution for each individual paramater. Instead, here is a vector solution for all of the parameters: \\[ \\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\hat{\\beta_0} \\\\ \\hat{\\beta_1} \\\\ \\vdots \\\\ \\hat{\\beta_p} \\end{pmatrix} = (\\boldsymbol{X'X})^{-1}\\boldsymbol{X'y} \\] Note that since there are \\(p+1\\) equations that must be solved, the model has \\(n-p+1\\) degrees of freedom. Following the same logic, there must be at least \\(p+1\\) observations in order to solve the equations and hence construct the model. This results in the following fitted regression model, which can be graphically expressed as a Regression Plane : \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_P x_ip \\] Manual Computation \u00b6 The tricky part is that \\((\\boldsymbol{X'X})^{-1}\\) is hard to compute by manually , except for the special case where \\(p=1\\) (SLR). Thus, it is likely that the parameters will be provided by the question. If required to compute them manually, then \\((\\boldsymbol{X'X})^{-1}\\) is likely to be provided. The remaining \\(\\boldsymbol{X'y}\\) still needs to be computed and put together to obtain the regression parameters. However, if the model has \\(p=2\\) but no intercept , then \\((\\boldsymbol{X'X})^{-1}\\) is a 2 x 2 Matrix whose inverse can be easily calculated. Similarly, if \\((\\boldsymbol{X'X})^{-1}\\) is a Diagonal Matrix , its inverse can be easily calculated as well. Goodness of Fit \u00b6 The ANOVA for MLR follows the same intuition as the SLR version, adjusted for the new degrees of freedom: Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(p\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-(p+1)\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) - The coefficient of determination still represents the proportion of variance explained by the regression, but has a slightly different formula: \\[ R^2 = \\frac{RegSS}{TSS} = r_{y,\\hat{y}}^2 \\] The multiple IVs of the model are now captured through \\(\\hat{y}\\) instead of \\(x\\) directly. However, the hypothesis test under the MLR is vastly different from the SLR version. Instead of testing if an individual IV is useful, it tests if all the IVs are collectively useful in helping to explain the DV. \\[ \\begin{aligned} H_0 &: \\beta_1 = \\beta_2 = ... = \\beta_p = 0 \\\\ H_1 &: \\text{At least one } \\beta_j \\text{ is non-zero} \\end{aligned} \\] Thus, rejecting the null hypothesis implies that at least one of the IVs used is useful, but does not provide much insight into which of them are useful. Partial F Test \u00b6 A partial F-test can be used to precisely determine which of the IVs are useful in explaining the DV. A regular F test compares the null model with no IVs to the desired model with all the IVs. If the sum of squares are significantly different, then it implies that the additional IVs are jointly useful in explaining \\(Y\\) . The partial F test generalizes this idea. Instead of considering a null model with no IVs, a Reduced Model with a limited number IVs ( \\(q\\) ) is considered instead. Consequently, the desired model is known as the Full Model with all \\(p\\) IVs, where \\(q < p\\) . \\[ \\begin{aligned} H_0: \\beta_{p-q+1} = ... = \\beta_p = 0 \\\\ H_1: \\beta_{p-q+1} = ... = \\beta_p \\ne 0 \\end{aligned} \\] The test is also commonly referred to as the Generalized F test, where the models are referred to as the Restricted and Unrestricted Models. The difference in the RSS between the Full and Reduced Model is known as the Extra Sum of Squares (ExtraSS) . It represents the contribution of the missing variables in explaining the variance of \\(Y\\) . Under the null hypothesis, there should be no difference between the two RSS, and thus \\(ExtraSS = 0\\) . \\[ ExtraSS = RSS_{Reduced} - RSS_{Full} \\] Thus, the Partial F-statistic is testing for the equality of variance between the two RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_{p-q+1} = ... = \\beta_p \\ne 0\\) . \\[ \\begin{aligned} F &= \\frac{MS_{ExtraSS}}{MS_{RSS_{Full}}} \\\\ &= \\frac{ExtraSS/q}{RSS/(n-2)} \\\\ &= \\frac{(RSS_{Reduced} - RSS_{Full})/q}{RSS/(n-2)} \\\\ &= (n-2) * \\frac{(1- R_{Reduced}^2) - (1 - R_{Full}^2)}{1-RSS_{Full}^2}, \\text{divide both by TSS} \\\\ &= (n-2) * \\frac{R_{Full}^2 - R_{Reduced}^2}{1-RSS_{Full}^2} \\end{aligned} \\] Statistical Inference \u00b6 Sampling Distributions \u00b6 Similar to SLR, the regression parameters are normally distributed as well. However, since there are multiple regression parameters, they collectively follow a multivariate normal distribution . \\[ \\hat{\\beta} \\sim N_{p+1}(\\beta, \\sigma^2 (\\boldsymbol{X'X})^{-1}) \\] The variance of the distribution is known as the Variance Covariance Matrix , which provides the covariances between every possible pair of regression parameters. Since the covariance of a variable with itself is its variance, the diagonals are the respective variances of the parameters. Note that the first element of the diagonal is the intercept, thus the variance of the jth IV is the (j+1)th element of the diagonal . \\[ Var(\\hat{\\beta}) = \\begin{pmatrix} Var(\\hat{\\beta}_0) & Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) & ... & Cov(\\hat{\\beta}_0, \\hat{\\beta}_p) \\\\ Cov(\\hat{\\beta}_1, \\hat{\\beta}_0) & Var(\\hat{\\beta}_1) & ... & Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ Cov(\\hat{\\beta}_p, \\hat{\\beta}_0) & Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) & ... & Var(\\hat{\\beta}_p) \\end{pmatrix} \\] Note that the covariances are symmetrical about the diagonal - \\(Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = Cov(\\hat{\\beta}_1, \\hat{\\beta}_0)\\) . Hypothesis Testing \u00b6 Similar to SLR, the t-test can be used to test for the significance of an individual IV, but the intepretation of the test is different from the SLR case. It tests the usefulness of an individual IV in the presence of the other predictors . \\[ \\begin{aligned} H_0: \\beta_j = 0 \\\\ H_1: \\beta_j \\ne 0 \\end{aligned} \\] \\[ t(\\beta_j) = \\frac{\\hat{\\beta_j} - \\beta_j}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\] Recall that the variance of the jth IV si the (j+1)th element of the variance covariance matrix. \\[ t(\\beta_j) \\sim t_{n-p-1} \\] However, this leads to several odd results which needs to be accounted for: Predictor is not significant individually but significant when taken alone . Predictors are not significant individually but significant when taken together . There are now two possible ways to test for the significance of IVs: Conduct a single F-test to test for the joint significance of all IVs Conduct multiple t-tests to test for the joint significance of each IV The problem with the multiple t-test approach lies with the type I error of the tests. For \\(\\alpha = 0.05\\) , the probability of correctly rejecting the null is \\(0.95\\) . Assuming that all the tests are independent, the probability of correctly rejecting all the nulls is \\(0.95^p\\) , which drastically decreases with the number of tests conducted. Given enough predictors, this means that the probability drops to approximately 0, which means that there is bound to be a wrongly rejected null; a type I error is guaranteed even though it was supposed to be limited at a 0.05 chance. The Bonferroni Correction is a method of adjusting \\(\\alpha\\) of each hypothesis test such that the overall type I error is kept at its desired level. However, this has the consequence of increasing the probability of type II errors, which is why it is not popular. The F-test has the advantage of controlling the type I error regardless of the number of predictors , which is why it is preferred for hypothesis testing in the MLR. Confidence Intervals \u00b6 Similar to SLR, the confidence intervals can be constructed using the distribution of the test-statistic: \\[ \\text{Confidence Interval} = \\hat{\\beta}_j \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_j}} \\] Prediction Intervals \u00b6 Unlike in SLR, it is difficult to determine the distribution of the prediction error. Thus, the final result can be found below: \\[ \\begin{aligned} \\text{Prediction Interval} &= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\\\ &= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\sqrt{s^2 [1 + x'_* (\\boldsymbol{X'X})^-1 * x_*]} \\end{aligned} \\] Despite the result looking more complicated, the key takeaway remains the same - the further away \\(x_*\\) is from \\(\\bar{x}\\) , the greater Variations of MLR \u00b6 Qualitative IV \u00b6 The discussion so far has mostly focused on Quantitative IVs, thus this section will explore Qualitative IVs. They can only take values from a list of pre-defined values, known as the Levels of the variable. In a regression context, most qualitative IVs are represented in the form of a Dummy Variable which can only take two possible levels - Yes (1) or No (0). Note that there are other ways of encoding a dummy variable (-1/0/1 etc), but the principles stay large the same. \\[ \\text{Dummy Variable} = \\begin{cases} 1, & \\text{First Level} \\\\ 0, & \\text{Second Level} \\end{cases} \\] In general, \\(n-1\\) dummy variables are needed to represent a qualitative variable with \\(n\\) levels. This is because the status of the last level can be deduced from the other dummy variables . Thus, including a seperate dummy variable for this last level is redundant and will lead to the problem of Collinearity , which will be explored in a later section. For instance, consider four levels (North, East, South & West), represented by the three dummy variables ( \\(N, E , S\\) ). If any of the variables are 1, they represent their respective direction (North, East & South). If all of them are 0, then the direction is the remaining level (West). The last remaining level is often referred to as the Baseline Level as it is the default level of the variable when all other dummies are 0. Any level can be used as the baseline, but the parameters will differ across models with different baselines. \\[ E(Y|X) = \\beta_0 + \\beta_{1, North} x_{North} + \\beta_{2, East} * x_{East} + \\beta_{3, South} * x_{South} \\] \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) at the baseline level Change in \\(E(Y)\\) from the baseline to the chosen level \\(X_1 = X_2 = ... = X_j = 0\\) \\(X_1 = X_2 = ... = 0; X_j = 1\\) Dummy variables are usually used in conjunction with quantitative ones . This essentially creates a \"seperate\" regression model for each of the levels. For the simplest case of one quantiative and one dummy, \\(\\beta_2\\) is the difference in the intercept of the two resulting SLR models. \\[ E(Y|X) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\] \\[ E(Y|X) = \\begin{cases} (\\beta_0 + \\beta_2) + \\beta_1 x_1, & x_2 = 1 \\\\ \\beta_0 + \\beta_1 x_1, & x_2 = 0 \\end{cases} \\] Interaction Model \u00b6 So far, it was assumed that each IV had an independent effect on the DV. However, IVs may interact to produce a joint effect on the DV, where the effect of one IV depends on the value of another IV. For instance, the production of a factory may depend on the number of Machines and Workers. However, the more machines there are, the greater the effect of an additional worker . Thus, this interaction effect can be captured through an Interaction Variable , which is the product of both IVs: \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 \\\\ &= \\beta_0 + (\\beta_1 + \\beta_3 x_2)x_1 + \\beta_2 x_2 \\\\ &= \\beta_0 + \\beta_1 x_1 + (\\beta_2 + \\beta_3 x_1) x_2 \\\\ \\end{aligned} \\] A one unit increase in \\(x_1\\) will increase E(Y) by \\(\\beta_1 + \\beta_3 x_2\\) , which depends on the value of \\(x_2\\) as well, which is is why they interact with each another. Phrased another way, for every one unit increase in \\(x_2\\) , the change in E(Y) from a unit increase in \\(x_1\\) increases by \\(\\beta_3\\) . Something unusual to take note of is that the Interaction Variable tests significant but the constituent variables do not. In this case, it is common practice to retain both the interaction and the consistuent variables in the model. This is practice is known as the Hierarchical Principle . Models containing dummy variables can also have an interaction effect. Building off the example from the previous section, \\(\\beta_2\\) is still the difference in the intercept but with the new \\(\\beta_3\\) being the difference in slopes of the two resulting SLR models. \\[ E(Y|X) = \\begin{cases} (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3)x_1, & x_2 = 1 \\\\ \\beta_0 + \\beta_1 x_1, & x_2 = 0 \\end{cases} \\] Piecewise Model \u00b6 If the DV has an abrupt change in behaviour over different values of the IVs, it can be accounted for the through the use of a Piecewise Regression . The first method of creating a piecewise regression involves the use of an Indicator Function . It is essentially a dummy variable which depends on the value of the other IVs. \\[ z_{\\{x>=c\\}} = \\begin{cases} 0, & x < c \\\\ 1, & x \\ge c \\end{cases} \\] \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 z(x-c) \\\\ &= \\begin{cases} \\beta_0 + \\beta_1 x_1, & x < c \\\\ (\\beta_0 - \\beta_2c) + (\\beta_1 + \\beta_2) x_1, & x \\ge c \\end{cases} \\end{aligned} \\] Note that \\(z(x-c)\\) is treated as a distinct IV and hence can be equivalently expressed as \\(x_2\\) ; the full notation is used here for clarity. \\(c\\) is the value at which the DV abruptly changes in behaviour, known as a Kink in the graph, which continuously connects the two regression lines. The other method is to use an interaction variable instead. Similar to how the interaction variables resulted in the model to \"split\", the model now splits at \\(x = c\\) , resulting in a non-continuous gap. \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2z + \\beta_3 zx \\\\ &= \\begin{cases} \\beta_0 + \\beta_1 x_1, & x < c \\\\ (\\beta_0 - \\beta_2) + (\\beta_1 + \\beta_3) x_1, & x \\ge c \\end{cases} \\end{aligned} \\] Polynomial Model \u00b6 If the relationship between the DV and IV is complex (non-linear), then a Polynomial Regression can be used to better model the relationship between the two. \\[ E(Y|X) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 \\] Note that it is the same IV used in the regression, just with additional powers. Although a polynomial regression may capture the true relationship better, the regression parameters become hard to intepret. The partial derivatives can no longer be intepreted as holding other IVs constant as each IV is dependent on the same quantity, just to different powers. \\[ \\frac{\\partial E(Y|X)}{\\partial x} = \\beta_1 + 2\\beta_2 x + ... + m \\beta_m x^{m-1} \\]","title":"Multiple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#multiple-linear-regression","text":"The natural extension of the SLR model is to include more than one independent variable , which thus results in the more generalized Multiple Linear Regression (MLR) model. \\[ E(Y|X_1, ... X_p) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_{p}X_{p} \\] Unlike the SLR which studies how each individual IV influences the DV, the goal of the MLR model is to study how all the IVs operate together to influence the DV. \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) when \\(X_1 = X_2 = ... = 0\\) Change in \\(E(Y)\\) given a one unit increase in \\(X_j\\) , holding all other \\(X\\) 's constant Intercept Parameter \"Slope\" Parameter For avoidance of doubt, the subscript \\(i\\) will be used to denote observations while \\(j\\) will be used to denote independent variables. Every observation can also be expressed as the sum of the regression and the error term. However, due to the multi-dimensional nature of the model, it is commonly expressed in matrix notation: \\[ \\begin{aligned} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} &= \\begin{pmatrix} 1 & x_{11} & x_{12} & ... & x_{1p} \\\\ 1 & x_{21} & x_{22} & ... & x_{2p} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & ... & x_{np} \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix} + \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix} \\\\ \\boldsymbol{y} &= \\boldsymbol{X\\beta + \\varepsilon} \\end{aligned} \\]","title":"Multiple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#ordinary-least-squares","text":"Similar to the SLR model, the regression parameters can be found by minimizing the sum of squared residuals: \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_p x_ip)]^2 \\] There are \\(p+1\\) FOC equations to solve through the minimization, with the additional one reflecting the intercept parameter. It is difficult to algebraically solve this system of equations, thus there is no closed form tsolution for each individual paramater. Instead, here is a vector solution for all of the parameters: \\[ \\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\hat{\\beta_0} \\\\ \\hat{\\beta_1} \\\\ \\vdots \\\\ \\hat{\\beta_p} \\end{pmatrix} = (\\boldsymbol{X'X})^{-1}\\boldsymbol{X'y} \\] Note that since there are \\(p+1\\) equations that must be solved, the model has \\(n-p+1\\) degrees of freedom. Following the same logic, there must be at least \\(p+1\\) observations in order to solve the equations and hence construct the model. This results in the following fitted regression model, which can be graphically expressed as a Regression Plane : \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_P x_ip \\]","title":"Ordinary Least Squares"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#manual-computation","text":"The tricky part is that \\((\\boldsymbol{X'X})^{-1}\\) is hard to compute by manually , except for the special case where \\(p=1\\) (SLR). Thus, it is likely that the parameters will be provided by the question. If required to compute them manually, then \\((\\boldsymbol{X'X})^{-1}\\) is likely to be provided. The remaining \\(\\boldsymbol{X'y}\\) still needs to be computed and put together to obtain the regression parameters. However, if the model has \\(p=2\\) but no intercept , then \\((\\boldsymbol{X'X})^{-1}\\) is a 2 x 2 Matrix whose inverse can be easily calculated. Similarly, if \\((\\boldsymbol{X'X})^{-1}\\) is a Diagonal Matrix , its inverse can be easily calculated as well.","title":"Manual Computation"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#goodness-of-fit","text":"The ANOVA for MLR follows the same intuition as the SLR version, adjusted for the new degrees of freedom: Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(p\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-(p+1)\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) - The coefficient of determination still represents the proportion of variance explained by the regression, but has a slightly different formula: \\[ R^2 = \\frac{RegSS}{TSS} = r_{y,\\hat{y}}^2 \\] The multiple IVs of the model are now captured through \\(\\hat{y}\\) instead of \\(x\\) directly. However, the hypothesis test under the MLR is vastly different from the SLR version. Instead of testing if an individual IV is useful, it tests if all the IVs are collectively useful in helping to explain the DV. \\[ \\begin{aligned} H_0 &: \\beta_1 = \\beta_2 = ... = \\beta_p = 0 \\\\ H_1 &: \\text{At least one } \\beta_j \\text{ is non-zero} \\end{aligned} \\] Thus, rejecting the null hypothesis implies that at least one of the IVs used is useful, but does not provide much insight into which of them are useful.","title":"Goodness of Fit"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#partial-f-test","text":"A partial F-test can be used to precisely determine which of the IVs are useful in explaining the DV. A regular F test compares the null model with no IVs to the desired model with all the IVs. If the sum of squares are significantly different, then it implies that the additional IVs are jointly useful in explaining \\(Y\\) . The partial F test generalizes this idea. Instead of considering a null model with no IVs, a Reduced Model with a limited number IVs ( \\(q\\) ) is considered instead. Consequently, the desired model is known as the Full Model with all \\(p\\) IVs, where \\(q < p\\) . \\[ \\begin{aligned} H_0: \\beta_{p-q+1} = ... = \\beta_p = 0 \\\\ H_1: \\beta_{p-q+1} = ... = \\beta_p \\ne 0 \\end{aligned} \\] The test is also commonly referred to as the Generalized F test, where the models are referred to as the Restricted and Unrestricted Models. The difference in the RSS between the Full and Reduced Model is known as the Extra Sum of Squares (ExtraSS) . It represents the contribution of the missing variables in explaining the variance of \\(Y\\) . Under the null hypothesis, there should be no difference between the two RSS, and thus \\(ExtraSS = 0\\) . \\[ ExtraSS = RSS_{Reduced} - RSS_{Full} \\] Thus, the Partial F-statistic is testing for the equality of variance between the two RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_{p-q+1} = ... = \\beta_p \\ne 0\\) . \\[ \\begin{aligned} F &= \\frac{MS_{ExtraSS}}{MS_{RSS_{Full}}} \\\\ &= \\frac{ExtraSS/q}{RSS/(n-2)} \\\\ &= \\frac{(RSS_{Reduced} - RSS_{Full})/q}{RSS/(n-2)} \\\\ &= (n-2) * \\frac{(1- R_{Reduced}^2) - (1 - R_{Full}^2)}{1-RSS_{Full}^2}, \\text{divide both by TSS} \\\\ &= (n-2) * \\frac{R_{Full}^2 - R_{Reduced}^2}{1-RSS_{Full}^2} \\end{aligned} \\]","title":"Partial F Test"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#statistical-inference","text":"","title":"Statistical Inference"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#sampling-distributions","text":"Similar to SLR, the regression parameters are normally distributed as well. However, since there are multiple regression parameters, they collectively follow a multivariate normal distribution . \\[ \\hat{\\beta} \\sim N_{p+1}(\\beta, \\sigma^2 (\\boldsymbol{X'X})^{-1}) \\] The variance of the distribution is known as the Variance Covariance Matrix , which provides the covariances between every possible pair of regression parameters. Since the covariance of a variable with itself is its variance, the diagonals are the respective variances of the parameters. Note that the first element of the diagonal is the intercept, thus the variance of the jth IV is the (j+1)th element of the diagonal . \\[ Var(\\hat{\\beta}) = \\begin{pmatrix} Var(\\hat{\\beta}_0) & Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) & ... & Cov(\\hat{\\beta}_0, \\hat{\\beta}_p) \\\\ Cov(\\hat{\\beta}_1, \\hat{\\beta}_0) & Var(\\hat{\\beta}_1) & ... & Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ Cov(\\hat{\\beta}_p, \\hat{\\beta}_0) & Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) & ... & Var(\\hat{\\beta}_p) \\end{pmatrix} \\] Note that the covariances are symmetrical about the diagonal - \\(Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = Cov(\\hat{\\beta}_1, \\hat{\\beta}_0)\\) .","title":"Sampling Distributions"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#hypothesis-testing","text":"Similar to SLR, the t-test can be used to test for the significance of an individual IV, but the intepretation of the test is different from the SLR case. It tests the usefulness of an individual IV in the presence of the other predictors . \\[ \\begin{aligned} H_0: \\beta_j = 0 \\\\ H_1: \\beta_j \\ne 0 \\end{aligned} \\] \\[ t(\\beta_j) = \\frac{\\hat{\\beta_j} - \\beta_j}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\] Recall that the variance of the jth IV si the (j+1)th element of the variance covariance matrix. \\[ t(\\beta_j) \\sim t_{n-p-1} \\] However, this leads to several odd results which needs to be accounted for: Predictor is not significant individually but significant when taken alone . Predictors are not significant individually but significant when taken together . There are now two possible ways to test for the significance of IVs: Conduct a single F-test to test for the joint significance of all IVs Conduct multiple t-tests to test for the joint significance of each IV The problem with the multiple t-test approach lies with the type I error of the tests. For \\(\\alpha = 0.05\\) , the probability of correctly rejecting the null is \\(0.95\\) . Assuming that all the tests are independent, the probability of correctly rejecting all the nulls is \\(0.95^p\\) , which drastically decreases with the number of tests conducted. Given enough predictors, this means that the probability drops to approximately 0, which means that there is bound to be a wrongly rejected null; a type I error is guaranteed even though it was supposed to be limited at a 0.05 chance. The Bonferroni Correction is a method of adjusting \\(\\alpha\\) of each hypothesis test such that the overall type I error is kept at its desired level. However, this has the consequence of increasing the probability of type II errors, which is why it is not popular. The F-test has the advantage of controlling the type I error regardless of the number of predictors , which is why it is preferred for hypothesis testing in the MLR.","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#confidence-intervals","text":"Similar to SLR, the confidence intervals can be constructed using the distribution of the test-statistic: \\[ \\text{Confidence Interval} = \\hat{\\beta}_j \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_j}} \\]","title":"Confidence Intervals"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#prediction-intervals","text":"Unlike in SLR, it is difficult to determine the distribution of the prediction error. Thus, the final result can be found below: \\[ \\begin{aligned} \\text{Prediction Interval} &= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\\\ &= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\sqrt{s^2 [1 + x'_* (\\boldsymbol{X'X})^-1 * x_*]} \\end{aligned} \\] Despite the result looking more complicated, the key takeaway remains the same - the further away \\(x_*\\) is from \\(\\bar{x}\\) , the greater","title":"Prediction Intervals"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#variations-of-mlr","text":"","title":"Variations of MLR"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#qualitative-iv","text":"The discussion so far has mostly focused on Quantitative IVs, thus this section will explore Qualitative IVs. They can only take values from a list of pre-defined values, known as the Levels of the variable. In a regression context, most qualitative IVs are represented in the form of a Dummy Variable which can only take two possible levels - Yes (1) or No (0). Note that there are other ways of encoding a dummy variable (-1/0/1 etc), but the principles stay large the same. \\[ \\text{Dummy Variable} = \\begin{cases} 1, & \\text{First Level} \\\\ 0, & \\text{Second Level} \\end{cases} \\] In general, \\(n-1\\) dummy variables are needed to represent a qualitative variable with \\(n\\) levels. This is because the status of the last level can be deduced from the other dummy variables . Thus, including a seperate dummy variable for this last level is redundant and will lead to the problem of Collinearity , which will be explored in a later section. For instance, consider four levels (North, East, South & West), represented by the three dummy variables ( \\(N, E , S\\) ). If any of the variables are 1, they represent their respective direction (North, East & South). If all of them are 0, then the direction is the remaining level (West). The last remaining level is often referred to as the Baseline Level as it is the default level of the variable when all other dummies are 0. Any level can be used as the baseline, but the parameters will differ across models with different baselines. \\[ E(Y|X) = \\beta_0 + \\beta_{1, North} x_{North} + \\beta_{2, East} * x_{East} + \\beta_{3, South} * x_{South} \\] \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) at the baseline level Change in \\(E(Y)\\) from the baseline to the chosen level \\(X_1 = X_2 = ... = X_j = 0\\) \\(X_1 = X_2 = ... = 0; X_j = 1\\) Dummy variables are usually used in conjunction with quantitative ones . This essentially creates a \"seperate\" regression model for each of the levels. For the simplest case of one quantiative and one dummy, \\(\\beta_2\\) is the difference in the intercept of the two resulting SLR models. \\[ E(Y|X) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\] \\[ E(Y|X) = \\begin{cases} (\\beta_0 + \\beta_2) + \\beta_1 x_1, & x_2 = 1 \\\\ \\beta_0 + \\beta_1 x_1, & x_2 = 0 \\end{cases} \\]","title":"Qualitative IV"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#interaction-model","text":"So far, it was assumed that each IV had an independent effect on the DV. However, IVs may interact to produce a joint effect on the DV, where the effect of one IV depends on the value of another IV. For instance, the production of a factory may depend on the number of Machines and Workers. However, the more machines there are, the greater the effect of an additional worker . Thus, this interaction effect can be captured through an Interaction Variable , which is the product of both IVs: \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 \\\\ &= \\beta_0 + (\\beta_1 + \\beta_3 x_2)x_1 + \\beta_2 x_2 \\\\ &= \\beta_0 + \\beta_1 x_1 + (\\beta_2 + \\beta_3 x_1) x_2 \\\\ \\end{aligned} \\] A one unit increase in \\(x_1\\) will increase E(Y) by \\(\\beta_1 + \\beta_3 x_2\\) , which depends on the value of \\(x_2\\) as well, which is is why they interact with each another. Phrased another way, for every one unit increase in \\(x_2\\) , the change in E(Y) from a unit increase in \\(x_1\\) increases by \\(\\beta_3\\) . Something unusual to take note of is that the Interaction Variable tests significant but the constituent variables do not. In this case, it is common practice to retain both the interaction and the consistuent variables in the model. This is practice is known as the Hierarchical Principle . Models containing dummy variables can also have an interaction effect. Building off the example from the previous section, \\(\\beta_2\\) is still the difference in the intercept but with the new \\(\\beta_3\\) being the difference in slopes of the two resulting SLR models. \\[ E(Y|X) = \\begin{cases} (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3)x_1, & x_2 = 1 \\\\ \\beta_0 + \\beta_1 x_1, & x_2 = 0 \\end{cases} \\]","title":"Interaction Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#piecewise-model","text":"If the DV has an abrupt change in behaviour over different values of the IVs, it can be accounted for the through the use of a Piecewise Regression . The first method of creating a piecewise regression involves the use of an Indicator Function . It is essentially a dummy variable which depends on the value of the other IVs. \\[ z_{\\{x>=c\\}} = \\begin{cases} 0, & x < c \\\\ 1, & x \\ge c \\end{cases} \\] \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 z(x-c) \\\\ &= \\begin{cases} \\beta_0 + \\beta_1 x_1, & x < c \\\\ (\\beta_0 - \\beta_2c) + (\\beta_1 + \\beta_2) x_1, & x \\ge c \\end{cases} \\end{aligned} \\] Note that \\(z(x-c)\\) is treated as a distinct IV and hence can be equivalently expressed as \\(x_2\\) ; the full notation is used here for clarity. \\(c\\) is the value at which the DV abruptly changes in behaviour, known as a Kink in the graph, which continuously connects the two regression lines. The other method is to use an interaction variable instead. Similar to how the interaction variables resulted in the model to \"split\", the model now splits at \\(x = c\\) , resulting in a non-continuous gap. \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2z + \\beta_3 zx \\\\ &= \\begin{cases} \\beta_0 + \\beta_1 x_1, & x < c \\\\ (\\beta_0 - \\beta_2) + (\\beta_1 + \\beta_3) x_1, & x \\ge c \\end{cases} \\end{aligned} \\]","title":"Piecewise Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#polynomial-model","text":"If the relationship between the DV and IV is complex (non-linear), then a Polynomial Regression can be used to better model the relationship between the two. \\[ E(Y|X) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 \\] Note that it is the same IV used in the regression, just with additional powers. Although a polynomial regression may capture the true relationship better, the regression parameters become hard to intepret. The partial derivatives can no longer be intepreted as holding other IVs constant as each IV is dependent on the same quantity, just to different powers. \\[ \\frac{\\partial E(Y|X)}{\\partial x} = \\beta_1 + 2\\beta_2 x + ... + m \\beta_m x^{m-1} \\]","title":"Polynomial Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/","text":"Gauss Markov Theorem \u00b6 Using OLS, the estimated regression parameters will always be unbiased under certain assumptions. The Gauss Markov Theorem extends this, which states that under certain assumptions, the OLS estimators will have the lowest variance among all possible linear unbiased estimators. In a statistics context, they are said to be the most efficient among all other linear unbiased estimators. In a regression context, the OLS estimators are said to be the Best Linear Unbiased Estimators (BLUE). This section will go over the various assumptions for both OLS and Gauss Markov Theorem. It will also cover the diagnostics to determine if the assumptions have been violated. The assumptions needed for OLS and the Gauss Markov theorem are often mixed up with each other as the assumptions needed for OLS are also needed for the theorem. This set of notes makes a clear distinction between the two. OLS Assumptions \u00b6 #1: Linearity \u00b6 Linear regression is a model where the relationship between the DV and IVs are linear. Thus, the regression parameters must be linear , but NOT the DV or IV. This means that the model is still considered a \"Linear Regression\" even after a transformation of the DV and/or IV. \\[ \\begin{aligned} y_i &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\\\ y_i &= \\beta_0 + \\beta_1 x^2 + \\beta_2 x^3 \\\\ \\ln y_i &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\end{aligned} \\] #2: Exogenity \u00b6 Exogenity refers to how a variable comes from outside the model and is thus independent of any other variables within the model . In a regression context, this comes in the form of the errors having a conditional mean of 0 , ensuring that the errors are random and thus not related to the IVs. \\[ E(\\varepsilon_i | x_{ij}) = 0 \\\\ \\] \"Endo\" and \"Exo\" in Greek means \"In\" and \"Out\" respectively, which is how the meaning of the words were derived. There are two key implications of Exogenity: By the Law of Total Expectations, the unconditional expectation of the error is also 0. By the Linearity of Conditional Expectations, the expectation of the product of the Error and IVs is 0. \\[ \\begin{aligned} E(\\varepsilon_i) = 0 \\\\ E(\\varepsilon_i x_{ij}) = 0 \\end{aligned} \\] Following these two implications, it can be shown that the Covariance between the Error and IVs are also 0, which is another consequence of independence (NOT the other way around). \\[ \\begin{aligned} Cov (\\varepsilon_i, x_{ij}) &= E(\\varepsilon_i x_{ij}) - E(\\varepsilon_i) * E(x_{ij}) \\\\ &= 0 - 0 * E(x_{ij}) \\\\ &= 0 \\end{aligned} \\] Without exogenity, the regression parameters would reflect the effect of both the IV and the unmodelled variable within the error term. This causes the OLS estimate to be biased , known as the Omitted Variable Bias . Since the unmodelled variable confounds the results of the regression, it is known as a Confounding Variable . Residual Analysis \u00b6 Since the errors are unobservable, the residuals are used to estimate the errors. If the fitted model is adequate - all relavent IVs are included in the right form, then the residuals should closely resemble the errors and therefore be structureless (random). However, if there are patterns in the residuals , it indicates that there is additional information that can be used to improve the model and thus should be included. Due to the OLS, the correlation between residuals and existing IVs will always be 0 indicating no linear relationship . To check for unmodelled non-linear relationships , a Residual Plot of the IVs against the Residuals can be used. For instance, if the residual plot shows a quadractic pattern (curve), then a quadractic IV should be added into the model. Mathematically, it can be expressed as a function of the existing estimates: \\[ \\begin{aligned} \\hat{y_i} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\varepsilon}_i \\\\ \\hat{\\varepsilon_i} &= \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\text{ (From residual plot)} \\\\ \\hat{y_i} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &= (\\hat{\\beta}_0 + \\hat{\\gamma}_0) + (\\hat{\\beta}_1 + \\hat{\\gamma}_1)x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &= \\hat{\\beta}_0' + \\hat{\\beta}_1'x + \\hat{\\beta}_2' x^2 \\end{aligned} \\] #3: No Perfect Collinearity \u00b6 Collinearity refers to when an IV can be expressed as a linear combination of one or more other IVs . Perfect collinearity is an extreme case where an IV can be perfectly expressed as a combination of another. Technically speaking, Collinearity refers to one to one variable relationship, while Multicollinearity refers to one to many variable relationship, hence \"Multi\". Variable is a multiple of another: \\(x_1 = cx_2\\) Variable differs by a constant from another: \\(x_1 = x_2 \\pm c\\) Variable is an affine transformation of another: Sum of several variables is fixed: Dummy Variable Trap The issue with perfect collinearity is that it affects the linear algebra used to solve for the coefficients (EG. Two equations to solve for three unknowns). There will be no unique solutions - many different values for the coefficients could work equally well. Imperfect Collinearity \u00b6 Imperfect Collinearity is a less extreme case where an IV is highly (but not perfectly) correlated with one or more other IVs. Recall that correlation refers to the extent of a Linear relationship. Non-linear relationships between variables are fine (EG. Polynomial Regression). This means that including the IV does not bring much additional predctive power into the model as its effects are already captured through related predictors and thus can be removed from the model. Unlike in the perfect case, high collinearity does not prevent OLS from finding a solution. However, the intepretation of the variables become complicated: The original intepretation of coefficients \"holding other variables constant\" is no longer true as highly correlated variables tend to move together. Thus, it is hard to seperate the effects of an individual variable . Consequently, OLS has difficulty estimating these coefficients, which could result in weird meaningless estimates ; EG. Large positive coefficient but large negative for its correlated counterpart. This also results in higher standard errors for the coefficients of correlated variables. This reduces the magnitude of t-statistic , which results in more false negatives , failing to reject the null when it should. This results in important variables being omitted from the regression. Technically speaking, there is nothing wrong with collinearity if the purpose of the model is solely for prediction. However, if the purpose of the model was to establish causality, then collinearity poses a problem as it interferes with statistical inference. Detecting Collinearity \u00b6 The simplest way to detect collinearity is through a Scatterplot or Correlation Matrix , which shows the correlations between pairs of variables . A correlation of 0.8 and higher is typically considered high enough where the collinearity becomes problematic. However, the issue is that this method can only detect collinearity between pairs of variable at a time. In order to detect collinearity among three or more variables ( multicollinearity ), then the Variance Inflation Factor (VIF) should be used instead. \\[ VIF = \\frac{1}{1-R^2_j} \\] The VIF is derived from the variance of the regression coefficient. As mentioned previously, under the presence of collinearity, the standard error and hence variance of the coefficient increases (\"inflated\"). The extent of the increase is known as the VIF. \\[ \\hat{Var}(\\hat{\\beta_1}) = \\frac{MS_{RSS}}{(n-1) s^2} * \\frac{1}{1-R^2_j} \\] The \\(R^2_j\\) in the VIF is the coefficient of determination of a model where the jth IV is regressed against all other IVs . A high \\(R^2_j\\) means that the IV is well explained by the other IVs (high correlation), which indicates the presence of collinearity. Generally, a \\(VIF > 10\\) is deemed to have severe collinearity . #4: No Extreme Outliers \u00b6 Outliers are observations with unusual values of the DV relative to fitted regression model. The last OLS assumption is that there are no extreme outliers in the dataset used to create the regression model. Generally, as long as the DV and IVs have a positive and finite Kurtosis , then the probability of such observations occuring are low. Outliers are problematic as OLS is sensitive to outliers . Extreme outliers have large residuals which receive more weight in the optimization process, which causes the resulting model to accomodate it (when it should not), causing the resulting coefficients to be biased. Identifying Outliers \u00b6 By definition, Outliers have unusually large residuals . In order to gauge what is considered a \"large residual\", the residuals are standardized for comparison. Standardization requires knowledge of the sampling distribution of the residuals . Given that the errors have a constant variance of \\(\\sigma^2\\) , the variance of the residuals can be shown to be: \\[ var(\\hat{\\varepsilon}_i) = \\sigma^2 (1-h_{ii}) \\] \\(h_ii\\) is known as the Leverage of the observation, which will be covered in the following section. The sampling distribution can then be determined: \\[ \\hat{\\varepsilon}_i ~ N(0, \\sigma^2 (1-h_{ii})) \\] Thus, the standardized residuals are the raw residuals scaled by their standard error: \\[ \\hat{\\varepsilon}_i^{\\text{standardized}} = \\frac{\\hat{\\varepsilon}_i}{\\sqrt{\\sigma^2 (1-h_{ii})}} \\] In practice, the variance of the errors are unknown, thus it is estimated using the Sample Variance of the residuals instead. Residuals with standardized values of larger than 2 or 3 are considered large and thus can be considered as an outlier. High Leverage Points \u00b6 While outliers are unusual points of the DV, High Leverage observations have unusual values of the IV relative to the majority of the values. It is easy to identify high leverage points when there is only one IV through a scatterplot - simply find the observation that is away from the rest. It becomes much more complicated when there are multiple IVs. The observation's values for each of the IVs could be in the common range of each IV, but unusual when taken collectively : The leverage of the observation can be determined from the MLR: \\[ \\begin{aligned} \\hat{\\boldsymbol{y}} &= \\boldsymbol{X}\\boldsymbol{\\beta} \\\\ &= \\boldsymbol{X}[(X'X)^{-1}y] \\\\ &= \\boldsymbol{H}\\boldsymbol{y} \\end{aligned} \\] \\(\\boldsymbol{H}\\) is known as the Hat Matrix as it puts a hat on y in the notation. The leverage of the \\(ith\\) observation is the \\(ith\\) diagonal element of the matrix, \\(h_{ii}\\) . An observation is considered to have high leverage if its leverage is greater than three times the average leverage: \\[ h_{ii} \\gt 3 \\left(\\frac{p+1}{n}\\right) \\] Influential Points \u00b6 The effect of Outliers and High leverage points can both be summarized into a concept known as Influence . An observation is influential if the exclusion of the observation from the regression leads to significantly differently results. In general the process involves three steps: Fit the original model with all \\(n\\) observations; determine the \\(j-th\\) fitted value \\(\\hat{y}_j\\) Fit an adjusted model with omitting the \\(i-th\\) observation, determine the \\(j-th\\) fitted value \\(\\hat{y}_{j(i)}\\) Calculate the change in the \\(j-th\\) fitted value This process has to be repeated for all fitted values for all observations . Cook's Distance summarizes the effect of the \\(i-th\\) observation on the whole model: \\[ D_i = \\frac{\\sum^n_{j=1} (\\hat{y}_j - \\hat{y}_{j(i)})^2}{(p+1) \\cdot MS_{Residuals}} \\] This method of computation requires \\(n+1\\) datasets - 1 dataset with all the observation and \\(n\\) datasets with the \\(i-th\\) observation omitted. It is also extremely time consuming to have to fit a model to each dataset. An alternative method of determining Cook's Distance is to make use of both the Outliers and Leverage: \\[ D_i = \\frac{1}{p+1} \\cdot (e^{\\text{standardized}})^2 \\cdot \\frac{h_{ii}}{1-h_{ii}} \\] Thus, an observation must be unusual in BOTH the DV and IV in order to be considered influential. Outliers and High leverage points are necessary but not sufficient conditions to be influential. Gauss Markov Assumptions \u00b6 #1: Conditional Homoscedasticity \u00b6 Homoscedasticity refers to the error terms having constant variance while Heteroscedasticity refers to having non-constant variance. \\[ var(\\varepsilon_i | x_i) = \\sigma^2 \\] Under homoscedasticity, the sampling distribution of the estimates are easily derived and thus it can be shown that they are the most efficient estimators. The same cannot be proven under heteroscedasticity. Note that that the OLS estimates are still unbiased; they are just not the most efficient. Identifying Heteroscedasticity \u00b6 Similar to before, if the model is adequate, then the residuals should resemble the errors and have constant variance . Thus, this can be easily determined through a residual plot of the Residuals against the fitted values. If the points are equally spread out about the mean (0) and show no pattern , then homoscedasticity is present. However, if the points show an increasing or decreasing variance (typically in the shape of a funnel ), then heteroscedasticity is present. Alternatively, a hypothesis test can be conducted to determine if heteroscedasticity is present, known as the Bresuch Pagan Test . \\[ \\begin{aligned} H_0 &: \\sigma^2 \\\\ H_1 &: \\sigma^2 + \\boldsymbol{Z\\gamma} \\end{aligned} \\] The test-statistic is computed as follows: Compute the squared standardized residuals from the original model Regress them onto the variables in Z (LOL need to change this part) Compute the RegSS of the new regression \\[ T = \\frac{RegSS}{2} \\] The test-statistic follows a chi-square distribution with \\(q\\) degrees of freedom, where \\(q\\) is the number of variables in \\(\\boldsymbol{Z}\\) . \\[ T \\sim \\chi^2_q \\] Dealing with Heteroscedasticity \u00b6 If prior information is known about the structure of the data, then the most intuitive method would be to incorporate that information into the data. \\[ Var(\\varepsilon_i) = \\frac{\\sigma^2}{w_i} \\] If no prior information is known, then the Heteroscedasticity can be reduced by using a Variance Stabilizing Transformation , such as the Log or Squareroot . Note that since they require positive data, a constant can be added to each term before the transformation to ensure that the values are positive. It is out of the scope for this set of notes to show why these help to stabilize variance. Alternatively, if there is only mild heteroscedasticity in the data, then OLS can be used but with an adjustment to the standard errors of the coefficients, known as heteroscedastic-robust standard errors . Due to complexity of the computations, it will not be covered in this set of notes. However, the general idea is that an weighted estimate of the variance covariance matrix is computed and the standard errors are computed from there. #2: No Serial Correlation \u00b6 If errors are correlated with one another, it is known as Serial Correlation or Autocorrelation . It implies that there are other unmodelled factors that can be used for prediction, which would imply the current model specification to be wrong. Thus, for the SLR model to be true, the errors must be independent of one another . \\[ Cov(\\varepsilon_i,\\varepsilon_j) = 0 \\] Confidence Intervals and PI are narrower than it should be > 95% PI is actually < 95%> P values lower > Appear statisticlaly significant when they shld not be Time series tends to have errors that are positively correlated, which is why it has its own dedicated section No Serial Correlation > Outcome of zero conditional mean, but most likely in time series data Error Distribution \u00b6 Although not needed for OLS estimation or Guass Markov, the errors of the regression are usually assumed to be normally distributed . \\[ \\varepsilon \\sim N(0, \\sigma^2) \\] If the errors are normally distributed, then it follows that \\(\\beta\\) is normally distributed as well since they are linear and additive. This greatly eases the computation needed to determine the sampling distribution for statistical inference. Q-Q Plots \u00b6 Since the errors are normally distributed, the residuals should be normally distributed as well. This can be verified using a Quantile-Quantile Plot (QQ Plot) , which compares the quantiles of two distributions. The first distribution is plotted on the x-axis while the second on the y-axis. If the quantiles are the same (same distribution), then the points should lie on \\(y = x\\) , the 45 degree line.","title":"Gauss Markov Theorem"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#gauss-markov-theorem","text":"Using OLS, the estimated regression parameters will always be unbiased under certain assumptions. The Gauss Markov Theorem extends this, which states that under certain assumptions, the OLS estimators will have the lowest variance among all possible linear unbiased estimators. In a statistics context, they are said to be the most efficient among all other linear unbiased estimators. In a regression context, the OLS estimators are said to be the Best Linear Unbiased Estimators (BLUE). This section will go over the various assumptions for both OLS and Gauss Markov Theorem. It will also cover the diagnostics to determine if the assumptions have been violated. The assumptions needed for OLS and the Gauss Markov theorem are often mixed up with each other as the assumptions needed for OLS are also needed for the theorem. This set of notes makes a clear distinction between the two.","title":"Gauss Markov Theorem"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#ols-assumptions","text":"","title":"OLS Assumptions"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#1-linearity","text":"Linear regression is a model where the relationship between the DV and IVs are linear. Thus, the regression parameters must be linear , but NOT the DV or IV. This means that the model is still considered a \"Linear Regression\" even after a transformation of the DV and/or IV. \\[ \\begin{aligned} y_i &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\\\ y_i &= \\beta_0 + \\beta_1 x^2 + \\beta_2 x^3 \\\\ \\ln y_i &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\end{aligned} \\]","title":"#1: Linearity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#2-exogenity","text":"Exogenity refers to how a variable comes from outside the model and is thus independent of any other variables within the model . In a regression context, this comes in the form of the errors having a conditional mean of 0 , ensuring that the errors are random and thus not related to the IVs. \\[ E(\\varepsilon_i | x_{ij}) = 0 \\\\ \\] \"Endo\" and \"Exo\" in Greek means \"In\" and \"Out\" respectively, which is how the meaning of the words were derived. There are two key implications of Exogenity: By the Law of Total Expectations, the unconditional expectation of the error is also 0. By the Linearity of Conditional Expectations, the expectation of the product of the Error and IVs is 0. \\[ \\begin{aligned} E(\\varepsilon_i) = 0 \\\\ E(\\varepsilon_i x_{ij}) = 0 \\end{aligned} \\] Following these two implications, it can be shown that the Covariance between the Error and IVs are also 0, which is another consequence of independence (NOT the other way around). \\[ \\begin{aligned} Cov (\\varepsilon_i, x_{ij}) &= E(\\varepsilon_i x_{ij}) - E(\\varepsilon_i) * E(x_{ij}) \\\\ &= 0 - 0 * E(x_{ij}) \\\\ &= 0 \\end{aligned} \\] Without exogenity, the regression parameters would reflect the effect of both the IV and the unmodelled variable within the error term. This causes the OLS estimate to be biased , known as the Omitted Variable Bias . Since the unmodelled variable confounds the results of the regression, it is known as a Confounding Variable .","title":"#2: Exogenity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#residual-analysis","text":"Since the errors are unobservable, the residuals are used to estimate the errors. If the fitted model is adequate - all relavent IVs are included in the right form, then the residuals should closely resemble the errors and therefore be structureless (random). However, if there are patterns in the residuals , it indicates that there is additional information that can be used to improve the model and thus should be included. Due to the OLS, the correlation between residuals and existing IVs will always be 0 indicating no linear relationship . To check for unmodelled non-linear relationships , a Residual Plot of the IVs against the Residuals can be used. For instance, if the residual plot shows a quadractic pattern (curve), then a quadractic IV should be added into the model. Mathematically, it can be expressed as a function of the existing estimates: \\[ \\begin{aligned} \\hat{y_i} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\varepsilon}_i \\\\ \\hat{\\varepsilon_i} &= \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\text{ (From residual plot)} \\\\ \\hat{y_i} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &= (\\hat{\\beta}_0 + \\hat{\\gamma}_0) + (\\hat{\\beta}_1 + \\hat{\\gamma}_1)x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &= \\hat{\\beta}_0' + \\hat{\\beta}_1'x + \\hat{\\beta}_2' x^2 \\end{aligned} \\]","title":"Residual Analysis"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#3-no-perfect-collinearity","text":"Collinearity refers to when an IV can be expressed as a linear combination of one or more other IVs . Perfect collinearity is an extreme case where an IV can be perfectly expressed as a combination of another. Technically speaking, Collinearity refers to one to one variable relationship, while Multicollinearity refers to one to many variable relationship, hence \"Multi\". Variable is a multiple of another: \\(x_1 = cx_2\\) Variable differs by a constant from another: \\(x_1 = x_2 \\pm c\\) Variable is an affine transformation of another: Sum of several variables is fixed: Dummy Variable Trap The issue with perfect collinearity is that it affects the linear algebra used to solve for the coefficients (EG. Two equations to solve for three unknowns). There will be no unique solutions - many different values for the coefficients could work equally well.","title":"#3: No Perfect Collinearity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#imperfect-collinearity","text":"Imperfect Collinearity is a less extreme case where an IV is highly (but not perfectly) correlated with one or more other IVs. Recall that correlation refers to the extent of a Linear relationship. Non-linear relationships between variables are fine (EG. Polynomial Regression). This means that including the IV does not bring much additional predctive power into the model as its effects are already captured through related predictors and thus can be removed from the model. Unlike in the perfect case, high collinearity does not prevent OLS from finding a solution. However, the intepretation of the variables become complicated: The original intepretation of coefficients \"holding other variables constant\" is no longer true as highly correlated variables tend to move together. Thus, it is hard to seperate the effects of an individual variable . Consequently, OLS has difficulty estimating these coefficients, which could result in weird meaningless estimates ; EG. Large positive coefficient but large negative for its correlated counterpart. This also results in higher standard errors for the coefficients of correlated variables. This reduces the magnitude of t-statistic , which results in more false negatives , failing to reject the null when it should. This results in important variables being omitted from the regression. Technically speaking, there is nothing wrong with collinearity if the purpose of the model is solely for prediction. However, if the purpose of the model was to establish causality, then collinearity poses a problem as it interferes with statistical inference.","title":"Imperfect Collinearity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#detecting-collinearity","text":"The simplest way to detect collinearity is through a Scatterplot or Correlation Matrix , which shows the correlations between pairs of variables . A correlation of 0.8 and higher is typically considered high enough where the collinearity becomes problematic. However, the issue is that this method can only detect collinearity between pairs of variable at a time. In order to detect collinearity among three or more variables ( multicollinearity ), then the Variance Inflation Factor (VIF) should be used instead. \\[ VIF = \\frac{1}{1-R^2_j} \\] The VIF is derived from the variance of the regression coefficient. As mentioned previously, under the presence of collinearity, the standard error and hence variance of the coefficient increases (\"inflated\"). The extent of the increase is known as the VIF. \\[ \\hat{Var}(\\hat{\\beta_1}) = \\frac{MS_{RSS}}{(n-1) s^2} * \\frac{1}{1-R^2_j} \\] The \\(R^2_j\\) in the VIF is the coefficient of determination of a model where the jth IV is regressed against all other IVs . A high \\(R^2_j\\) means that the IV is well explained by the other IVs (high correlation), which indicates the presence of collinearity. Generally, a \\(VIF > 10\\) is deemed to have severe collinearity .","title":"Detecting Collinearity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#4-no-extreme-outliers","text":"Outliers are observations with unusual values of the DV relative to fitted regression model. The last OLS assumption is that there are no extreme outliers in the dataset used to create the regression model. Generally, as long as the DV and IVs have a positive and finite Kurtosis , then the probability of such observations occuring are low. Outliers are problematic as OLS is sensitive to outliers . Extreme outliers have large residuals which receive more weight in the optimization process, which causes the resulting model to accomodate it (when it should not), causing the resulting coefficients to be biased.","title":"#4: No Extreme Outliers"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#identifying-outliers","text":"By definition, Outliers have unusually large residuals . In order to gauge what is considered a \"large residual\", the residuals are standardized for comparison. Standardization requires knowledge of the sampling distribution of the residuals . Given that the errors have a constant variance of \\(\\sigma^2\\) , the variance of the residuals can be shown to be: \\[ var(\\hat{\\varepsilon}_i) = \\sigma^2 (1-h_{ii}) \\] \\(h_ii\\) is known as the Leverage of the observation, which will be covered in the following section. The sampling distribution can then be determined: \\[ \\hat{\\varepsilon}_i ~ N(0, \\sigma^2 (1-h_{ii})) \\] Thus, the standardized residuals are the raw residuals scaled by their standard error: \\[ \\hat{\\varepsilon}_i^{\\text{standardized}} = \\frac{\\hat{\\varepsilon}_i}{\\sqrt{\\sigma^2 (1-h_{ii})}} \\] In practice, the variance of the errors are unknown, thus it is estimated using the Sample Variance of the residuals instead. Residuals with standardized values of larger than 2 or 3 are considered large and thus can be considered as an outlier.","title":"Identifying Outliers"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#high-leverage-points","text":"While outliers are unusual points of the DV, High Leverage observations have unusual values of the IV relative to the majority of the values. It is easy to identify high leverage points when there is only one IV through a scatterplot - simply find the observation that is away from the rest. It becomes much more complicated when there are multiple IVs. The observation's values for each of the IVs could be in the common range of each IV, but unusual when taken collectively : The leverage of the observation can be determined from the MLR: \\[ \\begin{aligned} \\hat{\\boldsymbol{y}} &= \\boldsymbol{X}\\boldsymbol{\\beta} \\\\ &= \\boldsymbol{X}[(X'X)^{-1}y] \\\\ &= \\boldsymbol{H}\\boldsymbol{y} \\end{aligned} \\] \\(\\boldsymbol{H}\\) is known as the Hat Matrix as it puts a hat on y in the notation. The leverage of the \\(ith\\) observation is the \\(ith\\) diagonal element of the matrix, \\(h_{ii}\\) . An observation is considered to have high leverage if its leverage is greater than three times the average leverage: \\[ h_{ii} \\gt 3 \\left(\\frac{p+1}{n}\\right) \\]","title":"High Leverage Points"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#influential-points","text":"The effect of Outliers and High leverage points can both be summarized into a concept known as Influence . An observation is influential if the exclusion of the observation from the regression leads to significantly differently results. In general the process involves three steps: Fit the original model with all \\(n\\) observations; determine the \\(j-th\\) fitted value \\(\\hat{y}_j\\) Fit an adjusted model with omitting the \\(i-th\\) observation, determine the \\(j-th\\) fitted value \\(\\hat{y}_{j(i)}\\) Calculate the change in the \\(j-th\\) fitted value This process has to be repeated for all fitted values for all observations . Cook's Distance summarizes the effect of the \\(i-th\\) observation on the whole model: \\[ D_i = \\frac{\\sum^n_{j=1} (\\hat{y}_j - \\hat{y}_{j(i)})^2}{(p+1) \\cdot MS_{Residuals}} \\] This method of computation requires \\(n+1\\) datasets - 1 dataset with all the observation and \\(n\\) datasets with the \\(i-th\\) observation omitted. It is also extremely time consuming to have to fit a model to each dataset. An alternative method of determining Cook's Distance is to make use of both the Outliers and Leverage: \\[ D_i = \\frac{1}{p+1} \\cdot (e^{\\text{standardized}})^2 \\cdot \\frac{h_{ii}}{1-h_{ii}} \\] Thus, an observation must be unusual in BOTH the DV and IV in order to be considered influential. Outliers and High leverage points are necessary but not sufficient conditions to be influential.","title":"Influential Points"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#gauss-markov-assumptions","text":"","title":"Gauss Markov Assumptions"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#1-conditional-homoscedasticity","text":"Homoscedasticity refers to the error terms having constant variance while Heteroscedasticity refers to having non-constant variance. \\[ var(\\varepsilon_i | x_i) = \\sigma^2 \\] Under homoscedasticity, the sampling distribution of the estimates are easily derived and thus it can be shown that they are the most efficient estimators. The same cannot be proven under heteroscedasticity. Note that that the OLS estimates are still unbiased; they are just not the most efficient.","title":"#1: Conditional Homoscedasticity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#identifying-heteroscedasticity","text":"Similar to before, if the model is adequate, then the residuals should resemble the errors and have constant variance . Thus, this can be easily determined through a residual plot of the Residuals against the fitted values. If the points are equally spread out about the mean (0) and show no pattern , then homoscedasticity is present. However, if the points show an increasing or decreasing variance (typically in the shape of a funnel ), then heteroscedasticity is present. Alternatively, a hypothesis test can be conducted to determine if heteroscedasticity is present, known as the Bresuch Pagan Test . \\[ \\begin{aligned} H_0 &: \\sigma^2 \\\\ H_1 &: \\sigma^2 + \\boldsymbol{Z\\gamma} \\end{aligned} \\] The test-statistic is computed as follows: Compute the squared standardized residuals from the original model Regress them onto the variables in Z (LOL need to change this part) Compute the RegSS of the new regression \\[ T = \\frac{RegSS}{2} \\] The test-statistic follows a chi-square distribution with \\(q\\) degrees of freedom, where \\(q\\) is the number of variables in \\(\\boldsymbol{Z}\\) . \\[ T \\sim \\chi^2_q \\]","title":"Identifying Heteroscedasticity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#dealing-with-heteroscedasticity","text":"If prior information is known about the structure of the data, then the most intuitive method would be to incorporate that information into the data. \\[ Var(\\varepsilon_i) = \\frac{\\sigma^2}{w_i} \\] If no prior information is known, then the Heteroscedasticity can be reduced by using a Variance Stabilizing Transformation , such as the Log or Squareroot . Note that since they require positive data, a constant can be added to each term before the transformation to ensure that the values are positive. It is out of the scope for this set of notes to show why these help to stabilize variance. Alternatively, if there is only mild heteroscedasticity in the data, then OLS can be used but with an adjustment to the standard errors of the coefficients, known as heteroscedastic-robust standard errors . Due to complexity of the computations, it will not be covered in this set of notes. However, the general idea is that an weighted estimate of the variance covariance matrix is computed and the standard errors are computed from there.","title":"Dealing with Heteroscedasticity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#2-no-serial-correlation","text":"If errors are correlated with one another, it is known as Serial Correlation or Autocorrelation . It implies that there are other unmodelled factors that can be used for prediction, which would imply the current model specification to be wrong. Thus, for the SLR model to be true, the errors must be independent of one another . \\[ Cov(\\varepsilon_i,\\varepsilon_j) = 0 \\] Confidence Intervals and PI are narrower than it should be > 95% PI is actually < 95%> P values lower > Appear statisticlaly significant when they shld not be Time series tends to have errors that are positively correlated, which is why it has its own dedicated section No Serial Correlation > Outcome of zero conditional mean, but most likely in time series data","title":"#2: No Serial Correlation"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#error-distribution","text":"Although not needed for OLS estimation or Guass Markov, the errors of the regression are usually assumed to be normally distributed . \\[ \\varepsilon \\sim N(0, \\sigma^2) \\] If the errors are normally distributed, then it follows that \\(\\beta\\) is normally distributed as well since they are linear and additive. This greatly eases the computation needed to determine the sampling distribution for statistical inference.","title":"Error Distribution"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#q-q-plots","text":"Since the errors are normally distributed, the residuals should be normally distributed as well. This can be verified using a Quantile-Quantile Plot (QQ Plot) , which compares the quantiles of two distributions. The first distribution is plotted on the x-axis while the second on the y-axis. If the quantiles are the same (same distribution), then the points should lie on \\(y = x\\) , the 45 degree line.","title":"Q-Q Plots"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/","text":"Statistical Learning \u00b6 The regression concepts covered in the previous sections are widely considered to be traditional applied statistics . In a contemporary context, regression is just one of the many methods that fall under Statistical Learning . It is a framework of harnessing data to gain an understanding of how the data is related to one another and/or how a group of variables can be used to accurately predict another. Similar to regression, the relationship between variables can be expressed as a combination of a Signal Function and a Noise term: \\[ y = f(x) + \\varepsilon \\] The two terms originate from Engineering, where Signal refers to the meaningful component of the data while Noise refers to the random variation that inteferes with the signal. Statistical learning models are distinguished based on their signal function. There are two main kinds of models: Parametric Models Non-Parametric Assumes DV follows a specific functional form Does not assume any functional form Function parameters determined from the data - Does not require a large amount of data Requires a large amount of data to work well EG. Linear Regression EG. Clustering | May not fit the data well | Fits the data well | | Described by parameters | No parameters used | | Simple to implement | Requires large amount of data to implement | | Risk that assumed function is wrong | Does not make any assumption about data | They can also be further split according to: Supervised Learning Unsupervised Learning Specified DV to supervise the learning No specific variable chosen Inference/Prediction with respect to the DV Inference/Prediction for all variables EG. Linear Regression EG. Clustering Regression vs classification classification - classifying the observations to a certian level Model Accuracy \u00b6 Since the end goal of the model is to make predictions on new unobserved data, the quality of the model should be evaluated against its performance on unobserved data as well. The observed data used to create the model is known as the Training Data as it helps train the signal function to identify relationships between variables, while the unobserved data used to evaluate the model is known as the Test Data . The quality of the model can then be quantified by the extent to which the model predictions match the data . Similar to regression, this quantity is known as the Error of the model and is summarized through the Mean Square Error (MSE) statistic. The MSE is the average of the sum of squared errors and can be calculated for both the training and test data: \\[ \\begin{aligned} \\text{Training MSE} &= \\frac{[y_i - \\hat{f}(x_0)]^2}{n_{training}} \\\\ \\text{Test MSE} &= \\frac{[y_0 - \\hat{f}(x_0)]^2}{n_{test}} \\end{aligned} \\] Note that this is different from the MSE defined in ANOVA, where the MS is divded by its degrees of freedom rather than the number of observations. The MSE defined in this section is a general concept while the ANOVA MSE is a purely regression concept. The Training MSE reflects the goodness of fit of the model while the Test MSE reflects its prediction accuracy. As alluded to earlier, the goal of statistical learning is to choose the model with the lowest Test MSE . In general, the training MSE should always be smaller than the testing MSE, which is why they are not interchangeable. This is because all models are trained to match the training data to various extents, thus they should naturally have relatively smaller errors. On the flipside, test MSE should always be higher because the model is likely to have mistakenly captured some of the noise in the training data that do not generalize to the test data, resulting in a higher testing error. The extent of the difference is dependent on how well the model fits the training data; the extent to which it learns from it: High Flexibility/Complexity - Tends to overfit the training data; matches data too much; learns too much Low Flexibility/Complexity - Tends to underfit the training data; matches data too little; learns too little This is not to say that low flexibility models are better. In fact, some level of flexibility is needed for the model to pick up most of the signals in the training data, but not too much such that the noise is captured as well. Thus, the test MSE generally decreases with flexibility up till a certain point, following which it increases, forming a U shaped curve : Bias Variance Tradeoff \u00b6 The test MSE can be better understood by decomposing it into its consistuent commponents: \\[ \\text{MSE} = \\text{Bias}[\\hat{f}(x_0)]^2 + \\text{Variance}[\\hat{f}(x_0)] \\] The Bias of the model (also known as the Accuracy ) is the difference in expected value of the estimated signal function and the true signal function. More complex models are better able to capture the signal in the data, thus tends to have a lower bias. The Variance of the model (also known as Precision ) is the change in the estimated signal function across different datasets. Ideally, the model should have low variance such that it would be relatively stable across different training data. While more complex models are better able to capture signals, this makes them prone to overfitting and hence more sensitive to differences in the training data , leading to higher variance. Note that although Precision & Accuracy are both synonyms in English, they have distinct meanings in statistics. Ideally, a model should have both low bias and low variance. However, as explained above, there is an inherent tension between Bias and Variance due to the complexity of the model, known as the Bias-Variance Tradeoff . A relatively simple model (underfitted) tends to have a high bias but low variance . As the complexity increases, the bias initially decreases more than the variance increases , causing the test MSE to fall. At some point, the model becomes too complex (overfitted), where the increase in variance outweighs the fall in bias , resulting in the U-shaped curve as seen previously. Thus, the goal is to find an optimal balance in between Bias and Variance where the test MSE is minimized. Resampling Methods \u00b6 Validation Set \u00b6 LOO Cross Validation \u00b6 K fold Cross Validation \u00b6 Model Selection \u00b6 Feature Selection \u00b6 Forward Stepwise Selection \u00b6 Backward Stepwise Selection \u00b6 Stepwise Selection \u00b6 Shrinkage Methods \u00b6","title":"Statistical Learning"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#statistical-learning","text":"The regression concepts covered in the previous sections are widely considered to be traditional applied statistics . In a contemporary context, regression is just one of the many methods that fall under Statistical Learning . It is a framework of harnessing data to gain an understanding of how the data is related to one another and/or how a group of variables can be used to accurately predict another. Similar to regression, the relationship between variables can be expressed as a combination of a Signal Function and a Noise term: \\[ y = f(x) + \\varepsilon \\] The two terms originate from Engineering, where Signal refers to the meaningful component of the data while Noise refers to the random variation that inteferes with the signal. Statistical learning models are distinguished based on their signal function. There are two main kinds of models: Parametric Models Non-Parametric Assumes DV follows a specific functional form Does not assume any functional form Function parameters determined from the data - Does not require a large amount of data Requires a large amount of data to work well EG. Linear Regression EG. Clustering | May not fit the data well | Fits the data well | | Described by parameters | No parameters used | | Simple to implement | Requires large amount of data to implement | | Risk that assumed function is wrong | Does not make any assumption about data | They can also be further split according to: Supervised Learning Unsupervised Learning Specified DV to supervise the learning No specific variable chosen Inference/Prediction with respect to the DV Inference/Prediction for all variables EG. Linear Regression EG. Clustering Regression vs classification classification - classifying the observations to a certian level","title":"Statistical Learning"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#model-accuracy","text":"Since the end goal of the model is to make predictions on new unobserved data, the quality of the model should be evaluated against its performance on unobserved data as well. The observed data used to create the model is known as the Training Data as it helps train the signal function to identify relationships between variables, while the unobserved data used to evaluate the model is known as the Test Data . The quality of the model can then be quantified by the extent to which the model predictions match the data . Similar to regression, this quantity is known as the Error of the model and is summarized through the Mean Square Error (MSE) statistic. The MSE is the average of the sum of squared errors and can be calculated for both the training and test data: \\[ \\begin{aligned} \\text{Training MSE} &= \\frac{[y_i - \\hat{f}(x_0)]^2}{n_{training}} \\\\ \\text{Test MSE} &= \\frac{[y_0 - \\hat{f}(x_0)]^2}{n_{test}} \\end{aligned} \\] Note that this is different from the MSE defined in ANOVA, where the MS is divded by its degrees of freedom rather than the number of observations. The MSE defined in this section is a general concept while the ANOVA MSE is a purely regression concept. The Training MSE reflects the goodness of fit of the model while the Test MSE reflects its prediction accuracy. As alluded to earlier, the goal of statistical learning is to choose the model with the lowest Test MSE . In general, the training MSE should always be smaller than the testing MSE, which is why they are not interchangeable. This is because all models are trained to match the training data to various extents, thus they should naturally have relatively smaller errors. On the flipside, test MSE should always be higher because the model is likely to have mistakenly captured some of the noise in the training data that do not generalize to the test data, resulting in a higher testing error. The extent of the difference is dependent on how well the model fits the training data; the extent to which it learns from it: High Flexibility/Complexity - Tends to overfit the training data; matches data too much; learns too much Low Flexibility/Complexity - Tends to underfit the training data; matches data too little; learns too little This is not to say that low flexibility models are better. In fact, some level of flexibility is needed for the model to pick up most of the signals in the training data, but not too much such that the noise is captured as well. Thus, the test MSE generally decreases with flexibility up till a certain point, following which it increases, forming a U shaped curve :","title":"Model Accuracy"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#bias-variance-tradeoff","text":"The test MSE can be better understood by decomposing it into its consistuent commponents: \\[ \\text{MSE} = \\text{Bias}[\\hat{f}(x_0)]^2 + \\text{Variance}[\\hat{f}(x_0)] \\] The Bias of the model (also known as the Accuracy ) is the difference in expected value of the estimated signal function and the true signal function. More complex models are better able to capture the signal in the data, thus tends to have a lower bias. The Variance of the model (also known as Precision ) is the change in the estimated signal function across different datasets. Ideally, the model should have low variance such that it would be relatively stable across different training data. While more complex models are better able to capture signals, this makes them prone to overfitting and hence more sensitive to differences in the training data , leading to higher variance. Note that although Precision & Accuracy are both synonyms in English, they have distinct meanings in statistics. Ideally, a model should have both low bias and low variance. However, as explained above, there is an inherent tension between Bias and Variance due to the complexity of the model, known as the Bias-Variance Tradeoff . A relatively simple model (underfitted) tends to have a high bias but low variance . As the complexity increases, the bias initially decreases more than the variance increases , causing the test MSE to fall. At some point, the model becomes too complex (overfitted), where the increase in variance outweighs the fall in bias , resulting in the U-shaped curve as seen previously. Thus, the goal is to find an optimal balance in between Bias and Variance where the test MSE is minimized.","title":"Bias Variance Tradeoff"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#resampling-methods","text":"","title":"Resampling Methods"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#validation-set","text":"","title":"Validation Set"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#loo-cross-validation","text":"","title":"LOO Cross Validation"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#k-fold-cross-validation","text":"","title":"K fold Cross Validation"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#model-selection","text":"","title":"Model Selection"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#feature-selection","text":"","title":"Feature Selection"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#forward-stepwise-selection","text":"","title":"Forward Stepwise Selection"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#backward-stepwise-selection","text":"","title":"Backward Stepwise Selection"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#stepwise-selection","text":"","title":"Stepwise Selection"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#shrinkage-methods","text":"","title":"Shrinkage Methods"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/6.%20Generalized%20Linear%20Models/","text":"","title":"6. Generalized Linear Models"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/7.%20Time%20Series%20Models/","text":"","title":"7. Time Series Models"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/8.%20Tree%20Models/","text":"","title":"8. Tree Models"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/9.%20Principal%20Component%20Analysis/","text":"","title":"9. Principal Component Analysis"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/","text":"Review of Statistical Theory \u00b6 This section assumes some basic knowledge on Random Variables , which can be found under another set of notes covering a Review of Probability Theory . Overview of Statistics \u00b6 Statistics is a discipline revolving around data. A Population refers to the theoretical set of all possible data of the event of interest. The goal of statistics is to determine certain attributes that summarizes or describes the population, known as Parameters . However, it is impossible to study the entire population at once, thus a subset of the population is studied instead, known as the Sample . Attributes that summarize or describe the sample are known as Statistics . Ideally, the sample is representative of the population, which means that findings from the sample can be applied to the population as a whole. This means that the sample statistics can be used estimate population parameters. We distinguish between the two (when they have the same notation) through the Hat accent (^) - Population Parameters are their written without the hat ( \\(x\\) ) while their corresponding sample statistics are written with the hat ( \\(\\hat{x}\\) ). Common Sample Statistics \u00b6 The Mean is the average of the population. Population Mean Sample Mean \\(\\mu = \\sum\\limits_{i=i}^n x_i * p(x_i)\\) \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=i}^n x_i\\) The Variance measures the spread of values about the mean. However, the units of Variance are unintuitive, thus we consider the Standard Deviation for more practical purposes, which is the square root of the variance . Population Variance Sample Variance \\(\\sigma^2 = \\sum\\limits_{i=i}^n (x_i - \\mu)^2 * p(x_i)\\) \\(s^2 = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x})^2\\) Covariance is a measure of the linear relationship between two variables: Positive Covariance - Variables move in the same direction Negative Covariance - Variables move in opposite directions Population Covariance Sample Covariance \\(\\sigma_{x, y} = \\mu_{xy} - \\mu_x \\mu_y\\) \\(\\hat{\\sigma}_{x, y} = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x}) * (y_i - \\bar{y})\\) However, there are two issues with Covariance - the units are unintuitive (similar to variance) and there is no benchmark as to what constitutes a strong/weak relationship. Thus, the Correlation is an adjusted measure of the relationship between -1 and 1 . Population Correlation Sample Correlation \\(\\rho = \\frac{\\sigma_{x,y}}{\\sigma_x * \\sigma_y}\\) \\(r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x} \\sum(y_i - \\bar{y})}\\) Properties of Estimators Biased Consistent Efficient Degrees of freedom Sampling Distribution \u00b6 Whenever a sample is drawn from a population and a statistic is calculated, it is known as a Point Estimate . Due to measurement error, a different sample would be drawn each time, thus leading to a different point estimate . If this process were to be repeated a large number times, the probability distribution of the resulting point estimates is known as the Sampling Distribution of the statistic. There is no rule surrounding the sampling distribution - it depends on the distribution of the population, statistic being measured, sampling method etc. The standard deviation of this sampling distribution is known as the Standard Error of the statistic: \\[ \\sigma_{\\theta} = \\sqrt {\\sigma^2_{\\theta}} \\] A special case is the Sample Mean . If the population is normally distributed, then it is normally distributed as well. Regardless of the population distribution, it is also approximately normally distributed through the Central Limit Theorem or Law of Large Numbers . \\[ \\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\] Following that, we can compute the standard error: \\[ \\sigma_{\\bar{x}} = \\sqrt \\frac{\\sigma^2_{\\bar{x}}}{n} = \\frac{\\sigma}{\\sqrt{n}} \\] However, the population variance is usually unknown. Thus, it can be approximated using the Sample Variance, which is an unbiased estimator for it. The result is known as an Estimate for the Standard Error : \\[ \\hat{\\sigma}_{\\bar{x}} = \\sqrt \\frac{s^2}{n} = \\frac{s}{\\sqrt{n}} \\] Note that only the sample variance is an unbiased estimator for the population variance. Although it may look like it, the sample SD is NOT an unbiased estimator for the population SD. Confidence Interval \u00b6 Given that there is only one true value for the population parameter and a whole distribution of estimators, it is unlikely that a point estimate will be equal to the population parameter. Thus, instead of a point estimate, a range of estimates is used, known as a Confidence Interval . The interval is made a chosen Confidence Level which represents the proportion of confidence intervals that will contain the true value . In other words, if a large number of confidence intervals constructed in the same manner were to be made, \\((1-\\alpha)%\\) of them would contain the true value. \\[ P(- \\text{Margin of Error} < \\theta < \\text{Margin of Error}) = 1 - \\alpha \\] The Margin of Error represents the range of values on either side of the point estimate that the true value could lie. For instance, for a 95% confidence interval, the confidence lies within the 0.025 and 0.975 percentile of the sampling distribution. The margin of error can be calculated by finding the corresponding values of the sampling distribution at these percentiles. Consider the 95% confidence interval for the Sample Mean , which is normally distributed. For convenience, it is usually normalized such that it will become a Standard Normal Distribution : \\[ \\begin{aligned} P(-1.96 < \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt n}} < 1.96) &= 0.95 \\\\ P(\\bar{x} - 1.96 \\frac{\\sigma}{\\sqrt n} < \\mu < \\bar{x} + 1.96 \\frac{\\sigma}{\\sqrt n}) &= 0.95 \\\\ \\end{aligned} \\] \\[ \\therefore \\text{Margin of Error} = \\bar{x} + Z_{\\frac{\\alpha}{2}} * \\frac{\\sigma}{\\sqrt n} \\] Hypothesis Testing \u00b6 Hypothesis Testing is a formal method of making inferences about the population parameters based on the sample statistic. It starts with a Hypothesis which is a conjecture about the population parameters: Null Hypothesis - What is currently believed to be true Alternative Hypothesis - What is to be proven A Test Statistic is then calculated, which would quantify the behaviour of population such that it would distinguish the null and alternative hypothesis. Assuming the Null Hypothesis is true , the sampling distribution of the test statistic is determined. From the sampling distribution, the p-value is calculated, which is the probability of observing the calculated sample statistic or more extreme . If the p-value is smaller than a pre-determined level of Statistical Significance ( \\(\\alpha\\) ), then the test is \"successful\" and the null hypothesis is rejected. If not, then the test \"fails\" and the null is not rejected. Note that it rejecting the null hypothesis does NOT mean that the alternative hypothesis is accepted - EG. Rejected then hypothesis of a large paw print being from a bear does not mean that it is from Bigfoot. Thus, the hypotheses are usually constructed such that the two hypothesis are complementary , such that rejecting the null allows acceptance of the alternative, leading to a definitive insight. In layman terms, a hypothesis test is a test of extremeness . Assuming the null hypothesis is true, how extreme (low probability) is the observed sample? If the p-value is sufficiently low, it means that the sample is rare - it could be due to pure chance or that it is actually not rare because the null is not true. We distinguish between the two mathematically through \\(\\alpha\\) . It is the probability of a False Positive - that the result obtained was by pure chance. It is typically set at 5%, which means that 5% of all statistics calculated are expected to be extreme under the null. Thus, if the p-value of the sample is smaller than \\(\\alpha\\) , then it is likely that the observation was not due to chance but instead because the null was false . Alternatively, instead of comparing p-value to \\(alpha\\) , the test-statistic and the corresponding value of \\(\\alpha\\) on the sampling distribution can be used. It is known as the Critical Value , which represents the boundary of Reject Null Do not Reject Null p-value smaller than \\(\\alpha\\) p-value smaller than \\(\\alpha\\) test-statistic larger than critical value test-statistic smaller than critical value Let the random variable \\(T\\) denote the test statistics. There are many different kinds of test statistics depending on the distribution and what is being investigated. Z-statistic \u00b6 The most simple test statistic involve the Sample Mean , which is normally distributed: \\[ T = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] If the population variance is known , then the test statistic has a Standard Normal Distribution and thus the test-statistic is known as a Z-Statistic . \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\sqrt n \\\\ &= Z * \\sqrt n \\\\ \\therefore T &\\sim N(0,1) \\end{aligned} \\] t-statistic \u00b6 If the population variance is unknown, then it will be approximated by the Sample Variance . Through algebraic manipulation, the test statistic can still be expressed in the form of a Z variable, but with an additional term: \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}} \\\\ &= \\frac{\\bar{x} - \\mu}{s} * \\sqrt n \\\\ &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\frac{\\sigma}{s} * \\sqrt n \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{s^2}{\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{(n-1)s^2}{(n-1)\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt {\\frac{(n-1)s^2}{\\sigma^2} * \\frac{1}{n-1}} \\\\ \\end{aligned} \\] The additional term can be shown to have a Chi-Squared Distribution of \\(n-1\\) degrees of freedom, which by definition is the sum of \\(n-1\\) independent standard normal variables: \\[ \\begin{aligned} \\chi^2_n &= Z^2 \\\\ &= \\sum \\left(\\frac {\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &= \\sum \\left(\\frac {(x_i - \\bar{x}) + (\\bar{x} - \\mu)}{\\sigma}\\right)^2 \\\\ &= \\sum \\left(\\frac{x_i - \\bar{x}}{\\sigma}\\right)^2 + \\sum \\left(\\frac{\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &= \\frac{1}{\\sigma^2} \\sum (x_i - \\bar{x})^2 + \\sum Z^2 + 0 \\\\ &= \\frac{(n-1)}{\\sigma^2} \\frac{\\sum (x_i - \\bar{x})^2}{n-1} + \\chi^2_1 \\\\ &= \\frac{(n-1)s^2}{\\sigma^2} + \\chi^2_1 \\\\ \\end{aligned} \\] \\[\\therefore \\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\] Thus, the test statistic is the ratio of a Standard Normal Variable to the squareroot of a Chi Squared Variable (divided by its degrees of freedom). By definition, this test statistic has a t-distribution with the same degrees of freedom and is known as the t-statistic : \\[ \\begin{align*} T &= \\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\\\ &= t_{n-1} * \\sqrt{n} \\\\ \\end{align*} \\] \\[\\therefore T \\sim t_{n-1}\\] Despite the slightly convoluted proof, the t-distribution is simply a standard normal distribution with heavier tails . This means that extreme values are slightly more likely, which is meant to account for the increase in variability due to the use of the sample variance rather than the population variance. F-statistic \u00b6 The square of t-statisic has an F-Distribution , which is defined as the ratio of two independent chi-square variables (divided by their respective degrees of freedom). It has two dimensions for its degree of freedom, reflecting the two chi-square variables. \\[ F_{m,n} = \\frac{\\frac{\\chi_m}{m}}{\\frac{\\chi_n}{n}} \\] The square of a t-statistic follows an F-distribution because: The square of the standard normal variable in the numerator becomes a \\(\\chi_1\\) variable The squareroot is removed in the denominator, becoming a \\(\\chi_{n-1}\\) over its degree of freedom \\[ \\begin{aligned} t_{n-1}^2 &= \\left(\\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\right)^2 \\\\ &= \\frac{\\chi_1}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &= \\frac{\\frac{\\chi_1}{1}}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &= F_{1,n-1} * n \\end{aligned} \\] \\[ \\therefore t_{n-1}^2 \\sim F_{1, n-1} \\] Thus, the square of the t-statistic is known as the F-statistic , which is usually used to test for the equality of variance . Maximum Likelihood Estimation \u00b6 If the population distribution is known , there is an alternative method of estimating the parameters apart from calculating the corresponding sample statistics, known as Maximum Likelihood Estimation (MLE). There are an infinite number of variations of the distribution that could have resulted in the sample, each with different parameters . Technically speaking, any set of parameters could have resulted in the sample. However, the goal of MLE is to find the set of parameters that are most likely to result in the sample ; in other words, the probability of obtaining this sample is the highest with this set of paramters than any other set. The probability of obtaining the sample is known as its Likelihood : \\[ L(\\theta \\mid x) = P_{\\theta} (X = x) \\] Warning Likelihood functions and PMF/PDFs are often confused with one another as they involve the same expression. The key is understanding what is given and what is random , which results in the subtle but differing notation: PMF/PDF : Given parameters, outcomes are random; \\(P_{X}(x)\\) Likelihood : Given outcomes, parameters are random; \\(P_{\\theta}(x)\\) Assuming that the sample is iid, the likehood for the entire sample is the product of the likelihood for each observation , known as the Likelihood Function : \\[ L(\\theta) = \\prod P_{\\theta}(X = x_i) \\] The goal is to find the parameters that maximizes the likelihood function through calculus: \\[ \\frac{d}{d\\theta} L(\\theta) = 0 \\] In practice, especially when dealing with multiple parameters, the likelihood function is complicated to work with. Thus, a log transformation is often applied to simplify it, turning the product into a summation . This is known as the Log-Likelihood Function . Since the logarithm transform is monotonic, both the likelihood and log-likelihood functions share the same maximum . \\[ \\begin{aligned} \\ell (\\theta) &= \\ln L(\\theta) \\\\ \\therefore \\frac{d}{d\\theta} \\ell (\\theta) &= 0 \\end{aligned} \\] Practical Tips \u00b6 Some distributions have complicated PMF/PDFs that make working with them more complicated. Most questions will usually have some method to simplify the likelihood function . The first tip is to understand that since the likelihood function will be logarithm transformed and then differentiated, factors that contains ONLY constants can be dropped since they will inevitably be removed later: \\[ \\begin{aligned} L(\\theta) &= a * x \\\\ \\ell (\\theta) &= \\ln a + \\ln x \\\\ \\ell' (\\theta) &= \\frac{1}{x} \\\\ \\\\ \\therefore L(\\theta) \\propto x \\end{aligned} \\] The next tip is that if the parameters are embedded in the power of some constant, they should be combined together: \\[ \\begin{aligned} L(\\theta) &= a^{\\theta} \\cdot b^{\\theta} \\cdot c^{\\theta} \\\\ &= (a \\cdot b \\cdot c)^{\\theta} \\end{aligned} \\] The reverse also applies...divide power, may not be in the same term, could come from another term However, if the above terms for some reason are added instead of multiplied , then a substituition method would be better: \\[ \\begin{aligned} L(\\theta) &= e^{-\\frac{k}{100}} - \\left(e^{-\\frac{k}{100}} \\right)^2 \\\\ L(\\theta) &= p - p^2 \\end{aligned} \\] Method of Moments \u00b6 An alternative method for estimating population parameters is the Method of Moments (MOM). It is based on the Law of Large Numbers , which states that the sample mean converges to the population mean (first raw moment), given a sufficiently large sample size . Thus, by equating the sample raw moments to the population raw moments , up to the number of parameters to estimate, we can solve for an estimate of the parameters. Note If there is only one parameter to estimate, then only the first raw moments are equated. If there are two, then the second raw moment should be equated as well. \\[ \\begin{aligned} E(X^k) &= \\bar{x} \\\\ &= \\frac{\\sum x^k_i}{n} \\end{aligned} \\] The main advantage of this method is that it is computationally simpler than MLE. For certain known distributions, the MOM estimate and MLE estimate are the same, thus MOM can be used as a shortcut for MLE .","title":"Review of Statistical Theory"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#review-of-statistical-theory","text":"This section assumes some basic knowledge on Random Variables , which can be found under another set of notes covering a Review of Probability Theory .","title":"Review of Statistical Theory"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#overview-of-statistics","text":"Statistics is a discipline revolving around data. A Population refers to the theoretical set of all possible data of the event of interest. The goal of statistics is to determine certain attributes that summarizes or describes the population, known as Parameters . However, it is impossible to study the entire population at once, thus a subset of the population is studied instead, known as the Sample . Attributes that summarize or describe the sample are known as Statistics . Ideally, the sample is representative of the population, which means that findings from the sample can be applied to the population as a whole. This means that the sample statistics can be used estimate population parameters. We distinguish between the two (when they have the same notation) through the Hat accent (^) - Population Parameters are their written without the hat ( \\(x\\) ) while their corresponding sample statistics are written with the hat ( \\(\\hat{x}\\) ).","title":"Overview of Statistics"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#common-sample-statistics","text":"The Mean is the average of the population. Population Mean Sample Mean \\(\\mu = \\sum\\limits_{i=i}^n x_i * p(x_i)\\) \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=i}^n x_i\\) The Variance measures the spread of values about the mean. However, the units of Variance are unintuitive, thus we consider the Standard Deviation for more practical purposes, which is the square root of the variance . Population Variance Sample Variance \\(\\sigma^2 = \\sum\\limits_{i=i}^n (x_i - \\mu)^2 * p(x_i)\\) \\(s^2 = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x})^2\\) Covariance is a measure of the linear relationship between two variables: Positive Covariance - Variables move in the same direction Negative Covariance - Variables move in opposite directions Population Covariance Sample Covariance \\(\\sigma_{x, y} = \\mu_{xy} - \\mu_x \\mu_y\\) \\(\\hat{\\sigma}_{x, y} = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x}) * (y_i - \\bar{y})\\) However, there are two issues with Covariance - the units are unintuitive (similar to variance) and there is no benchmark as to what constitutes a strong/weak relationship. Thus, the Correlation is an adjusted measure of the relationship between -1 and 1 . Population Correlation Sample Correlation \\(\\rho = \\frac{\\sigma_{x,y}}{\\sigma_x * \\sigma_y}\\) \\(r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x} \\sum(y_i - \\bar{y})}\\) Properties of Estimators Biased Consistent Efficient Degrees of freedom","title":"Common Sample Statistics"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#sampling-distribution","text":"Whenever a sample is drawn from a population and a statistic is calculated, it is known as a Point Estimate . Due to measurement error, a different sample would be drawn each time, thus leading to a different point estimate . If this process were to be repeated a large number times, the probability distribution of the resulting point estimates is known as the Sampling Distribution of the statistic. There is no rule surrounding the sampling distribution - it depends on the distribution of the population, statistic being measured, sampling method etc. The standard deviation of this sampling distribution is known as the Standard Error of the statistic: \\[ \\sigma_{\\theta} = \\sqrt {\\sigma^2_{\\theta}} \\] A special case is the Sample Mean . If the population is normally distributed, then it is normally distributed as well. Regardless of the population distribution, it is also approximately normally distributed through the Central Limit Theorem or Law of Large Numbers . \\[ \\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\] Following that, we can compute the standard error: \\[ \\sigma_{\\bar{x}} = \\sqrt \\frac{\\sigma^2_{\\bar{x}}}{n} = \\frac{\\sigma}{\\sqrt{n}} \\] However, the population variance is usually unknown. Thus, it can be approximated using the Sample Variance, which is an unbiased estimator for it. The result is known as an Estimate for the Standard Error : \\[ \\hat{\\sigma}_{\\bar{x}} = \\sqrt \\frac{s^2}{n} = \\frac{s}{\\sqrt{n}} \\] Note that only the sample variance is an unbiased estimator for the population variance. Although it may look like it, the sample SD is NOT an unbiased estimator for the population SD.","title":"Sampling Distribution"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#confidence-interval","text":"Given that there is only one true value for the population parameter and a whole distribution of estimators, it is unlikely that a point estimate will be equal to the population parameter. Thus, instead of a point estimate, a range of estimates is used, known as a Confidence Interval . The interval is made a chosen Confidence Level which represents the proportion of confidence intervals that will contain the true value . In other words, if a large number of confidence intervals constructed in the same manner were to be made, \\((1-\\alpha)%\\) of them would contain the true value. \\[ P(- \\text{Margin of Error} < \\theta < \\text{Margin of Error}) = 1 - \\alpha \\] The Margin of Error represents the range of values on either side of the point estimate that the true value could lie. For instance, for a 95% confidence interval, the confidence lies within the 0.025 and 0.975 percentile of the sampling distribution. The margin of error can be calculated by finding the corresponding values of the sampling distribution at these percentiles. Consider the 95% confidence interval for the Sample Mean , which is normally distributed. For convenience, it is usually normalized such that it will become a Standard Normal Distribution : \\[ \\begin{aligned} P(-1.96 < \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt n}} < 1.96) &= 0.95 \\\\ P(\\bar{x} - 1.96 \\frac{\\sigma}{\\sqrt n} < \\mu < \\bar{x} + 1.96 \\frac{\\sigma}{\\sqrt n}) &= 0.95 \\\\ \\end{aligned} \\] \\[ \\therefore \\text{Margin of Error} = \\bar{x} + Z_{\\frac{\\alpha}{2}} * \\frac{\\sigma}{\\sqrt n} \\]","title":"Confidence Interval"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#hypothesis-testing","text":"Hypothesis Testing is a formal method of making inferences about the population parameters based on the sample statistic. It starts with a Hypothesis which is a conjecture about the population parameters: Null Hypothesis - What is currently believed to be true Alternative Hypothesis - What is to be proven A Test Statistic is then calculated, which would quantify the behaviour of population such that it would distinguish the null and alternative hypothesis. Assuming the Null Hypothesis is true , the sampling distribution of the test statistic is determined. From the sampling distribution, the p-value is calculated, which is the probability of observing the calculated sample statistic or more extreme . If the p-value is smaller than a pre-determined level of Statistical Significance ( \\(\\alpha\\) ), then the test is \"successful\" and the null hypothesis is rejected. If not, then the test \"fails\" and the null is not rejected. Note that it rejecting the null hypothesis does NOT mean that the alternative hypothesis is accepted - EG. Rejected then hypothesis of a large paw print being from a bear does not mean that it is from Bigfoot. Thus, the hypotheses are usually constructed such that the two hypothesis are complementary , such that rejecting the null allows acceptance of the alternative, leading to a definitive insight. In layman terms, a hypothesis test is a test of extremeness . Assuming the null hypothesis is true, how extreme (low probability) is the observed sample? If the p-value is sufficiently low, it means that the sample is rare - it could be due to pure chance or that it is actually not rare because the null is not true. We distinguish between the two mathematically through \\(\\alpha\\) . It is the probability of a False Positive - that the result obtained was by pure chance. It is typically set at 5%, which means that 5% of all statistics calculated are expected to be extreme under the null. Thus, if the p-value of the sample is smaller than \\(\\alpha\\) , then it is likely that the observation was not due to chance but instead because the null was false . Alternatively, instead of comparing p-value to \\(alpha\\) , the test-statistic and the corresponding value of \\(\\alpha\\) on the sampling distribution can be used. It is known as the Critical Value , which represents the boundary of Reject Null Do not Reject Null p-value smaller than \\(\\alpha\\) p-value smaller than \\(\\alpha\\) test-statistic larger than critical value test-statistic smaller than critical value Let the random variable \\(T\\) denote the test statistics. There are many different kinds of test statistics depending on the distribution and what is being investigated.","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#z-statistic","text":"The most simple test statistic involve the Sample Mean , which is normally distributed: \\[ T = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] If the population variance is known , then the test statistic has a Standard Normal Distribution and thus the test-statistic is known as a Z-Statistic . \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\sqrt n \\\\ &= Z * \\sqrt n \\\\ \\therefore T &\\sim N(0,1) \\end{aligned} \\]","title":"Z-statistic"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#t-statistic","text":"If the population variance is unknown, then it will be approximated by the Sample Variance . Through algebraic manipulation, the test statistic can still be expressed in the form of a Z variable, but with an additional term: \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}} \\\\ &= \\frac{\\bar{x} - \\mu}{s} * \\sqrt n \\\\ &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\frac{\\sigma}{s} * \\sqrt n \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{s^2}{\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{(n-1)s^2}{(n-1)\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt {\\frac{(n-1)s^2}{\\sigma^2} * \\frac{1}{n-1}} \\\\ \\end{aligned} \\] The additional term can be shown to have a Chi-Squared Distribution of \\(n-1\\) degrees of freedom, which by definition is the sum of \\(n-1\\) independent standard normal variables: \\[ \\begin{aligned} \\chi^2_n &= Z^2 \\\\ &= \\sum \\left(\\frac {\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &= \\sum \\left(\\frac {(x_i - \\bar{x}) + (\\bar{x} - \\mu)}{\\sigma}\\right)^2 \\\\ &= \\sum \\left(\\frac{x_i - \\bar{x}}{\\sigma}\\right)^2 + \\sum \\left(\\frac{\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &= \\frac{1}{\\sigma^2} \\sum (x_i - \\bar{x})^2 + \\sum Z^2 + 0 \\\\ &= \\frac{(n-1)}{\\sigma^2} \\frac{\\sum (x_i - \\bar{x})^2}{n-1} + \\chi^2_1 \\\\ &= \\frac{(n-1)s^2}{\\sigma^2} + \\chi^2_1 \\\\ \\end{aligned} \\] \\[\\therefore \\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\] Thus, the test statistic is the ratio of a Standard Normal Variable to the squareroot of a Chi Squared Variable (divided by its degrees of freedom). By definition, this test statistic has a t-distribution with the same degrees of freedom and is known as the t-statistic : \\[ \\begin{align*} T &= \\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\\\ &= t_{n-1} * \\sqrt{n} \\\\ \\end{align*} \\] \\[\\therefore T \\sim t_{n-1}\\] Despite the slightly convoluted proof, the t-distribution is simply a standard normal distribution with heavier tails . This means that extreme values are slightly more likely, which is meant to account for the increase in variability due to the use of the sample variance rather than the population variance.","title":"t-statistic"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#f-statistic","text":"The square of t-statisic has an F-Distribution , which is defined as the ratio of two independent chi-square variables (divided by their respective degrees of freedom). It has two dimensions for its degree of freedom, reflecting the two chi-square variables. \\[ F_{m,n} = \\frac{\\frac{\\chi_m}{m}}{\\frac{\\chi_n}{n}} \\] The square of a t-statistic follows an F-distribution because: The square of the standard normal variable in the numerator becomes a \\(\\chi_1\\) variable The squareroot is removed in the denominator, becoming a \\(\\chi_{n-1}\\) over its degree of freedom \\[ \\begin{aligned} t_{n-1}^2 &= \\left(\\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\right)^2 \\\\ &= \\frac{\\chi_1}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &= \\frac{\\frac{\\chi_1}{1}}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &= F_{1,n-1} * n \\end{aligned} \\] \\[ \\therefore t_{n-1}^2 \\sim F_{1, n-1} \\] Thus, the square of the t-statistic is known as the F-statistic , which is usually used to test for the equality of variance .","title":"F-statistic"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#maximum-likelihood-estimation","text":"If the population distribution is known , there is an alternative method of estimating the parameters apart from calculating the corresponding sample statistics, known as Maximum Likelihood Estimation (MLE). There are an infinite number of variations of the distribution that could have resulted in the sample, each with different parameters . Technically speaking, any set of parameters could have resulted in the sample. However, the goal of MLE is to find the set of parameters that are most likely to result in the sample ; in other words, the probability of obtaining this sample is the highest with this set of paramters than any other set. The probability of obtaining the sample is known as its Likelihood : \\[ L(\\theta \\mid x) = P_{\\theta} (X = x) \\] Warning Likelihood functions and PMF/PDFs are often confused with one another as they involve the same expression. The key is understanding what is given and what is random , which results in the subtle but differing notation: PMF/PDF : Given parameters, outcomes are random; \\(P_{X}(x)\\) Likelihood : Given outcomes, parameters are random; \\(P_{\\theta}(x)\\) Assuming that the sample is iid, the likehood for the entire sample is the product of the likelihood for each observation , known as the Likelihood Function : \\[ L(\\theta) = \\prod P_{\\theta}(X = x_i) \\] The goal is to find the parameters that maximizes the likelihood function through calculus: \\[ \\frac{d}{d\\theta} L(\\theta) = 0 \\] In practice, especially when dealing with multiple parameters, the likelihood function is complicated to work with. Thus, a log transformation is often applied to simplify it, turning the product into a summation . This is known as the Log-Likelihood Function . Since the logarithm transform is monotonic, both the likelihood and log-likelihood functions share the same maximum . \\[ \\begin{aligned} \\ell (\\theta) &= \\ln L(\\theta) \\\\ \\therefore \\frac{d}{d\\theta} \\ell (\\theta) &= 0 \\end{aligned} \\]","title":"Maximum Likelihood Estimation"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#practical-tips","text":"Some distributions have complicated PMF/PDFs that make working with them more complicated. Most questions will usually have some method to simplify the likelihood function . The first tip is to understand that since the likelihood function will be logarithm transformed and then differentiated, factors that contains ONLY constants can be dropped since they will inevitably be removed later: \\[ \\begin{aligned} L(\\theta) &= a * x \\\\ \\ell (\\theta) &= \\ln a + \\ln x \\\\ \\ell' (\\theta) &= \\frac{1}{x} \\\\ \\\\ \\therefore L(\\theta) \\propto x \\end{aligned} \\] The next tip is that if the parameters are embedded in the power of some constant, they should be combined together: \\[ \\begin{aligned} L(\\theta) &= a^{\\theta} \\cdot b^{\\theta} \\cdot c^{\\theta} \\\\ &= (a \\cdot b \\cdot c)^{\\theta} \\end{aligned} \\] The reverse also applies...divide power, may not be in the same term, could come from another term However, if the above terms for some reason are added instead of multiplied , then a substituition method would be better: \\[ \\begin{aligned} L(\\theta) &= e^{-\\frac{k}{100}} - \\left(e^{-\\frac{k}{100}} \\right)^2 \\\\ L(\\theta) &= p - p^2 \\end{aligned} \\]","title":"Practical Tips"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#method-of-moments","text":"An alternative method for estimating population parameters is the Method of Moments (MOM). It is based on the Law of Large Numbers , which states that the sample mean converges to the population mean (first raw moment), given a sufficiently large sample size . Thus, by equating the sample raw moments to the population raw moments , up to the number of parameters to estimate, we can solve for an estimate of the parameters. Note If there is only one parameter to estimate, then only the first raw moments are equated. If there are two, then the second raw moment should be equated as well. \\[ \\begin{aligned} E(X^k) &= \\bar{x} \\\\ &= \\frac{\\sum x^k_i}{n} \\end{aligned} \\] The main advantage of this method is that it is computationally simpler than MLE. For certain known distributions, the MOM estimate and MLE estimate are the same, thus MOM can be used as a shortcut for MLE .","title":"Method of Moments"}]}