{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Actuarial Exam Notes Test","title":"**Actuarial Exam Notes**"},{"location":"#actuarial-exam-notes","text":"Test","title":"Actuarial Exam Notes"},{"location":"Placeholder/","text":"Work in progress!","title":"ASA-PA"},{"location":"Placeholder/#work-in-progress","text":"","title":"Work in progress!"},{"location":"0.%20Foundational/1.%20Review%20of%20Linear%20Algebra/","text":"Vector Matrix System of equations","title":"Review of Linear Algebra"},{"location":"0.%20Foundational/2.%20Review%20of%20Calculus/","text":"Review of Calculus Differentiation Application of differentiation - Optimization Integration Application - Area/Volume","title":"Review of Calculus"},{"location":"0.%20Foundational/2.%20Review%20of%20Calculus/#review-of-calculus","text":"Differentiation Application of differentiation - Optimization Integration Application - Area/Volume","title":"Review of Calculus"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/","text":"Review of Statistical Theory This is meant to be a quick review of the basic statistical concepts of VEE: Mathematical Statistics and other undergrad statitics courses that will be relevant for this exam. Overview of Statistics Statistics is a discipline revolving around data. A Population refers to the theoretical set of all possible data of the event of interest. The goal of statistics is to determine certain attributes that summarizes or describes the population, known as Parameters . However, it is impossible to study the entire population at once, thus a subset of the population is studied instead, known as the Sample . Attributes that summarize or describe the sample are known as Statistics . Ideally, the sample is representative of the population, which means that findings from the sample can be applied to the population as a whole. This means that the sample statistics can be used estimate population parameters. We distinguish between the two (when they have the same notation) through the Hat accent (^) - Population Parameters are their written without the hat ( \\(x\\) ) while their corresponding sample statistics are written with the hat ( \\(\\hat{x}\\) ). Common Sample Statistics The Mean is the average of the population. Population Mean Sample Mean \\(\\mu = \\sum\\limits_{i=i}^n x_i * p(x_i)\\) \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=i}^n x_i\\) The Variance measures the spread of values about the mean. However, the units of Variance are unintuitive, thus we consider the Standard Deviation for more practical purposes, which is the square root of the variance . Population Variance Sample Variance \\(\\sigma^2 = \\sum\\limits_{i=i}^n (x_i - \\mu)^2 * p(x_i)\\) \\(s^2 = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x})^2\\) Covariance is a measure of the linear relationship between two variables: Positive Covariance - Variables move in the same direction Negative Covariance - Variables move in opposite directions Population Covariance Sample Covariance \\(\\sigma_{x, y} = \\mu_{xy} - \\mu_x \\mu_y\\) \\(\\hat{\\sigma}_{x, y} = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x}) * (y_i - \\bar{y})\\) However, there are two issues with Covariance - the units are unintuitive (similar to variance) and there is no benchmark as to what constitutes a strong/weak relationship. Thus, the Correlation is an adjusted measure of the relationship between -1 and 1 . Population Correlation Sample Correlation \\(\\rho = \\frac{\\sigma_{x,y}}{\\sigma_x * \\sigma_y}\\) \\(r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x} \\sum(y_i - \\bar{y})}\\) Properties of Estimators Biased Consistent Efficient Degrees of freedom Sampling Distribution Whenever a sample is drawn from a population and a statistic is calculated, it is known as a Point Estimate . Due to measurement error, a different sample would be drawn each time, thus leading to a different point estimate . If this process were to be repeated a large number times, the probability distribution of the resulting point estimates is known as the Sampling Distribution of the statistic. There is no rule surrounding the sampling distribution - it depends on the distribution of the population, statistic being measured, sampling method etc. The standard deviation of this sampling distribution is known as the Standard Error of the statistic: \\[ \\sigma_{\\theta} = \\sqrt {\\sigma^2_{\\theta}} \\] A special case is the Sample Mean . If the population is normally distributed, then it is normally distributed as well. Regardless of the population distribution, it is also approximately normally distributed through the Central Limit Theorem or Law of Large Numbers . \\[ \\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\] Following that, we can compute the standard error: \\[ \\sigma_{\\bar{x}} = \\sqrt \\frac{\\sigma^2_{\\bar{x}}}{n} = \\frac{\\sigma}{\\sqrt{n}} \\] However, the population variance is usually unknown. Thus, it can be approximated using the Sample Variance, which is an unbiased estimator for it. The result is known as an Estimate for the Standard Error : \\[ \\hat{\\sigma}_{\\bar{x}} = \\sqrt \\frac{s^2}{n} = \\frac{s}{\\sqrt{n}} \\] Note that only the sample variance is an unbiased estimator for the population variance. Although it may look like it, the sample SD is NOT an unbiased estimator for the population SD. Confidence Interval Given that there is only one true value for the population parameter and a whole distribution of estimators, it is unlikely that a point estimate will be equal to the population parameter. Thus, instead of a point estimate, a range of estimates is used, known as a Confidence Interval . The interval is made a chosen Confidence Level which represents the proportion of confidence intervals that will contain the true value . In other words, if a large number of confidence intervals constructed in the same manner were to be made, \\((1-\\alpha)%\\) of them would contain the true value. \\[ P(- \\text{Margin of Error} < \\theta < \\text{Margin of Error}) = 1 - \\alpha \\] The Margin of Error represents the range of values on either side of the point estimate that the true value could lie. For instance, for a 95% confidence interval, the confidence lies within the 0.025 and 0.975 percentile of the sampling distribution. The margin of error can be calculated by finding the corresponding values of the sampling distribution at these percentiles. Consider the 95% confidence interval for the Sample Mean , which is normally distributed. For convenience, it is usually normalized such that it will become a Standard Normal Distribution : \\[ \\begin{aligned} P(-1.96 < \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt n}} < 1.96) &= 0.95 \\\\ P(\\bar{x} - 1.96 \\frac{\\sigma}{\\sqrt n} < \\mu < \\bar{x} + 1.96 \\frac{\\sigma}{\\sqrt n}) &= 0.95 \\\\ \\end{aligned} \\] \\[ \\therefore \\text{Margin of Error} = \\bar{x} + Z_{\\frac{\\alpha}{2}} * \\frac{\\sigma}{\\sqrt n} \\] Hypothesis Testing Hypothesis Testing is a formal method of making inferences about the population parameters based on the sample statistic. It starts with a Hypothesis which is a conjecture about the population parameters: Null Hypothesis - What is currently believed to be true Alternative Hypothesis - What is to be proven A Test Statistic is then calculated, which would quantify the behaviour of population such that it would distinguish the null and alternative hypothesis. Assuming the Null Hypothesis is true , the sampling distribution of the test statistic is determined. From the sampling distribution, the p-value is calculated, which is the probability of observing the calculated sample statistic or more extreme . If the p-value is smaller than a pre-determined level of Statistical Significance ( \\(\\alpha\\) ), then the test is \"successful\" and the null hypothesis is rejected. If not, then the test \"fails\" and the null is not rejected. Note that it rejecting the null hypothesis does NOT mean that the alternative hypothesis is accepted - EG. Rejected then hypothesis of a large paw print being from a bear does not mean that it is from Bigfoot. Thus, the hypotheses are usually constructed such that the two hypothesis are complementary , such that rejecting the null allows acceptance of the alternative, leading to a definitive insight. In layman terms, a hypothesis test is a test of extremeness . Assuming the null hypothesis is true, how extreme (low probability) is the observed sample? If the p-value is sufficiently low, it means that the sample is rare - it could be due to pure chance or that it is actually not rare because the null is not true. We distinguish between the two mathematically through \\(\\alpha\\) . It is the probability of a False Positive - that the result obtained was by pure chance. It is typically set at 5%, which means that 5% of all statistics calculated are expected to be extreme under the null. Thus, if the p-value of the sample is smaller than \\(\\alpha\\) , then it is likely that the observation was not due to chance but instead because the null was false . Alternatively, instead of comparing p-value to \\(alpha\\) , the test-statistic and the corresponding value of \\(\\alpha\\) on the sampling distribution can be used. It is known as the Critical Value , which represents the boundary of Reject Null Do not Reject Null p-value smaller than \\(\\alpha\\) p-value smaller than \\(\\alpha\\) test-statistic larger than critical value test-statistic smaller than critical value Let the random variable \\(T\\) denote the test statistics. There are many different kinds of test statistics depending on the distribution and what is being investigated. Z-statistic The most simple test statistic involve the Sample Mean , which is normally distributed: \\[ T = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] If the population variance is known , then the test statistic has a Standard Normal Distribution and thus the test-statistic is known as a Z-Statistic . \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\sqrt n \\\\ &= Z * \\sqrt n \\\\ \\therefore T &\\sim N(0,1) \\end{aligned} \\] t-statistic If the population variance is unknown, then it will be approximated by the Sample Variance . Through algebraic manipulation, the test statistic can still be expressed in the form of a Z variable, but with an additional term: \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}} \\\\ &= \\frac{\\bar{x} - \\mu}{s} * \\sqrt n \\\\ &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\frac{\\sigma}{s} * \\sqrt n \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{s^2}{\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{(n-1)s^2}{(n-1)\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt {\\frac{(n-1)s^2}{\\sigma^2} * \\frac{1}{n-1}} \\\\ \\end{aligned} \\] The additional term can be shown to have a Chi-Squared Distribution of \\(n-1\\) degrees of freedom, which by definition is the sum of \\(n-1\\) independent standard normal variables: \\[ \\begin{aligned} \\chi^2_n &= Z^2 \\\\ &= \\sum \\left(\\frac {\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &= \\sum \\left(\\frac {(x_i - \\bar{x}) + (\\bar{x} - \\mu)}{\\sigma}\\right)^2 \\\\ &= \\sum \\left(\\frac{x_i - \\bar{x}}{\\sigma}\\right)^2 + \\sum \\left(\\frac{\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &= \\frac{1}{\\sigma^2} \\sum (x_i - \\bar{x})^2 + \\sum Z^2 + 0 \\\\ &= \\frac{(n-1)}{\\sigma^2} \\frac{\\sum (x_i - \\bar{x})^2}{n-1} + \\chi^2_1 \\\\ &= \\frac{(n-1)s^2}{\\sigma^2} + \\chi^2_1 \\\\ \\end{aligned} \\] \\[\\therefore \\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\] Thus, the test statistic is the ratio of a Standard Normal Variable to the squareroot of a Chi Squared Variable (divided by its degrees of freedom). By definition, this test statistic has a t-distribution with the same degrees of freedom and is known as the t-statistic : \\[ \\begin{align*} T &= \\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\\\ &= t_{n-1} * \\sqrt{n} \\\\ \\end{align*} \\] \\[\\therefore T \\sim t_{n-1}\\] Despite the slightly convoluted proof, the t-distribution is simply a standard normal distribution with heavier tails . This means that extreme values are slightly more likely, which is meant to account for the increase in variability due to the use of the sample variance rather than the population variance. F-statistic The square of t-statisic has an F-Distribution , which is defined as the ratio of two independent chi-square variables (divided by their respective degrees of freedom). It has two dimensions for its degree of freedom, reflecting the two chi-square variables. \\[ F_{m,n} = \\frac{\\frac{\\chi_m}{m}}{\\frac{\\chi_n}{n}} \\] The square of a t-statistic follows an F-distribution because: The square of the standard normal variable in the numerator becomes a \\(\\chi_1\\) variable The squareroot is removed in the denominator, becoming a \\(\\chi_{n-1}\\) over its degree of freedom \\[ \\begin{aligned} t_{n-1}^2 &= \\left(\\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\right)^2 \\\\ &= \\frac{\\chi_1}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &= \\frac{\\frac{\\chi_1}{1}}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &= F_{1,n-1} * n \\end{aligned} \\] \\[ \\therefore t_{n-1}^2 \\sim F_{1, n-1} \\] Thus, the square of the t-statistic is known as the F-statistic , which is usually used to test for the equality of variance . Maximum Likelihood Estimation If the population distribution is known, there is an alternative method of estimating the parameters apart from calculating the corresponding sample statistics. We model a set of observations as a random sample from an unknown joint probability distribution which is expressed in terms of a set of parameters. The goal of maximum likelihood estimation is to determine the parameters for which the observed data have the highest joint probability. The goal of maximum likelihood estimation is to find the values of the model parameters that maximize the likelihood function over the parameter space","title":"Review of Statistical Theory"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#review-of-statistical-theory","text":"This is meant to be a quick review of the basic statistical concepts of VEE: Mathematical Statistics and other undergrad statitics courses that will be relevant for this exam.","title":"Review of Statistical Theory"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#overview-of-statistics","text":"Statistics is a discipline revolving around data. A Population refers to the theoretical set of all possible data of the event of interest. The goal of statistics is to determine certain attributes that summarizes or describes the population, known as Parameters . However, it is impossible to study the entire population at once, thus a subset of the population is studied instead, known as the Sample . Attributes that summarize or describe the sample are known as Statistics . Ideally, the sample is representative of the population, which means that findings from the sample can be applied to the population as a whole. This means that the sample statistics can be used estimate population parameters. We distinguish between the two (when they have the same notation) through the Hat accent (^) - Population Parameters are their written without the hat ( \\(x\\) ) while their corresponding sample statistics are written with the hat ( \\(\\hat{x}\\) ).","title":"Overview of Statistics"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#common-sample-statistics","text":"The Mean is the average of the population. Population Mean Sample Mean \\(\\mu = \\sum\\limits_{i=i}^n x_i * p(x_i)\\) \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=i}^n x_i\\) The Variance measures the spread of values about the mean. However, the units of Variance are unintuitive, thus we consider the Standard Deviation for more practical purposes, which is the square root of the variance . Population Variance Sample Variance \\(\\sigma^2 = \\sum\\limits_{i=i}^n (x_i - \\mu)^2 * p(x_i)\\) \\(s^2 = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x})^2\\) Covariance is a measure of the linear relationship between two variables: Positive Covariance - Variables move in the same direction Negative Covariance - Variables move in opposite directions Population Covariance Sample Covariance \\(\\sigma_{x, y} = \\mu_{xy} - \\mu_x \\mu_y\\) \\(\\hat{\\sigma}_{x, y} = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x}) * (y_i - \\bar{y})\\) However, there are two issues with Covariance - the units are unintuitive (similar to variance) and there is no benchmark as to what constitutes a strong/weak relationship. Thus, the Correlation is an adjusted measure of the relationship between -1 and 1 . Population Correlation Sample Correlation \\(\\rho = \\frac{\\sigma_{x,y}}{\\sigma_x * \\sigma_y}\\) \\(r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x} \\sum(y_i - \\bar{y})}\\) Properties of Estimators Biased Consistent Efficient Degrees of freedom","title":"Common Sample Statistics"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#sampling-distribution","text":"Whenever a sample is drawn from a population and a statistic is calculated, it is known as a Point Estimate . Due to measurement error, a different sample would be drawn each time, thus leading to a different point estimate . If this process were to be repeated a large number times, the probability distribution of the resulting point estimates is known as the Sampling Distribution of the statistic. There is no rule surrounding the sampling distribution - it depends on the distribution of the population, statistic being measured, sampling method etc. The standard deviation of this sampling distribution is known as the Standard Error of the statistic: \\[ \\sigma_{\\theta} = \\sqrt {\\sigma^2_{\\theta}} \\] A special case is the Sample Mean . If the population is normally distributed, then it is normally distributed as well. Regardless of the population distribution, it is also approximately normally distributed through the Central Limit Theorem or Law of Large Numbers . \\[ \\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\] Following that, we can compute the standard error: \\[ \\sigma_{\\bar{x}} = \\sqrt \\frac{\\sigma^2_{\\bar{x}}}{n} = \\frac{\\sigma}{\\sqrt{n}} \\] However, the population variance is usually unknown. Thus, it can be approximated using the Sample Variance, which is an unbiased estimator for it. The result is known as an Estimate for the Standard Error : \\[ \\hat{\\sigma}_{\\bar{x}} = \\sqrt \\frac{s^2}{n} = \\frac{s}{\\sqrt{n}} \\] Note that only the sample variance is an unbiased estimator for the population variance. Although it may look like it, the sample SD is NOT an unbiased estimator for the population SD.","title":"Sampling Distribution"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#confidence-interval","text":"Given that there is only one true value for the population parameter and a whole distribution of estimators, it is unlikely that a point estimate will be equal to the population parameter. Thus, instead of a point estimate, a range of estimates is used, known as a Confidence Interval . The interval is made a chosen Confidence Level which represents the proportion of confidence intervals that will contain the true value . In other words, if a large number of confidence intervals constructed in the same manner were to be made, \\((1-\\alpha)%\\) of them would contain the true value. \\[ P(- \\text{Margin of Error} < \\theta < \\text{Margin of Error}) = 1 - \\alpha \\] The Margin of Error represents the range of values on either side of the point estimate that the true value could lie. For instance, for a 95% confidence interval, the confidence lies within the 0.025 and 0.975 percentile of the sampling distribution. The margin of error can be calculated by finding the corresponding values of the sampling distribution at these percentiles. Consider the 95% confidence interval for the Sample Mean , which is normally distributed. For convenience, it is usually normalized such that it will become a Standard Normal Distribution : \\[ \\begin{aligned} P(-1.96 < \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt n}} < 1.96) &= 0.95 \\\\ P(\\bar{x} - 1.96 \\frac{\\sigma}{\\sqrt n} < \\mu < \\bar{x} + 1.96 \\frac{\\sigma}{\\sqrt n}) &= 0.95 \\\\ \\end{aligned} \\] \\[ \\therefore \\text{Margin of Error} = \\bar{x} + Z_{\\frac{\\alpha}{2}} * \\frac{\\sigma}{\\sqrt n} \\]","title":"Confidence Interval"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#hypothesis-testing","text":"Hypothesis Testing is a formal method of making inferences about the population parameters based on the sample statistic. It starts with a Hypothesis which is a conjecture about the population parameters: Null Hypothesis - What is currently believed to be true Alternative Hypothesis - What is to be proven A Test Statistic is then calculated, which would quantify the behaviour of population such that it would distinguish the null and alternative hypothesis. Assuming the Null Hypothesis is true , the sampling distribution of the test statistic is determined. From the sampling distribution, the p-value is calculated, which is the probability of observing the calculated sample statistic or more extreme . If the p-value is smaller than a pre-determined level of Statistical Significance ( \\(\\alpha\\) ), then the test is \"successful\" and the null hypothesis is rejected. If not, then the test \"fails\" and the null is not rejected. Note that it rejecting the null hypothesis does NOT mean that the alternative hypothesis is accepted - EG. Rejected then hypothesis of a large paw print being from a bear does not mean that it is from Bigfoot. Thus, the hypotheses are usually constructed such that the two hypothesis are complementary , such that rejecting the null allows acceptance of the alternative, leading to a definitive insight. In layman terms, a hypothesis test is a test of extremeness . Assuming the null hypothesis is true, how extreme (low probability) is the observed sample? If the p-value is sufficiently low, it means that the sample is rare - it could be due to pure chance or that it is actually not rare because the null is not true. We distinguish between the two mathematically through \\(\\alpha\\) . It is the probability of a False Positive - that the result obtained was by pure chance. It is typically set at 5%, which means that 5% of all statistics calculated are expected to be extreme under the null. Thus, if the p-value of the sample is smaller than \\(\\alpha\\) , then it is likely that the observation was not due to chance but instead because the null was false . Alternatively, instead of comparing p-value to \\(alpha\\) , the test-statistic and the corresponding value of \\(\\alpha\\) on the sampling distribution can be used. It is known as the Critical Value , which represents the boundary of Reject Null Do not Reject Null p-value smaller than \\(\\alpha\\) p-value smaller than \\(\\alpha\\) test-statistic larger than critical value test-statistic smaller than critical value Let the random variable \\(T\\) denote the test statistics. There are many different kinds of test statistics depending on the distribution and what is being investigated.","title":"Hypothesis Testing"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#z-statistic","text":"The most simple test statistic involve the Sample Mean , which is normally distributed: \\[ T = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] If the population variance is known , then the test statistic has a Standard Normal Distribution and thus the test-statistic is known as a Z-Statistic . \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\sqrt n \\\\ &= Z * \\sqrt n \\\\ \\therefore T &\\sim N(0,1) \\end{aligned} \\]","title":"Z-statistic"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#t-statistic","text":"If the population variance is unknown, then it will be approximated by the Sample Variance . Through algebraic manipulation, the test statistic can still be expressed in the form of a Z variable, but with an additional term: \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}} \\\\ &= \\frac{\\bar{x} - \\mu}{s} * \\sqrt n \\\\ &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\frac{\\sigma}{s} * \\sqrt n \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{s^2}{\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{(n-1)s^2}{(n-1)\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt {\\frac{(n-1)s^2}{\\sigma^2} * \\frac{1}{n-1}} \\\\ \\end{aligned} \\] The additional term can be shown to have a Chi-Squared Distribution of \\(n-1\\) degrees of freedom, which by definition is the sum of \\(n-1\\) independent standard normal variables: \\[ \\begin{aligned} \\chi^2_n &= Z^2 \\\\ &= \\sum \\left(\\frac {\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &= \\sum \\left(\\frac {(x_i - \\bar{x}) + (\\bar{x} - \\mu)}{\\sigma}\\right)^2 \\\\ &= \\sum \\left(\\frac{x_i - \\bar{x}}{\\sigma}\\right)^2 + \\sum \\left(\\frac{\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &= \\frac{1}{\\sigma^2} \\sum (x_i - \\bar{x})^2 + \\sum Z^2 + 0 \\\\ &= \\frac{(n-1)}{\\sigma^2} \\frac{\\sum (x_i - \\bar{x})^2}{n-1} + \\chi^2_1 \\\\ &= \\frac{(n-1)s^2}{\\sigma^2} + \\chi^2_1 \\\\ \\end{aligned} \\] \\[\\therefore \\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\] Thus, the test statistic is the ratio of a Standard Normal Variable to the squareroot of a Chi Squared Variable (divided by its degrees of freedom). By definition, this test statistic has a t-distribution with the same degrees of freedom and is known as the t-statistic : \\[ \\begin{align*} T &= \\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\\\ &= t_{n-1} * \\sqrt{n} \\\\ \\end{align*} \\] \\[\\therefore T \\sim t_{n-1}\\] Despite the slightly convoluted proof, the t-distribution is simply a standard normal distribution with heavier tails . This means that extreme values are slightly more likely, which is meant to account for the increase in variability due to the use of the sample variance rather than the population variance.","title":"t-statistic"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#f-statistic","text":"The square of t-statisic has an F-Distribution , which is defined as the ratio of two independent chi-square variables (divided by their respective degrees of freedom). It has two dimensions for its degree of freedom, reflecting the two chi-square variables. \\[ F_{m,n} = \\frac{\\frac{\\chi_m}{m}}{\\frac{\\chi_n}{n}} \\] The square of a t-statistic follows an F-distribution because: The square of the standard normal variable in the numerator becomes a \\(\\chi_1\\) variable The squareroot is removed in the denominator, becoming a \\(\\chi_{n-1}\\) over its degree of freedom \\[ \\begin{aligned} t_{n-1}^2 &= \\left(\\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\right)^2 \\\\ &= \\frac{\\chi_1}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &= \\frac{\\frac{\\chi_1}{1}}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &= F_{1,n-1} * n \\end{aligned} \\] \\[ \\therefore t_{n-1}^2 \\sim F_{1, n-1} \\] Thus, the square of the t-statistic is known as the F-statistic , which is usually used to test for the equality of variance .","title":"F-statistic"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#maximum-likelihood-estimation","text":"If the population distribution is known, there is an alternative method of estimating the parameters apart from calculating the corresponding sample statistics. We model a set of observations as a random sample from an unknown joint probability distribution which is expressed in terms of a set of parameters. The goal of maximum likelihood estimation is to determine the parameters for which the observed data have the highest joint probability. The goal of maximum likelihood estimation is to find the values of the model parameters that maximize the likelihood function over the parameter space","title":"Maximum Likelihood Estimation"},{"location":"0.%20Foundational/4.%20Review%20of%20Accounting/","text":"","title":"4. Review of Accounting"},{"location":"0.%20Foundational/5.%20Review%20of%20Corporate%20Finance/","text":"","title":"5. Review of Corporate Finance"},{"location":"0.%20Foundational/Overview/","text":"","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/Overview/","text":"Overview of Actuarial Mathematics These set of exams cover the main concepts needed to evaluate insurance related risks. They are split into two main exams: Fundamentals of Actuarial Mathematics (FAM) Advanced Long Term or Short Term Actuarial Mathematics (ALTAM or ASTAM) This set of notes splits the FAM component into two seperate components for clarity, as the components are not related to one another: FAM-S (Short Term) FAM-L (Long Term) However, the actual FAM exam tests both components together in an MCQ format. It is regarded as one of the tougher examinations due to its wide breadth (due to the unrelated components).","title":"**Overview of Actuarial Mathematics**"},{"location":"2.%20Actuarial%20Mathematics/Overview/#overview-of-actuarial-mathematics","text":"These set of exams cover the main concepts needed to evaluate insurance related risks. They are split into two main exams: Fundamentals of Actuarial Mathematics (FAM) Advanced Long Term or Short Term Actuarial Mathematics (ALTAM or ASTAM) This set of notes splits the FAM component into two seperate components for clarity, as the components are not related to one another: FAM-S (Short Term) FAM-L (Long Term) However, the actual FAM exam tests both components together in an MCQ format. It is regarded as one of the tougher examinations due to its wide breadth (due to the unrelated components).","title":"Overview of Actuarial Mathematics"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/","text":"Survival Models Survival Models are probability distributions that measure the time to failure of an entity, or phrased another way, the future lifetime of an entity. In an Actuarial context, it measures the time to death of a person. Since survival models measure time , they are denoted by the continuous random variable \\(T_i\\) , where the subscript represents the age of the person being studied. Newborn Lifetime The base survival model measures the future lifetime of a newborn ( aged 0 ), denoted by the random variable \\(T_0\\) . The CDF thus represents the probability that the newborn dies before age \\(t\\) : \\[ F_0(t) = P(T_0 \\le t) \\] The Survival Function is the complement of the CDF and is the probability that the newborn survives till a certain age \\(t\\) . From a different perspective, it can also be seen as the probability of the newborn dying AFTER age \\(t\\) : \\[ \\begin{aligned} S_0(t) &= P(T_0 \\ge t) \\\\ &= 1 - P(T_0 \\le t) \\\\ &= 1 - F_0(t) \\end{aligned} \\] Note that all humans will inevitably die : \\[ \\begin{aligned} \\lim_{t \\to \\infty} S_0 (t) = 0 \\iff \\lim_{t \\to \\infty} F_0 (t) = 1 \\end{aligned} \\] Continuous Lifetime The above can be generalized for a person aged \\(x\\) , denoted by \\(T_x\\) . However, the intepretation of the two functions changes slightly. The CDF instead represents the probability that the person dies within \\(t\\) years from age \\(x\\) : \\[ \\begin{aligned} F_x(t) &= P(T_x \\le t) \\\\ T_x &= 1, 2, \\dots \\end{aligned} \\] It can also be written in terms of the newborn lifetime . The key is to understand that the newborn is assumed to survive till age \\(x\\) and then die within the next \\(t\\) years: \\[ \\begin{aligned} P(T_x \\le t) &= P(T_0 \\le x + t | T_0 > x) \\\\ &= \\frac{P(x < T_0 < x +t)}{P(T_0 > x)} \\\\ &= \\frac{P(T_0 < x +t)-P(T_0 < x)}{P(T_0 > x)} \\\\ &= \\frac{F_0(x+t) - F_0(x)}{S_0(x)} \\end{aligned} \\] Conversely, the survival function is the probability that the person survives another \\(t\\) years or dies AFTER \\(t\\) years : \\[ S_x(t) = P(T_x \\ge t) \\] It can also be written in terms of the newborn lifetime following the same logic as above. The newborn is assumed to survive till age \\(x\\) and then survive another \\(t\\) years: \\[ \\begin{aligned} P(T_x \\ge t) &= P(T_0 \\ge x + t | T_0 > x) \\\\ &= \\frac{P(T_0 > x + t)}{P(T_0 > x)} \\\\ &= \\frac{S_0(x+t)}{S_0(x)} \\end{aligned} \\] Note that since the surviving till age \\(x\\) is a subset of surviving till age \\(x+t\\) , the numerator can be simplified to just surviving till \\(x+t\\) . Note the survival function can thus be re-written as the following: \\[ \\begin{aligned} S_x(t) &= \\frac{S_0(x+t)}{S_0(x)} \\\\ S_0(x+t) &= S_0(x)S_x(t) \\\\ \\end{aligned} \\] The probability of the survival of a newborn till age \\(x+t\\) is the product of: The probability of survival of a newborn till age \\(x\\) Given that the newborn survived till age \\(x\\) , that they will survive another \\(t\\) years Probability Tree This can be generalized for people of any age to obtain: \\[ S_x(t+u) = S_x(t) S_{x+t}(u) \\] It is important to remember to \"update\" the starting age of the second term to the \"new age\". flowchart LR A([x]) ---> B([x+t, Survive t years]) ---> C([x+t+u, Survive t+u years]) A([x]) ---> E([Die within t years]) B([x+t, Survive t years]) ---> F([Death within t+u years]) \\[ \\begin{aligned} \\text{Death within t years} &= F_x(t) \\\\ \\\\ \\text{Survive t years} &= S_x(t) \\\\ \\\\ \\text{Death within (t+u) years} &= F_x(t+u) \\\\ &= S_x(t) F_{x+t}(u) \\\\ \\\\ \\text{Survive (t+u) years} &= S_x(t+u) \\\\ &= S_x(t) S_{x+t}(u) \\\\ \\\\ \\therefore \\text{Survive t years} &= \\text{Death within (t+u) years} + \\text{Survive (t+u) years} \\\\ S_x(t) &= S_x(t) F_{x+t}(u) + S_x(t) S_{x+t}(u) \\end{aligned} \\] Deferred Death The probability of \"deferred\" death can also be calculated, whereby the person surives another \\(t\\) years and then dies within the following \\(u\\) years : \\[ \\begin{aligned} P(t < T_x < t+u) &= P(T_x < t+u) - P(T_x < t) \\\\ &= F_x(t+u) - F_x(t) \\\\ &= 1 - S_x(t+u) - (1 - S_x(t)) \\\\ &= 1 - S_x(t)S_{x+t}(u) - 1 + S_x(t) \\\\ &= S_x(t) - S_x(t)S_{x+t}(u) \\\\ &= S_x(t)(1-S_{x+t}(u)) \\\\ &= S_x(t)F_{x+t}(u) \\end{aligned} \\] Note that this is consistent with the probability tree approach earlier. Force of Mortality The Force of Mortality is the probability of dying instantly ; in an infinitely small unit of time (normalized by unit time): \\[ \\begin{aligned} \\mu_(x) &= \\lim_{h \\to 0} \\frac{P(T_x < h)}{h} \\\\ &= \\lim_{h \\to 0} \\frac{1 - S_x(h)}{h} \\\\ &= \\lim_{h \\to 0} \\frac{1 - (\\frac{S_0(x+h)}{S_0(x)})}{h} \\\\ &= \\lim_{h \\to 0} \\frac{S_0(x) - S_0(x+h)}{S_0(x)h} \\\\ &= \\frac{1}{S_0(x)} \\lim_{h \\to 0} \\frac{S_0(x) - S_0(x+h)}{h} \\\\ &= - \\frac{1}{S_0(x)} \\lim_{h \\to 0} \\frac{S_0(x+h) - S_0(x)}{h} \\\\ &= - \\frac{1}{S_0(x)} S'_0(x) \\\\ &= - \\frac{S'_0(x)}{S_0(x)} \\end{aligned} \\] Since \\(S_0(x)\\) is the complement of \\(F_0(x)\\) , the derivative of the survival function can be written in terms of the PDF of the future lifetime: \\[ \\begin{aligned} f_0(x) &= \\frac{d}{dx} F_0(x) \\\\ &= \\frac{d}{dx} (1-S_0(x)) \\\\ &= -S'_0(x) \\end{aligned} \\] The force of mortality can be rewritten as the following: \\[ \\mu_x = \\frac{f_0(x)}{S_0(x)} \\] Thus, the force of mortality can also be intepreted as the CONDITIONAL probability distribution of death for a person aged \\(x\\) , assuming that the person survives till age \\(x\\) . \\(f_0(x)\\) is UNCONDITIONAL distribution of the future lifetime as it does not assume the individual lives to any age. Alternative Death Function More generally, the force of mortality can be rewritten as: \\[ \\begin{aligned} \\mu_x &= \\frac{f_0(x)}{S_0(x)} \\\\ \\mu_{x+t} &= \\frac{f_x(t)}{S_x(t)} \\\\ f_x(t) &= S_x(t)\\mu_{x+t} \\\\ F_x(t) &= \\int_0^t S_x(t)\\mu_{x+t} \\end{aligned} \\] The probability of death in an interval is the SUM of the product of the probability of surviving till a given age and then dying in an infinitely small time afterwards. The sum of all the infinitely small time periods is equal to the given time interval. Following this definition, for extremely small time intervals , the force of mortality can be used to approximate the probability of death in that interval: \\[ P(T_x < h) \\approx h * \\mu_{x} \\] Alternative Survival Function More generally, the force of mortality can be rewritten as: \\[ \\begin{aligned} \\mu_(x) &= - \\frac{S'_0(x)}{S_0(x)} \\\\ \\mu_{x + t} &= \\frac{-S'_x(t)}{S_x(t)} \\\\ \\mu_{x + t} &= - \\frac{d}{dt} (\\ln S_x(t)) \\\\ \\ln S_x(t) &= - \\int_{0}^{t} \\mu_{x + t} \\\\ S_x(t) &= e ^ {- \\int_{0}^{t} \\mu_{x + t}} \\\\ \\end{aligned} \\] Thus, if \\(mu_x\\) is known for all ages (x), then the survival probabilites for any age can be calculated from it. In other words, the distribution of \\(T_x\\) can be determined from the force of mortality . Continuous Expectation Life Expectancy measures the number of years a person is expected to live. It most commonly used with respect to newborns. It is calculated as the Expectation of the distribution of future lifetime: \\[ \\begin{aligned} \\mathring{e}_x &= E(T_x) \\\\ &= \\int_{0}^{t} t f_x(t) \\\\ &= - \\int_{0}^{t} t S_x(t) \\end{aligned} \\] Let \\(u=t\\) and \\(dv=S'_x(t)\\) . Through integration by parts, \\[ \\begin{aligned} \\mathring{e}_x &= -\\left(uv - \\int_{0}^{\\infty} v du\\right) \\\\ &= -\\left(\\Bigl[tS_x(t)\\Bigr]_0^\\infty - \\int_0^\\infty S_x(t)\\right) \\\\ &= \\int_0^\\infty S_x(t) \\end{aligned} \\] Recall that \\([-t * S_x(t)]^{\\infty}_0\\) is 0 as the probability of living forever is 0. More formally, the above is known as the Complete Expectation of Life . If the expectation was calculated over a specified term instead of indefinitely, then the result is known as the Term Expectation of Life . Continuous Variance The second moment can be calculated in a similar fashion: \\[ \\begin{aligned} E(T^2_x) &= \\int_{0}^{\\infty} t^2 f_x(t) \\\\ &= -\\left(\\Bigl[t^2 S_x(t)\\Bigr]_0^\\infty - \\int_{0}^{\\infty} 2tS_x(t) \\right) \\\\ &= \\int_{0}^{\\infty} 2tS_x(t) \\end{aligned} \\] Thus, the Variance of the future lifetime about the Expectation is the following: \\[ \\begin{aligned} Var(T_x) &= E(T^2_x) - [E(T_x)]^2 \\\\ &= \\int_{0}^{\\infty} 2tS_x(t) - \\left(\\int_0^\\infty S_x(t) \\right)^2 \\end{aligned} \\] Actuarial Notation Given how often these expressions are used, they are often abbreviated using the International Actuarial Notation : \\[ \\begin{aligned} {}_{t}p_{x} &= S_x(t) \\\\ {}_{t}q_{x} &= F_x(t) \\\\ {}_{t}q_{x} + {}_{t}p_{x} &= 1 \\end{aligned} \\] If \\(t=1\\) , it is omitted; EG. \\({}_{1}q_{x} = q_{x}\\) . Deferred Deaths are expressed using the pipe symbol: \\[ \\begin{aligned} {}_{t|u}q_{x} &= S_x(t)F_{x+t}(u) \\\\ &= {}_{t}p_{x} * {}_{u}q_{x+t} \\end{aligned} \\] Discrete Lifetime If only the integer components of the future lifetime are considered, then it becomes a discrete distribution known as the Curtate Future Lifetime of the person. It is denoted by the random variable \\(K_x\\) , which is a truncated version of \\(T_x\\) . Formally, it is known as a Floor Function : \\[ \\begin{aligned} K_x &= \\lfloor T_x \\rfloor \\\\ K_x &= 0, 1, \\dots \\end{aligned} \\] It measures the number of full years that the individual is expected to live for. If \\(K_x = k\\) , then it means the individual will another \\(k\\) full years but not live past \\(k+1\\) years full years. In other words, they will die between \\(k\\) (inclusive) and \\(k+1\\) (Exclusive) years . Note that the distribution starts from 0 . If \\(K_x = 0\\) , then they will not live another full year; they will die within the current year. \\[ \\begin{aligned} P(K_x = k) &= P(x+k \\le T_0 < x+k+1) \\\\ &= P(k \\le T_x < k+1) \\\\ &= P(T_x < k+1) - P(T_x \\le k) \\\\ &= F_x(k+1) - F_x(k) \\\\ &= S_x(k) - S_x(k+1) \\\\ &= S_x(k) - S_x(k) * S_{x+k}(1) \\\\ &= S_x(k) (1 - S_{x+k}(1)) \\\\ &= S_x(k) F_{x+k}(1) \\\\ &= {} {}_{k}p_{x} {}_{}q_{x + k} \\\\ &= {}_{k|}q_{x} \\end{aligned} \\] Discrete Expectation Similarly, the Expected Curtate Lifetime is the expectation of \\(K_x\\) : \\[ \\begin{aligned} e_x &=(K_x) \\\\ &= \\sum_{k=0}^{\\infty} k ({}_{k}p_{x} - {}_{k+1}p_{x}) \\\\ &= 1 ({}_{1}p_{x} - {}_{2}p_{x}) + 2 ({}_{2}p_{x} - {}_{3}p_{x}) + 3 ({}_{3}p_{x} - {}_{4}p_{x}) + ... \\\\ &= {}_{1}p_{x} - {}_{2}p_{x} + 2{}_{2}p_{x} - 2{}_{3}p_{x} + 3 {}_{3}p_{x} - 3 {}_{4}p_{x} \\\\ &= {}_{1}p_{x} + 2{}_{2}p_{x} + 3 {}_{3}p_{x} + ... \\\\ &= \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\end{aligned} \\] If the expectation for one period is known, the expectation for all other periods can be calculated recurively: \\[ \\begin{aligned} e _{x+1} &= \\sum _{k=1}^{\\infty} {}_{k}p_{x+1} \\\\ &= \\sum _{k=1}^{\\infty} \\frac{{}_{k+1}p_{x}}{{}_{1}p_{x}} \\\\ &= \\frac{1}{p_x}\\cdot \\left(\\sum _{k=1}^{\\infty} {}_{k}p_{x} - {}_{1}p_{x} \\right)\\\\ &= \\frac{1}{p_x}(e_x - {}_{1}p_{x}) \\\\ &= \\frac{e_x}{p_x} - 1 \\end{aligned} \\] Generally speaking, the discrete expectation can be thought of as an approximation of the continuous one. Mathematically speaking, the continuous one calculates the area under the survival function, which the discrete version approximates using rectangles. Since the rectangles under approximate the area, \\[ e_x < \\mathring{e}_x \\] If it is asssumed that the lifetime variable is uniformly distributed within the year (which is a fair assumption because the chance of dying in December is not much larger than dying in January), then it can be shown that: \\[ \\mathring{e}_x \\approx e_x + \\frac{1}{2} \\] Note that the assumption falls off for older years as the difference in mortality between the end of year and the beginning of year is larger for older lives than for younger lives. Discrete Variance The second moment can be calculated in a similar fashion: \\[ \\begin{aligned} E(K^2_x) &= \\sum_{k=0}^{\\infty} k^2 ({}_{k}p_{x} - {}_{k+1}p_{x}) \\\\ &= 1 ({}_{1}p_{x} - {}_{2}p_{x}) + 4 ({}_{2}p_{x} - {}_{3}p_{x}) + 9 ({}_{3}p_{x} - {}_{4}p_{x}) + ... \\\\ &= {}_{1}p_{x} - {}_{2}p_{x} + 4{}_{2}p_{x} - 4{}_{3}p_{x} + 9 {}_{3}p_{x} - 9 {}_{4}p_{x} \\\\ &= {}_{1}p_{x} + 3{}_{2}p_{x} + 5 {}_{3}p_{x} + ... \\\\ &= \\sum_{k=1}^{\\infty} (2k-1) {}_{k}p_{x} \\\\ &= \\sum_{k=1}^{\\infty} 2k{}_{k}p_{x} - {}_{k}p_{x} \\\\ &= \\sum_{k=1}^{\\infty} 2k{}_{k}p_{x} - \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\\\ &= 2 \\sum_{k=1}^{\\infty} k{}_{k}p_{x} - e_x \\end{aligned} \\] Thus, the variance can be calculated as: \\[ \\begin{aligned} Var(K_x) &= E(K_x^2) - [E(K_x)]^2 \\\\ &= 2 \\sum_{k=1}^{\\infty} k{}_{k}p_{x} - e_x - (e_x)^2 \\end{aligned} \\] Parametric Survival Models Given the importance of \\(\\mu_x\\) , several mathematical functions have been made to describe the force of mortality, known as a Parametric Survival Model . De Moivre Model Gompertz Model Refer to youtube video for proof Makeham Model Gompertz Makeham Distributions Gompertz Law law suggests that mortality increases exponentially with age : \\[ \\mu_x = Bc^x \\] \\(B\\) represents the initial level of mortality while \\(c\\) represents the rate of change of mortality Makeha","title":"Survival Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#survival-models","text":"Survival Models are probability distributions that measure the time to failure of an entity, or phrased another way, the future lifetime of an entity. In an Actuarial context, it measures the time to death of a person. Since survival models measure time , they are denoted by the continuous random variable \\(T_i\\) , where the subscript represents the age of the person being studied.","title":"Survival Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#newborn-lifetime","text":"The base survival model measures the future lifetime of a newborn ( aged 0 ), denoted by the random variable \\(T_0\\) . The CDF thus represents the probability that the newborn dies before age \\(t\\) : \\[ F_0(t) = P(T_0 \\le t) \\] The Survival Function is the complement of the CDF and is the probability that the newborn survives till a certain age \\(t\\) . From a different perspective, it can also be seen as the probability of the newborn dying AFTER age \\(t\\) : \\[ \\begin{aligned} S_0(t) &= P(T_0 \\ge t) \\\\ &= 1 - P(T_0 \\le t) \\\\ &= 1 - F_0(t) \\end{aligned} \\] Note that all humans will inevitably die : \\[ \\begin{aligned} \\lim_{t \\to \\infty} S_0 (t) = 0 \\iff \\lim_{t \\to \\infty} F_0 (t) = 1 \\end{aligned} \\]","title":"Newborn Lifetime"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#continuous-lifetime","text":"The above can be generalized for a person aged \\(x\\) , denoted by \\(T_x\\) . However, the intepretation of the two functions changes slightly. The CDF instead represents the probability that the person dies within \\(t\\) years from age \\(x\\) : \\[ \\begin{aligned} F_x(t) &= P(T_x \\le t) \\\\ T_x &= 1, 2, \\dots \\end{aligned} \\] It can also be written in terms of the newborn lifetime . The key is to understand that the newborn is assumed to survive till age \\(x\\) and then die within the next \\(t\\) years: \\[ \\begin{aligned} P(T_x \\le t) &= P(T_0 \\le x + t | T_0 > x) \\\\ &= \\frac{P(x < T_0 < x +t)}{P(T_0 > x)} \\\\ &= \\frac{P(T_0 < x +t)-P(T_0 < x)}{P(T_0 > x)} \\\\ &= \\frac{F_0(x+t) - F_0(x)}{S_0(x)} \\end{aligned} \\] Conversely, the survival function is the probability that the person survives another \\(t\\) years or dies AFTER \\(t\\) years : \\[ S_x(t) = P(T_x \\ge t) \\] It can also be written in terms of the newborn lifetime following the same logic as above. The newborn is assumed to survive till age \\(x\\) and then survive another \\(t\\) years: \\[ \\begin{aligned} P(T_x \\ge t) &= P(T_0 \\ge x + t | T_0 > x) \\\\ &= \\frac{P(T_0 > x + t)}{P(T_0 > x)} \\\\ &= \\frac{S_0(x+t)}{S_0(x)} \\end{aligned} \\] Note that since the surviving till age \\(x\\) is a subset of surviving till age \\(x+t\\) , the numerator can be simplified to just surviving till \\(x+t\\) . Note the survival function can thus be re-written as the following: \\[ \\begin{aligned} S_x(t) &= \\frac{S_0(x+t)}{S_0(x)} \\\\ S_0(x+t) &= S_0(x)S_x(t) \\\\ \\end{aligned} \\] The probability of the survival of a newborn till age \\(x+t\\) is the product of: The probability of survival of a newborn till age \\(x\\) Given that the newborn survived till age \\(x\\) , that they will survive another \\(t\\) years","title":"Continuous Lifetime"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#probability-tree","text":"This can be generalized for people of any age to obtain: \\[ S_x(t+u) = S_x(t) S_{x+t}(u) \\] It is important to remember to \"update\" the starting age of the second term to the \"new age\". flowchart LR A([x]) ---> B([x+t, Survive t years]) ---> C([x+t+u, Survive t+u years]) A([x]) ---> E([Die within t years]) B([x+t, Survive t years]) ---> F([Death within t+u years]) \\[ \\begin{aligned} \\text{Death within t years} &= F_x(t) \\\\ \\\\ \\text{Survive t years} &= S_x(t) \\\\ \\\\ \\text{Death within (t+u) years} &= F_x(t+u) \\\\ &= S_x(t) F_{x+t}(u) \\\\ \\\\ \\text{Survive (t+u) years} &= S_x(t+u) \\\\ &= S_x(t) S_{x+t}(u) \\\\ \\\\ \\therefore \\text{Survive t years} &= \\text{Death within (t+u) years} + \\text{Survive (t+u) years} \\\\ S_x(t) &= S_x(t) F_{x+t}(u) + S_x(t) S_{x+t}(u) \\end{aligned} \\]","title":"Probability Tree"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#deferred-death","text":"The probability of \"deferred\" death can also be calculated, whereby the person surives another \\(t\\) years and then dies within the following \\(u\\) years : \\[ \\begin{aligned} P(t < T_x < t+u) &= P(T_x < t+u) - P(T_x < t) \\\\ &= F_x(t+u) - F_x(t) \\\\ &= 1 - S_x(t+u) - (1 - S_x(t)) \\\\ &= 1 - S_x(t)S_{x+t}(u) - 1 + S_x(t) \\\\ &= S_x(t) - S_x(t)S_{x+t}(u) \\\\ &= S_x(t)(1-S_{x+t}(u)) \\\\ &= S_x(t)F_{x+t}(u) \\end{aligned} \\] Note that this is consistent with the probability tree approach earlier.","title":"Deferred Death"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#force-of-mortality","text":"The Force of Mortality is the probability of dying instantly ; in an infinitely small unit of time (normalized by unit time): \\[ \\begin{aligned} \\mu_(x) &= \\lim_{h \\to 0} \\frac{P(T_x < h)}{h} \\\\ &= \\lim_{h \\to 0} \\frac{1 - S_x(h)}{h} \\\\ &= \\lim_{h \\to 0} \\frac{1 - (\\frac{S_0(x+h)}{S_0(x)})}{h} \\\\ &= \\lim_{h \\to 0} \\frac{S_0(x) - S_0(x+h)}{S_0(x)h} \\\\ &= \\frac{1}{S_0(x)} \\lim_{h \\to 0} \\frac{S_0(x) - S_0(x+h)}{h} \\\\ &= - \\frac{1}{S_0(x)} \\lim_{h \\to 0} \\frac{S_0(x+h) - S_0(x)}{h} \\\\ &= - \\frac{1}{S_0(x)} S'_0(x) \\\\ &= - \\frac{S'_0(x)}{S_0(x)} \\end{aligned} \\] Since \\(S_0(x)\\) is the complement of \\(F_0(x)\\) , the derivative of the survival function can be written in terms of the PDF of the future lifetime: \\[ \\begin{aligned} f_0(x) &= \\frac{d}{dx} F_0(x) \\\\ &= \\frac{d}{dx} (1-S_0(x)) \\\\ &= -S'_0(x) \\end{aligned} \\] The force of mortality can be rewritten as the following: \\[ \\mu_x = \\frac{f_0(x)}{S_0(x)} \\] Thus, the force of mortality can also be intepreted as the CONDITIONAL probability distribution of death for a person aged \\(x\\) , assuming that the person survives till age \\(x\\) . \\(f_0(x)\\) is UNCONDITIONAL distribution of the future lifetime as it does not assume the individual lives to any age.","title":"Force of Mortality"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#alternative-death-function","text":"More generally, the force of mortality can be rewritten as: \\[ \\begin{aligned} \\mu_x &= \\frac{f_0(x)}{S_0(x)} \\\\ \\mu_{x+t} &= \\frac{f_x(t)}{S_x(t)} \\\\ f_x(t) &= S_x(t)\\mu_{x+t} \\\\ F_x(t) &= \\int_0^t S_x(t)\\mu_{x+t} \\end{aligned} \\] The probability of death in an interval is the SUM of the product of the probability of surviving till a given age and then dying in an infinitely small time afterwards. The sum of all the infinitely small time periods is equal to the given time interval. Following this definition, for extremely small time intervals , the force of mortality can be used to approximate the probability of death in that interval: \\[ P(T_x < h) \\approx h * \\mu_{x} \\]","title":"Alternative Death Function"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#alternative-survival-function","text":"More generally, the force of mortality can be rewritten as: \\[ \\begin{aligned} \\mu_(x) &= - \\frac{S'_0(x)}{S_0(x)} \\\\ \\mu_{x + t} &= \\frac{-S'_x(t)}{S_x(t)} \\\\ \\mu_{x + t} &= - \\frac{d}{dt} (\\ln S_x(t)) \\\\ \\ln S_x(t) &= - \\int_{0}^{t} \\mu_{x + t} \\\\ S_x(t) &= e ^ {- \\int_{0}^{t} \\mu_{x + t}} \\\\ \\end{aligned} \\] Thus, if \\(mu_x\\) is known for all ages (x), then the survival probabilites for any age can be calculated from it. In other words, the distribution of \\(T_x\\) can be determined from the force of mortality .","title":"Alternative Survival Function"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#continuous-expectation","text":"Life Expectancy measures the number of years a person is expected to live. It most commonly used with respect to newborns. It is calculated as the Expectation of the distribution of future lifetime: \\[ \\begin{aligned} \\mathring{e}_x &= E(T_x) \\\\ &= \\int_{0}^{t} t f_x(t) \\\\ &= - \\int_{0}^{t} t S_x(t) \\end{aligned} \\] Let \\(u=t\\) and \\(dv=S'_x(t)\\) . Through integration by parts, \\[ \\begin{aligned} \\mathring{e}_x &= -\\left(uv - \\int_{0}^{\\infty} v du\\right) \\\\ &= -\\left(\\Bigl[tS_x(t)\\Bigr]_0^\\infty - \\int_0^\\infty S_x(t)\\right) \\\\ &= \\int_0^\\infty S_x(t) \\end{aligned} \\] Recall that \\([-t * S_x(t)]^{\\infty}_0\\) is 0 as the probability of living forever is 0. More formally, the above is known as the Complete Expectation of Life . If the expectation was calculated over a specified term instead of indefinitely, then the result is known as the Term Expectation of Life .","title":"Continuous Expectation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#continuous-variance","text":"The second moment can be calculated in a similar fashion: \\[ \\begin{aligned} E(T^2_x) &= \\int_{0}^{\\infty} t^2 f_x(t) \\\\ &= -\\left(\\Bigl[t^2 S_x(t)\\Bigr]_0^\\infty - \\int_{0}^{\\infty} 2tS_x(t) \\right) \\\\ &= \\int_{0}^{\\infty} 2tS_x(t) \\end{aligned} \\] Thus, the Variance of the future lifetime about the Expectation is the following: \\[ \\begin{aligned} Var(T_x) &= E(T^2_x) - [E(T_x)]^2 \\\\ &= \\int_{0}^{\\infty} 2tS_x(t) - \\left(\\int_0^\\infty S_x(t) \\right)^2 \\end{aligned} \\]","title":"Continuous Variance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#actuarial-notation","text":"Given how often these expressions are used, they are often abbreviated using the International Actuarial Notation : \\[ \\begin{aligned} {}_{t}p_{x} &= S_x(t) \\\\ {}_{t}q_{x} &= F_x(t) \\\\ {}_{t}q_{x} + {}_{t}p_{x} &= 1 \\end{aligned} \\] If \\(t=1\\) , it is omitted; EG. \\({}_{1}q_{x} = q_{x}\\) . Deferred Deaths are expressed using the pipe symbol: \\[ \\begin{aligned} {}_{t|u}q_{x} &= S_x(t)F_{x+t}(u) \\\\ &= {}_{t}p_{x} * {}_{u}q_{x+t} \\end{aligned} \\]","title":"Actuarial Notation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#discrete-lifetime","text":"If only the integer components of the future lifetime are considered, then it becomes a discrete distribution known as the Curtate Future Lifetime of the person. It is denoted by the random variable \\(K_x\\) , which is a truncated version of \\(T_x\\) . Formally, it is known as a Floor Function : \\[ \\begin{aligned} K_x &= \\lfloor T_x \\rfloor \\\\ K_x &= 0, 1, \\dots \\end{aligned} \\] It measures the number of full years that the individual is expected to live for. If \\(K_x = k\\) , then it means the individual will another \\(k\\) full years but not live past \\(k+1\\) years full years. In other words, they will die between \\(k\\) (inclusive) and \\(k+1\\) (Exclusive) years . Note that the distribution starts from 0 . If \\(K_x = 0\\) , then they will not live another full year; they will die within the current year. \\[ \\begin{aligned} P(K_x = k) &= P(x+k \\le T_0 < x+k+1) \\\\ &= P(k \\le T_x < k+1) \\\\ &= P(T_x < k+1) - P(T_x \\le k) \\\\ &= F_x(k+1) - F_x(k) \\\\ &= S_x(k) - S_x(k+1) \\\\ &= S_x(k) - S_x(k) * S_{x+k}(1) \\\\ &= S_x(k) (1 - S_{x+k}(1)) \\\\ &= S_x(k) F_{x+k}(1) \\\\ &= {} {}_{k}p_{x} {}_{}q_{x + k} \\\\ &= {}_{k|}q_{x} \\end{aligned} \\]","title":"Discrete Lifetime"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#discrete-expectation","text":"Similarly, the Expected Curtate Lifetime is the expectation of \\(K_x\\) : \\[ \\begin{aligned} e_x &=(K_x) \\\\ &= \\sum_{k=0}^{\\infty} k ({}_{k}p_{x} - {}_{k+1}p_{x}) \\\\ &= 1 ({}_{1}p_{x} - {}_{2}p_{x}) + 2 ({}_{2}p_{x} - {}_{3}p_{x}) + 3 ({}_{3}p_{x} - {}_{4}p_{x}) + ... \\\\ &= {}_{1}p_{x} - {}_{2}p_{x} + 2{}_{2}p_{x} - 2{}_{3}p_{x} + 3 {}_{3}p_{x} - 3 {}_{4}p_{x} \\\\ &= {}_{1}p_{x} + 2{}_{2}p_{x} + 3 {}_{3}p_{x} + ... \\\\ &= \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\end{aligned} \\] If the expectation for one period is known, the expectation for all other periods can be calculated recurively: \\[ \\begin{aligned} e _{x+1} &= \\sum _{k=1}^{\\infty} {}_{k}p_{x+1} \\\\ &= \\sum _{k=1}^{\\infty} \\frac{{}_{k+1}p_{x}}{{}_{1}p_{x}} \\\\ &= \\frac{1}{p_x}\\cdot \\left(\\sum _{k=1}^{\\infty} {}_{k}p_{x} - {}_{1}p_{x} \\right)\\\\ &= \\frac{1}{p_x}(e_x - {}_{1}p_{x}) \\\\ &= \\frac{e_x}{p_x} - 1 \\end{aligned} \\] Generally speaking, the discrete expectation can be thought of as an approximation of the continuous one. Mathematically speaking, the continuous one calculates the area under the survival function, which the discrete version approximates using rectangles. Since the rectangles under approximate the area, \\[ e_x < \\mathring{e}_x \\] If it is asssumed that the lifetime variable is uniformly distributed within the year (which is a fair assumption because the chance of dying in December is not much larger than dying in January), then it can be shown that: \\[ \\mathring{e}_x \\approx e_x + \\frac{1}{2} \\] Note that the assumption falls off for older years as the difference in mortality between the end of year and the beginning of year is larger for older lives than for younger lives.","title":"Discrete Expectation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#discrete-variance","text":"The second moment can be calculated in a similar fashion: \\[ \\begin{aligned} E(K^2_x) &= \\sum_{k=0}^{\\infty} k^2 ({}_{k}p_{x} - {}_{k+1}p_{x}) \\\\ &= 1 ({}_{1}p_{x} - {}_{2}p_{x}) + 4 ({}_{2}p_{x} - {}_{3}p_{x}) + 9 ({}_{3}p_{x} - {}_{4}p_{x}) + ... \\\\ &= {}_{1}p_{x} - {}_{2}p_{x} + 4{}_{2}p_{x} - 4{}_{3}p_{x} + 9 {}_{3}p_{x} - 9 {}_{4}p_{x} \\\\ &= {}_{1}p_{x} + 3{}_{2}p_{x} + 5 {}_{3}p_{x} + ... \\\\ &= \\sum_{k=1}^{\\infty} (2k-1) {}_{k}p_{x} \\\\ &= \\sum_{k=1}^{\\infty} 2k{}_{k}p_{x} - {}_{k}p_{x} \\\\ &= \\sum_{k=1}^{\\infty} 2k{}_{k}p_{x} - \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\\\ &= 2 \\sum_{k=1}^{\\infty} k{}_{k}p_{x} - e_x \\end{aligned} \\] Thus, the variance can be calculated as: \\[ \\begin{aligned} Var(K_x) &= E(K_x^2) - [E(K_x)]^2 \\\\ &= 2 \\sum_{k=1}^{\\infty} k{}_{k}p_{x} - e_x - (e_x)^2 \\end{aligned} \\]","title":"Discrete Variance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#parametric-survival-models","text":"Given the importance of \\(\\mu_x\\) , several mathematical functions have been made to describe the force of mortality, known as a Parametric Survival Model .","title":"Parametric Survival Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#de-moivre-model","text":"","title":"De Moivre Model"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#gompertz-model","text":"Refer to youtube video for proof","title":"Gompertz Model"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#makeham-model","text":"","title":"Makeham Model"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#gompertz-makeham-distributions","text":"Gompertz Law law suggests that mortality increases exponentially with age : \\[ \\mu_x = Bc^x \\] \\(B\\) represents the initial level of mortality while \\(c\\) represents the rate of change of mortality Makeha","title":"Gompertz Makeham Distributions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/","text":"Life Tables The Discrete Survival Model above can be expressed in tabular form as well, known as a Life Table . Instead of providing the probabilities directly, it provides the expected number of people alive at a given age \\(l_x\\) . From these values, the probability rates can be calculated. The lowest age of the life table is known as the Starting Age , denoted by \\(\\alpha\\) . Conversely, the highest age is known as the Terminal Age , denoted by \\(\\omega\\) . The number of people alive at the starting age is known as the Radix of the life table, which is usually a large round number; \\(l_{\\alpha} = 100,000\\) . All members of the poulation are expected to gradually die by the terminal age; \\(l_{\\omega} = 0\\) . \\[ l_x > l_{x+t} \\] An additional column for \\(d_x\\) can also be calculated, which represents the expected number of people who die within a year ; from age \\(x\\) to age \\(x+1\\) . Note that because the life table is decreasing, it is written from the perspective of \\(x\\) , not \\(x+1\\) . \\[ d_x = l_x - l_{x+1} \\] If the life table is incomplete, \\(d_x\\) can be used to fill up the remaining information: \\[ l_x - l_{x+n} = d_x + d_{x+1} + ... d_{x+n-1} \\] The probability of death/survival over a period can be calculated as the proportion of people who died/survived in that period: \\[ \\begin{aligned} {}_{t}p_{x} &= \\frac{l_{x+t}}{l_x} \\\\ {}_{t}q_{x} &= \\frac{l_x - l_{x+t}}{l_x} \\\\ {}_{s|t}q_{x} &= \\frac{l_{x+s} - l_{x+s+t}}{l_{x}} \\\\ \\end{aligned} \\] If considering only the probabilities for a single period , then \\(d_x\\) can be used as well: \\[ \\begin{aligned} p_{x} &= \\frac{l_x - d_x}{l_x} \\\\ q_{x} &= \\frac{d_x}{l_x} \\\\ \\end{aligned} \\] If the life table only includes the yearly probabilities, then the probability over a specific period can be calculated via first principles using a probability tree: \\[ {}_{t}q_{x} = q_x + p_x * q_{x+1} + p_x * p_{x+1} * q_{x+2} + ... p_{x} * p _{x+1} * ... p_{x+n-2} * q_{x+n-1} \\]","title":"Life Tables"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#life-tables","text":"The Discrete Survival Model above can be expressed in tabular form as well, known as a Life Table . Instead of providing the probabilities directly, it provides the expected number of people alive at a given age \\(l_x\\) . From these values, the probability rates can be calculated. The lowest age of the life table is known as the Starting Age , denoted by \\(\\alpha\\) . Conversely, the highest age is known as the Terminal Age , denoted by \\(\\omega\\) . The number of people alive at the starting age is known as the Radix of the life table, which is usually a large round number; \\(l_{\\alpha} = 100,000\\) . All members of the poulation are expected to gradually die by the terminal age; \\(l_{\\omega} = 0\\) . \\[ l_x > l_{x+t} \\] An additional column for \\(d_x\\) can also be calculated, which represents the expected number of people who die within a year ; from age \\(x\\) to age \\(x+1\\) . Note that because the life table is decreasing, it is written from the perspective of \\(x\\) , not \\(x+1\\) . \\[ d_x = l_x - l_{x+1} \\] If the life table is incomplete, \\(d_x\\) can be used to fill up the remaining information: \\[ l_x - l_{x+n} = d_x + d_{x+1} + ... d_{x+n-1} \\] The probability of death/survival over a period can be calculated as the proportion of people who died/survived in that period: \\[ \\begin{aligned} {}_{t}p_{x} &= \\frac{l_{x+t}}{l_x} \\\\ {}_{t}q_{x} &= \\frac{l_x - l_{x+t}}{l_x} \\\\ {}_{s|t}q_{x} &= \\frac{l_{x+s} - l_{x+s+t}}{l_{x}} \\\\ \\end{aligned} \\] If considering only the probabilities for a single period , then \\(d_x\\) can be used as well: \\[ \\begin{aligned} p_{x} &= \\frac{l_x - d_x}{l_x} \\\\ q_{x} &= \\frac{d_x}{l_x} \\\\ \\end{aligned} \\] If the life table only includes the yearly probabilities, then the probability over a specific period can be calculated via first principles using a probability tree: \\[ {}_{t}q_{x} = q_x + p_x * q_{x+1} + p_x * p_{x+1} * q_{x+2} + ... p_{x} * p _{x+1} * ... p_{x+n-2} * q_{x+n-1} \\]","title":"Life Tables"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/","text":"Life Insurance Functions Life Insurance contracts promise to pay out a benefit if the insured event (Death) occurs in the future . The value of a life insurance contract must reflect these two aspects: Uncertainty of cashflows - Expected Value , based on Survival Models Time value of money - Present Value , based on Interest Theory Thus, the value of a life insurance contract is the Expected Present Value (EPV) of the promised cashflows. There are two categories of insurance contracts, based on when the benefits are paid out: Discrete Continuous Paid at the end of the period that the insured died Paid out at immediately upon the death of the insured Payable Discretely If the contract pays out benefits discretely, then \\(K_x\\) is used as the survival model. Given a benefit of $1, the PV of the benefit is \\(v^{K_x + 1}\\) . Thus, the EPV of the contract is the sum product of the PV of the benefit in each period and the probability of death in that period: \\[ \\begin{aligned} EPV &= \\sum v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\\\ &= \\sum v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Whole Life Assurance Whole Life Assurance covers the insured indefinitely and thus will pay out whenever the insured dies . Let WL be the random variable denoting the PV of the death benefit: \\[ \\begin{aligned} \\text{WL} &= v^{K_x + 1} \\\\ K_x &= 0, 1, \\dots, \\infty \\end{aligned} \\] Technically speaking, the upper limit of \\(K_x\\) should \\(\\omega-x\\) as it represents the maximum attainable age in the discrete survival model. However, since the \\({}_{k}p_{x} = 0\\) for all \\(k \\ge \\omega-x\\) , it does not matter what the upper limit is as long as it is larger than \\(\\omega-x\\) . Thus, in order to be concise, \\(\\infty\\) is often used instead. Thus, the EPV is the Expectation/First Moment of the WL random variable: \\[ E(\\text{WL}) = \\sum^\\infty_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\] The Second Moment can be similarly caclulated: \\[ E({\\text{WL}}^2) = \\sum^\\infty_{K_x = 0} (v^{K_x + 1})^2 \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\] This can be simplified to a formula similar to the first moment by substituting \\(v' = v^2\\) , which can be further generalized to the k-th moment of the random variable by substituting \\(v' = v^k\\) instead. The simplified second moment and thus Variance be calculated: \\[ \\begin{aligned} E({\\text{WL}}^2) &= \\sum^\\infty_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\\\ &= \\sum^\\infty_{K_x = 0} (v')^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\\\ \\\\ \\therefore Var(\\text{WL}) &= E({\\text{WL}}^2) - [E(\\text{WL})]^2 \\end{aligned} \\] Term Assurance Term Assurance covers the insured for a specified period \\(n\\) and only pays out if the insured dies within that period. Nothing is paid out if the insured survives beyond that. Let TA be the random variable denoting the PV of the death benefit: \\[ \\begin{aligned} \\text{TA} &= \\begin{cases} v^{K_x + 1} ,& K_x = 0, 1, \\dots, n-1 \\\\ 0 ,& K_x \\ge n \\end{cases} \\end{aligned} \\] Thus, the EPV is the expectation of the TA random variable: \\[ E(\\text{TA}) = \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\] Similarly, the simplified second moment and thus Variance can be calculated using the \\(v' = v^2\\) substitution: \\[ \\begin{aligned} E({\\text{TA}}^2) &= \\sum^{n-1}_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\\\ &= \\sum^\\infty_{K_x = 0} (v')^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\\\ \\\\ \\therefore Var(\\text{TA}) &= E({\\text{TA}}^2) - E(\\text{TA}) \\end{aligned} \\] Pure Endowment Pure Endowments are a special kind of insurance that instead only pays out if the insured survives past a certain period . Nothing is paid out if the insured dies before that. Let PE be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{PE} &= \\begin{cases} 0 ,& K_x < n \\\\ v^n ,& K_x \\ge n \\end{cases} \\end{aligned} \\] The probability of surviving past the period is given by a single probability \\({}_{n}p_{x}\\) . Thus, the EPV is the expectation of the PE random variable: \\[ E(\\text{PE}) = v^n {}_{n}p_{x} \\] Add more intuition Similarly, the simplified second moment and thus Variance can be calculated using the \\(v' = v^2\\) substitution: \\[ \\begin{aligned} E({\\text{PE}}^2) &= (v^2)^n {}_{n}p_{x} \\\\ &= (v')^n {}_{n}p_{x} \\\\ \\\\ \\therefore Var(\\text{PE}) &= E({\\text{PE}}^2) - E(\\text{PE}) \\end{aligned} \\] Endowment Assurance Endowment Assurances are a combination of term assurances and pure endowments: Term Assurance - Pays out if the insured dies within the period Pure Endowment - Pays out if the insured survives past the period Similar to whole life policies, endowment assurances will make a payout. Let EA be the random variable denoting the PV of the benefits: \\[ \\begin{aligned} \\text{EA} &= \\text{TA} + \\text{PE} \\\\ &= \\begin{cases} v^{K_x + 1} ,& K_x = 0, 1, \\dots, n-1 \\\\ v^n ,& K_x \\ge n \\end{cases} \\\\ &= v^{min(K_x + 1, n)} \\end{aligned} \\] Thus, the EPV is the expectation of the EA random variable: \\[ \\begin{aligned} E(\\text{EA}) &= E(\\text{TA}) + E(\\text{PE}) \\\\ &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} + v^n {}_{n}p_{x} \\end{aligned} \\] Note that the in the final year of the contract, the individual will receive \\(v^n\\) if they die (from the TA) or receive the same \\(v^n\\) if they survive (from the PE). Thus, a simplification can be made to the EPV: \\[ \\begin{aligned} E(\\text{EA}) &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} + v^n {}_{n}p_{x} \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} + v^n {}_{n-1}p_{x} {}_{}q_{x + n - 1} + v^n {}_{n}p_{x} \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} + v^n ({}_{n-1}p_{x} {}_{}q_{x + n - 1} + {}_{n}p_{x}) \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} + v^n {}_{n-1}p_{x} \\end{aligned} \\] Deferred Assurances Deferred Assurances are variations of the above contracts where the coverage is deferred by \\(n\\) years . While any contract can be deferred, the most useful is the Deferred Whole Life Assurance . Let DWL be the random variable denoting the PV of the benefits of a deferred whole life assurance: \\[ \\begin{aligned} \\text{DWL} &= \\begin{cases} 0, K_x = 0 ,& 1, 2, \\dots, n-1 \\\\ v^{K_x + 1} ,& K_x \\ge n \\end{cases} \\end{aligned} \\] Thus, the EPV is the expectation of the DWL random variable: \\[ \\begin{aligned} E(\\text{DWL}) &= \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\\\ &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} - \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\\\ &= E(WL) - E(TA) \\end{aligned} \\] Intuitively, a DWL can be thought of as an WL that is issued \\(n\\) years later . Thus, the EPV of this WL policy can be brought forward time by adjusting for both interest and survival : \\[ E(\\text{DWL}) = v^n {}_{n}p_{x} * E(WL_{x+n}) \\] This allows a TA to be expressed as the difference of two WL policies issued at different times : \\[ \\begin{aligned} E(\\text{DWL}) &= v^n {}_{n}p_{x} * E(WL_{x+n}) \\\\ E(WL) - E(TA) &= v^n {}_{n}p_{x} * E(WL_{x+n}) \\\\ E(TA) &= E(WL) - v^n {}_{n}p_{x} * E(WL_{x+n}) \\end{aligned} \\] Actuarial Notation Similar to the survival model, given how often these values are calculated, they are abbreviated using the International Actuarial Notation as well. \\({}^{k}A\\) represents the k-th moment of a contract where a benefit of 1 is paid discretely when the status fails (Life Insurance). \\(k\\) is the multiplier on the interest rate for that moment. As before, if \\(k=1\\) , it is omitted; EG. \\({}^{1}A = A\\) . This notation will mainly be used for the first and second moments for the EPV and variance respectively. The subscript \\(x\\) is the age of the policyholder, known as the Life Status . The life status fails when the policyholder dies. The additional subscript \\(\\enclose{actuarial}{n}\\) represents the duration of the policy , known as the Duration Status . It fails if the policyholder survives past the policyterm. A 1 above the status indicates that the policy pays only pays if that particular status fails first . If no \"1\" is included, then the policy pays whichever status fails first. If the duration status is omitted, then the policy is assumed to last forever. Thus putting everything together, the EPV of each policy can be denoted as follows: Whole Life Assurance - Payable whenever policyholder dies; \\(A_x\\) Term Assurance - Payable only if policyholder dies during policy period; \\(A^{1}_{x:\\enclose{actuarial}{n}}\\) Pure Endowment - Payable only if policyholder survives past policy period; \\(A^{\\>\\>\\>\\> 1}_{x:\\enclose{actuarial}{n}}\\) Endowment Assurance - Payable whichever of the above two occur first; \\(A_{x:\\enclose{actuarial}{n}}\\) Recursions The EPV of each contract can also be expressed through backwards recursion , where it is calculated as a function of itself. Consider the WL random variable: If the policyholder dies in the year with probability \\({}_{}q_{x}\\) , then a benefit of 1 is paid at the end of the year. If the policyholder survives past the year with probability \\({}_{}p_{x}\\) , then the policyholder will die in some future year. The PV of the benefit at the end of the year is given is the EPV of the same contract but at that future time ; \\(A_{x+1}\\) . \\[ \\begin{aligned} WL &= \\begin{cases} v \\cdot 1 ,& {}_{}q_{x} \\\\ v \\cdot A_{x+1} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore A_x = v{}_{}q_{x} + v{}_{}p_{x}A_{x+1} \\] Consider the same exercise for the TA random variable: The main difference is that the EPV of the same contract at a future time is denoted by \\(A^1_{x+1:\\enclose{actuarial}{n-1}}\\) , where the \\(n-1\\) reflects that the policyholder survived the first year of the policy, thus only \\(n-1\\) years of coverage remain. \\[ \\begin{aligned} TA &= \\begin{cases} v \\cdot 1 ,& {}_{}q_{x} \\\\ v \\cdot A^1_{x+1:\\enclose{actuarial}{n-1}} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore A^1_{x:\\enclose{actuarial}{n-1}} = v{}_{}q_{x} + v{}_{}p_{x}A^1_{x+1:\\enclose{actuarial}{n-1}} \\] Consider the same exercise for the PE random variable: The main difference is that if the policyholder dies in the current year, no benefit will be paid out at the end of the year. Thus, there only the second component remains. \\[ \\begin{aligned} PE &= \\begin{cases} 0 ,& {}_{}q_{x} \\\\ v \\cdot A^{\\>\\>\\>\\>\\>\\>\\>\\>\\> 1}_{x+1:\\enclose{actuarial}{n-1}} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore A^{\\>\\>\\> 1}_{x:\\enclose{actuarial}{n}} = v{}_{}p_{x}A^{\\>\\>\\>\\>\\>\\>\\>\\>\\> 1}_{x+1:\\enclose{actuarial}{n-1}} \\] EA is omitted as it is simply the combination of a TA and PE. These identities are most useful in a spreadsheet setting where the calculations can be easily repeated to fill up an entire life table. Intuitions WL at the end of the life table is Term same 1 PE 0 Death products increase over time Survival products decrease EA tends to term Term becomes WL when policy period exceeds end Two policies of the same type Higher probability of occuring is higher EA: For policies which are guaranteed to pay, it depends on the tendency to pay out earlier > Paid out earlier, discounted less, higher EPV 5 year EA 30 5 year EA 60 Comparing values Depends on age Young Age (Probability death low) TA is the lowest, as least likely to pay out WL is the next lowest, unlikely to pay out as well but at least WILL pay out PE is the next lowest because the probability of survival is high EA is the highest because PE + TA > PE Old age (Probability death high) PE is the lowest because least likely to pay ouf Term is next because likely to pay out EA and WL depends Both share the same probability depends on the additional > If the death probability is high enough, then the PE component of the EA becomes worhtless, the PV of the indefinite coverage is higher EA is the next highest because combination Payable Continuously Continuous Whole Life Continuous Term Continuous Endowment No such thing as continuous PE Claims Acceleration Approximate continuous claims by assuming they occur in the middle of the period x + k and x + k + 1 at x+k+1/2 \\[ \\begin{aligned} \\bar{A}_x &\\approx v^{\\frac{1}{2}} q_x + v^{\\frac{3}{2}} p_x {}_{}q_{x+1} + v^{\\frac{5}{2}} {}_{2}p_{x} {}_{}q_{x+2} + \\dots \\\\ &\\approx (1+i)^{\\frac{1}{2}} (vq_x + v^1 p_x {}_{}q_{x+1} + v^2 {}_{2}p_{x} {}_{}q_{x+2} + \\dots) \\\\ &\\approx (1+i)^{\\frac{1}{2}} A_x \\end{aligned} \\] Uniform Death Distribution Can think of Whole life as a series of terms Uniform distribution each year \\[ \\begin{aligned} \\int^1_0 v^t {}_{t}p_{x+k} \\mu_{x+k+t} &\\approx \\int^1_0 v^t {}_{}q_{x+k} &\\approx {}_{}q_{x+k} \\int^1_0 e^{\\deltat} \\end{aligned} \\] \\[ \\begin{aligned} \\bar{A}_x &= {}_{0|}\\bar{A}^1_{x+1:\\enclose{actuarial}{1}} + {}_{1|}\\bar{A}^1_{x+1:\\enclose{actuarial}{1}} + {}_{2|}\\bar{A}^1_{x+1:\\enclose{actuarial}{1}} + \\dots \\\\ &= q_x \\frac{iv}{\\delta} + vp_xq_{x+1} \\frac{iv}{\\delta} + v^2 {}_{2}p_{x} q_{x+2} \\frac{iv}{\\delta} + \\dots \\\\ &= \\frac{i}{\\delta} (q_x + v^1 p_x {}_{}q_{x+1} + v^2 {}_{2}p_{x} {}_{}q_{x+2} + \\dots) \\\\ &= \\frac{i}{\\delta} A_x \\\\ \\end{aligned} \\]","title":"Life Insurance Functions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#life-insurance-functions","text":"Life Insurance contracts promise to pay out a benefit if the insured event (Death) occurs in the future . The value of a life insurance contract must reflect these two aspects: Uncertainty of cashflows - Expected Value , based on Survival Models Time value of money - Present Value , based on Interest Theory Thus, the value of a life insurance contract is the Expected Present Value (EPV) of the promised cashflows. There are two categories of insurance contracts, based on when the benefits are paid out: Discrete Continuous Paid at the end of the period that the insured died Paid out at immediately upon the death of the insured","title":"Life Insurance Functions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#payable-discretely","text":"If the contract pays out benefits discretely, then \\(K_x\\) is used as the survival model. Given a benefit of $1, the PV of the benefit is \\(v^{K_x + 1}\\) . Thus, the EPV of the contract is the sum product of the PV of the benefit in each period and the probability of death in that period: \\[ \\begin{aligned} EPV &= \\sum v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\\\ &= \\sum v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\]","title":"Payable Discretely"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#whole-life-assurance","text":"Whole Life Assurance covers the insured indefinitely and thus will pay out whenever the insured dies . Let WL be the random variable denoting the PV of the death benefit: \\[ \\begin{aligned} \\text{WL} &= v^{K_x + 1} \\\\ K_x &= 0, 1, \\dots, \\infty \\end{aligned} \\] Technically speaking, the upper limit of \\(K_x\\) should \\(\\omega-x\\) as it represents the maximum attainable age in the discrete survival model. However, since the \\({}_{k}p_{x} = 0\\) for all \\(k \\ge \\omega-x\\) , it does not matter what the upper limit is as long as it is larger than \\(\\omega-x\\) . Thus, in order to be concise, \\(\\infty\\) is often used instead. Thus, the EPV is the Expectation/First Moment of the WL random variable: \\[ E(\\text{WL}) = \\sum^\\infty_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\] The Second Moment can be similarly caclulated: \\[ E({\\text{WL}}^2) = \\sum^\\infty_{K_x = 0} (v^{K_x + 1})^2 \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\] This can be simplified to a formula similar to the first moment by substituting \\(v' = v^2\\) , which can be further generalized to the k-th moment of the random variable by substituting \\(v' = v^k\\) instead. The simplified second moment and thus Variance be calculated: \\[ \\begin{aligned} E({\\text{WL}}^2) &= \\sum^\\infty_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\\\ &= \\sum^\\infty_{K_x = 0} (v')^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\\\ \\\\ \\therefore Var(\\text{WL}) &= E({\\text{WL}}^2) - [E(\\text{WL})]^2 \\end{aligned} \\]","title":"Whole Life Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#term-assurance","text":"Term Assurance covers the insured for a specified period \\(n\\) and only pays out if the insured dies within that period. Nothing is paid out if the insured survives beyond that. Let TA be the random variable denoting the PV of the death benefit: \\[ \\begin{aligned} \\text{TA} &= \\begin{cases} v^{K_x + 1} ,& K_x = 0, 1, \\dots, n-1 \\\\ 0 ,& K_x \\ge n \\end{cases} \\end{aligned} \\] Thus, the EPV is the expectation of the TA random variable: \\[ E(\\text{TA}) = \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\] Similarly, the simplified second moment and thus Variance can be calculated using the \\(v' = v^2\\) substitution: \\[ \\begin{aligned} E({\\text{TA}}^2) &= \\sum^{n-1}_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\\\ &= \\sum^\\infty_{K_x = 0} (v')^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\\\ \\\\ \\therefore Var(\\text{TA}) &= E({\\text{TA}}^2) - E(\\text{TA}) \\end{aligned} \\]","title":"Term Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#pure-endowment","text":"Pure Endowments are a special kind of insurance that instead only pays out if the insured survives past a certain period . Nothing is paid out if the insured dies before that. Let PE be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{PE} &= \\begin{cases} 0 ,& K_x < n \\\\ v^n ,& K_x \\ge n \\end{cases} \\end{aligned} \\] The probability of surviving past the period is given by a single probability \\({}_{n}p_{x}\\) . Thus, the EPV is the expectation of the PE random variable: \\[ E(\\text{PE}) = v^n {}_{n}p_{x} \\] Add more intuition Similarly, the simplified second moment and thus Variance can be calculated using the \\(v' = v^2\\) substitution: \\[ \\begin{aligned} E({\\text{PE}}^2) &= (v^2)^n {}_{n}p_{x} \\\\ &= (v')^n {}_{n}p_{x} \\\\ \\\\ \\therefore Var(\\text{PE}) &= E({\\text{PE}}^2) - E(\\text{PE}) \\end{aligned} \\]","title":"Pure Endowment"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#endowment-assurance","text":"Endowment Assurances are a combination of term assurances and pure endowments: Term Assurance - Pays out if the insured dies within the period Pure Endowment - Pays out if the insured survives past the period Similar to whole life policies, endowment assurances will make a payout. Let EA be the random variable denoting the PV of the benefits: \\[ \\begin{aligned} \\text{EA} &= \\text{TA} + \\text{PE} \\\\ &= \\begin{cases} v^{K_x + 1} ,& K_x = 0, 1, \\dots, n-1 \\\\ v^n ,& K_x \\ge n \\end{cases} \\\\ &= v^{min(K_x + 1, n)} \\end{aligned} \\] Thus, the EPV is the expectation of the EA random variable: \\[ \\begin{aligned} E(\\text{EA}) &= E(\\text{TA}) + E(\\text{PE}) \\\\ &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} + v^n {}_{n}p_{x} \\end{aligned} \\] Note that the in the final year of the contract, the individual will receive \\(v^n\\) if they die (from the TA) or receive the same \\(v^n\\) if they survive (from the PE). Thus, a simplification can be made to the EPV: \\[ \\begin{aligned} E(\\text{EA}) &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} + v^n {}_{n}p_{x} \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} + v^n {}_{n-1}p_{x} {}_{}q_{x + n - 1} + v^n {}_{n}p_{x} \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} + v^n ({}_{n-1}p_{x} {}_{}q_{x + n - 1} + {}_{n}p_{x}) \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} + v^n {}_{n-1}p_{x} \\end{aligned} \\]","title":"Endowment Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#deferred-assurances","text":"Deferred Assurances are variations of the above contracts where the coverage is deferred by \\(n\\) years . While any contract can be deferred, the most useful is the Deferred Whole Life Assurance . Let DWL be the random variable denoting the PV of the benefits of a deferred whole life assurance: \\[ \\begin{aligned} \\text{DWL} &= \\begin{cases} 0, K_x = 0 ,& 1, 2, \\dots, n-1 \\\\ v^{K_x + 1} ,& K_x \\ge n \\end{cases} \\end{aligned} \\] Thus, the EPV is the expectation of the DWL random variable: \\[ \\begin{aligned} E(\\text{DWL}) &= \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\\\ &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} - \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\\\ &= E(WL) - E(TA) \\end{aligned} \\] Intuitively, a DWL can be thought of as an WL that is issued \\(n\\) years later . Thus, the EPV of this WL policy can be brought forward time by adjusting for both interest and survival : \\[ E(\\text{DWL}) = v^n {}_{n}p_{x} * E(WL_{x+n}) \\] This allows a TA to be expressed as the difference of two WL policies issued at different times : \\[ \\begin{aligned} E(\\text{DWL}) &= v^n {}_{n}p_{x} * E(WL_{x+n}) \\\\ E(WL) - E(TA) &= v^n {}_{n}p_{x} * E(WL_{x+n}) \\\\ E(TA) &= E(WL) - v^n {}_{n}p_{x} * E(WL_{x+n}) \\end{aligned} \\]","title":"Deferred Assurances"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#actuarial-notation","text":"Similar to the survival model, given how often these values are calculated, they are abbreviated using the International Actuarial Notation as well. \\({}^{k}A\\) represents the k-th moment of a contract where a benefit of 1 is paid discretely when the status fails (Life Insurance). \\(k\\) is the multiplier on the interest rate for that moment. As before, if \\(k=1\\) , it is omitted; EG. \\({}^{1}A = A\\) . This notation will mainly be used for the first and second moments for the EPV and variance respectively. The subscript \\(x\\) is the age of the policyholder, known as the Life Status . The life status fails when the policyholder dies. The additional subscript \\(\\enclose{actuarial}{n}\\) represents the duration of the policy , known as the Duration Status . It fails if the policyholder survives past the policyterm. A 1 above the status indicates that the policy pays only pays if that particular status fails first . If no \"1\" is included, then the policy pays whichever status fails first. If the duration status is omitted, then the policy is assumed to last forever. Thus putting everything together, the EPV of each policy can be denoted as follows: Whole Life Assurance - Payable whenever policyholder dies; \\(A_x\\) Term Assurance - Payable only if policyholder dies during policy period; \\(A^{1}_{x:\\enclose{actuarial}{n}}\\) Pure Endowment - Payable only if policyholder survives past policy period; \\(A^{\\>\\>\\>\\> 1}_{x:\\enclose{actuarial}{n}}\\) Endowment Assurance - Payable whichever of the above two occur first; \\(A_{x:\\enclose{actuarial}{n}}\\)","title":"Actuarial Notation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#recursions","text":"The EPV of each contract can also be expressed through backwards recursion , where it is calculated as a function of itself. Consider the WL random variable: If the policyholder dies in the year with probability \\({}_{}q_{x}\\) , then a benefit of 1 is paid at the end of the year. If the policyholder survives past the year with probability \\({}_{}p_{x}\\) , then the policyholder will die in some future year. The PV of the benefit at the end of the year is given is the EPV of the same contract but at that future time ; \\(A_{x+1}\\) . \\[ \\begin{aligned} WL &= \\begin{cases} v \\cdot 1 ,& {}_{}q_{x} \\\\ v \\cdot A_{x+1} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore A_x = v{}_{}q_{x} + v{}_{}p_{x}A_{x+1} \\] Consider the same exercise for the TA random variable: The main difference is that the EPV of the same contract at a future time is denoted by \\(A^1_{x+1:\\enclose{actuarial}{n-1}}\\) , where the \\(n-1\\) reflects that the policyholder survived the first year of the policy, thus only \\(n-1\\) years of coverage remain. \\[ \\begin{aligned} TA &= \\begin{cases} v \\cdot 1 ,& {}_{}q_{x} \\\\ v \\cdot A^1_{x+1:\\enclose{actuarial}{n-1}} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore A^1_{x:\\enclose{actuarial}{n-1}} = v{}_{}q_{x} + v{}_{}p_{x}A^1_{x+1:\\enclose{actuarial}{n-1}} \\] Consider the same exercise for the PE random variable: The main difference is that if the policyholder dies in the current year, no benefit will be paid out at the end of the year. Thus, there only the second component remains. \\[ \\begin{aligned} PE &= \\begin{cases} 0 ,& {}_{}q_{x} \\\\ v \\cdot A^{\\>\\>\\>\\>\\>\\>\\>\\>\\> 1}_{x+1:\\enclose{actuarial}{n-1}} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore A^{\\>\\>\\> 1}_{x:\\enclose{actuarial}{n}} = v{}_{}p_{x}A^{\\>\\>\\>\\>\\>\\>\\>\\>\\> 1}_{x+1:\\enclose{actuarial}{n-1}} \\] EA is omitted as it is simply the combination of a TA and PE. These identities are most useful in a spreadsheet setting where the calculations can be easily repeated to fill up an entire life table.","title":"Recursions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#intuitions","text":"WL at the end of the life table is Term same 1 PE 0 Death products increase over time Survival products decrease EA tends to term Term becomes WL when policy period exceeds end Two policies of the same type Higher probability of occuring is higher EA: For policies which are guaranteed to pay, it depends on the tendency to pay out earlier > Paid out earlier, discounted less, higher EPV 5 year EA 30 5 year EA 60 Comparing values Depends on age Young Age (Probability death low) TA is the lowest, as least likely to pay out WL is the next lowest, unlikely to pay out as well but at least WILL pay out PE is the next lowest because the probability of survival is high EA is the highest because PE + TA > PE Old age (Probability death high) PE is the lowest because least likely to pay ouf Term is next because likely to pay out EA and WL depends Both share the same probability depends on the additional > If the death probability is high enough, then the PE component of the EA becomes worhtless, the PV of the indefinite coverage is higher EA is the next highest because combination","title":"Intuitions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#payable-continuously","text":"","title":"Payable Continuously"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#continuous-whole-life","text":"","title":"Continuous Whole Life"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#continuous-term","text":"","title":"Continuous Term"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#continuous-endowment","text":"No such thing as continuous PE","title":"Continuous Endowment"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#claims-acceleration","text":"Approximate continuous claims by assuming they occur in the middle of the period x + k and x + k + 1 at x+k+1/2 \\[ \\begin{aligned} \\bar{A}_x &\\approx v^{\\frac{1}{2}} q_x + v^{\\frac{3}{2}} p_x {}_{}q_{x+1} + v^{\\frac{5}{2}} {}_{2}p_{x} {}_{}q_{x+2} + \\dots \\\\ &\\approx (1+i)^{\\frac{1}{2}} (vq_x + v^1 p_x {}_{}q_{x+1} + v^2 {}_{2}p_{x} {}_{}q_{x+2} + \\dots) \\\\ &\\approx (1+i)^{\\frac{1}{2}} A_x \\end{aligned} \\]","title":"Claims Acceleration"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#uniform-death-distribution","text":"Can think of Whole life as a series of terms Uniform distribution each year \\[ \\begin{aligned} \\int^1_0 v^t {}_{t}p_{x+k} \\mu_{x+k+t} &\\approx \\int^1_0 v^t {}_{}q_{x+k} &\\approx {}_{}q_{x+k} \\int^1_0 e^{\\deltat} \\end{aligned} \\] \\[ \\begin{aligned} \\bar{A}_x &= {}_{0|}\\bar{A}^1_{x+1:\\enclose{actuarial}{1}} + {}_{1|}\\bar{A}^1_{x+1:\\enclose{actuarial}{1}} + {}_{2|}\\bar{A}^1_{x+1:\\enclose{actuarial}{1}} + \\dots \\\\ &= q_x \\frac{iv}{\\delta} + vp_xq_{x+1} \\frac{iv}{\\delta} + v^2 {}_{2}p_{x} q_{x+2} \\frac{iv}{\\delta} + \\dots \\\\ &= \\frac{i}{\\delta} (q_x + v^1 p_x {}_{}q_{x+1} + v^2 {}_{2}p_{x} {}_{}q_{x+2} + \\dots) \\\\ &= \\frac{i}{\\delta} A_x \\\\ \\end{aligned} \\]","title":"Uniform Death Distribution"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuity%20Functions/","text":"Life Annuity Functions","title":"Life Annuity Functions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuity%20Functions/#life-annuity-functions","text":"","title":"Life Annuity Functions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Premium%20Calculations/","text":"","title":"Premium Calculations"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Reserve%20Caclulations/","text":"","title":"6. Reserve Caclulations"},{"location":"3.%20Predictive%20Analytics/Exam%20Overview/","text":"Overview of Predictive Analytics Predictive Analytics is the usage of statistical models to analyze historical or current data to make predictions about the future or unknown events. Due to the growing relevance of Predictive Analytics , the SOA has added a significant amount of material on the topic into the credentialling process: Exam Statistics for Risk Modelling (SRM) Exam Predictive Analytics (PA) Exam Advanced Topics for Predictive Analytics (ATPA) Exam ATPA was added midway through 2022 to replace Exam IFM. Credit for IFM also counts towards credit for ATPA, thus this set of notes will NOT be covering ATPA. All of them share the same theme of working with statistical models: Constructing statistical models Intepreting their outputs Evaluating their performance Exam SRM covers the theory about the various types of models, tested in the typical MCQ format . Unlike the other exams where most of the questions are quantitative, most of SRM's questions are qualitative . Exam PA tests the same concepts in an applied manner , providing real data and a business problem to navigate through in a written format . Exam ATPA builds on both of these exams, covering more advanced concepts in a business context.","title":"Overview"},{"location":"3.%20Predictive%20Analytics/Exam%20Overview/#overview-of-predictive-analytics","text":"Predictive Analytics is the usage of statistical models to analyze historical or current data to make predictions about the future or unknown events. Due to the growing relevance of Predictive Analytics , the SOA has added a significant amount of material on the topic into the credentialling process: Exam Statistics for Risk Modelling (SRM) Exam Predictive Analytics (PA) Exam Advanced Topics for Predictive Analytics (ATPA) Exam ATPA was added midway through 2022 to replace Exam IFM. Credit for IFM also counts towards credit for ATPA, thus this set of notes will NOT be covering ATPA. All of them share the same theme of working with statistical models: Constructing statistical models Intepreting their outputs Evaluating their performance Exam SRM covers the theory about the various types of models, tested in the typical MCQ format . Unlike the other exams where most of the questions are quantitative, most of SRM's questions are qualitative . Exam PA tests the same concepts in an applied manner , providing real data and a business problem to navigate through in a written format . Exam ATPA builds on both of these exams, covering more advanced concepts in a business context.","title":"Overview of Predictive Analytics"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/","text":"Regression Overview Population Regression Model Regression is a statistical model that relates a Dependent Variable (DV) to one or more Independent Variables (IV). The dependent variable is regressed on to the independent variable. They are fundamentally a function of the independent variables and several Regression Parameters , \\(\\beta\\) . The functional form of the regression is based on the relationship between the variables. The goal is to use a model that best captures the relationship between the variables. \\[ Y = f(X, \\beta) \\] Independent Variable(s) Dependent Variables Variable used to make predictions Variable being predicted Free to change the value Depends on the value of indepenent variable Deterministic Random Variable Denoted as \\(X\\) Denoted as \\(Y\\) To be precise, for every set of IVs, the DV has a Conditional Distribution dependent on the given IV. For instance, the the \\(Y\\) could take any possible value (Marginal Distribution), but given these set of \\(X\\) , the possible values can be narrowed down to a certain range (conditional distribution). \\[ \\displaylines{ Y \\sim Distribution \\\\ Y|X \\sim Conditional~Distribution} \\] Thus, the output of the regression model is actually the Expected Value of the conditional distribution, \\(E(Y|X)\\) , for every possible \\(X\\) . \\[ E(Y|X) = f(X, \\beta) \\] The actual observations are unlikely to be exactly equal to its expectation, thus there is a difference between an observation and the corresponding regression output. It known as the Random Error Term which accounts for all other factors that affect the DV that are not captured in the regression. This means that the relationship between the \\(Y\\) and \\(X\\) is only approximate , as the true relationship is probably different due to the possibility of unaccounted variables. Note that the sign of the errors are significant - positive implies the actual value lies above the regression output while negative implies it lies below. \\[ \\varepsilon_i = y_i - f(x_i, \\beta) \\] This means that \\(Y\\) (not its expectation!) can be expressed as a sum of the regression model and the error terms: \\[ y_i = f(x_i,\\beta) + \\varepsilon_i \\] The Regression is known as the Systematic component as it is shared among all observations The Error is known as the Non-Systematic component as it is unique to each observation Sample Regression Model In practice, the population is unobservable hence it is impossible to construct the population regression model. Instead, a regression model is constructed from a sample instead, which aims to estimate the population model. \\[ \\hat{y} = f(X,\\hat{\\beta}) \\] Similarly, the output of this model can be compared to the actual observations. However, the resulting difference is known as the Residual of the model, which like all the other components, is an estimate for the Error term. \\[ \\hat{\\varepsilon_i} = y_i - \\hat{y_i} \\] There are several different methods to estimate the regression parameters, but they usually involve minimizing the residuals of the model, such that the resulting model best fits the given sample, which is why it is also known as the Fitted Regression Model . Hypothesis Testing Once a regression model has been fit, the next step is to determine if the relationship found in the sample is indicative of a relationship in the population. This can be determined through the following two-sided hypothesis test : Null Hypothesis: \\(\\beta_1 = 0\\) Alternative Hypothesis: \\(\\beta_1 \\ne 0\\) Under the null, the regression parameters are assumed to be 0, implying that there is no relationship between \\(Y\\) and \\(X\\) . The test should reject the null , proving that there IS a relationship between the DV and IVs. Prediction Once the best model has been determined, it can be used to make Predictions about future unobserved values. Let these future values be denoted by the subscript \\(*\\) . These unobserved DVs come from the population, thus can be expressed as a function of the population model: \\[ y_* = f(x_*, \\beta) + \\varepsilon_* \\] The corresponding values from the sample regression model is an estimate for this unobserved value: \\[ \\hat{y}_* = f(x_*, \\hat{\\beta}) \\] Like before, the predicted value is unlikely to be exactly equal to the actual value. Thus, the difference between both values can be measured as the Prediction Error : \\[ y_* - \\hat{y_*} = \\varepsilon_* + [f(x_*, \\beta) - f(x_*, \\hat{\\beta})] \\] The prediction error is thus made up of two components : Inherent error present in the DV ( \\(\\varepsilon_*\\) ) Error in estimating the population model ( \\(f(x_*, \\beta) - f(x_*, \\hat{\\beta})\\) ) Based on the distribution of the prediction error, a Prediction Interval at a given confidence level can be calculated to accompany the regression estimate, which is essentially a confidence interval for the predicted value . Note that the prediction intervals will always be wider than confidence intervals . This is because CIs only takes into the account the error in estimating the population model/parameters while PIs take into account the inherent error of the DV as well.","title":"Regression Overview"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#regression-overview","text":"","title":"Regression Overview"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#population-regression-model","text":"Regression is a statistical model that relates a Dependent Variable (DV) to one or more Independent Variables (IV). The dependent variable is regressed on to the independent variable. They are fundamentally a function of the independent variables and several Regression Parameters , \\(\\beta\\) . The functional form of the regression is based on the relationship between the variables. The goal is to use a model that best captures the relationship between the variables. \\[ Y = f(X, \\beta) \\] Independent Variable(s) Dependent Variables Variable used to make predictions Variable being predicted Free to change the value Depends on the value of indepenent variable Deterministic Random Variable Denoted as \\(X\\) Denoted as \\(Y\\) To be precise, for every set of IVs, the DV has a Conditional Distribution dependent on the given IV. For instance, the the \\(Y\\) could take any possible value (Marginal Distribution), but given these set of \\(X\\) , the possible values can be narrowed down to a certain range (conditional distribution). \\[ \\displaylines{ Y \\sim Distribution \\\\ Y|X \\sim Conditional~Distribution} \\] Thus, the output of the regression model is actually the Expected Value of the conditional distribution, \\(E(Y|X)\\) , for every possible \\(X\\) . \\[ E(Y|X) = f(X, \\beta) \\] The actual observations are unlikely to be exactly equal to its expectation, thus there is a difference between an observation and the corresponding regression output. It known as the Random Error Term which accounts for all other factors that affect the DV that are not captured in the regression. This means that the relationship between the \\(Y\\) and \\(X\\) is only approximate , as the true relationship is probably different due to the possibility of unaccounted variables. Note that the sign of the errors are significant - positive implies the actual value lies above the regression output while negative implies it lies below. \\[ \\varepsilon_i = y_i - f(x_i, \\beta) \\] This means that \\(Y\\) (not its expectation!) can be expressed as a sum of the regression model and the error terms: \\[ y_i = f(x_i,\\beta) + \\varepsilon_i \\] The Regression is known as the Systematic component as it is shared among all observations The Error is known as the Non-Systematic component as it is unique to each observation","title":"Population Regression Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#sample-regression-model","text":"In practice, the population is unobservable hence it is impossible to construct the population regression model. Instead, a regression model is constructed from a sample instead, which aims to estimate the population model. \\[ \\hat{y} = f(X,\\hat{\\beta}) \\] Similarly, the output of this model can be compared to the actual observations. However, the resulting difference is known as the Residual of the model, which like all the other components, is an estimate for the Error term. \\[ \\hat{\\varepsilon_i} = y_i - \\hat{y_i} \\] There are several different methods to estimate the regression parameters, but they usually involve minimizing the residuals of the model, such that the resulting model best fits the given sample, which is why it is also known as the Fitted Regression Model .","title":"Sample Regression Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#hypothesis-testing","text":"Once a regression model has been fit, the next step is to determine if the relationship found in the sample is indicative of a relationship in the population. This can be determined through the following two-sided hypothesis test : Null Hypothesis: \\(\\beta_1 = 0\\) Alternative Hypothesis: \\(\\beta_1 \\ne 0\\) Under the null, the regression parameters are assumed to be 0, implying that there is no relationship between \\(Y\\) and \\(X\\) . The test should reject the null , proving that there IS a relationship between the DV and IVs.","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#prediction","text":"Once the best model has been determined, it can be used to make Predictions about future unobserved values. Let these future values be denoted by the subscript \\(*\\) . These unobserved DVs come from the population, thus can be expressed as a function of the population model: \\[ y_* = f(x_*, \\beta) + \\varepsilon_* \\] The corresponding values from the sample regression model is an estimate for this unobserved value: \\[ \\hat{y}_* = f(x_*, \\hat{\\beta}) \\] Like before, the predicted value is unlikely to be exactly equal to the actual value. Thus, the difference between both values can be measured as the Prediction Error : \\[ y_* - \\hat{y_*} = \\varepsilon_* + [f(x_*, \\beta) - f(x_*, \\hat{\\beta})] \\] The prediction error is thus made up of two components : Inherent error present in the DV ( \\(\\varepsilon_*\\) ) Error in estimating the population model ( \\(f(x_*, \\beta) - f(x_*, \\hat{\\beta})\\) ) Based on the distribution of the prediction error, a Prediction Interval at a given confidence level can be calculated to accompany the regression estimate, which is essentially a confidence interval for the predicted value . Note that the prediction intervals will always be wider than confidence intervals . This is because CIs only takes into the account the error in estimating the population model/parameters while PIs take into account the inherent error of the DV as well.","title":"Prediction"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/10.%20Clustering/","text":"","title":"10. Clustering"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/","text":"Simple Linear Regression Simple Linear Regression (SLR) assumes a Linear Relationship between a Numeric DV and single continuous quantitative IV. The model is considered to be simple because it only contains a single independent variable. \\[ E(Y|X) = \\beta_0 + \\beta_1 X \\] \\(\\beta_0\\) \\(\\beta_1\\) Expected value of \\(Y\\) when \\(X = 0\\) Change in the expected value of \\(Y\\) given a one unit increase in \\(X\\) Intercept Parameter Slope Parameter Each observation can also be expressed as sum of the regression and its error term: \\[ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\] Ordinary Least Squares SLR parameters are estimated using the Ordinary Least Squares method, which minimizes the Sum of Squared Residuals of the fitted model. It is commonly referred to as the Residual Sum Squared (RSS). \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 \\] The minimization is solved through calculus by setting the partial derivatives of the RSS to 0: For the intercept parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_0} RSS &= 0 \\\\ -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum y_i - \\sum \\hat{\\beta}_0 - \\sum \\hat{\\beta}_1 x &= 0 \\\\ n\\bar{y} -n\\hat{\\beta}_0 - n\\hat{\\beta}_1 \\bar{x} &= 0 \\\\ \\end{aligned} \\] \\[\\therefore \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\] For the slope parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_1} RSS &= 0 \\\\ -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum (y_i x_i) - \\hat{\\beta}_0 \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} + n\\hat{\\beta}_1 \\bar{x}^2 - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} &= \\hat{\\beta}_1 \\sum (x^2_i) - n\\bar{x}^2 \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 = \\frac{\\sum (x_i y_i) - n\\bar{x}\\bar{y}}{\\sum (x_i^2) - n\\bar{x}^2} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} = r * \\frac{s_y}{s_x} \\] This results in the following fitted regression model, which can be graphically expressed as a Regression Line : \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] Note that it should \\(\\hat{\\varepsilon}\\) in the above image, not \\(e_i\\) . OLS Properties By re-arranging the formula for \\(\\hat{\\beta}_0\\) , we can show that \\((\\bar{x}, \\bar{y})\\) always lies on the fitted regression model: \\[ \\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x} \\] Additionally, since the parameters are estimated through minimization, the resulting model must always fulfil the two first order conditions . The model thus has \\(n-2\\) degrees of freedom to reflect these \"constraints\". \\(\\beta_0\\) FOC \\(\\beta_1\\) FOC \\(\\frac{\\partial}{\\partial \\beta_0} = -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\frac{\\partial}{\\partial \\beta_1} = -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\sum \\hat{\\varepsilon_i} = 0\\) \\(\\sum x_i \\hat{\\varepsilon_i} = 0\\) Residuals are negatively correlated Residuals and Independendent variables are uncorrelated Using the above results, we can also show the following that the mean of the regression outputs is equal to the mean of the population: \\[ \\begin{aligned} \\hat{\\varepsilon_i} &= y_i - \\hat{y_i} \\\\ \\sum \\hat{\\varepsilon_i} &= \\sum y_i - \\hat{y_i} \\\\ 0 &= n\\bar{y} - n\\bar{\\hat{y}} \\\\ \\bar{\\hat{y}} &= \\bar{y} \\\\ \\end{aligned} \\] Goodness of Fit Ideally, the regression should fit the sample closely, having as small as residuals as possible . The size of all the residuals in the model can be summarized through the RSS. The lower the RSS, the better the fit of the model. Recall that the residuals naturally sum to 0 under OLS - the residuals are thus squared to remove the sign so that they can be summed together. \\[ RSS = \\sum (y_i - \\hat{y})^2 \\] However, the SSR on its own is hard to intepret as there is no indication of how low or high it actually is. Thus, the Total Sum of Squares (TSS) can be used as a benchmark for the RSS as it is at least equal to or higher than the RSS. The TSS represents the RSS for a Null Regression - a model with containing only the intercept parameter . The output of this regression is always the sample mean \\(\\bar{y}\\) , which is used for the computation of its residuals. It represents the worst possible model which thus has the highest possible RSS . The lower the RSS compared to the TSS, the better the fit of the model. \\[ TSS = \\sum (y_i - \\bar{y}) \\] Null Model Consider a regression with only the intercept; \\(\\beta_1 = 0\\) . It is known as the Null Model as there are no independent variables used. \\[ y = \\beta_0 \\] We can estimate \\(\\hat{\\beta_0}\\) using OLS, which results in the following result: \\[ \\begin{aligned} -2 \\sum (y_i - \\hat{\\beta_0}) &= 0 \\\\ n \\bar{y} - n \\hat{\\beta_0} &= 0 \\\\ \\hat{\\beta_0} &= \\bar{y} \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{y} = \\bar{y} \\] Sum of Squares The TSS can be further decomposed into two more parts for analysis: \\[ \\begin{aligned} TSS &= \\sum (y_i - \\bar{y})^2 \\\\ TSS &= \\sum[(y_i - \\hat{y}) + (\\hat{y}-\\bar{y})]^2 \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 2 \\sum((y_i - \\hat{y})(\\hat{y}-\\bar{y})) \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 0 \\\\ TSS &= RSS + RegSS \\end{aligned} \\] Residual SS (RSS) Regression SS (RegSS) \\(\\sum(\\hat{y}-\\bar{y})^2\\) \\(\\sum(y_i - \\hat{y})^2\\) Variation of the observed values about the regression Variation of the regression output about the sample mean Variation explained by the regression Variation unexplained by the regression Note that it can also be expressed in terms of the Slope Parameter : \\[ \\begin{align} RegSS &= \\sum(\\hat{y}-\\bar{y})^2 \\\\ &= \\sum(\\hat{\\beta}_0 + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum(\\bar{y} - \\beta_1 \\bar{x} + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum[\\hat{\\beta}_1 (x_i - \\bar{x})]^2 \\\\ &= \\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2 \\\\ \\end{align} \\] The Coefficient of Determination \\(R^2\\) can also be used to demonstrate goodness of fit. It measures the proportion of variation explained by the regression model: \\[ R^2 = \\frac{RegSS}{TSS} = 1 - \\frac{RSS}{TSS} \\] Building off the above expression, it can also be expressed in terms of the Sample Correlation : \\[ \\begin{align} R^2 &= \\frac{\\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\\right)^2 \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^4} \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x}) \\sum (y_i - \\bar{y})}\\right)^2 \\\\ &= r_{y,x}^2 \\end{align} \\] Degrees of Freedom The TSS is based on the naive model with only the intercept parameter, thus, it is subject to the single contraint of all residuals summing to 0. The TSS thus has \\(n-1\\) degrees of freedom . The RSS is based on the SLR with both the intercept and slope parameter, thus it is subject to an additional constraint of the sumproduct of all residuals and independent variables being 0. The RSS thus has \\(n-2\\) degrees of freedom. The sum of the RSS and RegSS is equal to the TSS, thus the sum of their degrees of freedom must also be equal to that of the TSS. By working backwards, the RegSS thus has only \\(1\\) degree of freedom . Mean Squared The division of any Sum of Square (TSS, RSS, RegSS) by its Degrees of Freedom is known as the Mean Squared (MS), which is a measure of its average variance . The MS of the TSS is the Unbiased Estimator for Population Variance , which is why this process is known as the Analysis of Variance , as it decomposes the variance of \\(Y\\) into its constituent components: \\[ s = \\frac{TSS}{n-1} \\] The MS of the RSS is known the Mean Squared Residuals , often also referred to as the Mean Squared Error , as it is an estimate for the population variance of the error \\(\\sigma^2\\) : \\[ MS_{\\text{Residuals}} = \\frac{RSS}{n-2} \\] The MS of the RegSS is known as the Mean Squared Regression , which represents the proportion of variance explained per \\(X\\) used. \\[ MS_{\\text{Regression}} = \\frac{RegSS}{1} \\] F Statistic The ANOVA parameters can be used to conduct a hypothesis test to determine if there is in fact a relationship between \\(X\\) and \\(Y\\) : \\(H_0\\) : \\(\\beta_1 = 0\\) \\(H_1\\) : \\(\\beta_1 \\ne 0\\) Under the null hypothesis, there should be no difference between the assumed model and a null model as both only contain the intercept parameter, thus \\(TSS = RSS\\) , where \\(RegSS = 0\\) . Thus, the F-statistic is testing for the equality of variance between the TSS and RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_1 \\ne 0\\) . The F-statistic can be constructed using the sum of squares: \\[ \\begin{aligned} F &= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &= \\frac{RegSS/1}{RSS/(n-2)} \\\\ &= \\frac{(TSS - RSS)/1}{RSS/(n-2)} \\\\ &= (n-2) * \\frac{R^2}{1-R^2}, \\text{divide both by TSS} \\end{aligned} \\] Similar to the variance test, it can be shown that under the null, this test-statistic follows an F distribution with \\(1\\) and \\(n-2\\) degrees of freedom: \\[ \\begin{aligned} T &= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &= \\frac{\\sigma^2_{RegSS} \\frac{MS_{Reg}}{\\sigma^2_{RegSS}}}{\\sigma^2_{RSS} \\frac{MS_{RSS}}{\\sigma^2_{RSS}}} \\\\ &= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\frac{1 * MS_{Reg}}{\\sigma^2_{RegSS}} * \\frac{1}{1}}{\\frac{(n-2) * MS_{RegSS}}{\\sigma^2_{RegSS}}* \\frac{1}{n-2}} \\\\ &= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\chi_1}{\\chi_{n-2}} * (n-2) \\\\ &= 1 * F_{1, n-2} * (n-2) \\\\ &= F_{1, n-2} * (n-2) \\end{aligned} \\] \\[ \\therefore F \\sim F_{1, n-2} \\] ANOVA Table All the above information is then summarized in a table for convenience, known as the ANOVA Table : Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(1\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-2\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) - Statistical Inference Sampling Distributions Since the errors are assumed to be normally distributed, then \\(Y\\) is assumed to be normally distributed as well. Since \\(Y\\) is a linear combination of the regression parameters, then the parameters (& their estimates) are normally distributed as well. Both estimates can be expressed in another form that makes it more convenient to find their expectation & variances. \\[ \\begin{aligned} \\hat{\\beta}_1 &= \\frac{\\sum [(x_i - \\bar{x})(y_i - \\bar{y})]}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})y_i}{\\sum (x^2_i - \\bar{x})} - \\frac{\\bar{y} \\sum (x_i - \\bar{x})}{\\sum (x^2_i - \\bar{x})} \\\\ &= \\sum \\frac{(x_i - \\bar{x})}{(x^2_i - \\bar{x})}* y_i - 0 \\\\ &= \\sum w_i * y_i \\\\ \\\\ \\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\ &= \\frac{1}{n} \\sum y_i - \\bar{x} \\sum w_i * y_i \\\\ &= \\sum y_i (\\frac{1}{n} - \\bar{x}w_i) \\end{aligned} \\] \\(w_i\\) is a sort of \"weight\" parameter of the sum of squares. It has three interesting properties that makes it useful: \\[ \\begin{aligned} \\sum w_i &= \\frac{\\sum (x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{n\\bar{x}-n\\bar{x}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{0}{\\sum (x^2_i - \\bar{x})} \\\\ &= 0 \\\\ \\\\ \\sum w_i x_i &= \\frac{\\sum x_i(x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x^2_i - \\bar{x} \\sum x_i)}{\\sum x^2_i - 2\\bar{x}\\sum x_i + \\sum \\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - \\bar{x}(n \\bar{x})}{\\sum x^2_i - 2\\bar{x}(n\\bar{x}) + n\\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - 2n\\bar{x}^2 + n\\bar{x}^2} \\\\ &=\\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - n\\bar{x}^2} \\\\ &= 1 \\\\ \\\\ \\sum w_i^2 &= \\frac{\\sum (x_i - \\bar{x})^2}{\\sum (x_i - \\bar{x})^4} \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i) &= n(\\frac{1}{n}) - \\bar{x} \\sum w_i \\\\ &= 1 - 0 \\\\ &= 1 \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i &= \\frac{1}{n} \\sum x_i - \\bar{x} \\sum w_i x_i \\\\ &= \\frac{1}{n} (n\\bar{x}) - \\bar{x} (1) \\\\ &= \\bar{x} - \\bar{x} \\\\ &= 0 \\end{aligned} \\] Using this, the Expectation & Variance can be determined: \\[ \\begin{aligned} E(\\hat{\\beta}_1) &= \\sum w_i E(y_i) \\\\ &= \\sum w_i E(\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum w_i + \\beta_1 \\sum w_i x_i \\\\ &= \\beta_0 (0) + \\beta_1 (1) \\\\ &= \\beta_1\\\\ \\\\ Var(\\hat{\\beta}_1) &= Var(\\sum w_i y_i) \\\\ &= \\sum w_i^2 Var (y_i) \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2} \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 \\sim N(\\beta_1, \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2}) \\] \\[ \\begin{aligned} E(\\hat{\\beta}_0) &= \\sum (\\frac{1}{n} - \\bar{x}w_i) E(y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i) (\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum (\\frac{1}{n} - \\bar{x}w_i) + \\beta_1 \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i \\\\ &= \\beta_0 (1) + \\beta_1 (0) \\\\ &= \\beta_0 \\\\ \\\\ Var(\\hat{\\beta}_0) &= Var(\\sum (\\frac{1}{n} - \\bar{x}w_i)y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i)^2 Var (y_i) \\\\ &= \\sigma^2 \\sum (\\frac{1}{n^2} -\\frac{2\\bar{x}w_i}{n} + \\bar{x}^2 w_i^2) \\\\ &= \\sigma^2 (\\sum \\frac{1}{n^2} - \\frac{2\\bar{x}}{n} \\sum w_i + \\bar{x}^2 \\sum w_i^2) \\\\ &= \\sigma^2 [n(\\frac{1}{n^2}) - \\frac{2\\bar{x}}{n} (0) + \\bar{x}^2 (\\frac{1}{\\sum (x_i - \\bar{x})^2})] \\\\ &= \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2}) \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_0 \\sim N(\\beta_0, \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2})) \\] Hypothesis Testing Since the regression parameters are normally distributed, a z-statistic can also be used to conduct the tests. However, since the population variance is not known, a t-statistic is used instead: \\[ \\begin{aligned} t &= \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\end{aligned} \\] Since the population variance is estimated by the MSE which has \\(n-2\\) degrees of freedom, the corresponding chi-squared and hence t-distribution has \\(n-2\\) degrees of freedom as well. \\[ \\begin{aligned} \\hat{Var}(\\hat{\\beta_1}) &= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} * \\frac{\\frac{1}{n-1}}{\\frac{1}{n-1}} \\\\ &= \\frac{MS_{RSS}}{(n-1) s^2} \\end{aligned} \\] \\[ t \\sim t_{n-2} \\] Since the square of the t-statistic is the F-statistic, both are equivalent ways of doing so and will always lead to the same conclusions. \\[ t^2 \\sim F_{1, n-2} \\] Confidence Intervals Since the distribution of t-statistic is known, it can be used to determine the confidence interval of the estimate: \\[ P\\left(\\text{Margin of Error} < \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} < \\text{Margin of Error}\\right) = 1 - \\alpha \\] \\[ \\text{Confidence Interval} = \\hat{\\beta}_1 \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_1}} \\] Prediction Intervals Consider the Prediction Error of the SLR model: \\[ y_* - \\hat{y_*} = \\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\] Since both \\(y_*\\) and \\(\\hat{y_*}\\) are normally distributed, the prediction errors are normally distributed as well: \\[ \\begin{aligned} E(y_* - \\hat{y_*}) &= E[\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)]] \\\\ &= 0 + \\beta_0 + \\beta_1 E(x_*) - \\beta_0 - \\beta_1 E(x_*) \\\\ &= 0 \\\\ \\\\ Var(y_* - \\hat{y_*}) &= Var(\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= Var(\\varepsilon_*) + Var[(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= ... \\\\ &= \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}] \\end{aligned} \\] \\[ \\therefore y_* - \\hat{y_*} \\sim N\\left(0, \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}]\\right) \\] Similar to before, since the population variance is unknown, it can be approximated using the MSE. Thus, a t-statistic constructed from this sample will have a t-distribution , allowing the following prediction interval to be calculated: \\[ \\text{Prediction Interval} = \\hat{y}_* \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\] Notice that the standard error of the prediction interval increases as \\(x_*\\) moves further away \\(\\bar{x}\\) , indicating that the predictions become less accurate for those values.","title":"Simple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#simple-linear-regression","text":"Simple Linear Regression (SLR) assumes a Linear Relationship between a Numeric DV and single continuous quantitative IV. The model is considered to be simple because it only contains a single independent variable. \\[ E(Y|X) = \\beta_0 + \\beta_1 X \\] \\(\\beta_0\\) \\(\\beta_1\\) Expected value of \\(Y\\) when \\(X = 0\\) Change in the expected value of \\(Y\\) given a one unit increase in \\(X\\) Intercept Parameter Slope Parameter Each observation can also be expressed as sum of the regression and its error term: \\[ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\]","title":"Simple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#ordinary-least-squares","text":"SLR parameters are estimated using the Ordinary Least Squares method, which minimizes the Sum of Squared Residuals of the fitted model. It is commonly referred to as the Residual Sum Squared (RSS). \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 \\] The minimization is solved through calculus by setting the partial derivatives of the RSS to 0: For the intercept parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_0} RSS &= 0 \\\\ -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum y_i - \\sum \\hat{\\beta}_0 - \\sum \\hat{\\beta}_1 x &= 0 \\\\ n\\bar{y} -n\\hat{\\beta}_0 - n\\hat{\\beta}_1 \\bar{x} &= 0 \\\\ \\end{aligned} \\] \\[\\therefore \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\] For the slope parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_1} RSS &= 0 \\\\ -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum (y_i x_i) - \\hat{\\beta}_0 \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} + n\\hat{\\beta}_1 \\bar{x}^2 - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} &= \\hat{\\beta}_1 \\sum (x^2_i) - n\\bar{x}^2 \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 = \\frac{\\sum (x_i y_i) - n\\bar{x}\\bar{y}}{\\sum (x_i^2) - n\\bar{x}^2} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} = r * \\frac{s_y}{s_x} \\] This results in the following fitted regression model, which can be graphically expressed as a Regression Line : \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] Note that it should \\(\\hat{\\varepsilon}\\) in the above image, not \\(e_i\\) .","title":"Ordinary Least Squares"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#ols-properties","text":"By re-arranging the formula for \\(\\hat{\\beta}_0\\) , we can show that \\((\\bar{x}, \\bar{y})\\) always lies on the fitted regression model: \\[ \\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x} \\] Additionally, since the parameters are estimated through minimization, the resulting model must always fulfil the two first order conditions . The model thus has \\(n-2\\) degrees of freedom to reflect these \"constraints\". \\(\\beta_0\\) FOC \\(\\beta_1\\) FOC \\(\\frac{\\partial}{\\partial \\beta_0} = -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\frac{\\partial}{\\partial \\beta_1} = -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\sum \\hat{\\varepsilon_i} = 0\\) \\(\\sum x_i \\hat{\\varepsilon_i} = 0\\) Residuals are negatively correlated Residuals and Independendent variables are uncorrelated Using the above results, we can also show the following that the mean of the regression outputs is equal to the mean of the population: \\[ \\begin{aligned} \\hat{\\varepsilon_i} &= y_i - \\hat{y_i} \\\\ \\sum \\hat{\\varepsilon_i} &= \\sum y_i - \\hat{y_i} \\\\ 0 &= n\\bar{y} - n\\bar{\\hat{y}} \\\\ \\bar{\\hat{y}} &= \\bar{y} \\\\ \\end{aligned} \\]","title":"OLS Properties"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#goodness-of-fit","text":"Ideally, the regression should fit the sample closely, having as small as residuals as possible . The size of all the residuals in the model can be summarized through the RSS. The lower the RSS, the better the fit of the model. Recall that the residuals naturally sum to 0 under OLS - the residuals are thus squared to remove the sign so that they can be summed together. \\[ RSS = \\sum (y_i - \\hat{y})^2 \\] However, the SSR on its own is hard to intepret as there is no indication of how low or high it actually is. Thus, the Total Sum of Squares (TSS) can be used as a benchmark for the RSS as it is at least equal to or higher than the RSS. The TSS represents the RSS for a Null Regression - a model with containing only the intercept parameter . The output of this regression is always the sample mean \\(\\bar{y}\\) , which is used for the computation of its residuals. It represents the worst possible model which thus has the highest possible RSS . The lower the RSS compared to the TSS, the better the fit of the model. \\[ TSS = \\sum (y_i - \\bar{y}) \\]","title":"Goodness of Fit"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#null-model","text":"Consider a regression with only the intercept; \\(\\beta_1 = 0\\) . It is known as the Null Model as there are no independent variables used. \\[ y = \\beta_0 \\] We can estimate \\(\\hat{\\beta_0}\\) using OLS, which results in the following result: \\[ \\begin{aligned} -2 \\sum (y_i - \\hat{\\beta_0}) &= 0 \\\\ n \\bar{y} - n \\hat{\\beta_0} &= 0 \\\\ \\hat{\\beta_0} &= \\bar{y} \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{y} = \\bar{y} \\]","title":"Null Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#sum-of-squares","text":"The TSS can be further decomposed into two more parts for analysis: \\[ \\begin{aligned} TSS &= \\sum (y_i - \\bar{y})^2 \\\\ TSS &= \\sum[(y_i - \\hat{y}) + (\\hat{y}-\\bar{y})]^2 \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 2 \\sum((y_i - \\hat{y})(\\hat{y}-\\bar{y})) \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 0 \\\\ TSS &= RSS + RegSS \\end{aligned} \\] Residual SS (RSS) Regression SS (RegSS) \\(\\sum(\\hat{y}-\\bar{y})^2\\) \\(\\sum(y_i - \\hat{y})^2\\) Variation of the observed values about the regression Variation of the regression output about the sample mean Variation explained by the regression Variation unexplained by the regression Note that it can also be expressed in terms of the Slope Parameter : \\[ \\begin{align} RegSS &= \\sum(\\hat{y}-\\bar{y})^2 \\\\ &= \\sum(\\hat{\\beta}_0 + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum(\\bar{y} - \\beta_1 \\bar{x} + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum[\\hat{\\beta}_1 (x_i - \\bar{x})]^2 \\\\ &= \\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2 \\\\ \\end{align} \\] The Coefficient of Determination \\(R^2\\) can also be used to demonstrate goodness of fit. It measures the proportion of variation explained by the regression model: \\[ R^2 = \\frac{RegSS}{TSS} = 1 - \\frac{RSS}{TSS} \\] Building off the above expression, it can also be expressed in terms of the Sample Correlation : \\[ \\begin{align} R^2 &= \\frac{\\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\\right)^2 \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^4} \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x}) \\sum (y_i - \\bar{y})}\\right)^2 \\\\ &= r_{y,x}^2 \\end{align} \\]","title":"Sum of Squares"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#degrees-of-freedom","text":"The TSS is based on the naive model with only the intercept parameter, thus, it is subject to the single contraint of all residuals summing to 0. The TSS thus has \\(n-1\\) degrees of freedom . The RSS is based on the SLR with both the intercept and slope parameter, thus it is subject to an additional constraint of the sumproduct of all residuals and independent variables being 0. The RSS thus has \\(n-2\\) degrees of freedom. The sum of the RSS and RegSS is equal to the TSS, thus the sum of their degrees of freedom must also be equal to that of the TSS. By working backwards, the RegSS thus has only \\(1\\) degree of freedom .","title":"Degrees of Freedom"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#mean-squared","text":"The division of any Sum of Square (TSS, RSS, RegSS) by its Degrees of Freedom is known as the Mean Squared (MS), which is a measure of its average variance . The MS of the TSS is the Unbiased Estimator for Population Variance , which is why this process is known as the Analysis of Variance , as it decomposes the variance of \\(Y\\) into its constituent components: \\[ s = \\frac{TSS}{n-1} \\] The MS of the RSS is known the Mean Squared Residuals , often also referred to as the Mean Squared Error , as it is an estimate for the population variance of the error \\(\\sigma^2\\) : \\[ MS_{\\text{Residuals}} = \\frac{RSS}{n-2} \\] The MS of the RegSS is known as the Mean Squared Regression , which represents the proportion of variance explained per \\(X\\) used. \\[ MS_{\\text{Regression}} = \\frac{RegSS}{1} \\]","title":"Mean Squared"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#f-statistic","text":"The ANOVA parameters can be used to conduct a hypothesis test to determine if there is in fact a relationship between \\(X\\) and \\(Y\\) : \\(H_0\\) : \\(\\beta_1 = 0\\) \\(H_1\\) : \\(\\beta_1 \\ne 0\\) Under the null hypothesis, there should be no difference between the assumed model and a null model as both only contain the intercept parameter, thus \\(TSS = RSS\\) , where \\(RegSS = 0\\) . Thus, the F-statistic is testing for the equality of variance between the TSS and RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_1 \\ne 0\\) . The F-statistic can be constructed using the sum of squares: \\[ \\begin{aligned} F &= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &= \\frac{RegSS/1}{RSS/(n-2)} \\\\ &= \\frac{(TSS - RSS)/1}{RSS/(n-2)} \\\\ &= (n-2) * \\frac{R^2}{1-R^2}, \\text{divide both by TSS} \\end{aligned} \\] Similar to the variance test, it can be shown that under the null, this test-statistic follows an F distribution with \\(1\\) and \\(n-2\\) degrees of freedom: \\[ \\begin{aligned} T &= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &= \\frac{\\sigma^2_{RegSS} \\frac{MS_{Reg}}{\\sigma^2_{RegSS}}}{\\sigma^2_{RSS} \\frac{MS_{RSS}}{\\sigma^2_{RSS}}} \\\\ &= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\frac{1 * MS_{Reg}}{\\sigma^2_{RegSS}} * \\frac{1}{1}}{\\frac{(n-2) * MS_{RegSS}}{\\sigma^2_{RegSS}}* \\frac{1}{n-2}} \\\\ &= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\chi_1}{\\chi_{n-2}} * (n-2) \\\\ &= 1 * F_{1, n-2} * (n-2) \\\\ &= F_{1, n-2} * (n-2) \\end{aligned} \\] \\[ \\therefore F \\sim F_{1, n-2} \\]","title":"F Statistic"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#anova-table","text":"All the above information is then summarized in a table for convenience, known as the ANOVA Table : Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(1\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-2\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) -","title":"ANOVA Table"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#statistical-inference","text":"","title":"Statistical Inference"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#sampling-distributions","text":"Since the errors are assumed to be normally distributed, then \\(Y\\) is assumed to be normally distributed as well. Since \\(Y\\) is a linear combination of the regression parameters, then the parameters (& their estimates) are normally distributed as well. Both estimates can be expressed in another form that makes it more convenient to find their expectation & variances. \\[ \\begin{aligned} \\hat{\\beta}_1 &= \\frac{\\sum [(x_i - \\bar{x})(y_i - \\bar{y})]}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})y_i}{\\sum (x^2_i - \\bar{x})} - \\frac{\\bar{y} \\sum (x_i - \\bar{x})}{\\sum (x^2_i - \\bar{x})} \\\\ &= \\sum \\frac{(x_i - \\bar{x})}{(x^2_i - \\bar{x})}* y_i - 0 \\\\ &= \\sum w_i * y_i \\\\ \\\\ \\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\ &= \\frac{1}{n} \\sum y_i - \\bar{x} \\sum w_i * y_i \\\\ &= \\sum y_i (\\frac{1}{n} - \\bar{x}w_i) \\end{aligned} \\] \\(w_i\\) is a sort of \"weight\" parameter of the sum of squares. It has three interesting properties that makes it useful: \\[ \\begin{aligned} \\sum w_i &= \\frac{\\sum (x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{n\\bar{x}-n\\bar{x}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{0}{\\sum (x^2_i - \\bar{x})} \\\\ &= 0 \\\\ \\\\ \\sum w_i x_i &= \\frac{\\sum x_i(x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x^2_i - \\bar{x} \\sum x_i)}{\\sum x^2_i - 2\\bar{x}\\sum x_i + \\sum \\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - \\bar{x}(n \\bar{x})}{\\sum x^2_i - 2\\bar{x}(n\\bar{x}) + n\\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - 2n\\bar{x}^2 + n\\bar{x}^2} \\\\ &=\\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - n\\bar{x}^2} \\\\ &= 1 \\\\ \\\\ \\sum w_i^2 &= \\frac{\\sum (x_i - \\bar{x})^2}{\\sum (x_i - \\bar{x})^4} \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i) &= n(\\frac{1}{n}) - \\bar{x} \\sum w_i \\\\ &= 1 - 0 \\\\ &= 1 \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i &= \\frac{1}{n} \\sum x_i - \\bar{x} \\sum w_i x_i \\\\ &= \\frac{1}{n} (n\\bar{x}) - \\bar{x} (1) \\\\ &= \\bar{x} - \\bar{x} \\\\ &= 0 \\end{aligned} \\] Using this, the Expectation & Variance can be determined: \\[ \\begin{aligned} E(\\hat{\\beta}_1) &= \\sum w_i E(y_i) \\\\ &= \\sum w_i E(\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum w_i + \\beta_1 \\sum w_i x_i \\\\ &= \\beta_0 (0) + \\beta_1 (1) \\\\ &= \\beta_1\\\\ \\\\ Var(\\hat{\\beta}_1) &= Var(\\sum w_i y_i) \\\\ &= \\sum w_i^2 Var (y_i) \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2} \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 \\sim N(\\beta_1, \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2}) \\] \\[ \\begin{aligned} E(\\hat{\\beta}_0) &= \\sum (\\frac{1}{n} - \\bar{x}w_i) E(y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i) (\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum (\\frac{1}{n} - \\bar{x}w_i) + \\beta_1 \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i \\\\ &= \\beta_0 (1) + \\beta_1 (0) \\\\ &= \\beta_0 \\\\ \\\\ Var(\\hat{\\beta}_0) &= Var(\\sum (\\frac{1}{n} - \\bar{x}w_i)y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i)^2 Var (y_i) \\\\ &= \\sigma^2 \\sum (\\frac{1}{n^2} -\\frac{2\\bar{x}w_i}{n} + \\bar{x}^2 w_i^2) \\\\ &= \\sigma^2 (\\sum \\frac{1}{n^2} - \\frac{2\\bar{x}}{n} \\sum w_i + \\bar{x}^2 \\sum w_i^2) \\\\ &= \\sigma^2 [n(\\frac{1}{n^2}) - \\frac{2\\bar{x}}{n} (0) + \\bar{x}^2 (\\frac{1}{\\sum (x_i - \\bar{x})^2})] \\\\ &= \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2}) \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_0 \\sim N(\\beta_0, \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2})) \\]","title":"Sampling Distributions"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#hypothesis-testing","text":"Since the regression parameters are normally distributed, a z-statistic can also be used to conduct the tests. However, since the population variance is not known, a t-statistic is used instead: \\[ \\begin{aligned} t &= \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\end{aligned} \\] Since the population variance is estimated by the MSE which has \\(n-2\\) degrees of freedom, the corresponding chi-squared and hence t-distribution has \\(n-2\\) degrees of freedom as well. \\[ \\begin{aligned} \\hat{Var}(\\hat{\\beta_1}) &= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} * \\frac{\\frac{1}{n-1}}{\\frac{1}{n-1}} \\\\ &= \\frac{MS_{RSS}}{(n-1) s^2} \\end{aligned} \\] \\[ t \\sim t_{n-2} \\] Since the square of the t-statistic is the F-statistic, both are equivalent ways of doing so and will always lead to the same conclusions. \\[ t^2 \\sim F_{1, n-2} \\]","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#confidence-intervals","text":"Since the distribution of t-statistic is known, it can be used to determine the confidence interval of the estimate: \\[ P\\left(\\text{Margin of Error} < \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} < \\text{Margin of Error}\\right) = 1 - \\alpha \\] \\[ \\text{Confidence Interval} = \\hat{\\beta}_1 \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_1}} \\]","title":"Confidence Intervals"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#prediction-intervals","text":"Consider the Prediction Error of the SLR model: \\[ y_* - \\hat{y_*} = \\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\] Since both \\(y_*\\) and \\(\\hat{y_*}\\) are normally distributed, the prediction errors are normally distributed as well: \\[ \\begin{aligned} E(y_* - \\hat{y_*}) &= E[\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)]] \\\\ &= 0 + \\beta_0 + \\beta_1 E(x_*) - \\beta_0 - \\beta_1 E(x_*) \\\\ &= 0 \\\\ \\\\ Var(y_* - \\hat{y_*}) &= Var(\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= Var(\\varepsilon_*) + Var[(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= ... \\\\ &= \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}] \\end{aligned} \\] \\[ \\therefore y_* - \\hat{y_*} \\sim N\\left(0, \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}]\\right) \\] Similar to before, since the population variance is unknown, it can be approximated using the MSE. Thus, a t-statistic constructed from this sample will have a t-distribution , allowing the following prediction interval to be calculated: \\[ \\text{Prediction Interval} = \\hat{y}_* \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\] Notice that the standard error of the prediction interval increases as \\(x_*\\) moves further away \\(\\bar{x}\\) , indicating that the predictions become less accurate for those values.","title":"Prediction Intervals"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/","text":"Multiple Linear Regression The natural extension of the SLR model is to include more than one independent variable , which thus results in the more generalized Multiple Linear Regression (MLR) model. \\[ E(Y|X_1, ... X_p) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_{p}X_{p} \\] Unlike the SLR which studies how each individual IV influences the DV, the goal of the MLR model is to study how all the IVs operate together to influence the DV. \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) when \\(X_1 = X_2 = ... = 0\\) Change in \\(E(Y)\\) given a one unit increase in \\(X_j\\) , holding all other \\(X\\) 's constant Intercept Parameter \"Slope\" Parameter For avoidance of doubt, the subscript \\(i\\) will be used to denote observations while \\(j\\) will be used to denote independent variables. Every observation can also be expressed as the sum of the regression and the error term. However, due to the multi-dimensional nature of the model, it is commonly expressed in matrix notation: \\[ \\begin{aligned} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} &= \\begin{pmatrix} 1 & x_{11} & x_{12} & ... & x_{1p} \\\\ 1 & x_{21} & x_{22} & ... & x_{2p} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & ... & x_{np} \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix} + \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix} \\\\ \\boldsymbol{y} &= \\boldsymbol{X\\beta + \\varepsilon} \\end{aligned} \\] Ordinary Least Squares Similar to the SLR model, the regression parameters can be found by minimizing the sum of squared residuals: \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_p x_ip)]^2 \\] There are \\(p+1\\) FOC equations to solve through the minimization, with the additional one reflecting the intercept parameter. It is difficult to algebraically solve this system of equations, thus there is no closed form tsolution for each individual paramater. Instead, here is a vector solution for all of the parameters: \\[ \\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\hat{\\beta_0} \\\\ \\hat{\\beta_1} \\\\ \\vdots \\\\ \\hat{\\beta_p} \\end{pmatrix} = (\\boldsymbol{X'X})^{-1}\\boldsymbol{X'y} \\] Note that since there are \\(p+1\\) equations that must be solved, the model has \\(n-p+1\\) degrees of freedom. Following the same logic, there must be at least \\(p+1\\) observations in order to solve the equations and hence construct the model. This results in the following fitted regression model, which can be graphically expressed as a Regression Plane : \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_P x_ip \\] Manual Computation The tricky part is that \\((\\boldsymbol{X'X})^{-1}\\) is hard to compute by manually , except for the special case where \\(p=1\\) (SLR). Thus, it is likely that the parameters will be provided by the question. If required to compute them manually, then \\((\\boldsymbol{X'X})^{-1}\\) is likely to be provided. The remaining \\(\\boldsymbol{X'y}\\) still needs to be computed and put together to obtain the regression parameters. However, if the model has \\(p=2\\) but no intercept , then \\((\\boldsymbol{X'X})^{-1}\\) is a 2 x 2 Matrix whose inverse can be easily calculated. Similarly, if \\((\\boldsymbol{X'X})^{-1}\\) is a Diagonal Matrix , its inverse can be easily calculated as well. Goodness of Fit The ANOVA for MLR follows the same intuition as the SLR version, adjusted for the new degrees of freedom: Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(p\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-(p+1)\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) - The coefficient of determination still represents the proportion of variance explained by the regression, but has a slightly different formula: \\[ R^2 = \\frac{RegSS}{TSS} = r_{y,\\hat{y}}^2 \\] The multiple IVs of the model are now captured through \\(\\hat{y}\\) instead of \\(x\\) directly. However, the hypothesis test under the MLR is vastly different from the SLR version. Instead of testing if an individual IV is useful, it tests if all the IVs are collectively useful in helping to explain the DV. \\[ \\begin{aligned} H_0 &: \\beta_1 = \\beta_2 = ... = \\beta_p = 0 \\\\ H_1 &: \\text{At least one } \\beta_j \\text{ is non-zero} \\end{aligned} \\] Thus, rejecting the null hypothesis implies that at least one of the IVs used is useful, but does not provide much insight into which of them are useful. Partial F Test A partial F-test can be used to precisely determine which of the IVs are useful in explaining the DV. A regular F test compares the null model with no IVs to the desired model with all the IVs. If the sum of squares are significantly different, then it implies that the additional IVs are jointly useful in explaining \\(Y\\) . The partial F test generalizes this idea. Instead of considering a null model with no IVs, a Reduced Model with a limited number IVs ( \\(q\\) ) is considered instead. Consequently, the desired model is known as the Full Model with all \\(p\\) IVs, where \\(q < p\\) . \\[ \\begin{aligned} H_0: \\beta_{p-q+1} = ... = \\beta_p = 0 \\\\ H_1: \\beta_{p-q+1} = ... = \\beta_p \\ne 0 \\end{aligned} \\] The test is also commonly referred to as the Generalized F test, where the models are referred to as the Restricted and Unrestricted Models. The difference in the RSS between the Full and Reduced Model is known as the Extra Sum of Squares (ExtraSS) . It represents the contribution of the missing variables in explaining the variance of \\(Y\\) . Under the null hypothesis, there should be no difference between the two RSS, and thus \\(ExtraSS = 0\\) . \\[ ExtraSS = RSS_{Reduced} - RSS_{Full} \\] Thus, the Partial F-statistic is testing for the equality of variance between the two RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_{p-q+1} = ... = \\beta_p \\ne 0\\) . \\[ \\begin{aligned} F &= \\frac{MS_{ExtraSS}}{MS_{RSS_{Full}}} \\\\ &= \\frac{ExtraSS/q}{RSS/(n-2)} \\\\ &= \\frac{(RSS_{Reduced} - RSS_{Full})/q}{RSS/(n-2)} \\\\ &= (n-2) * \\frac{(1- R_{Reduced}^2) - (1 - R_{Full}^2)}{1-RSS_{Full}^2}, \\text{divide both by TSS} \\\\ &= (n-2) * \\frac{R_{Full}^2 - R_{Reduced}^2}{1-RSS_{Full}^2} \\end{aligned} \\] Statistical Inference Sampling Distributions Similar to SLR, the regression parameters are normally distributed as well. However, since there are multiple regression parameters, they collectively follow a multivariate normal distribution . \\[ \\hat{\\beta} \\sim N_{p+1}(\\beta, \\sigma^2 (\\boldsymbol{X'X})^{-1}) \\] The variance of the distribution is known as the Variance Covariance Matrix , which provides the covariances between every possible pair of regression parameters. Since the covariance of a variable with itself is its variance, the diagonals are the respective variances of the parameters. Note that the first element of the diagonal is the intercept, thus the variance of the jth IV is the (j+1)th element of the diagonal . \\[ Var(\\hat{\\beta}) = \\begin{pmatrix} Var(\\hat{\\beta}_0) & Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) & ... & Cov(\\hat{\\beta}_0, \\hat{\\beta}_p) \\\\ Cov(\\hat{\\beta}_1, \\hat{\\beta}_0) & Var(\\hat{\\beta}_1) & ... & Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ Cov(\\hat{\\beta}_p, \\hat{\\beta}_0) & Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) & ... & Var(\\hat{\\beta}_p) \\end{pmatrix} \\] Note that the covariances are symmetrical about the diagonal - \\(Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = Cov(\\hat{\\beta}_1, \\hat{\\beta}_0)\\) . Hypothesis Testing Similar to SLR, the t-test can be used to test for the significance of an individual IV, but the intepretation of the test is different from the SLR case. It tests the usefulness of an individual IV in the presence of the other predictors . \\[ \\begin{aligned} H_0: \\beta_j = 0 \\\\ H_1: \\beta_j \\ne 0 \\end{aligned} \\] \\[ t(\\beta_j) = \\frac{\\hat{\\beta_j} - \\beta_j}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\] Recall that the variance of the jth IV si the (j+1)th element of the variance covariance matrix. \\[ t(\\beta_j) \\sim t_{n-p-1} \\] However, this leads to several odd results which needs to be accounted for: Predictor is not significant individually but significant when taken alone . Predictors are not significant individually but significant when taken together . There are now two possible ways to test for the significance of IVs: Conduct a single F-test to test for the joint significance of all IVs Conduct multiple t-tests to test for the joint significance of each IV The problem with the multiple t-test approach lies with the type I error of the tests. For \\(\\alpha = 0.05\\) , the probability of correctly rejecting the null is \\(0.95\\) . Assuming that all the tests are independent, the probability of correctly rejecting all the nulls is \\(0.95^p\\) , which drastically decreases with the number of tests conducted. Given enough predictors, this means that the probability drops to approximately 0, which means that there is bound to be a wrongly rejected null; a type I error is guaranteed even though it was supposed to be limited at a 0.05 chance. The Bonferroni Correction is a method of adjusting \\(\\alpha\\) of each hypothesis test such that the overall type I error is kept at its desired level. However, this has the consequence of increasing the probability of type II errors, which is why it is not popular. The F-test has the advantage of controlling the type I error regardless of the number of predictors , which is why it is preferred for hypothesis testing in the MLR. Confidence Intervals Similar to SLR, the confidence intervals can be constructed using the distribution of the test-statistic: \\[ \\text{Confidence Interval} = \\hat{\\beta}_j \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_j}} \\] Prediction Intervals Unlike in SLR, it is difficult to determine the distribution of the prediction error. Thus, the final result can be found below: \\[ \\begin{aligned} \\text{Prediction Interval} &= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\\\ &= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\sqrt{s^2 [1 + x'_* (\\boldsymbol{X'X})^-1 * x_*]} \\end{aligned} \\] Despite the result looking more complicated, the key takeaway remains the same - the further away \\(x_*\\) is from \\(\\bar{x}\\) , the greater Variations of MLR Qualitative IV The discussion so far has mostly focused on Quantitative IVs, thus this section will explore Qualitative IVs. They can only take values from a list of pre-defined values, known as the Levels of the variable. In a regression context, most qualitative IVs are represented in the form of a Dummy Variable which can only take two possible levels - Yes (1) or No (0). Note that there are other ways of encoding a dummy variable (-1/0/1 etc), but the principles stay large the same. \\[ \\text{Dummy Variable} = \\begin{cases} 1, & \\text{First Level} \\\\ 0, & \\text{Second Level} \\end{cases} \\] In general, \\(n-1\\) dummy variables are needed to represent a qualitative variable with \\(n\\) levels. This is because the status of the last level can be deduced from the other dummy variables . Thus, including a seperate dummy variable for this last level is redundant and will lead to the problem of Collinearity , which will be explored in a later section. For instance, consider four levels (North, East, South & West), represented by the three dummy variables ( \\(N, E , S\\) ). If any of the variables are 1, they represent their respective direction (North, East & South). If all of them are 0, then the direction is the remaining level (West). The last remaining level is often referred to as the Baseline Level as it is the default level of the variable when all other dummies are 0. Any level can be used as the baseline, but the parameters will differ across models with different baselines. \\[ E(Y|X) = \\beta_0 + \\beta_{1, North} x_{North} + \\beta_{2, East} * x_{East} + \\beta_{3, South} * x_{South} \\] \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) at the baseline level Change in \\(E(Y)\\) from the baseline to the chosen level \\(X_1 = X_2 = ... = X_j = 0\\) \\(X_1 = X_2 = ... = 0; X_j = 1\\) Dummy variables are usually used in conjunction with quantitative ones . This essentially creates a \"seperate\" regression model for each of the levels. For the simplest case of one quantiative and one dummy, \\(\\beta_2\\) is the difference in the intercept of the two resulting SLR models. \\[ E(Y|X) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\] \\[ E(Y|X) = \\begin{cases} (\\beta_0 + \\beta_2) + \\beta_1 x_1, & x_2 = 1 \\\\ \\beta_0 + \\beta_1 x_1, & x_2 = 0 \\end{cases} \\] Interaction Model So far, it was assumed that each IV had an independent effect on the DV. However, IVs may interact to produce a joint effect on the DV, where the effect of one IV depends on the value of another IV. For instance, the production of a factory may depend on the number of Machines and Workers. However, the more machines there are, the greater the effect of an additional worker . Thus, this interaction effect can be captured through an Interaction Variable , which is the product of both IVs: \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 \\\\ &= \\beta_0 + (\\beta_1 + \\beta_3 x_2)x_1 + \\beta_2 x_2 \\\\ &= \\beta_0 + \\beta_1 x_1 + (\\beta_2 + \\beta_3 x_1) x_2 \\\\ \\end{aligned} \\] A one unit increase in \\(x_1\\) will increase E(Y) by \\(\\beta_1 + \\beta_3 x_2\\) , which depends on the value of \\(x_2\\) as well, which is is why they interact with each another. Phrased another way, for every one unit increase in \\(x_2\\) , the change in E(Y) from a unit increase in \\(x_1\\) increases by \\(\\beta_3\\) . Something unusual to take note of is that the Interaction Variable tests significant but the constituent variables do not. In this case, it is common practice to retain both the interaction and the consistuent variables in the model. This is practice is known as the Hierarchical Principle . Models containing dummy variables can also have an interaction effect. Building off the example from the previous section, \\(\\beta_2\\) is still the difference in the intercept but with the new \\(\\beta_3\\) being the difference in slopes of the two resulting SLR models. \\[ E(Y|X) = \\begin{cases} (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3)x_1, & x_2 = 1 \\\\ \\beta_0 + \\beta_1 x_1, & x_2 = 0 \\end{cases} \\] Piecewise Model If the DV has an abrupt change in behaviour over different values of the IVs, it can be accounted for the through the use of a Piecewise Regression . The first method of creating a piecewise regression involves the use of an Indicator Function . It is essentially a dummy variable which depends on the value of the other IVs. \\[ z_{\\{x>=c\\}} = \\begin{cases} 0, & x < c \\\\ 1, & x \\ge c \\end{cases} \\] \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 z(x-c) \\\\ &= \\begin{cases} \\beta_0 + \\beta_1 x_1, & x < c \\\\ (\\beta_0 - \\beta_2c) + (\\beta_1 + \\beta_2) x_1, & x \\ge c \\end{cases} \\end{aligned} \\] Note that \\(z(x-c)\\) is treated as a distinct IV and hence can be equivalently expressed as \\(x_2\\) ; the full notation is used here for clarity. \\(c\\) is the value at which the DV abruptly changes in behaviour, known as a Kink in the graph, which continuously connects the two regression lines. The other method is to use an interaction variable instead. Similar to how the interaction variables resulted in the model to \"split\", the model now splits at \\(x = c\\) , resulting in a non-continuous gap. \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2z + \\beta_3 zx \\\\ &= \\begin{cases} \\beta_0 + \\beta_1 x_1, & x < c \\\\ (\\beta_0 - \\beta_2) + (\\beta_1 + \\beta_3) x_1, & x \\ge c \\end{cases} \\end{aligned} \\] Polynomial Model If the relationship between the DV and IV is complex (non-linear), then a Polynomial Regression can be used to better model the relationship between the two. \\[ E(Y|X) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 \\] Note that it is the same IV used in the regression, just with additional powers. Although a polynomial regression may capture the true relationship better, the regression parameters become hard to intepret. The partial derivatives can no longer be intepreted as holding other IVs constant as each IV is dependent on the same quantity, just to different powers. \\[ \\frac{\\partial E(Y|X)}{\\partial x} = \\beta_1 + 2\\beta_2 x + ... + m \\beta_m x^{m-1} \\]","title":"Multiple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#multiple-linear-regression","text":"The natural extension of the SLR model is to include more than one independent variable , which thus results in the more generalized Multiple Linear Regression (MLR) model. \\[ E(Y|X_1, ... X_p) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_{p}X_{p} \\] Unlike the SLR which studies how each individual IV influences the DV, the goal of the MLR model is to study how all the IVs operate together to influence the DV. \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) when \\(X_1 = X_2 = ... = 0\\) Change in \\(E(Y)\\) given a one unit increase in \\(X_j\\) , holding all other \\(X\\) 's constant Intercept Parameter \"Slope\" Parameter For avoidance of doubt, the subscript \\(i\\) will be used to denote observations while \\(j\\) will be used to denote independent variables. Every observation can also be expressed as the sum of the regression and the error term. However, due to the multi-dimensional nature of the model, it is commonly expressed in matrix notation: \\[ \\begin{aligned} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} &= \\begin{pmatrix} 1 & x_{11} & x_{12} & ... & x_{1p} \\\\ 1 & x_{21} & x_{22} & ... & x_{2p} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & ... & x_{np} \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix} + \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix} \\\\ \\boldsymbol{y} &= \\boldsymbol{X\\beta + \\varepsilon} \\end{aligned} \\]","title":"Multiple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#ordinary-least-squares","text":"Similar to the SLR model, the regression parameters can be found by minimizing the sum of squared residuals: \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_p x_ip)]^2 \\] There are \\(p+1\\) FOC equations to solve through the minimization, with the additional one reflecting the intercept parameter. It is difficult to algebraically solve this system of equations, thus there is no closed form tsolution for each individual paramater. Instead, here is a vector solution for all of the parameters: \\[ \\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\hat{\\beta_0} \\\\ \\hat{\\beta_1} \\\\ \\vdots \\\\ \\hat{\\beta_p} \\end{pmatrix} = (\\boldsymbol{X'X})^{-1}\\boldsymbol{X'y} \\] Note that since there are \\(p+1\\) equations that must be solved, the model has \\(n-p+1\\) degrees of freedom. Following the same logic, there must be at least \\(p+1\\) observations in order to solve the equations and hence construct the model. This results in the following fitted regression model, which can be graphically expressed as a Regression Plane : \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_P x_ip \\]","title":"Ordinary Least Squares"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#manual-computation","text":"The tricky part is that \\((\\boldsymbol{X'X})^{-1}\\) is hard to compute by manually , except for the special case where \\(p=1\\) (SLR). Thus, it is likely that the parameters will be provided by the question. If required to compute them manually, then \\((\\boldsymbol{X'X})^{-1}\\) is likely to be provided. The remaining \\(\\boldsymbol{X'y}\\) still needs to be computed and put together to obtain the regression parameters. However, if the model has \\(p=2\\) but no intercept , then \\((\\boldsymbol{X'X})^{-1}\\) is a 2 x 2 Matrix whose inverse can be easily calculated. Similarly, if \\((\\boldsymbol{X'X})^{-1}\\) is a Diagonal Matrix , its inverse can be easily calculated as well.","title":"Manual Computation"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#goodness-of-fit","text":"The ANOVA for MLR follows the same intuition as the SLR version, adjusted for the new degrees of freedom: Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(p\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-(p+1)\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) - The coefficient of determination still represents the proportion of variance explained by the regression, but has a slightly different formula: \\[ R^2 = \\frac{RegSS}{TSS} = r_{y,\\hat{y}}^2 \\] The multiple IVs of the model are now captured through \\(\\hat{y}\\) instead of \\(x\\) directly. However, the hypothesis test under the MLR is vastly different from the SLR version. Instead of testing if an individual IV is useful, it tests if all the IVs are collectively useful in helping to explain the DV. \\[ \\begin{aligned} H_0 &: \\beta_1 = \\beta_2 = ... = \\beta_p = 0 \\\\ H_1 &: \\text{At least one } \\beta_j \\text{ is non-zero} \\end{aligned} \\] Thus, rejecting the null hypothesis implies that at least one of the IVs used is useful, but does not provide much insight into which of them are useful.","title":"Goodness of Fit"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#partial-f-test","text":"A partial F-test can be used to precisely determine which of the IVs are useful in explaining the DV. A regular F test compares the null model with no IVs to the desired model with all the IVs. If the sum of squares are significantly different, then it implies that the additional IVs are jointly useful in explaining \\(Y\\) . The partial F test generalizes this idea. Instead of considering a null model with no IVs, a Reduced Model with a limited number IVs ( \\(q\\) ) is considered instead. Consequently, the desired model is known as the Full Model with all \\(p\\) IVs, where \\(q < p\\) . \\[ \\begin{aligned} H_0: \\beta_{p-q+1} = ... = \\beta_p = 0 \\\\ H_1: \\beta_{p-q+1} = ... = \\beta_p \\ne 0 \\end{aligned} \\] The test is also commonly referred to as the Generalized F test, where the models are referred to as the Restricted and Unrestricted Models. The difference in the RSS between the Full and Reduced Model is known as the Extra Sum of Squares (ExtraSS) . It represents the contribution of the missing variables in explaining the variance of \\(Y\\) . Under the null hypothesis, there should be no difference between the two RSS, and thus \\(ExtraSS = 0\\) . \\[ ExtraSS = RSS_{Reduced} - RSS_{Full} \\] Thus, the Partial F-statistic is testing for the equality of variance between the two RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_{p-q+1} = ... = \\beta_p \\ne 0\\) . \\[ \\begin{aligned} F &= \\frac{MS_{ExtraSS}}{MS_{RSS_{Full}}} \\\\ &= \\frac{ExtraSS/q}{RSS/(n-2)} \\\\ &= \\frac{(RSS_{Reduced} - RSS_{Full})/q}{RSS/(n-2)} \\\\ &= (n-2) * \\frac{(1- R_{Reduced}^2) - (1 - R_{Full}^2)}{1-RSS_{Full}^2}, \\text{divide both by TSS} \\\\ &= (n-2) * \\frac{R_{Full}^2 - R_{Reduced}^2}{1-RSS_{Full}^2} \\end{aligned} \\]","title":"Partial F Test"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#statistical-inference","text":"","title":"Statistical Inference"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#sampling-distributions","text":"Similar to SLR, the regression parameters are normally distributed as well. However, since there are multiple regression parameters, they collectively follow a multivariate normal distribution . \\[ \\hat{\\beta} \\sim N_{p+1}(\\beta, \\sigma^2 (\\boldsymbol{X'X})^{-1}) \\] The variance of the distribution is known as the Variance Covariance Matrix , which provides the covariances between every possible pair of regression parameters. Since the covariance of a variable with itself is its variance, the diagonals are the respective variances of the parameters. Note that the first element of the diagonal is the intercept, thus the variance of the jth IV is the (j+1)th element of the diagonal . \\[ Var(\\hat{\\beta}) = \\begin{pmatrix} Var(\\hat{\\beta}_0) & Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) & ... & Cov(\\hat{\\beta}_0, \\hat{\\beta}_p) \\\\ Cov(\\hat{\\beta}_1, \\hat{\\beta}_0) & Var(\\hat{\\beta}_1) & ... & Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ Cov(\\hat{\\beta}_p, \\hat{\\beta}_0) & Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) & ... & Var(\\hat{\\beta}_p) \\end{pmatrix} \\] Note that the covariances are symmetrical about the diagonal - \\(Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = Cov(\\hat{\\beta}_1, \\hat{\\beta}_0)\\) .","title":"Sampling Distributions"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#hypothesis-testing","text":"Similar to SLR, the t-test can be used to test for the significance of an individual IV, but the intepretation of the test is different from the SLR case. It tests the usefulness of an individual IV in the presence of the other predictors . \\[ \\begin{aligned} H_0: \\beta_j = 0 \\\\ H_1: \\beta_j \\ne 0 \\end{aligned} \\] \\[ t(\\beta_j) = \\frac{\\hat{\\beta_j} - \\beta_j}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\] Recall that the variance of the jth IV si the (j+1)th element of the variance covariance matrix. \\[ t(\\beta_j) \\sim t_{n-p-1} \\] However, this leads to several odd results which needs to be accounted for: Predictor is not significant individually but significant when taken alone . Predictors are not significant individually but significant when taken together . There are now two possible ways to test for the significance of IVs: Conduct a single F-test to test for the joint significance of all IVs Conduct multiple t-tests to test for the joint significance of each IV The problem with the multiple t-test approach lies with the type I error of the tests. For \\(\\alpha = 0.05\\) , the probability of correctly rejecting the null is \\(0.95\\) . Assuming that all the tests are independent, the probability of correctly rejecting all the nulls is \\(0.95^p\\) , which drastically decreases with the number of tests conducted. Given enough predictors, this means that the probability drops to approximately 0, which means that there is bound to be a wrongly rejected null; a type I error is guaranteed even though it was supposed to be limited at a 0.05 chance. The Bonferroni Correction is a method of adjusting \\(\\alpha\\) of each hypothesis test such that the overall type I error is kept at its desired level. However, this has the consequence of increasing the probability of type II errors, which is why it is not popular. The F-test has the advantage of controlling the type I error regardless of the number of predictors , which is why it is preferred for hypothesis testing in the MLR.","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#confidence-intervals","text":"Similar to SLR, the confidence intervals can be constructed using the distribution of the test-statistic: \\[ \\text{Confidence Interval} = \\hat{\\beta}_j \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_j}} \\]","title":"Confidence Intervals"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#prediction-intervals","text":"Unlike in SLR, it is difficult to determine the distribution of the prediction error. Thus, the final result can be found below: \\[ \\begin{aligned} \\text{Prediction Interval} &= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\\\ &= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\sqrt{s^2 [1 + x'_* (\\boldsymbol{X'X})^-1 * x_*]} \\end{aligned} \\] Despite the result looking more complicated, the key takeaway remains the same - the further away \\(x_*\\) is from \\(\\bar{x}\\) , the greater","title":"Prediction Intervals"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#variations-of-mlr","text":"","title":"Variations of MLR"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#qualitative-iv","text":"The discussion so far has mostly focused on Quantitative IVs, thus this section will explore Qualitative IVs. They can only take values from a list of pre-defined values, known as the Levels of the variable. In a regression context, most qualitative IVs are represented in the form of a Dummy Variable which can only take two possible levels - Yes (1) or No (0). Note that there are other ways of encoding a dummy variable (-1/0/1 etc), but the principles stay large the same. \\[ \\text{Dummy Variable} = \\begin{cases} 1, & \\text{First Level} \\\\ 0, & \\text{Second Level} \\end{cases} \\] In general, \\(n-1\\) dummy variables are needed to represent a qualitative variable with \\(n\\) levels. This is because the status of the last level can be deduced from the other dummy variables . Thus, including a seperate dummy variable for this last level is redundant and will lead to the problem of Collinearity , which will be explored in a later section. For instance, consider four levels (North, East, South & West), represented by the three dummy variables ( \\(N, E , S\\) ). If any of the variables are 1, they represent their respective direction (North, East & South). If all of them are 0, then the direction is the remaining level (West). The last remaining level is often referred to as the Baseline Level as it is the default level of the variable when all other dummies are 0. Any level can be used as the baseline, but the parameters will differ across models with different baselines. \\[ E(Y|X) = \\beta_0 + \\beta_{1, North} x_{North} + \\beta_{2, East} * x_{East} + \\beta_{3, South} * x_{South} \\] \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) at the baseline level Change in \\(E(Y)\\) from the baseline to the chosen level \\(X_1 = X_2 = ... = X_j = 0\\) \\(X_1 = X_2 = ... = 0; X_j = 1\\) Dummy variables are usually used in conjunction with quantitative ones . This essentially creates a \"seperate\" regression model for each of the levels. For the simplest case of one quantiative and one dummy, \\(\\beta_2\\) is the difference in the intercept of the two resulting SLR models. \\[ E(Y|X) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\] \\[ E(Y|X) = \\begin{cases} (\\beta_0 + \\beta_2) + \\beta_1 x_1, & x_2 = 1 \\\\ \\beta_0 + \\beta_1 x_1, & x_2 = 0 \\end{cases} \\]","title":"Qualitative IV"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#interaction-model","text":"So far, it was assumed that each IV had an independent effect on the DV. However, IVs may interact to produce a joint effect on the DV, where the effect of one IV depends on the value of another IV. For instance, the production of a factory may depend on the number of Machines and Workers. However, the more machines there are, the greater the effect of an additional worker . Thus, this interaction effect can be captured through an Interaction Variable , which is the product of both IVs: \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 \\\\ &= \\beta_0 + (\\beta_1 + \\beta_3 x_2)x_1 + \\beta_2 x_2 \\\\ &= \\beta_0 + \\beta_1 x_1 + (\\beta_2 + \\beta_3 x_1) x_2 \\\\ \\end{aligned} \\] A one unit increase in \\(x_1\\) will increase E(Y) by \\(\\beta_1 + \\beta_3 x_2\\) , which depends on the value of \\(x_2\\) as well, which is is why they interact with each another. Phrased another way, for every one unit increase in \\(x_2\\) , the change in E(Y) from a unit increase in \\(x_1\\) increases by \\(\\beta_3\\) . Something unusual to take note of is that the Interaction Variable tests significant but the constituent variables do not. In this case, it is common practice to retain both the interaction and the consistuent variables in the model. This is practice is known as the Hierarchical Principle . Models containing dummy variables can also have an interaction effect. Building off the example from the previous section, \\(\\beta_2\\) is still the difference in the intercept but with the new \\(\\beta_3\\) being the difference in slopes of the two resulting SLR models. \\[ E(Y|X) = \\begin{cases} (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3)x_1, & x_2 = 1 \\\\ \\beta_0 + \\beta_1 x_1, & x_2 = 0 \\end{cases} \\]","title":"Interaction Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#piecewise-model","text":"If the DV has an abrupt change in behaviour over different values of the IVs, it can be accounted for the through the use of a Piecewise Regression . The first method of creating a piecewise regression involves the use of an Indicator Function . It is essentially a dummy variable which depends on the value of the other IVs. \\[ z_{\\{x>=c\\}} = \\begin{cases} 0, & x < c \\\\ 1, & x \\ge c \\end{cases} \\] \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 z(x-c) \\\\ &= \\begin{cases} \\beta_0 + \\beta_1 x_1, & x < c \\\\ (\\beta_0 - \\beta_2c) + (\\beta_1 + \\beta_2) x_1, & x \\ge c \\end{cases} \\end{aligned} \\] Note that \\(z(x-c)\\) is treated as a distinct IV and hence can be equivalently expressed as \\(x_2\\) ; the full notation is used here for clarity. \\(c\\) is the value at which the DV abruptly changes in behaviour, known as a Kink in the graph, which continuously connects the two regression lines. The other method is to use an interaction variable instead. Similar to how the interaction variables resulted in the model to \"split\", the model now splits at \\(x = c\\) , resulting in a non-continuous gap. \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2z + \\beta_3 zx \\\\ &= \\begin{cases} \\beta_0 + \\beta_1 x_1, & x < c \\\\ (\\beta_0 - \\beta_2) + (\\beta_1 + \\beta_3) x_1, & x \\ge c \\end{cases} \\end{aligned} \\]","title":"Piecewise Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#polynomial-model","text":"If the relationship between the DV and IV is complex (non-linear), then a Polynomial Regression can be used to better model the relationship between the two. \\[ E(Y|X) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 \\] Note that it is the same IV used in the regression, just with additional powers. Although a polynomial regression may capture the true relationship better, the regression parameters become hard to intepret. The partial derivatives can no longer be intepreted as holding other IVs constant as each IV is dependent on the same quantity, just to different powers. \\[ \\frac{\\partial E(Y|X)}{\\partial x} = \\beta_1 + 2\\beta_2 x + ... + m \\beta_m x^{m-1} \\]","title":"Polynomial Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/","text":"Gauss Markov Theorem Using OLS, the estimated regression parameters will always be unbiased under certain assumptions. The Gauss Markov Theorem extends this, which states that under certain assumptions, the OLS estimators will have the lowest variance among all possible linear unbiased estimators. In a statistics context, they are said to be the most efficient among all other linear unbiased estimators. In a regression context, the OLS estimators are said to be the Best Linear Unbiased Estimators (BLUE). This section will go over the various assumptions for both OLS and Gauss Markov Theorem. It will also cover the diagnostics to determine if the assumptions have been violated. The assumptions needed for OLS and the Gauss Markov theorem are often mixed up with each other as the assumptions needed for OLS are also needed for the theorem. This set of notes makes a clear distinction between the two. OLS Assumptions #1: Linearity Linear regression is a model where the relationship between the DV and IVs are linear. Thus, the regression parameters must be linear , but NOT the DV or IV. This means that the model is still considered a \"Linear Regression\" even after a transformation of the DV and/or IV. \\[ \\begin{aligned} y_i &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\\\ y_i &= \\beta_0 + \\beta_1 x^2 + \\beta_2 x^3 \\\\ \\ln y_i &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\end{aligned} \\] #2: Exogenity Exogenity refers to how a variable comes from outside the model and is thus independent of any other variables within the model . In a regression context, this comes in the form of the errors having a conditional mean of 0 , ensuring that the errors are random and thus not related to the IVs. \\[ E(\\varepsilon_i | x_{ij}) = 0 \\\\ \\] \"Endo\" and \"Exo\" in Greek means \"In\" and \"Out\" respectively, which is how the meaning of the words were derived. There are two key implications of Exogenity: By the Law of Total Expectations, the unconditional expectation of the error is also 0. By the Linearity of Conditional Expectations, the expectation of the product of the Error and IVs is 0. \\[ \\begin{aligned} E(\\varepsilon_i) = 0 \\\\ E(\\varepsilon_i x_{ij}) = 0 \\end{aligned} \\] Following these two implications, it can be shown that the Covariance between the Error and IVs are also 0, which is another consequence of independence (NOT the other way around). \\[ \\begin{aligned} Cov (\\varepsilon_i, x_{ij}) &= E(\\varepsilon_i x_{ij}) - E(\\varepsilon_i) * E(x_{ij}) \\\\ &= 0 - 0 * E(x_{ij}) \\\\ &= 0 \\end{aligned} \\] Without exogenity, the regression parameters would reflect the effect of both the IV and the unmodelled variable within the error term. This causes the OLS estimate to be biased , known as the Omitted Variable Bias . Since the unmodelled variable confounds the results of the regression, it is known as a Confounding Variable . Residual Analysis Since the errors are unobservable, the residuals are used to estimate the errors. If the fitted model is adequate - all relavent IVs are included in the right form, then the residuals should closely resemble the errors and therefore be structureless (random). However, if there are patterns in the residuals , it indicates that there is additional information that can be used to improve the model and thus should be included. Due to the OLS, the correlation between residuals and existing IVs will always be 0 indicating no linear relationship . To check for unmodelled non-linear relationships , a Residual Plot of the IVs against the Residuals can be used. For instance, if the residual plot shows a quadractic pattern (curve), then a quadractic IV should be added into the model. Mathematically, it can be expressed as a function of the existing estimates: \\[ \\begin{aligned} \\hat{y_i} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\varepsilon}_i \\\\ \\hat{\\varepsilon_i} &= \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\text{ (From residual plot)} \\\\ \\hat{y_i} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &= (\\hat{\\beta}_0 + \\hat{\\gamma}_0) + (\\hat{\\beta}_1 + \\hat{\\gamma}_1)x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &= \\hat{\\beta}_0' + \\hat{\\beta}_1'x + \\hat{\\beta}_2' x^2 \\end{aligned} \\] #3: No Perfect Collinearity Collinearity refers to when an IV can be expressed as a linear combination of one or more other IVs . Perfect collinearity is an extreme case where an IV can be perfectly expressed as a combination of another. Technically speaking, Collinearity refers to one to one variable relationship, while Multicollinearity refers to one to many variable relationship, hence \"Multi\". Variable is a multiple of another: \\(x_1 = cx_2\\) Variable differs by a constant from another: \\(x_1 = x_2 \\pm c\\) Variable is an affine transformation of another: Sum of several variables is fixed: Dummy Variable Trap The issue with perfect collinearity is that it affects the linear algebra used to solve for the coefficients (EG. Two equations to solve for three unknowns). There will be no unique solutions - many different values for the coefficients could work equally well. Imperfect Collinearity Imperfect Collinearity is a less extreme case where an IV is highly (but not perfectly) correlated with one or more other IVs. Recall that correlation refers to the extent of a Linear relationship. Non-linear relationships between variables are fine (EG. Polynomial Regression). This means that including the IV does not bring much additional predctive power into the model as its effects are already captured through related predictors and thus can be removed from the model. Unlike in the perfect case, high collinearity does not prevent OLS from finding a solution. However, the intepretation of the variables become complicated: The original intepretation of coefficients \"holding other variables constant\" is no longer true as highly correlated variables tend to move together. Thus, it is hard to seperate the effects of an individual variable . Consequently, OLS has difficulty estimating these coefficients, which could result in weird meaningless estimates ; EG. Large positive coefficient but large negative for its correlated counterpart. This also results in higher standard errors for the coefficients of correlated variables. This reduces the magnitude of t-statistic , which results in more false negatives , failing to reject the null when it should. This results in important variables being omitted from the regression. Technically speaking, there is nothing wrong with collinearity if the purpose of the model is solely for prediction. However, if the purpose of the model was to establish causality, then collinearity poses a problem as it interferes with statistical inference. Detecting Collinearity The simplest way to detect collinearity is through a Scatterplot or Correlation Matrix , which shows the correlations between pairs of variables . A correlation of 0.8 and higher is typically considered high enough where the collinearity becomes problematic. However, the issue is that this method can only detect collinearity between pairs of variable at a time. In order to detect collinearity among three or more variables ( multicollinearity ), then the Variance Inflation Factor (VIF) should be used instead. \\[ VIF = \\frac{1}{1-R^2_j} \\] The VIF is derived from the variance of the regression coefficient. As mentioned previously, under the presence of collinearity, the standard error and hence variance of the coefficient increases (\"inflated\"). The extent of the increase is known as the VIF. \\[ \\hat{Var}(\\hat{\\beta_1}) = \\frac{MS_{RSS}}{(n-1) s^2} * \\frac{1}{1-R^2_j} \\] The \\(R^2_j\\) in the VIF is the coefficient of determination of a model where the jth IV is regressed against all other IVs . A high \\(R^2_j\\) means that the IV is well explained by the other IVs (high correlation), which indicates the presence of collinearity. Generally, a \\(VIF > 10\\) is deemed to have severe collinearity . #4: No Extreme Outliers Outliers are observations with unusual values of the DV relative to majority of the observations. The last OLS assumption is that there are no extreme outliers in the dataset used to create the regression model. Generally, as long as the DV and IVs have a positive and finite Kurtosis , then the probability of such observations occuring are low. Outliers are problematic as OLS is sensitive to outliers . Extreme outliers have large residuals which receive more weight in the optimization process, which causes the resulting model to accomodate it (when it should not), causing the resulting coefficients to be biased. {.center} Identifying Outliers Outliers Unusual values of Y relative to the predicted values, large residuals Hard to gauge how large is large > Standardize the residuals to compare However, need to know sampling distribution of the residuals Larger than 2 or 3 is considered outlier Example from actex: Not unusual in x1 or x2 but unusual when taken collectively Average leverage Exceed three times the average to be considered large Measure of remoteness Cooks Distance Influence definition Fit the main model Fit the model without the observation want to test the bigger the difference, the more influential that observation Cooks distance takes this for all points on the whole model etc Expected value of D is 1/n, so anything larger than that is large Need n+1 datasets dataset with all observations to get mse and y then n datasets that each exclude one of the observations Can be expressed algebraically as well Product of studentized residual and leverage > Need to be large in both X and Y axis to be influential Unity threshold Identifying Leverage Points Leverage is unusual values of X Visually easy for one variable > far away from the majority in the X axis Visually hard for multiple > could lie in the range of each variable but far away collectively Gauss Markov Assumptions #1: Conditional Homoscedasticity Homoscedasticity refers to the error terms having constant variance while Heteroscedasticity refers to having non-constant variance. \\[ var(\\varepsilon_i | x_i) = \\sigma^2 \\] Under homoscedasticity, the sampling distribution of the estimates are easily derived and thus it can be shown that they are the most efficient estimators. The same cannot be proven under heteroscedasticity. Note that that the OLS estimates are still unbiased; they are just not the most efficient. Identifying Heteroscedasticity Similar to before, if the model is adequate, then the residuals should resemble the errors and have constant variance . Thus, this can be easily determined through a residual plot of the Residuals against the fitted values. If the points are equally spread out about the mean (0) and show no pattern , then homoscedasticity is present. However, if the points show an increasing or decreasing variance (typically in the shape of a funnel ), then heteroscedasticity is present. Alternatively, a hypothesis test can be conducted to determine if heteroscedasticity is present, known as the Bresuch Pagan Test . \\[ \\begin{aligned} H_0: \\sigma^2 H_1: \\sigma^2 + \\boldsymbol{Z\\gamma} \\end{aligned} \\] The test-statistic is computed as follows: Compute the squared standardized residuals from the original model Regress them onto the variables in Z (LOL need to change this part) Compute the RegSS of the new regression \\[ T = \\frac{RegSS}{2} \\] The test-statistic follows a chi-square distribution with \\(q\\) degrees of freedom, where \\(q\\) is the number of variables in \\(\\boldsymbol{Z}\\) . \\[ T \\sim \\chi^2_q \\] Dealing with Heteroscedasticity If prior information is known about the structure of the data, then the most intuitive method would be to incorporate that information into the data. \\[ Var(\\varepsilon_i) = \\frac{\\sigma^2}{w_i} \\] If no prior information is known, then the Heteroscedasticity can be reduced by using a Variance Stabilizing Transformation , such as the Log or Squareroot . Note that since they require positive data, a constant can be added to each term before the transformation to ensure that the values are positive. It is out of the scope for this set of notes to show why these help to stabilize variance. Alternatively, if there is only mild heteroscedasticity in the data, then OLS can be used but with an adjustment to the standard errors of the coefficients, known as heteroscedastic-robust standard errors . Due to complexity of the computations, it will not be covered in this set of notes. However, the general idea is that an weighted estimate of the variance covariance matrix is computed and the standard errors are computed from there. #2: No Serial Correlation If errors are correlated with one another, it is known as Serial Correlation or Autocorrelation . It implies that there are other unmodelled factors that can be used for prediction, which would imply the current model specification to be wrong. Thus, for the SLR model to be true, the errors must be independent of one another . \\[ Cov(\\varepsilon_i,\\varepsilon_j) = 0 \\] Confidence Intervals and PI are narrower than it should be > 95% PI is actually < 95%> P values lower > Appear statisticlaly significant when they shld not be Time series tends to have errors that are positively correlated, which is why it has its own dedicated section No Serial Correlation > Outcome of zero conditional mean, but most likely in time series data Error Distribution Although not needed for OLS estimation or Guass Markov, the errors of the regression are usually assumed to be normally distributed . \\[ \\varepsilon \\sim N(0, \\sigma^2) \\] If the errors are normally distributed, then it follows that \\(\\beta\\) is normally distributed as well since they are linear and additive. This greatly eases the computation needed to determine the sampling distribution for statistical inference. Q-Q Plots Since the errors are normally distributed, the residuals should be normally distributed as well. This can be verified using a Quantile-Quantile Plot (QQ Plot) , which compares the quantiles of two distributions. The first distribution is plotted on the x-axis while the second on the y-axis. If the quantiles are the same, then the points should lie on \\(y = x\\) , the 45 degree line.","title":"Gauss Markov Theorem"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#gauss-markov-theorem","text":"Using OLS, the estimated regression parameters will always be unbiased under certain assumptions. The Gauss Markov Theorem extends this, which states that under certain assumptions, the OLS estimators will have the lowest variance among all possible linear unbiased estimators. In a statistics context, they are said to be the most efficient among all other linear unbiased estimators. In a regression context, the OLS estimators are said to be the Best Linear Unbiased Estimators (BLUE). This section will go over the various assumptions for both OLS and Gauss Markov Theorem. It will also cover the diagnostics to determine if the assumptions have been violated. The assumptions needed for OLS and the Gauss Markov theorem are often mixed up with each other as the assumptions needed for OLS are also needed for the theorem. This set of notes makes a clear distinction between the two.","title":"Gauss Markov Theorem"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#ols-assumptions","text":"","title":"OLS Assumptions"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#1-linearity","text":"Linear regression is a model where the relationship between the DV and IVs are linear. Thus, the regression parameters must be linear , but NOT the DV or IV. This means that the model is still considered a \"Linear Regression\" even after a transformation of the DV and/or IV. \\[ \\begin{aligned} y_i &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\\\ y_i &= \\beta_0 + \\beta_1 x^2 + \\beta_2 x^3 \\\\ \\ln y_i &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\end{aligned} \\]","title":"#1: Linearity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#2-exogenity","text":"Exogenity refers to how a variable comes from outside the model and is thus independent of any other variables within the model . In a regression context, this comes in the form of the errors having a conditional mean of 0 , ensuring that the errors are random and thus not related to the IVs. \\[ E(\\varepsilon_i | x_{ij}) = 0 \\\\ \\] \"Endo\" and \"Exo\" in Greek means \"In\" and \"Out\" respectively, which is how the meaning of the words were derived. There are two key implications of Exogenity: By the Law of Total Expectations, the unconditional expectation of the error is also 0. By the Linearity of Conditional Expectations, the expectation of the product of the Error and IVs is 0. \\[ \\begin{aligned} E(\\varepsilon_i) = 0 \\\\ E(\\varepsilon_i x_{ij}) = 0 \\end{aligned} \\] Following these two implications, it can be shown that the Covariance between the Error and IVs are also 0, which is another consequence of independence (NOT the other way around). \\[ \\begin{aligned} Cov (\\varepsilon_i, x_{ij}) &= E(\\varepsilon_i x_{ij}) - E(\\varepsilon_i) * E(x_{ij}) \\\\ &= 0 - 0 * E(x_{ij}) \\\\ &= 0 \\end{aligned} \\] Without exogenity, the regression parameters would reflect the effect of both the IV and the unmodelled variable within the error term. This causes the OLS estimate to be biased , known as the Omitted Variable Bias . Since the unmodelled variable confounds the results of the regression, it is known as a Confounding Variable .","title":"#2: Exogenity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#residual-analysis","text":"Since the errors are unobservable, the residuals are used to estimate the errors. If the fitted model is adequate - all relavent IVs are included in the right form, then the residuals should closely resemble the errors and therefore be structureless (random). However, if there are patterns in the residuals , it indicates that there is additional information that can be used to improve the model and thus should be included. Due to the OLS, the correlation between residuals and existing IVs will always be 0 indicating no linear relationship . To check for unmodelled non-linear relationships , a Residual Plot of the IVs against the Residuals can be used. For instance, if the residual plot shows a quadractic pattern (curve), then a quadractic IV should be added into the model. Mathematically, it can be expressed as a function of the existing estimates: \\[ \\begin{aligned} \\hat{y_i} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\varepsilon}_i \\\\ \\hat{\\varepsilon_i} &= \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\text{ (From residual plot)} \\\\ \\hat{y_i} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &= (\\hat{\\beta}_0 + \\hat{\\gamma}_0) + (\\hat{\\beta}_1 + \\hat{\\gamma}_1)x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &= \\hat{\\beta}_0' + \\hat{\\beta}_1'x + \\hat{\\beta}_2' x^2 \\end{aligned} \\]","title":"Residual Analysis"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#3-no-perfect-collinearity","text":"Collinearity refers to when an IV can be expressed as a linear combination of one or more other IVs . Perfect collinearity is an extreme case where an IV can be perfectly expressed as a combination of another. Technically speaking, Collinearity refers to one to one variable relationship, while Multicollinearity refers to one to many variable relationship, hence \"Multi\". Variable is a multiple of another: \\(x_1 = cx_2\\) Variable differs by a constant from another: \\(x_1 = x_2 \\pm c\\) Variable is an affine transformation of another: Sum of several variables is fixed: Dummy Variable Trap The issue with perfect collinearity is that it affects the linear algebra used to solve for the coefficients (EG. Two equations to solve for three unknowns). There will be no unique solutions - many different values for the coefficients could work equally well.","title":"#3: No Perfect Collinearity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#imperfect-collinearity","text":"Imperfect Collinearity is a less extreme case where an IV is highly (but not perfectly) correlated with one or more other IVs. Recall that correlation refers to the extent of a Linear relationship. Non-linear relationships between variables are fine (EG. Polynomial Regression). This means that including the IV does not bring much additional predctive power into the model as its effects are already captured through related predictors and thus can be removed from the model. Unlike in the perfect case, high collinearity does not prevent OLS from finding a solution. However, the intepretation of the variables become complicated: The original intepretation of coefficients \"holding other variables constant\" is no longer true as highly correlated variables tend to move together. Thus, it is hard to seperate the effects of an individual variable . Consequently, OLS has difficulty estimating these coefficients, which could result in weird meaningless estimates ; EG. Large positive coefficient but large negative for its correlated counterpart. This also results in higher standard errors for the coefficients of correlated variables. This reduces the magnitude of t-statistic , which results in more false negatives , failing to reject the null when it should. This results in important variables being omitted from the regression. Technically speaking, there is nothing wrong with collinearity if the purpose of the model is solely for prediction. However, if the purpose of the model was to establish causality, then collinearity poses a problem as it interferes with statistical inference.","title":"Imperfect Collinearity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#detecting-collinearity","text":"The simplest way to detect collinearity is through a Scatterplot or Correlation Matrix , which shows the correlations between pairs of variables . A correlation of 0.8 and higher is typically considered high enough where the collinearity becomes problematic. However, the issue is that this method can only detect collinearity between pairs of variable at a time. In order to detect collinearity among three or more variables ( multicollinearity ), then the Variance Inflation Factor (VIF) should be used instead. \\[ VIF = \\frac{1}{1-R^2_j} \\] The VIF is derived from the variance of the regression coefficient. As mentioned previously, under the presence of collinearity, the standard error and hence variance of the coefficient increases (\"inflated\"). The extent of the increase is known as the VIF. \\[ \\hat{Var}(\\hat{\\beta_1}) = \\frac{MS_{RSS}}{(n-1) s^2} * \\frac{1}{1-R^2_j} \\] The \\(R^2_j\\) in the VIF is the coefficient of determination of a model where the jth IV is regressed against all other IVs . A high \\(R^2_j\\) means that the IV is well explained by the other IVs (high correlation), which indicates the presence of collinearity. Generally, a \\(VIF > 10\\) is deemed to have severe collinearity .","title":"Detecting Collinearity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#4-no-extreme-outliers","text":"Outliers are observations with unusual values of the DV relative to majority of the observations. The last OLS assumption is that there are no extreme outliers in the dataset used to create the regression model. Generally, as long as the DV and IVs have a positive and finite Kurtosis , then the probability of such observations occuring are low. Outliers are problematic as OLS is sensitive to outliers . Extreme outliers have large residuals which receive more weight in the optimization process, which causes the resulting model to accomodate it (when it should not), causing the resulting coefficients to be biased. {.center}","title":"#4: No Extreme Outliers"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#identifying-outliers","text":"Outliers Unusual values of Y relative to the predicted values, large residuals Hard to gauge how large is large > Standardize the residuals to compare However, need to know sampling distribution of the residuals Larger than 2 or 3 is considered outlier Example from actex: Not unusual in x1 or x2 but unusual when taken collectively Average leverage Exceed three times the average to be considered large Measure of remoteness","title":"Identifying Outliers"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#cooks-distance","text":"Influence definition Fit the main model Fit the model without the observation want to test the bigger the difference, the more influential that observation Cooks distance takes this for all points on the whole model etc Expected value of D is 1/n, so anything larger than that is large Need n+1 datasets dataset with all observations to get mse and y then n datasets that each exclude one of the observations Can be expressed algebraically as well Product of studentized residual and leverage > Need to be large in both X and Y axis to be influential Unity threshold","title":"Cooks Distance"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#identifying-leverage-points","text":"Leverage is unusual values of X Visually easy for one variable > far away from the majority in the X axis Visually hard for multiple > could lie in the range of each variable but far away collectively","title":"Identifying Leverage Points"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#gauss-markov-assumptions","text":"","title":"Gauss Markov Assumptions"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#1-conditional-homoscedasticity","text":"Homoscedasticity refers to the error terms having constant variance while Heteroscedasticity refers to having non-constant variance. \\[ var(\\varepsilon_i | x_i) = \\sigma^2 \\] Under homoscedasticity, the sampling distribution of the estimates are easily derived and thus it can be shown that they are the most efficient estimators. The same cannot be proven under heteroscedasticity. Note that that the OLS estimates are still unbiased; they are just not the most efficient.","title":"#1: Conditional Homoscedasticity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#identifying-heteroscedasticity","text":"Similar to before, if the model is adequate, then the residuals should resemble the errors and have constant variance . Thus, this can be easily determined through a residual plot of the Residuals against the fitted values. If the points are equally spread out about the mean (0) and show no pattern , then homoscedasticity is present. However, if the points show an increasing or decreasing variance (typically in the shape of a funnel ), then heteroscedasticity is present. Alternatively, a hypothesis test can be conducted to determine if heteroscedasticity is present, known as the Bresuch Pagan Test . \\[ \\begin{aligned} H_0: \\sigma^2 H_1: \\sigma^2 + \\boldsymbol{Z\\gamma} \\end{aligned} \\] The test-statistic is computed as follows: Compute the squared standardized residuals from the original model Regress them onto the variables in Z (LOL need to change this part) Compute the RegSS of the new regression \\[ T = \\frac{RegSS}{2} \\] The test-statistic follows a chi-square distribution with \\(q\\) degrees of freedom, where \\(q\\) is the number of variables in \\(\\boldsymbol{Z}\\) . \\[ T \\sim \\chi^2_q \\]","title":"Identifying Heteroscedasticity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#dealing-with-heteroscedasticity","text":"If prior information is known about the structure of the data, then the most intuitive method would be to incorporate that information into the data. \\[ Var(\\varepsilon_i) = \\frac{\\sigma^2}{w_i} \\] If no prior information is known, then the Heteroscedasticity can be reduced by using a Variance Stabilizing Transformation , such as the Log or Squareroot . Note that since they require positive data, a constant can be added to each term before the transformation to ensure that the values are positive. It is out of the scope for this set of notes to show why these help to stabilize variance. Alternatively, if there is only mild heteroscedasticity in the data, then OLS can be used but with an adjustment to the standard errors of the coefficients, known as heteroscedastic-robust standard errors . Due to complexity of the computations, it will not be covered in this set of notes. However, the general idea is that an weighted estimate of the variance covariance matrix is computed and the standard errors are computed from there.","title":"Dealing with Heteroscedasticity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#2-no-serial-correlation","text":"If errors are correlated with one another, it is known as Serial Correlation or Autocorrelation . It implies that there are other unmodelled factors that can be used for prediction, which would imply the current model specification to be wrong. Thus, for the SLR model to be true, the errors must be independent of one another . \\[ Cov(\\varepsilon_i,\\varepsilon_j) = 0 \\] Confidence Intervals and PI are narrower than it should be > 95% PI is actually < 95%> P values lower > Appear statisticlaly significant when they shld not be Time series tends to have errors that are positively correlated, which is why it has its own dedicated section No Serial Correlation > Outcome of zero conditional mean, but most likely in time series data","title":"#2: No Serial Correlation"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#error-distribution","text":"Although not needed for OLS estimation or Guass Markov, the errors of the regression are usually assumed to be normally distributed . \\[ \\varepsilon \\sim N(0, \\sigma^2) \\] If the errors are normally distributed, then it follows that \\(\\beta\\) is normally distributed as well since they are linear and additive. This greatly eases the computation needed to determine the sampling distribution for statistical inference.","title":"Error Distribution"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#q-q-plots","text":"Since the errors are normally distributed, the residuals should be normally distributed as well. This can be verified using a Quantile-Quantile Plot (QQ Plot) , which compares the quantiles of two distributions. The first distribution is plotted on the x-axis while the second on the y-axis. If the quantiles are the same, then the points should lie on \\(y = x\\) , the 45 degree line.","title":"Q-Q Plots"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/","text":"Statistical Learning","title":"Statistical Learning"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#statistical-learning","text":"","title":"Statistical Learning"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/6.%20Generalized%20Linear%20Models/","text":"","title":"6. Generalized Linear Models"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/7.%20Time%20Series%20Models/","text":"","title":"7. Time Series Models"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/8.%20Tree%20Models/","text":"","title":"8. Tree Models"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/9.%20Principal%20Component%20Analysis/","text":"","title":"9. Principal Component Analysis"}]}