{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Actuarial Exam Notes Test","title":"**Actuarial Exam Notes**"},{"location":"#actuarial-exam-notes","text":"Test","title":"Actuarial Exam Notes"},{"location":"Placeholder/","text":"Work in progress!","title":"ASA-PA"},{"location":"Placeholder/#work-in-progress","text":"","title":"Work in progress!"},{"location":"0.%20Foundational/1.%20Review%20of%20Linear%20Algebra/","text":"Vector Matrix System of equations","title":"Review of Linear Algebra"},{"location":"0.%20Foundational/2.%20Review%20of%20Calculus/","text":"Review of Calculus Differentiation Application of differentiation - Optimization Integration Application - Area/Volume","title":"Review of Calculus"},{"location":"0.%20Foundational/2.%20Review%20of%20Calculus/#review-of-calculus","text":"Differentiation Application of differentiation - Optimization Integration Application - Area/Volume","title":"Review of Calculus"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/","text":"Review of Statistical Theory This is meant to be a quick review of the basic statistical concepts of VEE: Mathematical Statistics and other undergrad statitics courses that will be relevant for this exam. Overview of Statistics Statistics is a discipline revolving around data. A Population refers to the theoretical set of all possible data of the event of interest. The goal of statistics is to determine certain attributes that summarizes or describes the population, known as Parameters . However, it is impossible to study the entire population at once, thus a subset of the population is studied instead, known as the Sample . Attributes that summarize or describe the sample are known as Statistics . Ideally, the sample is representative of the population, which means that findings from the sample can be applied to the population as a whole. This means that the sample statistics can be used estimate population parameters. We distinguish between the two (when they have the same notation) through the Hat accent (^) - Population Parameters are their written without the hat ( \\(x\\) ) while their corresponding sample statistics are written with the hat ( \\(\\hat{x}\\) ). Common Sample Statistics The Mean is the average of the population. Population Mean Sample Mean \\(\\mu = \\sum\\limits_{i=i}^n x_i * p(x_i)\\) \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=i}^n x_i\\) The Variance measures the spread of values about the mean. However, the units of Variance are unintuitive, thus we consider the Standard Deviation for more practical purposes, which is the square root of the variance . Population Variance Sample Variance \\(\\sigma^2 = \\sum\\limits_{i=i}^n (x_i - \\mu)^2 * p(x_i)\\) \\(s^2 = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x})^2\\) Covariance is a measure of the linear relationship between two variables: Positive Covariance - Variables move in the same direction Negative Covariance - Variables move in opposite directions Population Covariance Sample Covariance \\(\\sigma_{x, y} = \\mu_{xy} - \\mu_x \\mu_y\\) \\(\\hat{\\sigma}_{x, y} = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x}) * (y_i - \\bar{y})\\) However, there are two issues with Covariance - the units are unintuitive (similar to variance) and there is no benchmark as to what constitutes a strong/weak relationship. Thus, the Correlation is an adjusted measure of the relationship between -1 and 1 . Population Correlation Sample Correlation \\(\\rho = \\frac{\\sigma_{x,y}}{\\sigma_x * \\sigma_y}\\) \\(r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x} \\sum(y_i - \\bar{y})}\\) Properties of Estimators Biased Consistent Efficient Degrees of freedom Sampling Distribution Whenever a sample is drawn from a population and a statistic is calculated, it is known as a Point Estimate . Due to measurement error, a different sample would be drawn each time, thus leading to a different point estimate . If this process were to be repeated a large number times, the probability distribution of the resulting point estimates is known as the Sampling Distribution of the statistic. There is no rule surrounding the sampling distribution - it depends on the distribution of the population, statistic being measured, sampling method etc. The standard deviation of this sampling distribution is known as the Standard Error of the statistic: \\[ \\sigma_{\\theta} = \\sqrt {\\sigma^2_{\\theta}} \\] A special case is the Sample Mean . If the population is normally distributed, then it is normally distributed as well. Regardless of the population distribution, it is also approximately normally distributed through the Central Limit Theorem or Law of Large Numbers . \\[ \\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\] Following that, we can compute the standard error: \\[ \\sigma_{\\bar{x}} = \\sqrt \\frac{\\sigma^2_{\\bar{x}}}{n} = \\frac{\\sigma}{\\sqrt{n}} \\] However, the population variance is usually unknown. Thus, it can be approximated using the Sample Variance, which is an unbiased estimator for it. The result is known as an Estimate for the Standard Error : \\[ \\hat{\\sigma}_{\\bar{x}} = \\sqrt \\frac{s^2}{n} = \\frac{s}{\\sqrt{n}} \\] Note that only the sample variance is an unbiased estimator for the population variance. Although it may look like it, the sample SD is NOT an unbiased estimator for the population SD. Confidence Interval Given that there is only one true value for the population parameter and a whole distribution of estimators, it is unlikely that a point estimate will be equal to the population parameter. Thus, instead of a point estimate, a range of estimates is used, known as a Confidence Interval . The interval is made a chosen Confidence Level which represents the proportion of confidence intervals that will contain the true value . In other words, if a large number of confidence intervals constructed in the same manner were to be made, \\((1-\\alpha)%\\) of them would contain the true value. \\[ P(- \\text{Margin of Error} < \\theta < \\text{Margin of Error}) = 1 - \\alpha \\] The Margin of Error represents the range of values on either side of the point estimate that the true value could lie. For instance, for a 95% confidence interval, the confidence lies within the 0.025 and 0.975 percentile of the sampling distribution. The margin of error can be calculated by finding the corresponding values of the sampling distribution at these percentiles. Consider the 95% confidence interval for the Sample Mean , which is normally distributed. For convenience, it is usually normalized such that it will become a Standard Normal Distribution : \\[ \\begin{aligned} P(-1.96 < \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt n}} < 1.96) &= 0.95 \\\\ P(\\bar{x} - 1.96 \\frac{\\sigma}{\\sqrt n} < \\mu < \\bar{x} + 1.96 \\frac{\\sigma}{\\sqrt n}) &= 0.95 \\\\ \\end{aligned} \\] \\[ \\therefore \\text{Margin of Error} = \\bar{x} + Z_{\\frac{\\alpha}{2}} * \\frac{\\sigma}{\\sqrt n} \\] Hypothesis Testing Hypothesis Testing is a formal method of making inferences about the population parameters based on the sample statistic. It starts with a Hypothesis which is a conjecture about the population parameters: Null Hypothesis - What is currently believed to be true Alternative Hypothesis - What is to be proven A Test Statistic is then calculated, which would quantify the behaviour of population such that it would distinguish the null and alternative hypothesis. Assuming the Null Hypothesis is true , the sampling distribution of the test statistic is determined. From the sampling distribution, the p-value is calculated, which is the probability of observing the calculated sample statistic or more extreme . If the p-value is smaller than a pre-determined level of Statistical Significance ( \\(\\alpha\\) ), then the test is \"successful\" and the null hypothesis is rejected. If not, then the test \"fails\" and the null is not rejected. Note that it rejecting the null hypothesis does NOT mean that the alternative hypothesis is accepted - EG. Rejected then hypothesis of a large paw print being from a bear does not mean that it is from Bigfoot. Thus, the hypotheses are usually constructed such that the two hypothesis are complementary , such that rejecting the null allows acceptance of the alternative, leading to a definitive insight. In layman terms, a hypothesis test is a test of extremeness . Assuming the null hypothesis is true, how extreme (low probability) is the observed sample? If the p-value is sufficiently low, it means that the sample is rare - it could be due to pure chance or that it is actually not rare because the null is not true. We distinguish between the two mathematically through \\(\\alpha\\) . It is the probability of a False Positive - that the result obtained was by pure chance. It is typically set at 5%, which means that 5% of all statistics calculated are expected to be extreme under the null. Thus, if the p-value of the sample is smaller than \\(\\alpha\\) , then it is likely that the observation was not due to chance but instead because the null was false . Alternatively, instead of comparing p-value to \\(alpha\\) , the test-statistic and the corresponding value of \\(\\alpha\\) on the sampling distribution can be used. It is known as the Critical Value , which represents the boundary of Reject Null Do not Reject Null p-value smaller than \\(\\alpha\\) p-value smaller than \\(\\alpha\\) test-statistic larger than critical value test-statistic smaller than critical value Let the random variable \\(T\\) denote the test statistics. There are many different kinds of test statistics depending on the distribution and what is being investigated. Z-statistic The most simple test statistic involve the Sample Mean , which is normally distributed: \\[ T = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] If the population variance is known , then the test statistic has a Standard Normal Distribution and thus the test-statistic is known as a Z-Statistic . \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\sqrt n \\\\ &= Z * \\sqrt n \\\\ \\therefore T &\\sim N(0,1) \\end{aligned} \\] t-statistic If the population variance is unknown, then it will be approximated by the Sample Variance . Through algebraic manipulation, the test statistic can still be expressed in the form of a Z variable, but with an additional term: \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}} \\\\ &= \\frac{\\bar{x} - \\mu}{s} * \\sqrt n \\\\ &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\frac{\\sigma}{s} * \\sqrt n \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{s^2}{\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{(n-1)s^2}{(n-1)\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt {\\frac{(n-1)s^2}{\\sigma^2} * \\frac{1}{n-1}} \\\\ \\end{aligned} \\] The additional term can be shown to have a Chi-Squared Distribution of \\(n-1\\) degrees of freedom, which by definition is the sum of \\(n-1\\) independent standard normal variables: \\[ \\begin{aligned} \\chi^2_n &= Z^2 \\\\ &= \\sum \\left(\\frac {\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &= \\sum \\left(\\frac {(x_i - \\bar{x}) + (\\bar{x} - \\mu)}{\\sigma}\\right)^2 \\\\ &= \\sum \\left(\\frac{x_i - \\bar{x}}{\\sigma}\\right)^2 + \\sum \\left(\\frac{\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &= \\frac{1}{\\sigma^2} \\sum (x_i - \\bar{x})^2 + \\sum Z^2 + 0 \\\\ &= \\frac{(n-1)}{\\sigma^2} \\frac{\\sum (x_i - \\bar{x})^2}{n-1} + \\chi^2_1 \\\\ &= \\frac{(n-1)s^2}{\\sigma^2} + \\chi^2_1 \\\\ \\end{aligned} \\] \\[\\therefore \\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\] Thus, the test statistic is the ratio of a Standard Normal Variable to the squareroot of a Chi Squared Variable (divided by its degrees of freedom). By definition, this test statistic has a t-distribution with the same degrees of freedom and is known as the t-statistic : \\[ \\begin{align*} T &= \\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\\\ &= t_{n-1} * \\sqrt{n} \\\\ \\end{align*} \\] \\[\\therefore T \\sim t_{n-1}\\] Despite the slightly convoluted proof, the t-distribution is simply a standard normal distribution with heavier tails . This means that extreme values are slightly more likely, which is meant to account for the increase in variability due to the use of the sample variance rather than the population variance. F-statistic The square of t-statisic has an F-Distribution , which is defined as the ratio of two independent chi-square variables (divided by their respective degrees of freedom). It has two dimensions for its degree of freedom, reflecting the two chi-square variables. \\[ F_{m,n} = \\frac{\\frac{\\chi_m}{m}}{\\frac{\\chi_n}{n}} \\] The square of a t-statistic follows an F-distribution because: The square of the standard normal variable in the numerator becomes a \\(\\chi_1\\) variable The squareroot is removed in the denominator, becoming a \\(\\chi_{n-1}\\) over its degree of freedom \\[ \\begin{aligned} t_{n-1}^2 &= \\left(\\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\right)^2 \\\\ &= \\frac{\\chi_1}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &= \\frac{\\frac{\\chi_1}{1}}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &= F_{1,n-1} * n \\end{aligned} \\] \\[ \\therefore t_{n-1}^2 \\sim F_{1, n-1} \\] Thus, the square of the t-statistic is known as the F-statistic , which is usually used to test for the equality of variance . Maximum Likelihood Estimation If the population distribution is known, there is an alternative method of estimating the parameters apart from calculating the corresponding sample statistics. We model a set of observations as a random sample from an unknown joint probability distribution which is expressed in terms of a set of parameters. The goal of maximum likelihood estimation is to determine the parameters for which the observed data have the highest joint probability. The goal of maximum likelihood estimation is to find the values of the model parameters that maximize the likelihood function over the parameter space","title":"Review of Statistical Theory"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#review-of-statistical-theory","text":"This is meant to be a quick review of the basic statistical concepts of VEE: Mathematical Statistics and other undergrad statitics courses that will be relevant for this exam.","title":"Review of Statistical Theory"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#overview-of-statistics","text":"Statistics is a discipline revolving around data. A Population refers to the theoretical set of all possible data of the event of interest. The goal of statistics is to determine certain attributes that summarizes or describes the population, known as Parameters . However, it is impossible to study the entire population at once, thus a subset of the population is studied instead, known as the Sample . Attributes that summarize or describe the sample are known as Statistics . Ideally, the sample is representative of the population, which means that findings from the sample can be applied to the population as a whole. This means that the sample statistics can be used estimate population parameters. We distinguish between the two (when they have the same notation) through the Hat accent (^) - Population Parameters are their written without the hat ( \\(x\\) ) while their corresponding sample statistics are written with the hat ( \\(\\hat{x}\\) ).","title":"Overview of Statistics"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#common-sample-statistics","text":"The Mean is the average of the population. Population Mean Sample Mean \\(\\mu = \\sum\\limits_{i=i}^n x_i * p(x_i)\\) \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=i}^n x_i\\) The Variance measures the spread of values about the mean. However, the units of Variance are unintuitive, thus we consider the Standard Deviation for more practical purposes, which is the square root of the variance . Population Variance Sample Variance \\(\\sigma^2 = \\sum\\limits_{i=i}^n (x_i - \\mu)^2 * p(x_i)\\) \\(s^2 = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x})^2\\) Covariance is a measure of the linear relationship between two variables: Positive Covariance - Variables move in the same direction Negative Covariance - Variables move in opposite directions Population Covariance Sample Covariance \\(\\sigma_{x, y} = \\mu_{xy} - \\mu_x \\mu_y\\) \\(\\hat{\\sigma}_{x, y} = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x}) * (y_i - \\bar{y})\\) However, there are two issues with Covariance - the units are unintuitive (similar to variance) and there is no benchmark as to what constitutes a strong/weak relationship. Thus, the Correlation is an adjusted measure of the relationship between -1 and 1 . Population Correlation Sample Correlation \\(\\rho = \\frac{\\sigma_{x,y}}{\\sigma_x * \\sigma_y}\\) \\(r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x} \\sum(y_i - \\bar{y})}\\) Properties of Estimators Biased Consistent Efficient Degrees of freedom","title":"Common Sample Statistics"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#sampling-distribution","text":"Whenever a sample is drawn from a population and a statistic is calculated, it is known as a Point Estimate . Due to measurement error, a different sample would be drawn each time, thus leading to a different point estimate . If this process were to be repeated a large number times, the probability distribution of the resulting point estimates is known as the Sampling Distribution of the statistic. There is no rule surrounding the sampling distribution - it depends on the distribution of the population, statistic being measured, sampling method etc. The standard deviation of this sampling distribution is known as the Standard Error of the statistic: \\[ \\sigma_{\\theta} = \\sqrt {\\sigma^2_{\\theta}} \\] A special case is the Sample Mean . If the population is normally distributed, then it is normally distributed as well. Regardless of the population distribution, it is also approximately normally distributed through the Central Limit Theorem or Law of Large Numbers . \\[ \\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\] Following that, we can compute the standard error: \\[ \\sigma_{\\bar{x}} = \\sqrt \\frac{\\sigma^2_{\\bar{x}}}{n} = \\frac{\\sigma}{\\sqrt{n}} \\] However, the population variance is usually unknown. Thus, it can be approximated using the Sample Variance, which is an unbiased estimator for it. The result is known as an Estimate for the Standard Error : \\[ \\hat{\\sigma}_{\\bar{x}} = \\sqrt \\frac{s^2}{n} = \\frac{s}{\\sqrt{n}} \\] Note that only the sample variance is an unbiased estimator for the population variance. Although it may look like it, the sample SD is NOT an unbiased estimator for the population SD.","title":"Sampling Distribution"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#confidence-interval","text":"Given that there is only one true value for the population parameter and a whole distribution of estimators, it is unlikely that a point estimate will be equal to the population parameter. Thus, instead of a point estimate, a range of estimates is used, known as a Confidence Interval . The interval is made a chosen Confidence Level which represents the proportion of confidence intervals that will contain the true value . In other words, if a large number of confidence intervals constructed in the same manner were to be made, \\((1-\\alpha)%\\) of them would contain the true value. \\[ P(- \\text{Margin of Error} < \\theta < \\text{Margin of Error}) = 1 - \\alpha \\] The Margin of Error represents the range of values on either side of the point estimate that the true value could lie. For instance, for a 95% confidence interval, the confidence lies within the 0.025 and 0.975 percentile of the sampling distribution. The margin of error can be calculated by finding the corresponding values of the sampling distribution at these percentiles. Consider the 95% confidence interval for the Sample Mean , which is normally distributed. For convenience, it is usually normalized such that it will become a Standard Normal Distribution : \\[ \\begin{aligned} P(-1.96 < \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt n}} < 1.96) &= 0.95 \\\\ P(\\bar{x} - 1.96 \\frac{\\sigma}{\\sqrt n} < \\mu < \\bar{x} + 1.96 \\frac{\\sigma}{\\sqrt n}) &= 0.95 \\\\ \\end{aligned} \\] \\[ \\therefore \\text{Margin of Error} = \\bar{x} + Z_{\\frac{\\alpha}{2}} * \\frac{\\sigma}{\\sqrt n} \\]","title":"Confidence Interval"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#hypothesis-testing","text":"Hypothesis Testing is a formal method of making inferences about the population parameters based on the sample statistic. It starts with a Hypothesis which is a conjecture about the population parameters: Null Hypothesis - What is currently believed to be true Alternative Hypothesis - What is to be proven A Test Statistic is then calculated, which would quantify the behaviour of population such that it would distinguish the null and alternative hypothesis. Assuming the Null Hypothesis is true , the sampling distribution of the test statistic is determined. From the sampling distribution, the p-value is calculated, which is the probability of observing the calculated sample statistic or more extreme . If the p-value is smaller than a pre-determined level of Statistical Significance ( \\(\\alpha\\) ), then the test is \"successful\" and the null hypothesis is rejected. If not, then the test \"fails\" and the null is not rejected. Note that it rejecting the null hypothesis does NOT mean that the alternative hypothesis is accepted - EG. Rejected then hypothesis of a large paw print being from a bear does not mean that it is from Bigfoot. Thus, the hypotheses are usually constructed such that the two hypothesis are complementary , such that rejecting the null allows acceptance of the alternative, leading to a definitive insight. In layman terms, a hypothesis test is a test of extremeness . Assuming the null hypothesis is true, how extreme (low probability) is the observed sample? If the p-value is sufficiently low, it means that the sample is rare - it could be due to pure chance or that it is actually not rare because the null is not true. We distinguish between the two mathematically through \\(\\alpha\\) . It is the probability of a False Positive - that the result obtained was by pure chance. It is typically set at 5%, which means that 5% of all statistics calculated are expected to be extreme under the null. Thus, if the p-value of the sample is smaller than \\(\\alpha\\) , then it is likely that the observation was not due to chance but instead because the null was false . Alternatively, instead of comparing p-value to \\(alpha\\) , the test-statistic and the corresponding value of \\(\\alpha\\) on the sampling distribution can be used. It is known as the Critical Value , which represents the boundary of Reject Null Do not Reject Null p-value smaller than \\(\\alpha\\) p-value smaller than \\(\\alpha\\) test-statistic larger than critical value test-statistic smaller than critical value Let the random variable \\(T\\) denote the test statistics. There are many different kinds of test statistics depending on the distribution and what is being investigated.","title":"Hypothesis Testing"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#z-statistic","text":"The most simple test statistic involve the Sample Mean , which is normally distributed: \\[ T = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] If the population variance is known , then the test statistic has a Standard Normal Distribution and thus the test-statistic is known as a Z-Statistic . \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\sqrt n \\\\ &= Z * \\sqrt n \\\\ \\therefore T &\\sim N(0,1) \\end{aligned} \\]","title":"Z-statistic"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#t-statistic","text":"If the population variance is unknown, then it will be approximated by the Sample Variance . Through algebraic manipulation, the test statistic can still be expressed in the form of a Z variable, but with an additional term: \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}} \\\\ &= \\frac{\\bar{x} - \\mu}{s} * \\sqrt n \\\\ &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\frac{\\sigma}{s} * \\sqrt n \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{s^2}{\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{(n-1)s^2}{(n-1)\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt {\\frac{(n-1)s^2}{\\sigma^2} * \\frac{1}{n-1}} \\\\ \\end{aligned} \\] The additional term can be shown to have a Chi-Squared Distribution of \\(n-1\\) degrees of freedom, which by definition is the sum of \\(n-1\\) independent standard normal variables: \\[ \\begin{aligned} \\chi^2_n &= Z^2 \\\\ &= \\sum \\left(\\frac {\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &= \\sum \\left(\\frac {(x_i - \\bar{x}) + (\\bar{x} - \\mu)}{\\sigma}\\right)^2 \\\\ &= \\sum \\left(\\frac{x_i - \\bar{x}}{\\sigma}\\right)^2 + \\sum \\left(\\frac{\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &= \\frac{1}{\\sigma^2} \\sum (x_i - \\bar{x})^2 + \\sum Z^2 + 0 \\\\ &= \\frac{(n-1)}{\\sigma^2} \\frac{\\sum (x_i - \\bar{x})^2}{n-1} + \\chi^2_1 \\\\ &= \\frac{(n-1)s^2}{\\sigma^2} + \\chi^2_1 \\\\ \\end{aligned} \\] \\[\\therefore \\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\] Thus, the test statistic is the ratio of a Standard Normal Variable to the squareroot of a Chi Squared Variable (divided by its degrees of freedom). By definition, this test statistic has a t-distribution with the same degrees of freedom and is known as the t-statistic : \\[ \\begin{align*} T &= \\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\\\ &= t_{n-1} * \\sqrt{n} \\\\ \\end{align*} \\] \\[\\therefore T \\sim t_{n-1}\\] Despite the slightly convoluted proof, the t-distribution is simply a standard normal distribution with heavier tails . This means that extreme values are slightly more likely, which is meant to account for the increase in variability due to the use of the sample variance rather than the population variance.","title":"t-statistic"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#f-statistic","text":"The square of t-statisic has an F-Distribution , which is defined as the ratio of two independent chi-square variables (divided by their respective degrees of freedom). It has two dimensions for its degree of freedom, reflecting the two chi-square variables. \\[ F_{m,n} = \\frac{\\frac{\\chi_m}{m}}{\\frac{\\chi_n}{n}} \\] The square of a t-statistic follows an F-distribution because: The square of the standard normal variable in the numerator becomes a \\(\\chi_1\\) variable The squareroot is removed in the denominator, becoming a \\(\\chi_{n-1}\\) over its degree of freedom \\[ \\begin{aligned} t_{n-1}^2 &= \\left(\\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\right)^2 \\\\ &= \\frac{\\chi_1}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &= \\frac{\\frac{\\chi_1}{1}}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &= F_{1,n-1} * n \\end{aligned} \\] \\[ \\therefore t_{n-1}^2 \\sim F_{1, n-1} \\] Thus, the square of the t-statistic is known as the F-statistic , which is usually used to test for the equality of variance .","title":"F-statistic"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#maximum-likelihood-estimation","text":"If the population distribution is known, there is an alternative method of estimating the parameters apart from calculating the corresponding sample statistics. We model a set of observations as a random sample from an unknown joint probability distribution which is expressed in terms of a set of parameters. The goal of maximum likelihood estimation is to determine the parameters for which the observed data have the highest joint probability. The goal of maximum likelihood estimation is to find the values of the model parameters that maximize the likelihood function over the parameter space","title":"Maximum Likelihood Estimation"},{"location":"0.%20Foundational/4.%20Review%20of%20Accounting/","text":"","title":"4. Review of Accounting"},{"location":"0.%20Foundational/5.%20Review%20of%20Corporate%20Finance/","text":"","title":"5. Review of Corporate Finance"},{"location":"0.%20Foundational/Overview/","text":"","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/Overview/","text":"Overview of Actuarial Mathematics These set of exams cover the main concepts needed to evaluate insurance related risks. They are split into two main exams: Fundamentals of Actuarial Mathematics (FAM) Advanced Long Term or Short Term Actuarial Mathematics (ALTAM or ASTAM) This set of notes splits the FAM component into two seperate components for clarity, as the components are not related to one another: FAM-S (Short Term) FAM-L (Long Term) However, the actual FAM exam tests both components together in an MCQ format. It is regarded as one of the tougher examinations due to its wide breadth (due to the unrelated components).","title":"**Overview of Actuarial Mathematics**"},{"location":"2.%20Actuarial%20Mathematics/Overview/#overview-of-actuarial-mathematics","text":"These set of exams cover the main concepts needed to evaluate insurance related risks. They are split into two main exams: Fundamentals of Actuarial Mathematics (FAM) Advanced Long Term or Short Term Actuarial Mathematics (ALTAM or ASTAM) This set of notes splits the FAM component into two seperate components for clarity, as the components are not related to one another: FAM-S (Short Term) FAM-L (Long Term) However, the actual FAM exam tests both components together in an MCQ format. It is regarded as one of the tougher examinations due to its wide breadth (due to the unrelated components).","title":"Overview of Actuarial Mathematics"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/","text":"Survival Models Survival Models are probability distributions that measure the time to failure of an entity, or phrased another way, the future lifetime of an entity. In an Actuarial context, it measures the time to death of a person. Since survival models measure time , they are denoted by the continuous random variable \\(T_i\\) , where the subscript represents the age of the person being studied. Newborn Lifetime The base survival model measures the future lifetime of a newborn ( aged 0 ), denoted by the random variable \\(T_0\\) . The CDF thus represents the probability that the newborn dies before age \\(t\\) : \\[ F_0(t) = P(T_0 \\le t) \\] The Survival Function is the complement of the CDF and is the probability that the newborn survives till a certain age \\(t\\) . From a different perspective, it can also be seen as the probability of the newborn dying AFTER age \\(t\\) : \\[ \\begin{aligned} S_0(t) &= P(T_0 \\ge t) \\\\ &= 1 - P(T_0 \\le t) \\\\ &= 1 - F_0(t) \\end{aligned} \\] Note that all humans will inevitably die : \\[ \\begin{aligned} \\lim_{t \\to \\infty} S_0 (t) = 0 \\iff \\lim_{t \\to \\infty} F_0 (t) = 1 \\end{aligned} \\] Continuous Lifetime The above can be generalized for a person aged \\(x\\) , denoted by \\(T_x\\) . However, the intepretation of the two functions changes slightly. The CDF instead represents the probability that the person dies within \\(t\\) years from age \\(x\\) : \\[ \\begin{aligned} F_x(t) &= P(T_x \\le t) \\\\ T_x &= 1, 2, \\dots \\end{aligned} \\] It can also be written in terms of the newborn lifetime . The key is to understand that the newborn is assumed to survive till age \\(x\\) and then die within the next \\(t\\) years: \\[ \\begin{aligned} P(T_x \\le t) &= P(T_0 \\le x + t | T_0 > x) \\\\ &= \\frac{P(x < T_0 < x +t)}{P(T_0 > x)} \\\\ &= \\frac{P(T_0 < x +t)-P(T_0 < x)}{P(T_0 > x)} \\\\ &= \\frac{F_0(x+t) - F_0(x)}{S_0(x)} \\end{aligned} \\] Conversely, the survival function is the probability that the person survives another \\(t\\) years or dies AFTER \\(t\\) years : \\[ S_x(t) = P(T_x \\ge t) \\] It can also be written in terms of the newborn lifetime following the same logic as above. The newborn is assumed to survive till age \\(x\\) and then survive another \\(t\\) years: \\[ \\begin{aligned} P(T_x \\ge t) &= P(T_0 \\ge x + t | T_0 > x) \\\\ &= \\frac{P(T_0 > x + t)}{P(T_0 > x)} \\\\ &= \\frac{S_0(x+t)}{S_0(x)} \\end{aligned} \\] Note that since the surviving till age \\(x\\) is a subset of surviving till age \\(x+t\\) , the numerator can be simplified to just surviving till \\(x+t\\) . Note the survival function can thus be re-written as the following: \\[ \\begin{aligned} S_x(t) &= \\frac{S_0(x+t)}{S_0(x)} \\\\ S_0(x+t) &= S_0(x)S_x(t) \\\\ \\end{aligned} \\] The probability of the survival of a newborn till age \\(x+t\\) is the product of: The probability of survival of a newborn till age \\(x\\) Given that the newborn survived till age \\(x\\) , that they will survive another \\(t\\) years This can be generalized for people of any age to obtain: \\[ S_x(t+u) = S_x(t) S_{x+t}(u) \\] It is important to remember to \"update\" the starting age of the second term to the \"new age\". The opposite of the above is when an individual surives another \\(t\\) years and then dies within the following \\(u\\) years . This is known as the probability of Deferred Death : \\[ \\begin{aligned} P(t < T_x < t+u) &= P(T_x < t+u) - P(T_x < t) \\\\ &= F_x(t+u) - F_x(t) \\\\ &= 1 - S_x(t+u) - (1 - S_x(t)) \\\\ &= 1 - S_x(t)S_{x+t}(u) - 1 + S_x(t) \\\\ &= S_x(t) - S_x(t)S_{x+t}(u) \\\\ &= S_x(t)(1-S_{x+t}(u)) \\\\ &= S_x(t)F_{x+t}(u) \\end{aligned} \\] Probability Tree Perspective Since the probabilites of death and survival are conditional on the age of the individual, they can be better expressed in the form of a probability tree: Thus, it can be shown through recursion that the probability of surviving \\(t\\) years is equal to the sum of the probabilities of deferred deaths for every year after: \\[ \\begin{aligned} S_x(1) &= S_x(1)F_{x+1}(1) + S_x(1)S_{x+1}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(1) [S_{x+1}(1)F_{x+2}(1) + S_{x+1}(1)S_{x+2}(1)] \\\\ &= S_x(1)F_{x+1}(1) + S_x(1) S_{x+1}(1)F_{x+2}(1) + S_x(1) S_{x+1}(1)S_{x+2}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(2)F_{x+2}(1) + S_x(2)S_{x+2}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(2)F_{x+2}(1) + \\dots \\\\ \\\\ \\therefore S_x(t) &= \\sum S_x(t)F{x+t}(1) \\end{aligned} \\] Recall that the probability of surviving till a certain age is simply the probability that the individual will die sometime after that age. The above expression solidifies this, where the RHS is the probability of dying in every possible age after . Force of Mortality The Force of Mortality is the probability of dying instantly ; in an infinitely small unit of time (normalized by unit time): \\[ \\begin{aligned} \\mu_(x) &= \\lim_{h \\to 0} \\frac{P(T_x < h)}{h} \\\\ &= \\lim_{h \\to 0} \\frac{1 - S_x(h)}{h} \\\\ &= \\lim_{h \\to 0} \\frac{1 - (\\frac{S_0(x+h)}{S_0(x)})}{h} \\\\ &= \\lim_{h \\to 0} \\frac{S_0(x) - S_0(x+h)}{S_0(x)h} \\\\ &= \\frac{1}{S_0(x)} \\lim_{h \\to 0} \\frac{S_0(x) - S_0(x+h)}{h} \\\\ &= - \\frac{1}{S_0(x)} \\lim_{h \\to 0} \\frac{S_0(x+h) - S_0(x)}{h} \\\\ &= - \\frac{1}{S_0(x)} S'_0(x) \\\\ &= - \\frac{S'_0(x)}{S_0(x)} \\end{aligned} \\] Since \\(S_0(x)\\) is the complement of \\(F_0(x)\\) , the derivative of the survival function can be written in terms of the PDF of the future lifetime: \\[ \\begin{aligned} f_0(x) &= \\frac{d}{dx} F_0(x) \\\\ &= \\frac{d}{dx} (1-S_0(x)) \\\\ &= -S'_0(x) \\end{aligned} \\] The force of mortality can be rewritten as the following: \\[ \\mu_x = \\frac{f_0(x)}{S_0(x)} \\] Thus, the force of mortality can also be intepreted as the CONDITIONAL probability distribution of death for a person aged \\(x\\) , assuming that the person survives till age \\(x\\) . \\(f_0(x)\\) is UNCONDITIONAL distribution of the future lifetime as it does not assume the individual lives to any age. Alternative Death Function More generally, the force of mortality can be rewritten as: \\[ \\begin{aligned} \\mu_x &= \\frac{f_0(x)}{S_0(x)} \\\\ \\mu_{x+t} &= \\frac{f_x(t)}{S_x(t)} \\\\ f_x(t) &= S_x(t)\\mu_{x+t} \\\\ F_x(t) &= \\int_0^t S_x(t)\\mu_{x+t} \\end{aligned} \\] The probability of death in an interval is the SUM of the product of the probability of surviving till a given age and then dying in an infinitely small time afterwards. The sum of all the infinitely small time periods is equal to the given time interval. Following this definition, for extremely small time intervals , the force of mortality can be used to approximate the probability of death in that interval: \\[ P(T_x < h) \\approx h * \\mu_{x} \\] Alternative Survival Function More generally, the force of mortality can be rewritten as: \\[ \\begin{aligned} \\mu_(x) &= - \\frac{S'_0(x)}{S_0(x)} \\\\ \\mu_{x + t} &= \\frac{-S'_x(t)}{S_x(t)} \\\\ \\mu_{x + t} &= - \\frac{d}{dt} (\\ln S_x(t)) \\\\ \\ln S_x(t) &= - \\int_{0}^{t} \\mu_{x + t} \\\\ S_x(t) &= e ^ {- \\int_{0}^{t} \\mu_{x + t}} \\\\ \\end{aligned} \\] Thus, if \\(\\mu_x\\) is known for all ages (x), then the survival probabilites for any age can be calculated from it. In other words, the distribution of \\(T_x\\) can be determined from the force of mortality . Continuous Expectation Life Expectancy measures the number of years a person is expected to live. It is calculated as the Expectation of the distribution of future lifetime: \\[ \\begin{aligned} E(T_x) &= \\int_{0}^{t} t f_x(t) \\\\ &= - \\int_{0}^{t} t S_x(t) \\end{aligned} \\] Let \\(u=t\\) and \\(dv=S'_x(t)\\) . Through integration by parts, \\[ \\begin{aligned} E(T_x) &= -\\left(uv - \\int_{0}^{\\infty} v du\\right) \\\\ &= -\\left(\\Bigl[tS_x(t)\\Bigr]_0^\\infty - \\int_0^\\infty S_x(t)\\right) \\\\ &= \\int_0^\\infty S_x(t) \\end{aligned} \\] Recall that \\([-t * S_x(t)]^{\\infty}_0\\) is 0 as the probability of living forever is 0. More formally, the above is known as the Complete Expectation of Life . If the expectation was calculated over a specified term instead of indefinitely, then the result is known as the Term Expectation of Life . Continuous Variance The second moment can be calculated in a similar fashion: \\[ \\begin{aligned} E(T^2_x) &= \\int_{0}^{\\infty} t^2 f_x(t) \\\\ &= -\\left(\\Bigl[t^2 S_x(t)\\Bigr]_0^\\infty - \\int_{0}^{\\infty} 2tS_x(t) \\right) \\\\ &= \\int_{0}^{\\infty} 2tS_x(t) \\end{aligned} \\] Thus, the Variance of the future lifetime about the Expectation is the following: \\[ \\begin{aligned} Var(T_x) &= E(T^2_x) - [E(T_x)]^2 \\\\ &= \\int_{0}^{\\infty} 2tS_x(t) - \\left(\\int_0^\\infty S_x(t) \\right)^2 \\end{aligned} \\] Actuarial Notation Given how often these expressions are used, they are often abbreviated using the International Actuarial Notation : \\[ \\begin{aligned} {}_{t}p_{x} &= S_x(t) \\\\ {}_{t}q_{x} &= F_x(t) \\\\ {}_{t}q_{x} + {}_{t}p_{x} &= 1 \\end{aligned} \\] If \\(t=1\\) , it is omitted; EG. \\({}_{1}q_{x} = q_{x}\\) . Deferred Deaths are expressed using the pipe symbol: \\[ \\begin{aligned} {}_{t|u}q_{x} &= S_x(t)F_{x+t}(u) \\\\ &= {}_{t}p_{x} * {}_{u}q_{x+t} \\end{aligned} \\] These notations are typically used when \\(t\\) is an integer, but not necessarily so, as will be seen in the next section. The expectation of the survival distribution is also commonly denoted as \\(\\mathring{e}_x\\) . Discrete Lifetime If only the integer components of the future lifetime are considered, then it becomes a discrete distribution known as the Curtate Future Lifetime of the person. It is denoted by the random variable \\(K_x\\) , which is a truncated version of \\(T_x\\) . Formally, it is known as a Floor Function : \\[ \\begin{aligned} K_x &= \\lfloor T_x \\rfloor \\\\ K_x &= 0, 1, \\dots \\end{aligned} \\] It measures the number of full years that the individual is expected to live for. If \\(K_x = k\\) , then it means the individual will another \\(k\\) full years but not live past \\(k+1\\) years full years. In other words, they will die between \\(k\\) (inclusive) and \\(k+1\\) (Exclusive) years . Note that the distribution starts from 0 . If \\(K_x = 0\\) , then they will not live another full year; they will die within the current year. \\[ \\begin{aligned} P(K_x = k) &= P(x+k \\le T_0 < x+k+1) \\\\ &= P(k \\le T_x < k+1) \\\\ &= P(T_x < k+1) - P(T_x \\le k) \\\\ &= F_x(k+1) - F_x(k) \\\\ &= S_x(k) - S_x(k+1) \\\\ &= S_x(k) - S_x(k) * S_{x+k}(1) \\\\ &= S_x(k) (1 - S_{x+k}(1)) \\\\ &= S_x(k) F_{x+k}(1) \\\\ &= {} {}_{k}p_{x} {}_{}q_{x + k} \\\\ &= {}_{k|}q_{x} \\end{aligned} \\] Discrete Expectation Similarly, the Expected Curtate Lifetime is the expectation of \\(K_x\\) : \\[ \\begin{aligned} e_x &=(K_x) \\\\ &= \\sum_{k=0}^{\\infty} k ({}_{k}p_{x} - {}_{k+1}p_{x}) \\\\ &= 1 ({}_{1}p_{x} - {}_{2}p_{x}) + 2 ({}_{2}p_{x} - {}_{3}p_{x}) + 3 ({}_{3}p_{x} - {}_{4}p_{x}) + ... \\\\ &= {}_{1}p_{x} - {}_{2}p_{x} + 2{}_{2}p_{x} - 2{}_{3}p_{x} + 3 {}_{3}p_{x} - 3 {}_{4}p_{x} \\\\ &= {}_{1}p_{x} + 2{}_{2}p_{x} + 3 {}_{3}p_{x} + ... \\\\ &= \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\end{aligned} \\] If the expectation for one period is known, the expectation for all other periods can be calculated recurively: \\[ \\begin{aligned} e _{x+1} &= \\sum _{k=1}^{\\infty} {}_{k}p_{x+1} \\\\ &= \\sum _{k=1}^{\\infty} \\frac{{}_{k+1}p_{x}}{{}_{1}p_{x}} \\\\ &= \\frac{1}{p_x}\\cdot \\left(\\sum _{k=1}^{\\infty} {}_{k}p_{x} - {}_{1}p_{x} \\right)\\\\ &= \\frac{1}{p_x}(e_x - {}_{1}p_{x}) \\\\ &= \\frac{e_x}{p_x} - 1 \\end{aligned} \\] Generally speaking, the discrete expectation can be thought of as an approximation of the continuous one. Mathematically speaking, the continuous one calculates the area under the survival function, which the discrete version approximates using rectangles. Since the rectangles under approximate the area, \\[ e_x < \\mathring{e}_x \\] If it is asssumed that the lifetime variable is uniformly distributed within the year (which is a fair assumption because the chance of dying in December is not much larger than dying in January), then it can be shown that: \\[ \\mathring{e}_x \\approx e_x + \\frac{1}{2} \\] Note that the assumption falls off for older years as the difference in mortality between the end of year and the beginning of year is larger for older lives than for younger lives. Discrete Variance The second moment can be calculated in a similar fashion: \\[ \\begin{aligned} E(K^2_x) &= \\sum_{k=0}^{\\infty} k^2 ({}_{k}p_{x} - {}_{k+1}p_{x}) \\\\ &= 1 ({}_{1}p_{x} - {}_{2}p_{x}) + 4 ({}_{2}p_{x} - {}_{3}p_{x}) + 9 ({}_{3}p_{x} - {}_{4}p_{x}) + ... \\\\ &= {}_{1}p_{x} - {}_{2}p_{x} + 4{}_{2}p_{x} - 4{}_{3}p_{x} + 9 {}_{3}p_{x} - 9 {}_{4}p_{x} \\\\ &= {}_{1}p_{x} + 3{}_{2}p_{x} + 5 {}_{3}p_{x} + ... \\\\ &= \\sum_{k=1}^{\\infty} (2k-1) {}_{k}p_{x} \\\\ &= \\sum_{k=1}^{\\infty} 2k{}_{k}p_{x} - {}_{k}p_{x} \\\\ &= \\sum_{k=1}^{\\infty} 2k{}_{k}p_{x} - \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\\\ &= 2 \\sum_{k=1}^{\\infty} k{}_{k}p_{x} - e_x \\end{aligned} \\] Thus, the variance can be calculated as: \\[ \\begin{aligned} Var(K_x) &= E(K_x^2) - [E(K_x)]^2 \\\\ &= 2 \\sum_{k=1}^{\\infty} k{}_{k}p_{x} - e_x - (e_x)^2 \\end{aligned} \\] Parametric Survival Models Given the importance of \\(\\mu_x\\) , several mathematical functions have been made to describe the force of mortality, known as a Parametric Survival Model . De Moivre Model Gompertz Model Refer to youtube video for proof Makeham Model Gompertz Makeham Distributions Gompertz Law law suggests that mortality increases exponentially with age : \\[ \\mu_x = Bc^x \\] \\(B\\) represents the initial level of mortality while \\(c\\) represents the rate of change of mortality Makeha","title":"Survival Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#survival-models","text":"Survival Models are probability distributions that measure the time to failure of an entity, or phrased another way, the future lifetime of an entity. In an Actuarial context, it measures the time to death of a person. Since survival models measure time , they are denoted by the continuous random variable \\(T_i\\) , where the subscript represents the age of the person being studied.","title":"Survival Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#newborn-lifetime","text":"The base survival model measures the future lifetime of a newborn ( aged 0 ), denoted by the random variable \\(T_0\\) . The CDF thus represents the probability that the newborn dies before age \\(t\\) : \\[ F_0(t) = P(T_0 \\le t) \\] The Survival Function is the complement of the CDF and is the probability that the newborn survives till a certain age \\(t\\) . From a different perspective, it can also be seen as the probability of the newborn dying AFTER age \\(t\\) : \\[ \\begin{aligned} S_0(t) &= P(T_0 \\ge t) \\\\ &= 1 - P(T_0 \\le t) \\\\ &= 1 - F_0(t) \\end{aligned} \\] Note that all humans will inevitably die : \\[ \\begin{aligned} \\lim_{t \\to \\infty} S_0 (t) = 0 \\iff \\lim_{t \\to \\infty} F_0 (t) = 1 \\end{aligned} \\]","title":"Newborn Lifetime"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#continuous-lifetime","text":"The above can be generalized for a person aged \\(x\\) , denoted by \\(T_x\\) . However, the intepretation of the two functions changes slightly. The CDF instead represents the probability that the person dies within \\(t\\) years from age \\(x\\) : \\[ \\begin{aligned} F_x(t) &= P(T_x \\le t) \\\\ T_x &= 1, 2, \\dots \\end{aligned} \\] It can also be written in terms of the newborn lifetime . The key is to understand that the newborn is assumed to survive till age \\(x\\) and then die within the next \\(t\\) years: \\[ \\begin{aligned} P(T_x \\le t) &= P(T_0 \\le x + t | T_0 > x) \\\\ &= \\frac{P(x < T_0 < x +t)}{P(T_0 > x)} \\\\ &= \\frac{P(T_0 < x +t)-P(T_0 < x)}{P(T_0 > x)} \\\\ &= \\frac{F_0(x+t) - F_0(x)}{S_0(x)} \\end{aligned} \\] Conversely, the survival function is the probability that the person survives another \\(t\\) years or dies AFTER \\(t\\) years : \\[ S_x(t) = P(T_x \\ge t) \\] It can also be written in terms of the newborn lifetime following the same logic as above. The newborn is assumed to survive till age \\(x\\) and then survive another \\(t\\) years: \\[ \\begin{aligned} P(T_x \\ge t) &= P(T_0 \\ge x + t | T_0 > x) \\\\ &= \\frac{P(T_0 > x + t)}{P(T_0 > x)} \\\\ &= \\frac{S_0(x+t)}{S_0(x)} \\end{aligned} \\] Note that since the surviving till age \\(x\\) is a subset of surviving till age \\(x+t\\) , the numerator can be simplified to just surviving till \\(x+t\\) . Note the survival function can thus be re-written as the following: \\[ \\begin{aligned} S_x(t) &= \\frac{S_0(x+t)}{S_0(x)} \\\\ S_0(x+t) &= S_0(x)S_x(t) \\\\ \\end{aligned} \\] The probability of the survival of a newborn till age \\(x+t\\) is the product of: The probability of survival of a newborn till age \\(x\\) Given that the newborn survived till age \\(x\\) , that they will survive another \\(t\\) years This can be generalized for people of any age to obtain: \\[ S_x(t+u) = S_x(t) S_{x+t}(u) \\] It is important to remember to \"update\" the starting age of the second term to the \"new age\". The opposite of the above is when an individual surives another \\(t\\) years and then dies within the following \\(u\\) years . This is known as the probability of Deferred Death : \\[ \\begin{aligned} P(t < T_x < t+u) &= P(T_x < t+u) - P(T_x < t) \\\\ &= F_x(t+u) - F_x(t) \\\\ &= 1 - S_x(t+u) - (1 - S_x(t)) \\\\ &= 1 - S_x(t)S_{x+t}(u) - 1 + S_x(t) \\\\ &= S_x(t) - S_x(t)S_{x+t}(u) \\\\ &= S_x(t)(1-S_{x+t}(u)) \\\\ &= S_x(t)F_{x+t}(u) \\end{aligned} \\]","title":"Continuous Lifetime"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#probability-tree-perspective","text":"Since the probabilites of death and survival are conditional on the age of the individual, they can be better expressed in the form of a probability tree: Thus, it can be shown through recursion that the probability of surviving \\(t\\) years is equal to the sum of the probabilities of deferred deaths for every year after: \\[ \\begin{aligned} S_x(1) &= S_x(1)F_{x+1}(1) + S_x(1)S_{x+1}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(1) [S_{x+1}(1)F_{x+2}(1) + S_{x+1}(1)S_{x+2}(1)] \\\\ &= S_x(1)F_{x+1}(1) + S_x(1) S_{x+1}(1)F_{x+2}(1) + S_x(1) S_{x+1}(1)S_{x+2}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(2)F_{x+2}(1) + S_x(2)S_{x+2}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(2)F_{x+2}(1) + \\dots \\\\ \\\\ \\therefore S_x(t) &= \\sum S_x(t)F{x+t}(1) \\end{aligned} \\] Recall that the probability of surviving till a certain age is simply the probability that the individual will die sometime after that age. The above expression solidifies this, where the RHS is the probability of dying in every possible age after .","title":"Probability Tree Perspective"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#force-of-mortality","text":"The Force of Mortality is the probability of dying instantly ; in an infinitely small unit of time (normalized by unit time): \\[ \\begin{aligned} \\mu_(x) &= \\lim_{h \\to 0} \\frac{P(T_x < h)}{h} \\\\ &= \\lim_{h \\to 0} \\frac{1 - S_x(h)}{h} \\\\ &= \\lim_{h \\to 0} \\frac{1 - (\\frac{S_0(x+h)}{S_0(x)})}{h} \\\\ &= \\lim_{h \\to 0} \\frac{S_0(x) - S_0(x+h)}{S_0(x)h} \\\\ &= \\frac{1}{S_0(x)} \\lim_{h \\to 0} \\frac{S_0(x) - S_0(x+h)}{h} \\\\ &= - \\frac{1}{S_0(x)} \\lim_{h \\to 0} \\frac{S_0(x+h) - S_0(x)}{h} \\\\ &= - \\frac{1}{S_0(x)} S'_0(x) \\\\ &= - \\frac{S'_0(x)}{S_0(x)} \\end{aligned} \\] Since \\(S_0(x)\\) is the complement of \\(F_0(x)\\) , the derivative of the survival function can be written in terms of the PDF of the future lifetime: \\[ \\begin{aligned} f_0(x) &= \\frac{d}{dx} F_0(x) \\\\ &= \\frac{d}{dx} (1-S_0(x)) \\\\ &= -S'_0(x) \\end{aligned} \\] The force of mortality can be rewritten as the following: \\[ \\mu_x = \\frac{f_0(x)}{S_0(x)} \\] Thus, the force of mortality can also be intepreted as the CONDITIONAL probability distribution of death for a person aged \\(x\\) , assuming that the person survives till age \\(x\\) . \\(f_0(x)\\) is UNCONDITIONAL distribution of the future lifetime as it does not assume the individual lives to any age.","title":"Force of Mortality"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#alternative-death-function","text":"More generally, the force of mortality can be rewritten as: \\[ \\begin{aligned} \\mu_x &= \\frac{f_0(x)}{S_0(x)} \\\\ \\mu_{x+t} &= \\frac{f_x(t)}{S_x(t)} \\\\ f_x(t) &= S_x(t)\\mu_{x+t} \\\\ F_x(t) &= \\int_0^t S_x(t)\\mu_{x+t} \\end{aligned} \\] The probability of death in an interval is the SUM of the product of the probability of surviving till a given age and then dying in an infinitely small time afterwards. The sum of all the infinitely small time periods is equal to the given time interval. Following this definition, for extremely small time intervals , the force of mortality can be used to approximate the probability of death in that interval: \\[ P(T_x < h) \\approx h * \\mu_{x} \\]","title":"Alternative Death Function"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#alternative-survival-function","text":"More generally, the force of mortality can be rewritten as: \\[ \\begin{aligned} \\mu_(x) &= - \\frac{S'_0(x)}{S_0(x)} \\\\ \\mu_{x + t} &= \\frac{-S'_x(t)}{S_x(t)} \\\\ \\mu_{x + t} &= - \\frac{d}{dt} (\\ln S_x(t)) \\\\ \\ln S_x(t) &= - \\int_{0}^{t} \\mu_{x + t} \\\\ S_x(t) &= e ^ {- \\int_{0}^{t} \\mu_{x + t}} \\\\ \\end{aligned} \\] Thus, if \\(\\mu_x\\) is known for all ages (x), then the survival probabilites for any age can be calculated from it. In other words, the distribution of \\(T_x\\) can be determined from the force of mortality .","title":"Alternative Survival Function"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#continuous-expectation","text":"Life Expectancy measures the number of years a person is expected to live. It is calculated as the Expectation of the distribution of future lifetime: \\[ \\begin{aligned} E(T_x) &= \\int_{0}^{t} t f_x(t) \\\\ &= - \\int_{0}^{t} t S_x(t) \\end{aligned} \\] Let \\(u=t\\) and \\(dv=S'_x(t)\\) . Through integration by parts, \\[ \\begin{aligned} E(T_x) &= -\\left(uv - \\int_{0}^{\\infty} v du\\right) \\\\ &= -\\left(\\Bigl[tS_x(t)\\Bigr]_0^\\infty - \\int_0^\\infty S_x(t)\\right) \\\\ &= \\int_0^\\infty S_x(t) \\end{aligned} \\] Recall that \\([-t * S_x(t)]^{\\infty}_0\\) is 0 as the probability of living forever is 0. More formally, the above is known as the Complete Expectation of Life . If the expectation was calculated over a specified term instead of indefinitely, then the result is known as the Term Expectation of Life .","title":"Continuous Expectation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#continuous-variance","text":"The second moment can be calculated in a similar fashion: \\[ \\begin{aligned} E(T^2_x) &= \\int_{0}^{\\infty} t^2 f_x(t) \\\\ &= -\\left(\\Bigl[t^2 S_x(t)\\Bigr]_0^\\infty - \\int_{0}^{\\infty} 2tS_x(t) \\right) \\\\ &= \\int_{0}^{\\infty} 2tS_x(t) \\end{aligned} \\] Thus, the Variance of the future lifetime about the Expectation is the following: \\[ \\begin{aligned} Var(T_x) &= E(T^2_x) - [E(T_x)]^2 \\\\ &= \\int_{0}^{\\infty} 2tS_x(t) - \\left(\\int_0^\\infty S_x(t) \\right)^2 \\end{aligned} \\]","title":"Continuous Variance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#actuarial-notation","text":"Given how often these expressions are used, they are often abbreviated using the International Actuarial Notation : \\[ \\begin{aligned} {}_{t}p_{x} &= S_x(t) \\\\ {}_{t}q_{x} &= F_x(t) \\\\ {}_{t}q_{x} + {}_{t}p_{x} &= 1 \\end{aligned} \\] If \\(t=1\\) , it is omitted; EG. \\({}_{1}q_{x} = q_{x}\\) . Deferred Deaths are expressed using the pipe symbol: \\[ \\begin{aligned} {}_{t|u}q_{x} &= S_x(t)F_{x+t}(u) \\\\ &= {}_{t}p_{x} * {}_{u}q_{x+t} \\end{aligned} \\] These notations are typically used when \\(t\\) is an integer, but not necessarily so, as will be seen in the next section. The expectation of the survival distribution is also commonly denoted as \\(\\mathring{e}_x\\) .","title":"Actuarial Notation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#discrete-lifetime","text":"If only the integer components of the future lifetime are considered, then it becomes a discrete distribution known as the Curtate Future Lifetime of the person. It is denoted by the random variable \\(K_x\\) , which is a truncated version of \\(T_x\\) . Formally, it is known as a Floor Function : \\[ \\begin{aligned} K_x &= \\lfloor T_x \\rfloor \\\\ K_x &= 0, 1, \\dots \\end{aligned} \\] It measures the number of full years that the individual is expected to live for. If \\(K_x = k\\) , then it means the individual will another \\(k\\) full years but not live past \\(k+1\\) years full years. In other words, they will die between \\(k\\) (inclusive) and \\(k+1\\) (Exclusive) years . Note that the distribution starts from 0 . If \\(K_x = 0\\) , then they will not live another full year; they will die within the current year. \\[ \\begin{aligned} P(K_x = k) &= P(x+k \\le T_0 < x+k+1) \\\\ &= P(k \\le T_x < k+1) \\\\ &= P(T_x < k+1) - P(T_x \\le k) \\\\ &= F_x(k+1) - F_x(k) \\\\ &= S_x(k) - S_x(k+1) \\\\ &= S_x(k) - S_x(k) * S_{x+k}(1) \\\\ &= S_x(k) (1 - S_{x+k}(1)) \\\\ &= S_x(k) F_{x+k}(1) \\\\ &= {} {}_{k}p_{x} {}_{}q_{x + k} \\\\ &= {}_{k|}q_{x} \\end{aligned} \\]","title":"Discrete Lifetime"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#discrete-expectation","text":"Similarly, the Expected Curtate Lifetime is the expectation of \\(K_x\\) : \\[ \\begin{aligned} e_x &=(K_x) \\\\ &= \\sum_{k=0}^{\\infty} k ({}_{k}p_{x} - {}_{k+1}p_{x}) \\\\ &= 1 ({}_{1}p_{x} - {}_{2}p_{x}) + 2 ({}_{2}p_{x} - {}_{3}p_{x}) + 3 ({}_{3}p_{x} - {}_{4}p_{x}) + ... \\\\ &= {}_{1}p_{x} - {}_{2}p_{x} + 2{}_{2}p_{x} - 2{}_{3}p_{x} + 3 {}_{3}p_{x} - 3 {}_{4}p_{x} \\\\ &= {}_{1}p_{x} + 2{}_{2}p_{x} + 3 {}_{3}p_{x} + ... \\\\ &= \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\end{aligned} \\] If the expectation for one period is known, the expectation for all other periods can be calculated recurively: \\[ \\begin{aligned} e _{x+1} &= \\sum _{k=1}^{\\infty} {}_{k}p_{x+1} \\\\ &= \\sum _{k=1}^{\\infty} \\frac{{}_{k+1}p_{x}}{{}_{1}p_{x}} \\\\ &= \\frac{1}{p_x}\\cdot \\left(\\sum _{k=1}^{\\infty} {}_{k}p_{x} - {}_{1}p_{x} \\right)\\\\ &= \\frac{1}{p_x}(e_x - {}_{1}p_{x}) \\\\ &= \\frac{e_x}{p_x} - 1 \\end{aligned} \\] Generally speaking, the discrete expectation can be thought of as an approximation of the continuous one. Mathematically speaking, the continuous one calculates the area under the survival function, which the discrete version approximates using rectangles. Since the rectangles under approximate the area, \\[ e_x < \\mathring{e}_x \\] If it is asssumed that the lifetime variable is uniformly distributed within the year (which is a fair assumption because the chance of dying in December is not much larger than dying in January), then it can be shown that: \\[ \\mathring{e}_x \\approx e_x + \\frac{1}{2} \\] Note that the assumption falls off for older years as the difference in mortality between the end of year and the beginning of year is larger for older lives than for younger lives.","title":"Discrete Expectation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#discrete-variance","text":"The second moment can be calculated in a similar fashion: \\[ \\begin{aligned} E(K^2_x) &= \\sum_{k=0}^{\\infty} k^2 ({}_{k}p_{x} - {}_{k+1}p_{x}) \\\\ &= 1 ({}_{1}p_{x} - {}_{2}p_{x}) + 4 ({}_{2}p_{x} - {}_{3}p_{x}) + 9 ({}_{3}p_{x} - {}_{4}p_{x}) + ... \\\\ &= {}_{1}p_{x} - {}_{2}p_{x} + 4{}_{2}p_{x} - 4{}_{3}p_{x} + 9 {}_{3}p_{x} - 9 {}_{4}p_{x} \\\\ &= {}_{1}p_{x} + 3{}_{2}p_{x} + 5 {}_{3}p_{x} + ... \\\\ &= \\sum_{k=1}^{\\infty} (2k-1) {}_{k}p_{x} \\\\ &= \\sum_{k=1}^{\\infty} 2k{}_{k}p_{x} - {}_{k}p_{x} \\\\ &= \\sum_{k=1}^{\\infty} 2k{}_{k}p_{x} - \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\\\ &= 2 \\sum_{k=1}^{\\infty} k{}_{k}p_{x} - e_x \\end{aligned} \\] Thus, the variance can be calculated as: \\[ \\begin{aligned} Var(K_x) &= E(K_x^2) - [E(K_x)]^2 \\\\ &= 2 \\sum_{k=1}^{\\infty} k{}_{k}p_{x} - e_x - (e_x)^2 \\end{aligned} \\]","title":"Discrete Variance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#parametric-survival-models","text":"Given the importance of \\(\\mu_x\\) , several mathematical functions have been made to describe the force of mortality, known as a Parametric Survival Model .","title":"Parametric Survival Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#de-moivre-model","text":"","title":"De Moivre Model"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#gompertz-model","text":"Refer to youtube video for proof","title":"Gompertz Model"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#makeham-model","text":"","title":"Makeham Model"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#gompertz-makeham-distributions","text":"Gompertz Law law suggests that mortality increases exponentially with age : \\[ \\mu_x = Bc^x \\] \\(B\\) represents the initial level of mortality while \\(c\\) represents the rate of change of mortality Makeha","title":"Gompertz Makeham Distributions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/","text":"Life Tables Life Tables existed long before the survival models in the previous section. The mortality functions were presented in tabular form, where the probabilities and expectations were then calculated from. Both the survival model and life table are equivalent ways of obtaining the same results. Basic Life Table The life table is constructed based on the mortality of a group of people known as the Cohort . The initial age of the group people is known as the Starting Age , denoted by \\(\\alpha\\) . The initial number of people in the cohort is known as the Radix , which is usually a large round number (EG. 1000) Conversely, the maximum age is known as the Terminal Age , denoted by \\(\\omega\\) . Everybody in the cohort is expected to gradually die by the terminal age . The number of people alive from the cohort at age \\(x\\) is denoted as \\(\\ell_x\\) : \\[ \\begin{aligned} \\ell_0 &= 1000 \\\\ \\ell_{\\omega} &= 0 \\\\ \\\\ \\therefore \\ell_{x+t} &< \\ell_{x} \\end{aligned} \\] From this basic life table, several important values can be calculated: The expected number of deaths over a period, \\({}_{n}d_{x}\\) The probability of survival past a period, \\({}_{n}p_{x}\\) The probability of death within a period, \\({}_{n}q_{x}\\) The deferred probability of death within a period, \\({}_{s|t}q_{x}\\) \\[ \\begin{aligned} {}_{n}d_{x} &= \\ell_{x} - \\ell_{x+n} \\\\ {}_{n}p_{x} &= \\frac{\\ell_{x+t}}{\\ell_{x}} \\\\ {}_{n}q_{x} &= \\frac{{}_{n}d_{x}}{\\ell_{x}} \\\\ {}_{s|t}q_{x} &= \\frac{{}_{t}d_{x+s}}{l_{x}} \\\\ \\end{aligned} \\] Thus, it can be seen that the probabilities are simply the proportion of people who died/survived over the a given period. Fractional Age Assumptions One limitation of the life table is that it is computed at discrete ages while many problems require computations for non-discrete ages. Thus, several assumptions about the life table that allows non-discrete values to be interpolated from the discrete ones, known as Fractional Age Assumptions . Uniform Distribution of Deaths The Uniform Distribution of Deaths (UDD) is an assumption that allows for Linear Interpolation between discrete ages. As its name suggests, it assumes that there is a uniform distribution of deaths within each year of age, such that the survival function decreases uniformly, resulting in a linear survival function between ages. Let the fractional age be \\(s\\) . Thus, the probability using fractional ages is simply the weighted average of the discrete points: \\[ \\begin{aligned} {}_{s}p_{x} &= (1-s) {}_{0}p_{x} + s {}_{1}p_{x} \\\\ &= (1-s)\\frac{\\ell_{x}}{\\ell_{x}} + s \\frac{\\ell_{x+1}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x}-s\\ell_{x} + s\\ell_{x+1}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x} - s(\\ell_{x}-\\ell_{x+1})}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x} - s d_{x}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x}}{\\ell_{x}} - s\\frac{d_{x}}{\\ell_{x}} \\\\ &= 1 - s q_{x} \\\\ \\\\ \\therefore {}_{s}q_{x} &= s \\cdot q_x \\end{aligned} \\] Constant Force of Mortality The Constant Force of Mortality is an assumption that allows for Exponential Interpolation between discrete ages. Select & Ultimate Mortality Mortality rates for the general population versus individuals who buy life insurance tend to be different. Generally speaking, people who purchase insurance tend to be richer and thus have better mortality rates than the general population . Within the individuals who purchase life insurance, those who have recently purchased a policy tend to have better mortality . This is because these individuals would have gone through Medical Underwriting and thus is expected to be in better health. The extent of the better mortality decreases over time and after a few years, they should experience the same mortality as the rest of the individuals who purchased life insurance. The duration of time is dependent on the rigorousness of the underwriting process . Formally speaking, the individuals who purchased life insurance were selected by the underwriting process and thus the better mortality experienced is known as Select Mortality . Similarly, the time whereby the select mortality is better is known as the Select Period . The select mortality ultimately converges with the non-select group, known as the Ultimate Mortality . In actuarial notation, subscript \\([x]\\) is used to distinguish select mortality from ultimate mortality, where the select period is denoted as \\(d\\) : \\[ \\begin{aligned} \\begin{cases} q_{[x]+t} < q_{x+t},& t < d \\\\ q_{[x]+t} = q_{x+t},& t \\ge d \\end{cases} \\end{aligned} \\]","title":"Life Tables"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#life-tables","text":"Life Tables existed long before the survival models in the previous section. The mortality functions were presented in tabular form, where the probabilities and expectations were then calculated from. Both the survival model and life table are equivalent ways of obtaining the same results.","title":"Life Tables"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#basic-life-table","text":"The life table is constructed based on the mortality of a group of people known as the Cohort . The initial age of the group people is known as the Starting Age , denoted by \\(\\alpha\\) . The initial number of people in the cohort is known as the Radix , which is usually a large round number (EG. 1000) Conversely, the maximum age is known as the Terminal Age , denoted by \\(\\omega\\) . Everybody in the cohort is expected to gradually die by the terminal age . The number of people alive from the cohort at age \\(x\\) is denoted as \\(\\ell_x\\) : \\[ \\begin{aligned} \\ell_0 &= 1000 \\\\ \\ell_{\\omega} &= 0 \\\\ \\\\ \\therefore \\ell_{x+t} &< \\ell_{x} \\end{aligned} \\] From this basic life table, several important values can be calculated: The expected number of deaths over a period, \\({}_{n}d_{x}\\) The probability of survival past a period, \\({}_{n}p_{x}\\) The probability of death within a period, \\({}_{n}q_{x}\\) The deferred probability of death within a period, \\({}_{s|t}q_{x}\\) \\[ \\begin{aligned} {}_{n}d_{x} &= \\ell_{x} - \\ell_{x+n} \\\\ {}_{n}p_{x} &= \\frac{\\ell_{x+t}}{\\ell_{x}} \\\\ {}_{n}q_{x} &= \\frac{{}_{n}d_{x}}{\\ell_{x}} \\\\ {}_{s|t}q_{x} &= \\frac{{}_{t}d_{x+s}}{l_{x}} \\\\ \\end{aligned} \\] Thus, it can be seen that the probabilities are simply the proportion of people who died/survived over the a given period.","title":"Basic Life Table"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#fractional-age-assumptions","text":"One limitation of the life table is that it is computed at discrete ages while many problems require computations for non-discrete ages. Thus, several assumptions about the life table that allows non-discrete values to be interpolated from the discrete ones, known as Fractional Age Assumptions .","title":"Fractional Age Assumptions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#uniform-distribution-of-deaths","text":"The Uniform Distribution of Deaths (UDD) is an assumption that allows for Linear Interpolation between discrete ages. As its name suggests, it assumes that there is a uniform distribution of deaths within each year of age, such that the survival function decreases uniformly, resulting in a linear survival function between ages. Let the fractional age be \\(s\\) . Thus, the probability using fractional ages is simply the weighted average of the discrete points: \\[ \\begin{aligned} {}_{s}p_{x} &= (1-s) {}_{0}p_{x} + s {}_{1}p_{x} \\\\ &= (1-s)\\frac{\\ell_{x}}{\\ell_{x}} + s \\frac{\\ell_{x+1}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x}-s\\ell_{x} + s\\ell_{x+1}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x} - s(\\ell_{x}-\\ell_{x+1})}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x} - s d_{x}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x}}{\\ell_{x}} - s\\frac{d_{x}}{\\ell_{x}} \\\\ &= 1 - s q_{x} \\\\ \\\\ \\therefore {}_{s}q_{x} &= s \\cdot q_x \\end{aligned} \\]","title":"Uniform Distribution of Deaths"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#constant-force-of-mortality","text":"The Constant Force of Mortality is an assumption that allows for Exponential Interpolation between discrete ages.","title":"Constant Force of Mortality"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#select-ultimate-mortality","text":"Mortality rates for the general population versus individuals who buy life insurance tend to be different. Generally speaking, people who purchase insurance tend to be richer and thus have better mortality rates than the general population . Within the individuals who purchase life insurance, those who have recently purchased a policy tend to have better mortality . This is because these individuals would have gone through Medical Underwriting and thus is expected to be in better health. The extent of the better mortality decreases over time and after a few years, they should experience the same mortality as the rest of the individuals who purchased life insurance. The duration of time is dependent on the rigorousness of the underwriting process . Formally speaking, the individuals who purchased life insurance were selected by the underwriting process and thus the better mortality experienced is known as Select Mortality . Similarly, the time whereby the select mortality is better is known as the Select Period . The select mortality ultimately converges with the non-select group, known as the Ultimate Mortality . In actuarial notation, subscript \\([x]\\) is used to distinguish select mortality from ultimate mortality, where the select period is denoted as \\(d\\) : \\[ \\begin{aligned} \\begin{cases} q_{[x]+t} < q_{x+t},& t < d \\\\ q_{[x]+t} = q_{x+t},& t \\ge d \\end{cases} \\end{aligned} \\]","title":"Select &amp; Ultimate Mortality"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/","text":"Life Assurance Functions Life Assurances are contracts that promise to pay out a benefit if the insured event occurs in the future . The value of a life assurance must reflect these two aspects: Uncertainty of cashflows - Expected Value , based on Survival Models Time value of money - Present Value , based on Interest Theory Thus, the value of a life assurance is the Expected Present Value (EPV) of the promised cashflows. Payable Discretely The simplest form of life assurance pays the benefits at the end of the year that the insured event occurs. For simplicity, the benefit of the contract is assumed to be 1. While not common in practice, it provides a simple framework to understand the core concepts. For an assurance payable discretely, \\(K_x\\) is used as the survival model. Thus, the present value of the benefit payable in each period is \\(v^{K_x + 1}\\) . The EPV of the contract is the sum product of the PV of the benefit in each period and the probability of death in that period: \\[ \\begin{aligned} EPV &= \\sum v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\\\ &= \\sum v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Actuarial Notation Similar to the survival model, given how often these values are calculated, they are abbreviated using the International Actuarial Notation as well. \\(A\\) represents the first moment (expectation) of the present value of a contract where a benefit of 1 is payable discretely when the status fails . The subscript \\(x\\) is the age of the policyholder, known as the Life Status . It fails when the policyholder dies . An additional subscript \\(:\\enclose{actuarial}{n}\\) is the duration that the assurance remains valid, known as the Duration Status . It fails if the policyholder survives past the policyterm. A \\(1\\) above the status indicates that the assurance pays only pays if that particular status fails first . If nothing is indicated, then the assurance pays whichever status fails first. If a particular status is omitted, then the assurance is not dependent on that status. For instance, omitting the duration status means that the contract is only dependent on the age of the policyholder. An assurance that begins \\(n\\) years later is known as a Deferred Assurance , which is denoted by \\({}_{n|}A\\) , following the same notation as deferred probabilities. Thus, putting everything together, the EPV of each assurance can be denoted as follows: Whole Life Assurance - Payable whenever policyholder dies; \\(A_x\\) Term Assurance - Payable only if policyholder dies during assurance period; \\(A^{1}_{x:\\enclose{actuarial}{n}}\\) Pure Endowment - Payable only if policyholder survives past assurance period; \\(A^{\\>\\>\\>\\> 1}_{x:\\enclose{actuarial}{n}}\\) Endowment Assurance - Payable whichever of the above two occur first; \\(A_{x:\\enclose{actuarial}{n}}\\) Deferred Whole Life Assurance - Payable only if policyholder dies after \\(n\\) years; \\({}_{n|}A_x\\) Pure Endowments can also be expressed as \\({}_{n}E_x\\) for easier typesetting. Whole Life Assurance Whole Life Assurances cover the insured indefinitely and thus will pay out whenever the insured dies. Let WL be the random variable denoting the PV of the death benefit: \\[ \\begin{aligned} \\text{WL} &= v^{K_x + 1} \\\\ K_x &= 0, 1, \\dots, \\infty \\end{aligned} \\] Technically speaking, the upper limit of \\(K_x\\) should \\(\\omega-x\\) as it represents the maximum attainable age in the discrete survival model. However, since the \\({}_{k}p_{x} = 0\\) for all \\(k \\ge \\omega-x\\) , it does not matter what the upper limit is as long as it is larger than \\(\\omega-x\\) . Thus, \\(\\infty\\) is used for conciseness instead. The EPV is the Expectation/First Moment of the WL random variable: \\[ \\begin{aligned} E(\\text{WL}) &= \\sum^\\infty_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ A_{x} &= \\sum^\\infty_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Another commonly used metric is the Variance of the contract. In order to get it, the Second Moment must first be determined: \\[ \\begin{aligned} E({\\text{WL}}^2) &= \\sum^\\infty_{K_x = 0} \\left(v^{K_x + 1}\\right)^2 \\cdot {}_{K_x|}q_{x} \\\\ &= \\sum^\\infty_{K_x = 0} \\left((v^2)^{K_x + 1}\\right) \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Notice that the expression can be simplified to a form that looks almost identical to the first moment - with the only difference being that is uses \\(v^2\\) instead of \\(v\\) . Thus, the second moment is simply the first moment evaluated at a higher interest rate \\(i^*=(1+i)^2-1\\) , such that \\(v^* = v^2\\) . Generally, the k-th moment is denoted as \\({}^{k}A_x\\) , where the \\(k\\) is the multiplier on the interest rate used to evaluate the moment, \\(i^*=(1+i)^k-1\\) . The Second Moment and hence Variance can be shown as: \\[ \\begin{aligned} E({\\text{WL}}^2) &= \\sum^\\infty_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ {}^{2} A_{x} &= \\sum^\\infty_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ \\\\ \\therefore Var(\\text{WL}) &= {}^{2} A_{x} - (A_{x})^2 \\end{aligned} \\] Term Assurance Term Assurance covers the insured for a specified period \\(n\\) and only pays out if the insured dies within that period. NOTHING is paid out if the insured survives beyond that. A WL Assurance can be thought of as a special Term Assurance with infinite coverage. Let TA be the random variable denoting the PV of the death benefit: \\[ \\begin{aligned} \\text{TA} &= \\begin{cases} v^{K_x + 1} ,& K_x = 0, 1, \\dots, n-1 \\\\ 0 ,& K_x \\ge n \\end{cases} \\\\ &= {}_{\\{K_x \\le n-1\\}} v^{K_x+1} \\end{aligned} \\] \\(\\{K_x \\le n-1\\}\\) is known as an Indicator Function , which is a binary variable that takes 1 if the condition is true and 0 if the condition if false . It provides a concise way to express a piecewise function in a single expression. The EPV is the expectation of the TA random variable: \\[ \\begin{aligned} E(\\text{TA}) &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + 0 \\cdot {}_{n}p_{x} \\\\ A^1_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Similar as before, the second moment and thus Variance can be calculated as the following: \\[ \\begin{aligned} E({\\text{TA}}^2) &= \\sum^{n-1}_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} + 0^2 \\cdot {}_{n}p_{x} \\\\ {}^{2} A^1_{x:\\enclose{actuarial}{n}} &= \\sum^\\infty_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ \\\\ \\therefore Var(\\text{TA}) &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} - (A^1_{x:\\enclose{actuarial}{n}})^2 \\end{aligned} \\] Pure Endowment Pure Endowments are a special kind of contract that instead only pays out if the insured survives past a specified period \\(n\\) . NOTHING is paid out if the insured dies before that. Let PE be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{PE} &= \\begin{cases} 0 ,& K_x = 0, 1, 2 \\dots, n \\\\ v^n ,& K_x \\ge n \\end{cases} \\end{aligned} \\] The EPV is the expectation of the PE random variable. However, note that the probability of surviving past the period is given by a single probability \\({}_{n}p_{x}\\) : \\[ \\begin{aligned} E(\\text{PE}) &= 0 \\cdot {}_{n}q_{x} + v^n {}_{n}p_{x} \\\\ {}_{n}E_x &= v^n {}_{n}p_{x} \\end{aligned} \\] Similar as before, the second moment and thus Variance can be calculated: \\[ \\begin{aligned} E({\\text{PE}}^2) &= 0^2 \\cdot {}_{n}q_{x} + (v^2)^n {}_{n}p_{x} \\\\ {}^{2}_{n}E_x &= (v^*)^n {}_{n}p_{x} \\\\ \\\\ \\therefore Var(\\text{PE}) &= {}^{2}_{n}E_x - ({}_{n}E_x)^2 \\end{aligned} \\] Endowment Assurance Endowment Assurances are a combination of term assurances and pure endowments: Term Assurance - Pays out if the insured dies within the period Pure Endowment - Pays out if the insured survives past the period Thus, endowment assurances WILL pay out no matter what Let EA be the random variable denoting the PV of the benefits: \\[ \\begin{aligned} \\text{EA} &= \\begin{cases} v^{K_x + 1} ,& K_x = 0, 1, \\dots, n-1 \\\\ v^n ,& K_x \\ge n \\end{cases} \\\\ &= v^{min(K_x + 1, n)} \\end{aligned} \\] The EPV is the expectation of the EA random variable: \\[ \\begin{aligned} E(\\text{EA}) &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n {}_{n}p_{x} \\\\ A_{x:\\enclose{actuarial}{n}} &= A^1_{x:\\enclose{actuarial}{n}} + {}_{n}E_{x} \\end{aligned} \\] Note that the in the final year of the contract, the assurance will pay \\(v^n\\) regardless of the outcome - TA pays if they die while PE pays if they survive. Thus, a simplification can be made to the EPV: \\[ \\begin{aligned} A_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n {}_{n}p_{x} \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} + v^n {}_{n-1}p_{x} {}_{}q_{x + n - 1} + v^n {}_{n}p_{x} \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n ({}_{n-1}p_{x} {}_{}q_{x + n - 1} + {}_{n}p_{x}) \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n {}_{n-1}p_{x} \\end{aligned} \\] Similar as before, the second moment and thus Variance can be calculated: \\[ \\begin{aligned} E(\\text{EA}^2) &= \\sum^{n-1}_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} + (v^2)^n {}_{n}p_{x} \\\\ {}^{2} A_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} + (v^*)^n {}_{n}p_{x} \\\\ \\\\ \\therefore Var(\\text{EA}) &= {}^{2} A_{x:\\enclose{actuarial}{n}} - \\left(A_{x:\\enclose{actuarial}{n}}\\right)^2 \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left(A^1_{x:\\enclose{actuarial}{n}} - ({}_{n}E_x) \\right)^2 \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left[(A^1_{x:\\enclose{actuarial}{n}})^2 + 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) + ({}_{n}E_x)^2 \\right] \\end{aligned} \\] The same outcome can be reached using a slightly different approach: \\[ \\begin{aligned} Var(EA) &= Var(\\text{TA} + \\text{PE}) \\\\ &= Var(\\text{TA}) + Var(\\text{PE}) + 2 Cov(\\text{TA}, \\text{PE}) \\\\ &= Var(\\text{TA}) + Var(\\text{PE}) + 2 [E(\\text{TA} \\cdot \\text{PE}) - E(\\text{TA}) \\cdot E(\\text{PE})] \\end{aligned} \\] Consider the distribution of TA and PE: \\[ \\begin{aligned} TA \\cdot PE &= \\begin{cases} v^{K_x + 1} \\cdot 0, K_x \\lt n \\\\ 0 \\cdot v^n, K_x \\ge n \\end{cases} \\\\ &= \\begin{cases} 0, K_x \\lt n \\\\ 0, K_x \\ge n \\end{cases} \\\\ \\\\ \\therefore E(\\text{TA} \\cdot \\text{PE}) &= 0 \\cdot {}_{n}q_x + 0 \\cdot {}_{n}p_x \\\\ &= 0 \\end{aligned} \\] This results in the same variance as before: \\[ \\begin{aligned} Var(EA) &= Var(\\text{TA}) + Var(\\text{PE}) + 2 [E(\\text{TA} \\cdot \\text{PE}) - E(\\text{TA}) \\cdot E(\\text{PE})] \\\\ &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} - (A^1_{x:\\enclose{actuarial}{n}})^2 + {}^{2}_{n}E_x - ({}_{n}E_x)^2 - 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left[(A^1_{x:\\enclose{actuarial}{n}})^2 + 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) + ({}_{n}E_x)^2 \\right] \\\\ \\end{aligned} \\] While this process might seem redundant, it provides an easy to understand example of how the variance of a combination of assurances is derived by considering the covariance. Deferred Assurances Deferred Assurances are variations of any of the above assurances, where the assurance starts \\(n\\) years later rather than immediately. While any assurance can be deferred, the two most common are the Deferred Whole Life and Deferred Term Assurances . Deferred Whole Life Assurance Let DWL be the random variable denoting the PV of the benefits of a deferred whole life assurance: \\[ \\begin{aligned} \\text{DWL} &= \\begin{cases} 0,& K_x = 0, 1, 2, \\dots, n-1 \\\\ v^{K_x + 1} ,& K_x \\ge n \\end{cases} \\\\ &= {}_{\\{K_x \\ge n\\}} v^{K_x + 1} \\end{aligned} \\] The EPV is the expectation of the EA random variable: \\[ \\begin{aligned} E(\\text{DWL}) &= 0 \\cdot {}_{n}p_{x} + \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ {}_{n|} A_{x} &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} - \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ &= A_x - A^1_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] Since a DWL is simply a WL assurance issued \\(n\\) years later, the EPV of the DWL is equivalent to the EPV of a WL issued at age \\(x+n\\) (after adjusting for interest AND survival): \\[ \\begin{aligned} {}_{n|} A_{x} &= \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ &= v^n {}_{n} p_{x} \\sum^{\\infty}_{K_x = n} v^{K_x - n + 1} \\cdot {}_{K_x-n|}q_{x+n} \\\\ &= v^n {}_{n} p_{x} \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x+n} \\\\ &= {}_{n}E_{x} \\cdot A_{x+n} \\\\ \\end{aligned} \\] PEs can be use as a discount factor for functions that takes mortality into consideration . If provided by the life table, this reduces the need for computation. When going \"back\" in time, it must reflect that the policyholder will eventually survive till the current age, which is why the probability of surviving the period must be multiplied. This allows a TA to be expressed as the difference of two WL assurances issued at different times : \\[ \\begin{aligned} {}_{n|} A_{x} &= {}_{n}E_{x} * A_{x+n} \\\\ A_x - A^1_{x:\\enclose{actuarial}{n}} &= {}_{n}E_{x} * A_{x+n} \\\\ A^1_{x:\\enclose{actuarial}{n}} &= A_{x} - {}_{n}E_{x} * A_{x+n} \\\\ \\end{aligned} \\] \\[ \\begin{aligned} Var (DWL) &= {}^{2}_{n|}A_x - ({}_{n|}A_x)^2 \\\\ &= \\left({}^{2}A_x - {}^{2}A^1_{x:\\enclose{actuarial}{n}}\\right) - \\left(A_x - A^1_{x:\\enclose{actuarial}{n}}\\right)^2 \\\\ &= \\left({}^{2}A_x - {}^{2}A^1_{x:\\enclose{actuarial}{n}}\\right) + \\left[(A_x)^2 - 2(A_x)(A^1_{x:\\enclose{actuarial}{n}}) + (A^1_{x:\\enclose{actuarial}{n}})^2 \\right] \\end{aligned} \\] Deferred Term Assurance Let DTA be the random variable denoting the PV of the benefits of a deferred term assurance. Note that most of the properties of the DWL apply to DTA as well: Difference of two terms - \\({}_{n|} A^1_{x:\\enclose{actuarial}{n}} = A^1_{x:\\enclose{actuarial}{n+k}} - A^1_{x:\\enclose{actuarial}{n}}\\) A WL assurance can be thought of as the infinite sum of a series of deferred term assurances, each deferred one year more than the previous : \\[ \\begin{aligned} E(\\text{WL}) &= E(\\text{DTA}_0) + E(\\text{DTA}_1) + E(\\text{DTA}_2) + E(\\text{DTA}_3) + \\dots + E(\\text{DTA}_\\infty) \\\\ &= \\sum^\\infty_{n = 1} \\\\ \\end{aligned} \\] Recursions The EPV of each contract can also be expressed through backwards recursion , where it is calculated as a function of itself. Consider the WL random variable: If the policyholder dies in the year with probability \\({}_{}q_{x}\\) , then a benefit of 1 is paid at the end of the year. If the policyholder survives past the year with probability \\({}_{}p_{x}\\) , then the policyholder will die in some future year. The PV of the benefit at the end of the year is given is the EPV of the same contract but at that future time ; \\(A_{x+1}\\) . \\[ \\begin{aligned} WL &= \\begin{cases} v \\cdot 1 ,& {}_{}q_{x} \\\\ v \\cdot A_{x+1} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore A_x = v{}_{}q_{x} + v{}_{}p_{x}A_{x+1} \\] The same exercise can be shown for the TA random variable. The main difference is understanding how the notation changed: \\(x+1\\) reflects the new age of the policyholder (same as WL) \\(n-1\\) reflects that one year of coverage has passed (not applicable for WL) \\[ \\begin{aligned} TA &= \\begin{cases} v \\cdot 1 ,& {}_{}q_{x} \\\\ v \\cdot A^1_{x+1:\\enclose{actuarial}{n-1}} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore A^1_{x:\\enclose{actuarial}{n-1}} = v{}_{}q_{x} + v{}_{}p_{x}A^1_{x+1:\\enclose{actuarial}{n-1}} \\] The PE variable is similar, with the main difference being that the policyholder will receive nothing if the policyholder dies . Thus, only the second component of the recursion remains: \\[ \\begin{aligned} PE &= \\begin{cases} 0 ,& {}_{}q_{x} \\\\ v \\cdot {}_{n-1}E_{x+1} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore {}_{n}E_{x} = v{}_{}p_{x} \\cdot {}_{n-1}E_{x+1} \\] EA is omitted from this section as it is simply the combination of a TA and PE. These identities are most useful in a spreadsheet setting where the calculations can be easily repeated to fill up an entire life table. However, even then it is necessary to have a starting point for the recusions to occur. The most common starting point is the terminal age as the EPVs can be intuitively determined since the policyholder will inevitably die at the end of the year: \\[ \\begin{aligned} A_{\\omega-1} &= v \\\\ A^{\\> \\> 1}_{\\omega-1:\\enclose{actuarial}{n}} &= v \\\\ {}_{n}E_{\\omega-1} &= 0 \\\\ A^{1}_{\\omega-1:\\enclose{actuarial}{n}} &= v \\end{aligned} \\] Intuitions Although the exam questions are mostly computational, it is good to have an understanding of how the different EPVs compare against one another to serve as a sort of \"sense check\" to determine if the answer is in the right direction. Same assurance comparisons Recall that the probability of death is an increasing function with age. The death benefit is more likely to be paid out to an older policyholder - in other words, they receive the death benefit \"sooner\" than a younger policyholder. Thus, an older policyholder has larger expected cashflows that are discounted less (due to receiving it sooner), which results in a higher EPV than a younger policyholder, all else equal: \\[ \\begin{aligned} A_{x+k} & \\gt A_{x} \\\\ A^{\\> \\> 1} _{x+k:\\enclose{actuarial}{n}} & \\gt A^{\\> \\> 1}_{x+k:\\enclose{actuarial}{n}} \\end{aligned} \\] Conversely, the probability of survival is a decreasing function with age. The survival benefit is less likely to be paid out to an older policyholder - smaller expected cashflows. Regardless of the age of the policyholder, the survival benefit is paid at the same time ( same discounting ). Thus, since an older policyholder has smaller expected cashflows , it has a lower EPV than a younger policyholder: \\[ {}_{n}E_{x+k} \\le {}_{n}E_{x} \\] Endowment Assurances have both a death and survival component , thus the comparison is a combination of the two: A younger policyholder is more likely to survive and receive the survival benefit at the end of the term (discounted more) An older policyholder is more likely to die and receive the death benefit during the term (discounted less) Assuming that the difference in expected cashflows are negligible , then an older policyholder would have an higher EPV due to the lower discounting : \\[ A_{x+k:\\enclose{actuarial}{n}} \\gt A_{x:\\enclose{actuarial}{n}} \\] Naturally, all else equal, assurances with a lower interest rate are discounted less and thus have a higher EPV . This is particularly useful for questions where the SULT cannot be directly used, but can still be used as a sense check for the answers. Different assurance comparisons At a young age where the probability of death is low, all else equal (where applicable), the EPVs of each assurance rank as follows: TA will have the smallest EPV . Although their benefits are paid out sooner, the expected benefits are small as the probability of death is small. WL has the next largest EPV . They have the same benefits as term in the short run, but have large expected benefits in the future . However, these large benefits are heavily discounted , still resuling in a small EPV. PE has the next largest EPV . Given the high probability of survival, the expected benefits are large . EA has the largest EPV . Since it is a combination of TA and PE, it is naturally the highest. \\[ \\begin{aligned} A^{1}_{30:\\enclose{actuarial}{n}} < A_{30} < {}_{n}E_{30} < A_{30:\\enclose{actuarial}{n}} \\\\ \\end{aligned} \\] At an old age where the probability of death is high, all else equal (where applicable), the EPVs of each assurance rank as follows: PE will have the smallest EPV . Given the low probability of survival, the expected benefits are are small . TA will have the next largest EPV . Given the high probability of death, the expected benefits are high . EA will have the next largest EPV . Since it is combination of TA and PE, it is naturally higher than both of them. WL has the largest EPV . Given the inevitable death of the policyholder, the expected benefits are the highest . \\[ \\begin{aligned} {}_{n}E_{100} < A^{1}_{100:\\enclose{actuarial}{n}} < A_{100:\\enclose{actuarial}{n}} < A_{100} \\\\ \\end{aligned} \\] As the policyholder approaches the terminal age, the EPVs tend to one another: \\[ \\begin{aligned} x &\\to \\omega \\\\ E(\\text{PE}) &\\to 0 \\\\ E(\\text{EA}) &\\to E(\\text{TA}) \\\\ E(\\text{TA}) &\\to E(\\text{WL}) \\end{aligned} \\] TA tends to WL whenever the end of the term exceeds the terminal age - thus the cashflows and hence EPV for both assurances become identical. Payable Continuously Continuous Whole Life Continuous Term Continuous Endowments NO SUCH THING AS CONTINUOUS PURE ENDOWMENTS Discrete to Continuous Approximation Uniform Death Distribution Alternatively, it can be assumed that people die with constant probability within a year . \\[ \\begin{aligned} f_{T_x} &= S_x(t)\\mu_{x+t} & \\end{aligned} \\] Can think of Whole life as a series of terms Uniform distribution each year \\[ \\begin{aligned} \\int^1_0 v^t {}_{t}p_{x+k} \\mu_{x+k+t} &\\approx \\int^1_0 v^t {}_{}q_{x+k} &\\approx {}_{}q_{x+k} \\int^1_0 e^{\\deltat} \\end{aligned} \\] \\[ \\begin{aligned} \\bar{A}_x &= {}_{0|}\\bar{A}^1_{x+1:\\enclose{actuarial}{1}} + {}_{1|}\\bar{A}^1_{x+1:\\enclose{actuarial}{1}} + {}_{2|}\\bar{A}^1_{x+1:\\enclose{actuarial}{1}} + \\dots \\\\ &= q_x \\frac{iv}{\\delta} + vp_xq_{x+1} \\frac{iv}{\\delta} + v^2 {}_{2}p_{x} q_{x+2} \\frac{iv}{\\delta} + \\dots \\\\ &= \\frac{i}{\\delta} (q_x + v^1 p_x {}_{}q_{x+1} + v^2 {}_{2}p_{x} {}_{}q_{x+2} + \\dots) \\\\ &= \\frac{i}{\\delta} A_x \\\\ \\end{aligned} \\] Claims Acceleration usually paid at \\(K_x + \\frac{1}{m}\\) Assume that deaths occur evenly thus should occur in \\(K_x + \\frac{m+1}{2m}\\) Since this is \\(\\frac{m-1}{2m}\\) earlier than the discrete case the discount factor for the discrete needs to be adjusted by that amount to be the same as the number of payments increase \\(m \\to \\infty\\) then the adjustment tends to \\(\\frac{1}{2}\\) Benefits that are paid out immediately versus at the end of the period are significantly different as the continuous benefits are discounted more . However, it can be assumed that on average , all the deaths in the year occur in the middle of the year - \\(x+k+\\frac{1}{2}\\) . The PV of the benefits are discounted using multiples of \\(v^{\\frac{1}{2}}\\) instead: Applying this to a continuous contract, it can be shown that it's EPV is is a function of the EPV of a discrete contract. This is known as the Claims Acceleration Approach . \\[ \\begin{aligned} \\bar{A}_x &\\approx v^{\\frac{1}{2}} q_x + v^{\\frac{3}{2}} p_x {}_{}q_{x+1} + v^{\\frac{5}{2}} {}_{2}p_{x} {}_{}q_{x+2} + \\dots \\\\ &\\approx (1+i)^{\\frac{1}{2}} (vq_x + v^1 p_x {}_{}q_{x+1} + v^2 {}_{2}p_{x} {}_{}q_{x+2} + \\dots) \\\\ &\\approx (1+i)^{\\frac{1}{2}} A_x \\end{aligned} \\] The method applies a higher discount rate to the discrete contract, to account for the \"under-discounting\" of the discrete method. While it is not is not perfect, it is more reflective of actual experience than the discrete method. NOTE THAT WHEN CALCULATING THE SECOND MOMENT USING UDD, EVEN THE I/DELTA TERM MUST REFLECT THE NEW INTEREST RATE Clamims Acceleration second moment also Variance UDD be careful","title":"Life Insurance Functions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#life-assurance-functions","text":"Life Assurances are contracts that promise to pay out a benefit if the insured event occurs in the future . The value of a life assurance must reflect these two aspects: Uncertainty of cashflows - Expected Value , based on Survival Models Time value of money - Present Value , based on Interest Theory Thus, the value of a life assurance is the Expected Present Value (EPV) of the promised cashflows.","title":"Life Assurance Functions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#payable-discretely","text":"The simplest form of life assurance pays the benefits at the end of the year that the insured event occurs. For simplicity, the benefit of the contract is assumed to be 1. While not common in practice, it provides a simple framework to understand the core concepts. For an assurance payable discretely, \\(K_x\\) is used as the survival model. Thus, the present value of the benefit payable in each period is \\(v^{K_x + 1}\\) . The EPV of the contract is the sum product of the PV of the benefit in each period and the probability of death in that period: \\[ \\begin{aligned} EPV &= \\sum v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\\\ &= \\sum v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\]","title":"Payable Discretely"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#actuarial-notation","text":"Similar to the survival model, given how often these values are calculated, they are abbreviated using the International Actuarial Notation as well. \\(A\\) represents the first moment (expectation) of the present value of a contract where a benefit of 1 is payable discretely when the status fails . The subscript \\(x\\) is the age of the policyholder, known as the Life Status . It fails when the policyholder dies . An additional subscript \\(:\\enclose{actuarial}{n}\\) is the duration that the assurance remains valid, known as the Duration Status . It fails if the policyholder survives past the policyterm. A \\(1\\) above the status indicates that the assurance pays only pays if that particular status fails first . If nothing is indicated, then the assurance pays whichever status fails first. If a particular status is omitted, then the assurance is not dependent on that status. For instance, omitting the duration status means that the contract is only dependent on the age of the policyholder. An assurance that begins \\(n\\) years later is known as a Deferred Assurance , which is denoted by \\({}_{n|}A\\) , following the same notation as deferred probabilities. Thus, putting everything together, the EPV of each assurance can be denoted as follows: Whole Life Assurance - Payable whenever policyholder dies; \\(A_x\\) Term Assurance - Payable only if policyholder dies during assurance period; \\(A^{1}_{x:\\enclose{actuarial}{n}}\\) Pure Endowment - Payable only if policyholder survives past assurance period; \\(A^{\\>\\>\\>\\> 1}_{x:\\enclose{actuarial}{n}}\\) Endowment Assurance - Payable whichever of the above two occur first; \\(A_{x:\\enclose{actuarial}{n}}\\) Deferred Whole Life Assurance - Payable only if policyholder dies after \\(n\\) years; \\({}_{n|}A_x\\) Pure Endowments can also be expressed as \\({}_{n}E_x\\) for easier typesetting.","title":"Actuarial Notation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#whole-life-assurance","text":"Whole Life Assurances cover the insured indefinitely and thus will pay out whenever the insured dies. Let WL be the random variable denoting the PV of the death benefit: \\[ \\begin{aligned} \\text{WL} &= v^{K_x + 1} \\\\ K_x &= 0, 1, \\dots, \\infty \\end{aligned} \\] Technically speaking, the upper limit of \\(K_x\\) should \\(\\omega-x\\) as it represents the maximum attainable age in the discrete survival model. However, since the \\({}_{k}p_{x} = 0\\) for all \\(k \\ge \\omega-x\\) , it does not matter what the upper limit is as long as it is larger than \\(\\omega-x\\) . Thus, \\(\\infty\\) is used for conciseness instead. The EPV is the Expectation/First Moment of the WL random variable: \\[ \\begin{aligned} E(\\text{WL}) &= \\sum^\\infty_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ A_{x} &= \\sum^\\infty_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Another commonly used metric is the Variance of the contract. In order to get it, the Second Moment must first be determined: \\[ \\begin{aligned} E({\\text{WL}}^2) &= \\sum^\\infty_{K_x = 0} \\left(v^{K_x + 1}\\right)^2 \\cdot {}_{K_x|}q_{x} \\\\ &= \\sum^\\infty_{K_x = 0} \\left((v^2)^{K_x + 1}\\right) \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Notice that the expression can be simplified to a form that looks almost identical to the first moment - with the only difference being that is uses \\(v^2\\) instead of \\(v\\) . Thus, the second moment is simply the first moment evaluated at a higher interest rate \\(i^*=(1+i)^2-1\\) , such that \\(v^* = v^2\\) . Generally, the k-th moment is denoted as \\({}^{k}A_x\\) , where the \\(k\\) is the multiplier on the interest rate used to evaluate the moment, \\(i^*=(1+i)^k-1\\) . The Second Moment and hence Variance can be shown as: \\[ \\begin{aligned} E({\\text{WL}}^2) &= \\sum^\\infty_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ {}^{2} A_{x} &= \\sum^\\infty_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ \\\\ \\therefore Var(\\text{WL}) &= {}^{2} A_{x} - (A_{x})^2 \\end{aligned} \\]","title":"Whole Life Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#term-assurance","text":"Term Assurance covers the insured for a specified period \\(n\\) and only pays out if the insured dies within that period. NOTHING is paid out if the insured survives beyond that. A WL Assurance can be thought of as a special Term Assurance with infinite coverage. Let TA be the random variable denoting the PV of the death benefit: \\[ \\begin{aligned} \\text{TA} &= \\begin{cases} v^{K_x + 1} ,& K_x = 0, 1, \\dots, n-1 \\\\ 0 ,& K_x \\ge n \\end{cases} \\\\ &= {}_{\\{K_x \\le n-1\\}} v^{K_x+1} \\end{aligned} \\] \\(\\{K_x \\le n-1\\}\\) is known as an Indicator Function , which is a binary variable that takes 1 if the condition is true and 0 if the condition if false . It provides a concise way to express a piecewise function in a single expression. The EPV is the expectation of the TA random variable: \\[ \\begin{aligned} E(\\text{TA}) &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + 0 \\cdot {}_{n}p_{x} \\\\ A^1_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Similar as before, the second moment and thus Variance can be calculated as the following: \\[ \\begin{aligned} E({\\text{TA}}^2) &= \\sum^{n-1}_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} + 0^2 \\cdot {}_{n}p_{x} \\\\ {}^{2} A^1_{x:\\enclose{actuarial}{n}} &= \\sum^\\infty_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ \\\\ \\therefore Var(\\text{TA}) &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} - (A^1_{x:\\enclose{actuarial}{n}})^2 \\end{aligned} \\]","title":"Term Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#pure-endowment","text":"Pure Endowments are a special kind of contract that instead only pays out if the insured survives past a specified period \\(n\\) . NOTHING is paid out if the insured dies before that. Let PE be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{PE} &= \\begin{cases} 0 ,& K_x = 0, 1, 2 \\dots, n \\\\ v^n ,& K_x \\ge n \\end{cases} \\end{aligned} \\] The EPV is the expectation of the PE random variable. However, note that the probability of surviving past the period is given by a single probability \\({}_{n}p_{x}\\) : \\[ \\begin{aligned} E(\\text{PE}) &= 0 \\cdot {}_{n}q_{x} + v^n {}_{n}p_{x} \\\\ {}_{n}E_x &= v^n {}_{n}p_{x} \\end{aligned} \\] Similar as before, the second moment and thus Variance can be calculated: \\[ \\begin{aligned} E({\\text{PE}}^2) &= 0^2 \\cdot {}_{n}q_{x} + (v^2)^n {}_{n}p_{x} \\\\ {}^{2}_{n}E_x &= (v^*)^n {}_{n}p_{x} \\\\ \\\\ \\therefore Var(\\text{PE}) &= {}^{2}_{n}E_x - ({}_{n}E_x)^2 \\end{aligned} \\]","title":"Pure Endowment"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#endowment-assurance","text":"Endowment Assurances are a combination of term assurances and pure endowments: Term Assurance - Pays out if the insured dies within the period Pure Endowment - Pays out if the insured survives past the period Thus, endowment assurances WILL pay out no matter what Let EA be the random variable denoting the PV of the benefits: \\[ \\begin{aligned} \\text{EA} &= \\begin{cases} v^{K_x + 1} ,& K_x = 0, 1, \\dots, n-1 \\\\ v^n ,& K_x \\ge n \\end{cases} \\\\ &= v^{min(K_x + 1, n)} \\end{aligned} \\] The EPV is the expectation of the EA random variable: \\[ \\begin{aligned} E(\\text{EA}) &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n {}_{n}p_{x} \\\\ A_{x:\\enclose{actuarial}{n}} &= A^1_{x:\\enclose{actuarial}{n}} + {}_{n}E_{x} \\end{aligned} \\] Note that the in the final year of the contract, the assurance will pay \\(v^n\\) regardless of the outcome - TA pays if they die while PE pays if they survive. Thus, a simplification can be made to the EPV: \\[ \\begin{aligned} A_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n {}_{n}p_{x} \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} + v^n {}_{n-1}p_{x} {}_{}q_{x + n - 1} + v^n {}_{n}p_{x} \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n ({}_{n-1}p_{x} {}_{}q_{x + n - 1} + {}_{n}p_{x}) \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n {}_{n-1}p_{x} \\end{aligned} \\] Similar as before, the second moment and thus Variance can be calculated: \\[ \\begin{aligned} E(\\text{EA}^2) &= \\sum^{n-1}_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} + (v^2)^n {}_{n}p_{x} \\\\ {}^{2} A_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} + (v^*)^n {}_{n}p_{x} \\\\ \\\\ \\therefore Var(\\text{EA}) &= {}^{2} A_{x:\\enclose{actuarial}{n}} - \\left(A_{x:\\enclose{actuarial}{n}}\\right)^2 \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left(A^1_{x:\\enclose{actuarial}{n}} - ({}_{n}E_x) \\right)^2 \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left[(A^1_{x:\\enclose{actuarial}{n}})^2 + 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) + ({}_{n}E_x)^2 \\right] \\end{aligned} \\] The same outcome can be reached using a slightly different approach: \\[ \\begin{aligned} Var(EA) &= Var(\\text{TA} + \\text{PE}) \\\\ &= Var(\\text{TA}) + Var(\\text{PE}) + 2 Cov(\\text{TA}, \\text{PE}) \\\\ &= Var(\\text{TA}) + Var(\\text{PE}) + 2 [E(\\text{TA} \\cdot \\text{PE}) - E(\\text{TA}) \\cdot E(\\text{PE})] \\end{aligned} \\] Consider the distribution of TA and PE: \\[ \\begin{aligned} TA \\cdot PE &= \\begin{cases} v^{K_x + 1} \\cdot 0, K_x \\lt n \\\\ 0 \\cdot v^n, K_x \\ge n \\end{cases} \\\\ &= \\begin{cases} 0, K_x \\lt n \\\\ 0, K_x \\ge n \\end{cases} \\\\ \\\\ \\therefore E(\\text{TA} \\cdot \\text{PE}) &= 0 \\cdot {}_{n}q_x + 0 \\cdot {}_{n}p_x \\\\ &= 0 \\end{aligned} \\] This results in the same variance as before: \\[ \\begin{aligned} Var(EA) &= Var(\\text{TA}) + Var(\\text{PE}) + 2 [E(\\text{TA} \\cdot \\text{PE}) - E(\\text{TA}) \\cdot E(\\text{PE})] \\\\ &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} - (A^1_{x:\\enclose{actuarial}{n}})^2 + {}^{2}_{n}E_x - ({}_{n}E_x)^2 - 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left[(A^1_{x:\\enclose{actuarial}{n}})^2 + 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) + ({}_{n}E_x)^2 \\right] \\\\ \\end{aligned} \\] While this process might seem redundant, it provides an easy to understand example of how the variance of a combination of assurances is derived by considering the covariance.","title":"Endowment Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#deferred-assurances","text":"Deferred Assurances are variations of any of the above assurances, where the assurance starts \\(n\\) years later rather than immediately. While any assurance can be deferred, the two most common are the Deferred Whole Life and Deferred Term Assurances .","title":"Deferred Assurances"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#deferred-whole-life-assurance","text":"Let DWL be the random variable denoting the PV of the benefits of a deferred whole life assurance: \\[ \\begin{aligned} \\text{DWL} &= \\begin{cases} 0,& K_x = 0, 1, 2, \\dots, n-1 \\\\ v^{K_x + 1} ,& K_x \\ge n \\end{cases} \\\\ &= {}_{\\{K_x \\ge n\\}} v^{K_x + 1} \\end{aligned} \\] The EPV is the expectation of the EA random variable: \\[ \\begin{aligned} E(\\text{DWL}) &= 0 \\cdot {}_{n}p_{x} + \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ {}_{n|} A_{x} &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} - \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ &= A_x - A^1_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] Since a DWL is simply a WL assurance issued \\(n\\) years later, the EPV of the DWL is equivalent to the EPV of a WL issued at age \\(x+n\\) (after adjusting for interest AND survival): \\[ \\begin{aligned} {}_{n|} A_{x} &= \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ &= v^n {}_{n} p_{x} \\sum^{\\infty}_{K_x = n} v^{K_x - n + 1} \\cdot {}_{K_x-n|}q_{x+n} \\\\ &= v^n {}_{n} p_{x} \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x+n} \\\\ &= {}_{n}E_{x} \\cdot A_{x+n} \\\\ \\end{aligned} \\] PEs can be use as a discount factor for functions that takes mortality into consideration . If provided by the life table, this reduces the need for computation. When going \"back\" in time, it must reflect that the policyholder will eventually survive till the current age, which is why the probability of surviving the period must be multiplied. This allows a TA to be expressed as the difference of two WL assurances issued at different times : \\[ \\begin{aligned} {}_{n|} A_{x} &= {}_{n}E_{x} * A_{x+n} \\\\ A_x - A^1_{x:\\enclose{actuarial}{n}} &= {}_{n}E_{x} * A_{x+n} \\\\ A^1_{x:\\enclose{actuarial}{n}} &= A_{x} - {}_{n}E_{x} * A_{x+n} \\\\ \\end{aligned} \\] \\[ \\begin{aligned} Var (DWL) &= {}^{2}_{n|}A_x - ({}_{n|}A_x)^2 \\\\ &= \\left({}^{2}A_x - {}^{2}A^1_{x:\\enclose{actuarial}{n}}\\right) - \\left(A_x - A^1_{x:\\enclose{actuarial}{n}}\\right)^2 \\\\ &= \\left({}^{2}A_x - {}^{2}A^1_{x:\\enclose{actuarial}{n}}\\right) + \\left[(A_x)^2 - 2(A_x)(A^1_{x:\\enclose{actuarial}{n}}) + (A^1_{x:\\enclose{actuarial}{n}})^2 \\right] \\end{aligned} \\]","title":"Deferred Whole Life Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#deferred-term-assurance","text":"Let DTA be the random variable denoting the PV of the benefits of a deferred term assurance. Note that most of the properties of the DWL apply to DTA as well: Difference of two terms - \\({}_{n|} A^1_{x:\\enclose{actuarial}{n}} = A^1_{x:\\enclose{actuarial}{n+k}} - A^1_{x:\\enclose{actuarial}{n}}\\) A WL assurance can be thought of as the infinite sum of a series of deferred term assurances, each deferred one year more than the previous : \\[ \\begin{aligned} E(\\text{WL}) &= E(\\text{DTA}_0) + E(\\text{DTA}_1) + E(\\text{DTA}_2) + E(\\text{DTA}_3) + \\dots + E(\\text{DTA}_\\infty) \\\\ &= \\sum^\\infty_{n = 1} \\\\ \\end{aligned} \\]","title":"Deferred Term Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#recursions","text":"The EPV of each contract can also be expressed through backwards recursion , where it is calculated as a function of itself. Consider the WL random variable: If the policyholder dies in the year with probability \\({}_{}q_{x}\\) , then a benefit of 1 is paid at the end of the year. If the policyholder survives past the year with probability \\({}_{}p_{x}\\) , then the policyholder will die in some future year. The PV of the benefit at the end of the year is given is the EPV of the same contract but at that future time ; \\(A_{x+1}\\) . \\[ \\begin{aligned} WL &= \\begin{cases} v \\cdot 1 ,& {}_{}q_{x} \\\\ v \\cdot A_{x+1} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore A_x = v{}_{}q_{x} + v{}_{}p_{x}A_{x+1} \\] The same exercise can be shown for the TA random variable. The main difference is understanding how the notation changed: \\(x+1\\) reflects the new age of the policyholder (same as WL) \\(n-1\\) reflects that one year of coverage has passed (not applicable for WL) \\[ \\begin{aligned} TA &= \\begin{cases} v \\cdot 1 ,& {}_{}q_{x} \\\\ v \\cdot A^1_{x+1:\\enclose{actuarial}{n-1}} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore A^1_{x:\\enclose{actuarial}{n-1}} = v{}_{}q_{x} + v{}_{}p_{x}A^1_{x+1:\\enclose{actuarial}{n-1}} \\] The PE variable is similar, with the main difference being that the policyholder will receive nothing if the policyholder dies . Thus, only the second component of the recursion remains: \\[ \\begin{aligned} PE &= \\begin{cases} 0 ,& {}_{}q_{x} \\\\ v \\cdot {}_{n-1}E_{x+1} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore {}_{n}E_{x} = v{}_{}p_{x} \\cdot {}_{n-1}E_{x+1} \\] EA is omitted from this section as it is simply the combination of a TA and PE. These identities are most useful in a spreadsheet setting where the calculations can be easily repeated to fill up an entire life table. However, even then it is necessary to have a starting point for the recusions to occur. The most common starting point is the terminal age as the EPVs can be intuitively determined since the policyholder will inevitably die at the end of the year: \\[ \\begin{aligned} A_{\\omega-1} &= v \\\\ A^{\\> \\> 1}_{\\omega-1:\\enclose{actuarial}{n}} &= v \\\\ {}_{n}E_{\\omega-1} &= 0 \\\\ A^{1}_{\\omega-1:\\enclose{actuarial}{n}} &= v \\end{aligned} \\]","title":"Recursions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#intuitions","text":"Although the exam questions are mostly computational, it is good to have an understanding of how the different EPVs compare against one another to serve as a sort of \"sense check\" to determine if the answer is in the right direction.","title":"Intuitions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#same-assurance-comparisons","text":"Recall that the probability of death is an increasing function with age. The death benefit is more likely to be paid out to an older policyholder - in other words, they receive the death benefit \"sooner\" than a younger policyholder. Thus, an older policyholder has larger expected cashflows that are discounted less (due to receiving it sooner), which results in a higher EPV than a younger policyholder, all else equal: \\[ \\begin{aligned} A_{x+k} & \\gt A_{x} \\\\ A^{\\> \\> 1} _{x+k:\\enclose{actuarial}{n}} & \\gt A^{\\> \\> 1}_{x+k:\\enclose{actuarial}{n}} \\end{aligned} \\] Conversely, the probability of survival is a decreasing function with age. The survival benefit is less likely to be paid out to an older policyholder - smaller expected cashflows. Regardless of the age of the policyholder, the survival benefit is paid at the same time ( same discounting ). Thus, since an older policyholder has smaller expected cashflows , it has a lower EPV than a younger policyholder: \\[ {}_{n}E_{x+k} \\le {}_{n}E_{x} \\] Endowment Assurances have both a death and survival component , thus the comparison is a combination of the two: A younger policyholder is more likely to survive and receive the survival benefit at the end of the term (discounted more) An older policyholder is more likely to die and receive the death benefit during the term (discounted less) Assuming that the difference in expected cashflows are negligible , then an older policyholder would have an higher EPV due to the lower discounting : \\[ A_{x+k:\\enclose{actuarial}{n}} \\gt A_{x:\\enclose{actuarial}{n}} \\] Naturally, all else equal, assurances with a lower interest rate are discounted less and thus have a higher EPV . This is particularly useful for questions where the SULT cannot be directly used, but can still be used as a sense check for the answers.","title":"Same assurance comparisons"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#different-assurance-comparisons","text":"At a young age where the probability of death is low, all else equal (where applicable), the EPVs of each assurance rank as follows: TA will have the smallest EPV . Although their benefits are paid out sooner, the expected benefits are small as the probability of death is small. WL has the next largest EPV . They have the same benefits as term in the short run, but have large expected benefits in the future . However, these large benefits are heavily discounted , still resuling in a small EPV. PE has the next largest EPV . Given the high probability of survival, the expected benefits are large . EA has the largest EPV . Since it is a combination of TA and PE, it is naturally the highest. \\[ \\begin{aligned} A^{1}_{30:\\enclose{actuarial}{n}} < A_{30} < {}_{n}E_{30} < A_{30:\\enclose{actuarial}{n}} \\\\ \\end{aligned} \\] At an old age where the probability of death is high, all else equal (where applicable), the EPVs of each assurance rank as follows: PE will have the smallest EPV . Given the low probability of survival, the expected benefits are are small . TA will have the next largest EPV . Given the high probability of death, the expected benefits are high . EA will have the next largest EPV . Since it is combination of TA and PE, it is naturally higher than both of them. WL has the largest EPV . Given the inevitable death of the policyholder, the expected benefits are the highest . \\[ \\begin{aligned} {}_{n}E_{100} < A^{1}_{100:\\enclose{actuarial}{n}} < A_{100:\\enclose{actuarial}{n}} < A_{100} \\\\ \\end{aligned} \\] As the policyholder approaches the terminal age, the EPVs tend to one another: \\[ \\begin{aligned} x &\\to \\omega \\\\ E(\\text{PE}) &\\to 0 \\\\ E(\\text{EA}) &\\to E(\\text{TA}) \\\\ E(\\text{TA}) &\\to E(\\text{WL}) \\end{aligned} \\] TA tends to WL whenever the end of the term exceeds the terminal age - thus the cashflows and hence EPV for both assurances become identical.","title":"Different assurance comparisons"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#payable-continuously","text":"","title":"Payable Continuously"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#continuous-whole-life","text":"","title":"Continuous Whole Life"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#continuous-term","text":"","title":"Continuous Term"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#continuous-endowments","text":"NO SUCH THING AS CONTINUOUS PURE ENDOWMENTS","title":"Continuous Endowments"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#discrete-to-continuous-approximation","text":"","title":"Discrete to Continuous Approximation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#uniform-death-distribution","text":"Alternatively, it can be assumed that people die with constant probability within a year . \\[ \\begin{aligned} f_{T_x} &= S_x(t)\\mu_{x+t} & \\end{aligned} \\] Can think of Whole life as a series of terms Uniform distribution each year \\[ \\begin{aligned} \\int^1_0 v^t {}_{t}p_{x+k} \\mu_{x+k+t} &\\approx \\int^1_0 v^t {}_{}q_{x+k} &\\approx {}_{}q_{x+k} \\int^1_0 e^{\\deltat} \\end{aligned} \\] \\[ \\begin{aligned} \\bar{A}_x &= {}_{0|}\\bar{A}^1_{x+1:\\enclose{actuarial}{1}} + {}_{1|}\\bar{A}^1_{x+1:\\enclose{actuarial}{1}} + {}_{2|}\\bar{A}^1_{x+1:\\enclose{actuarial}{1}} + \\dots \\\\ &= q_x \\frac{iv}{\\delta} + vp_xq_{x+1} \\frac{iv}{\\delta} + v^2 {}_{2}p_{x} q_{x+2} \\frac{iv}{\\delta} + \\dots \\\\ &= \\frac{i}{\\delta} (q_x + v^1 p_x {}_{}q_{x+1} + v^2 {}_{2}p_{x} {}_{}q_{x+2} + \\dots) \\\\ &= \\frac{i}{\\delta} A_x \\\\ \\end{aligned} \\]","title":"Uniform Death Distribution"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Insurance%20Functions/#claims-acceleration","text":"usually paid at \\(K_x + \\frac{1}{m}\\) Assume that deaths occur evenly thus should occur in \\(K_x + \\frac{m+1}{2m}\\) Since this is \\(\\frac{m-1}{2m}\\) earlier than the discrete case the discount factor for the discrete needs to be adjusted by that amount to be the same as the number of payments increase \\(m \\to \\infty\\) then the adjustment tends to \\(\\frac{1}{2}\\) Benefits that are paid out immediately versus at the end of the period are significantly different as the continuous benefits are discounted more . However, it can be assumed that on average , all the deaths in the year occur in the middle of the year - \\(x+k+\\frac{1}{2}\\) . The PV of the benefits are discounted using multiples of \\(v^{\\frac{1}{2}}\\) instead: Applying this to a continuous contract, it can be shown that it's EPV is is a function of the EPV of a discrete contract. This is known as the Claims Acceleration Approach . \\[ \\begin{aligned} \\bar{A}_x &\\approx v^{\\frac{1}{2}} q_x + v^{\\frac{3}{2}} p_x {}_{}q_{x+1} + v^{\\frac{5}{2}} {}_{2}p_{x} {}_{}q_{x+2} + \\dots \\\\ &\\approx (1+i)^{\\frac{1}{2}} (vq_x + v^1 p_x {}_{}q_{x+1} + v^2 {}_{2}p_{x} {}_{}q_{x+2} + \\dots) \\\\ &\\approx (1+i)^{\\frac{1}{2}} A_x \\end{aligned} \\] The method applies a higher discount rate to the discrete contract, to account for the \"under-discounting\" of the discrete method. While it is not is not perfect, it is more reflective of actual experience than the discrete method. NOTE THAT WHEN CALCULATING THE SECOND MOMENT USING UDD, EVEN THE I/DELTA TERM MUST REFLECT THE NEW INTEREST RATE Clamims Acceleration second moment also Variance UDD be careful","title":"Claims Acceleration"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuity%20Functions/","text":"Life Annuity Functions Life Annuity contracts promise to pay out a stream of benefits in the future for as long the policyholder remains alive. Similar to life insurance, the benefits can be paid discretely or continuously and can be valued through their EPV. Note that these are different from the annuities covered in Exam FM. The payments for those annuities are made regardless of the life of the policyholder, known as Annuity Certain . They serve as the foundation to understanding Life Annuities. Review: Annuities Certain There are two types of payment structures: Period Start Period End Paid in Advance Paid in Arrears Annuity Due Annuity Immediate \\(\\ddot{a}_{\\enclose{actuarial}{n}}\\) \\(a_{\\enclose{actuarial}{n}}\\) The overall PV of the annuity is the sum of the PV of the stream of payments: \\[ \\begin{aligned} a_{\\enclose{actuarial}{n}} &= v + v^2 + v^3 + \\dots + v^n \\\\ &= \\frac{v-v^{n+1}}{1-v} \\\\ &= \\sum^n_{k=1} v^k \\\\ &= \\frac{v(1-v^n)}{1-v} \\\\ &= \\frac{1-v^n}{i} \\\\ \\\\ \\ddot{a}_{\\enclose{actuarial}{n}} &= 1 + v + v^2 + \\dots + v^n \\\\ &= \\sum^{n-1}_{k=0} v^k \\\\ &= \\frac{1- v^{n}}{1-v} \\\\ &= \\frac{1- v^{n}}{d} \\end{aligned} \\] Notice that the payments simply differ by one period and hence one discounting factor: \\[ \\begin{aligned} \\ddot{a}_{\\enclose{actuarial}{n}} &= 1 + v + v^2 + v^3 + \\dots + v^n \\\\ &= (1+i) (v + v^2 + v^3 + \\dots + v^n) \\\\ &= (1+i) \\sum^{n}_{k=1} v^k \\\\ &= (1+i) a_{\\enclose{actuarial}{n}} \\end{aligned} \\] From another perspective, only the payments at \\(t=0\\) and \\(t=n\\) are different: \\[ \\begin{aligned} \\ddot{a}_{\\enclose{actuarial}{n}} &= \\sum^{n-1}_0 v^k \\\\ &= v^0 + \\sum^{n-1}_{k=1} v^k \\\\ &= 1 + a_{\\enclose{actuarial}{n-1}} \\end{aligned} \\] Payable Discretely If the contract pays out benefits discretely, then \\(K_x\\) is used as the survival model. Since a life annuity pays out a stream of benefits, the PV of the contract at every time period can be slightly confusing. It is important to remember the following: The PV is calculated with respect to time 0 The PV is the sum of the PV of all the payments the individual is expected to live \\[ \\begin{aligned} PV &= \\begin{cases} a_{\\enclose{actuarial}{K_x}}, & \\text{Life Annuity Immediate} \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x + 1}}, & \\text{Life Annuity Due} \\end{cases} \\end{aligned} \\] Thus, the EPV of the contract is the sum product of the PV of the benefit in each period and the probability of death in that period: \\[ \\begin{aligned} EPV &= \\begin{cases} \\text{Life Annuity Immediate} &=\\ \\sum a_{\\enclose{actuarial}{K_x}} \\cdot {}_{K_x|}q_{x} \\\\ \\text{Life Annuity Due} &= \\sum \\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\cdot {}_{K_x|}q_{x} \\end{cases} \\end{aligned} \\] For the remainder of the section, to be concise, only the formulas for Annuity Due will be given. The corresponding formulas for Annuity Immediates can be easily calculated from it. Actuarial Notation Most of the notation for Life Insurance applies to Life Annuities as well. The key difference is that \\({}^{k}a\\) is used to represent the k-th moment of the present value of a contract where a benefit of 1 is paid discretely for as long as the status does NOT fail . \\(A\\) stands for Assurance while \\(a\\) stands for Annuity. Whole Life Annuity Whole Life Annuities covers the insured indefinitely and thus will pay out for as long as the insured survives . Let WL be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{WL}_{\\text{Due}} &=\\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\end{aligned} \\] Thus, the EPV is the Expectation of the WLA random variable: \\[ \\begin{aligned} E(\\text{WLA}_{\\text{Due}}) &= \\sum^{\\infty}_{K_x = 0} \\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\cdot {}_{K_x|}q_{x} \\\\ \\ddot{a}_{x} &= \\sum^{\\infty}_{K_x = 0} \\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] The EPV can be furthered simplified, allowing a life annuity to be viewed as the sum of a series of pure endowments : \\[ \\begin{aligned} \\ddot{a}_{x} &= \\sum^{\\infty}_{K_x = 1} a_{\\enclose{actuarial}{K_x}} \\cdot {}_{K_x|}q_{x} \\\\ &=\\sum^{\\infty}_{K_x = 1} \\left(\\sum^n_{j=1} v^j \\right) \\cdot {}_{K_x|}q_{x} \\\\ &= v^1 \\cdot {}_{1|}q_{x} + (v^1 + v^2) \\cdot {}_{2|}q_{x} + (v^1 + v^2 + v^3) \\cdot {}_{3|}q_{x} + \\dots \\\\ &= v ({}_{1|}q_{x} + {}_{2|}q_{x} + \\dots) + v^2 ({}_{2|}q_{x} + {}_{3|}q_{x} + \\dots) + v^3 ({}_{3|}q_{x} + {}_{4|}q_{x} + \\dots) + \\dots \\\\ &= \\sum^{\\infty}_{j=1} v^j \\cdot \\left(\\sum^{\\infty}_{K_x = j} {}_{K_x|}q_{x} \\right) \\\\ &= \\sum^{\\infty}_{j=1} v^j \\cdot {}_{j}p_{x} \\end{aligned} \\] The second moment and variance for all life annuities will be covered in a later section. Temporary Life Annuity Temporary Life Annuities covers the insured for a specified period \\(n\\) and thus pays out for as long as the insured survives during that period only . Let TA be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{TA}_\\text{Due} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{K_x+1}},& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{n}},& K_x \\ge n \\\\ \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{min(K_x + 1, n)}} \\end{aligned} \\] Thus, the EPV is the expectation of the TA random variable, which can be simplified using the same approach as before: \\[ \\begin{aligned} E(\\text{TA}_\\text{Due}) &= \\sum^{n-1}_{K_x = 0} a_{\\enclose{actuarial}{K_x}} \\cdot {}_{K_x|}q_x + a_{\\enclose{actuarial}{n}} \\cdot {}_{n}p_x \\\\ \\ddot{a}_{x:\\enclose{actuarial}{n}} &= \\sum^{n}_{j=1} v^j \\cdot {}_{j}p_{x} \\end{aligned} \\] Note the change in limits for the simplified approach - an annuity is the combination of pure endowments for as long as the annuity lasts . Deferred Life Annuities Deferred Annuities are variations of the above contracts where the coverage is deferred by \\(n\\) years . While any contract can be deferred, the most useful is the Deferred Whole Life Annuity . Let DWL be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{DWL}_{\\text{Due}} &= \\begin{cases} 0, & K_x = 0, 1, 2, \\dots, n-1 \\\\ v^n a_{\\enclose{actuarial}{K_x-n}}, & K_x \\ge n \\end{cases} \\end{aligned} \\] Thus, the EPV is the expectation of the DWL random variable: \\[ \\begin{aligned} E(\\text{DWL}_{\\text{Due}}) &= \\sum^{\\infty}_{K_x = n} a_{\\enclose{actuarial}{K_x-n}} \\cdot {}_{K_x|}q_{x} \\\\ {}_{n|}a_{x} &= \\sum^{\\infty}_{K_x = 0} a_{\\enclose{actuarial}{K_x}} \\cdot {}_{K_x|}q_{x} - \\sum^{n-1}_{K_x = 0} a_{\\enclose{actuarial}{K_x}} \\cdot {}_{K_x|}q_{x} \\\\ {}_{n|}a_{x} &= \\ddot{a}_x - \\ddot{a}_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] Similar to before, the EPV can be shown to be a sum of pure endowments: \\[ {}_{n|}a_{x} = \\sum^{\\infty}_{j=1} v^j \\cdot {}_{j}p_{x} \\] Since a DWL is effectively a WL that is issued \\(n\\) years later, the EPV of a DWL is the EPV of the WL, adjusted for both interest and survival: \\[ \\begin{aligned} {}_{n|}a_{x} &= \\sum^{\\infty}_{K_x = 0} a_{\\enclose{actuarial}{K_x}} \\cdot {}_{K_x|}q_{x} - \\sum^{n-1}_{K_x = 0} a_{\\enclose{actuarial}{K_x}} \\cdot {}_{K_x|}q_{x} \\\\ &= \\sum^{\\infty}_{K_x = n} a_{\\enclose{actuarial}{K_x}} \\cdot {}_{K_x|}q_{x} \\\\ &= v^n {}_{n}p_{x} \\sum^{\\infty}_{K_x = n} a_{\\enclose{actuarial}{K_x-n}} \\cdot {}_{K_x-n|}q_{x+n} \\\\ &= v^n {}_{n}p_{x} \\sum^{\\infty}_{K_x = 0} a_{\\enclose{actuarial}{K_x}} \\cdot {}_{K_x|}q_{x+n} \\\\ &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\end{aligned} \\] This allows a TA to be expressed as the difference of two WL annuities issued at different times : \\[ \\begin{aligned} {}_{n|}a_{x} &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\\\ \\ddot{a}_x - \\ddot{a}_{x:\\enclose{actuarial}{n}} &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\\\ \\ddot{a}_{x:\\enclose{actuarial}{n}} &= \\ddot{a}_x - {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\end{aligned} \\] Guaranteed Life Annuities Guaranteed Life Annuities are whole life annuities with the benefits in the first \\(n\\) years being guaranteed - they will be paid out even if the insured dies during this period. Let GA be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{GA}_{\\text{Due}} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}},& K_x \\ge n \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{max(K_x+1,n)}} \\end{aligned} \\] Thus, the EPV is the expectation of the TA random variable: \\[ \\begin{aligned} E(\\text{GA}_{\\text{Due}}) &= hi \\\\ \\ddot{a}_{\\overline{x:\\enclose{actuarial}{n}}} &= \\ddot{a}_{\\enclose{actuarial}{n}} + {}_{n} \\ddot{a}_{x} \\end{aligned} \\] Assurances and Annuities Consider the random variable for the PV Whole Life Assurance and Annuity Due. Notice that both RVs are related through the certain annuity formula: \\[ \\begin{aligned} \\text{WL} &= v^{K_x + 1} \\\\ \\text{WLA} &= \\ddot{a}_{\\enclose{actuarial}{K_x+1}} = \\frac{1-v^{K_x + 1}}{d} \\end{aligned} \\] This is why Annuity Due, rather Annuity Immediates, are provided in the SULT. Thus, given the EPV of a life annuity, the EPV of a life assurance can be determined, vice-versa: \\[ \\begin{aligned} \\text{WLA} &= \\frac{1-\\text{WL}}{d} \\\\ E(\\text{WLA}) &= \\frac{1-E(\\text{WL})}{d} \\\\ \\ddot{a}_x &= \\frac{1-A_x}{d} \\\\ d \\ddot{a}_x &= 1 - A_x \\\\ A_x &= 1 - d \\ddot{a}_x \\end{aligned} \\] The same logic can be applied for the Variance as well: \\[ \\begin{aligned} \\text{WLA} &= \\frac{1-\\text{WL}}{d} \\\\ Var(\\text{WLA}) &= Var \\left( \\frac{1-\\text{WL}}{d} \\right) \\\\ Var(\\text{WLA}) &= \\frac{1}{d^2} var(\\text{WL}) \\\\ Var(\\text{WLA}) &= \\frac{1}{d^2} \\left({}_{}^2 A_x - (A_x)^2 \\right) \\end{aligned} \\] The same relationship can be shown for a temporary annuity and an Endowment Assurance , NOT a term assurance: \\[ \\begin{aligned} A_{x:\\enclose{actuarial}{n}} &= 1 - d \\ddot{a}_{x:\\enclose{actuarial}{n}} \\\\ Var(\\text{TA}) &= \\frac{1}{d^2} \\left({}_{}^2 A_{x:\\enclose{actuarial}{n}} - (A_{x:\\enclose{actuarial}{n}})^2 \\right) \\end{aligned} \\] Recursive Relationships \\[ \\ddot{a}_{x} = q_x(1) + p_x(1+v\\ddot{a}_{x+1}) \\] \\[ \\ddot{a}_{x:\\enclose{actuarial}{n}} = q_x(1) + p_x(1+v\\ddot{a}_{x+1:\\enclose{actuarial}{n-1}}) \\] Immediate and Due The same values can be calculated for Annuity Immediates by relating them to Annuity Dues: \\[ \\begin{aligned} \\ddot{a}_{\\enclose{actuarial}{n}} &= 1 + a_{\\enclose{actuarial}{n-1}} WL_Due &= 1 + WL_Immediate E(WL_Due) &= 1 + E(WL_Immediate) Var(WL_Due) &= Var(WL_Immediate) \\end{aligned} \\] Intuitions Payable Continuously Woolhouse Approximation","title":"Life Annuity Functions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuity%20Functions/#life-annuity-functions","text":"Life Annuity contracts promise to pay out a stream of benefits in the future for as long the policyholder remains alive. Similar to life insurance, the benefits can be paid discretely or continuously and can be valued through their EPV. Note that these are different from the annuities covered in Exam FM. The payments for those annuities are made regardless of the life of the policyholder, known as Annuity Certain . They serve as the foundation to understanding Life Annuities.","title":"Life Annuity Functions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuity%20Functions/#review-annuities-certain","text":"There are two types of payment structures: Period Start Period End Paid in Advance Paid in Arrears Annuity Due Annuity Immediate \\(\\ddot{a}_{\\enclose{actuarial}{n}}\\) \\(a_{\\enclose{actuarial}{n}}\\) The overall PV of the annuity is the sum of the PV of the stream of payments: \\[ \\begin{aligned} a_{\\enclose{actuarial}{n}} &= v + v^2 + v^3 + \\dots + v^n \\\\ &= \\frac{v-v^{n+1}}{1-v} \\\\ &= \\sum^n_{k=1} v^k \\\\ &= \\frac{v(1-v^n)}{1-v} \\\\ &= \\frac{1-v^n}{i} \\\\ \\\\ \\ddot{a}_{\\enclose{actuarial}{n}} &= 1 + v + v^2 + \\dots + v^n \\\\ &= \\sum^{n-1}_{k=0} v^k \\\\ &= \\frac{1- v^{n}}{1-v} \\\\ &= \\frac{1- v^{n}}{d} \\end{aligned} \\] Notice that the payments simply differ by one period and hence one discounting factor: \\[ \\begin{aligned} \\ddot{a}_{\\enclose{actuarial}{n}} &= 1 + v + v^2 + v^3 + \\dots + v^n \\\\ &= (1+i) (v + v^2 + v^3 + \\dots + v^n) \\\\ &= (1+i) \\sum^{n}_{k=1} v^k \\\\ &= (1+i) a_{\\enclose{actuarial}{n}} \\end{aligned} \\] From another perspective, only the payments at \\(t=0\\) and \\(t=n\\) are different: \\[ \\begin{aligned} \\ddot{a}_{\\enclose{actuarial}{n}} &= \\sum^{n-1}_0 v^k \\\\ &= v^0 + \\sum^{n-1}_{k=1} v^k \\\\ &= 1 + a_{\\enclose{actuarial}{n-1}} \\end{aligned} \\]","title":"Review: Annuities Certain"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuity%20Functions/#payable-discretely","text":"If the contract pays out benefits discretely, then \\(K_x\\) is used as the survival model. Since a life annuity pays out a stream of benefits, the PV of the contract at every time period can be slightly confusing. It is important to remember the following: The PV is calculated with respect to time 0 The PV is the sum of the PV of all the payments the individual is expected to live \\[ \\begin{aligned} PV &= \\begin{cases} a_{\\enclose{actuarial}{K_x}}, & \\text{Life Annuity Immediate} \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x + 1}}, & \\text{Life Annuity Due} \\end{cases} \\end{aligned} \\] Thus, the EPV of the contract is the sum product of the PV of the benefit in each period and the probability of death in that period: \\[ \\begin{aligned} EPV &= \\begin{cases} \\text{Life Annuity Immediate} &=\\ \\sum a_{\\enclose{actuarial}{K_x}} \\cdot {}_{K_x|}q_{x} \\\\ \\text{Life Annuity Due} &= \\sum \\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\cdot {}_{K_x|}q_{x} \\end{cases} \\end{aligned} \\] For the remainder of the section, to be concise, only the formulas for Annuity Due will be given. The corresponding formulas for Annuity Immediates can be easily calculated from it.","title":"Payable Discretely"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuity%20Functions/#actuarial-notation","text":"Most of the notation for Life Insurance applies to Life Annuities as well. The key difference is that \\({}^{k}a\\) is used to represent the k-th moment of the present value of a contract where a benefit of 1 is paid discretely for as long as the status does NOT fail . \\(A\\) stands for Assurance while \\(a\\) stands for Annuity.","title":"Actuarial Notation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuity%20Functions/#whole-life-annuity","text":"Whole Life Annuities covers the insured indefinitely and thus will pay out for as long as the insured survives . Let WL be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{WL}_{\\text{Due}} &=\\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\end{aligned} \\] Thus, the EPV is the Expectation of the WLA random variable: \\[ \\begin{aligned} E(\\text{WLA}_{\\text{Due}}) &= \\sum^{\\infty}_{K_x = 0} \\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\cdot {}_{K_x|}q_{x} \\\\ \\ddot{a}_{x} &= \\sum^{\\infty}_{K_x = 0} \\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] The EPV can be furthered simplified, allowing a life annuity to be viewed as the sum of a series of pure endowments : \\[ \\begin{aligned} \\ddot{a}_{x} &= \\sum^{\\infty}_{K_x = 1} a_{\\enclose{actuarial}{K_x}} \\cdot {}_{K_x|}q_{x} \\\\ &=\\sum^{\\infty}_{K_x = 1} \\left(\\sum^n_{j=1} v^j \\right) \\cdot {}_{K_x|}q_{x} \\\\ &= v^1 \\cdot {}_{1|}q_{x} + (v^1 + v^2) \\cdot {}_{2|}q_{x} + (v^1 + v^2 + v^3) \\cdot {}_{3|}q_{x} + \\dots \\\\ &= v ({}_{1|}q_{x} + {}_{2|}q_{x} + \\dots) + v^2 ({}_{2|}q_{x} + {}_{3|}q_{x} + \\dots) + v^3 ({}_{3|}q_{x} + {}_{4|}q_{x} + \\dots) + \\dots \\\\ &= \\sum^{\\infty}_{j=1} v^j \\cdot \\left(\\sum^{\\infty}_{K_x = j} {}_{K_x|}q_{x} \\right) \\\\ &= \\sum^{\\infty}_{j=1} v^j \\cdot {}_{j}p_{x} \\end{aligned} \\] The second moment and variance for all life annuities will be covered in a later section.","title":"Whole Life Annuity"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuity%20Functions/#temporary-life-annuity","text":"Temporary Life Annuities covers the insured for a specified period \\(n\\) and thus pays out for as long as the insured survives during that period only . Let TA be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{TA}_\\text{Due} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{K_x+1}},& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{n}},& K_x \\ge n \\\\ \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{min(K_x + 1, n)}} \\end{aligned} \\] Thus, the EPV is the expectation of the TA random variable, which can be simplified using the same approach as before: \\[ \\begin{aligned} E(\\text{TA}_\\text{Due}) &= \\sum^{n-1}_{K_x = 0} a_{\\enclose{actuarial}{K_x}} \\cdot {}_{K_x|}q_x + a_{\\enclose{actuarial}{n}} \\cdot {}_{n}p_x \\\\ \\ddot{a}_{x:\\enclose{actuarial}{n}} &= \\sum^{n}_{j=1} v^j \\cdot {}_{j}p_{x} \\end{aligned} \\] Note the change in limits for the simplified approach - an annuity is the combination of pure endowments for as long as the annuity lasts .","title":"Temporary Life Annuity"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuity%20Functions/#deferred-life-annuities","text":"Deferred Annuities are variations of the above contracts where the coverage is deferred by \\(n\\) years . While any contract can be deferred, the most useful is the Deferred Whole Life Annuity . Let DWL be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{DWL}_{\\text{Due}} &= \\begin{cases} 0, & K_x = 0, 1, 2, \\dots, n-1 \\\\ v^n a_{\\enclose{actuarial}{K_x-n}}, & K_x \\ge n \\end{cases} \\end{aligned} \\] Thus, the EPV is the expectation of the DWL random variable: \\[ \\begin{aligned} E(\\text{DWL}_{\\text{Due}}) &= \\sum^{\\infty}_{K_x = n} a_{\\enclose{actuarial}{K_x-n}} \\cdot {}_{K_x|}q_{x} \\\\ {}_{n|}a_{x} &= \\sum^{\\infty}_{K_x = 0} a_{\\enclose{actuarial}{K_x}} \\cdot {}_{K_x|}q_{x} - \\sum^{n-1}_{K_x = 0} a_{\\enclose{actuarial}{K_x}} \\cdot {}_{K_x|}q_{x} \\\\ {}_{n|}a_{x} &= \\ddot{a}_x - \\ddot{a}_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] Similar to before, the EPV can be shown to be a sum of pure endowments: \\[ {}_{n|}a_{x} = \\sum^{\\infty}_{j=1} v^j \\cdot {}_{j}p_{x} \\] Since a DWL is effectively a WL that is issued \\(n\\) years later, the EPV of a DWL is the EPV of the WL, adjusted for both interest and survival: \\[ \\begin{aligned} {}_{n|}a_{x} &= \\sum^{\\infty}_{K_x = 0} a_{\\enclose{actuarial}{K_x}} \\cdot {}_{K_x|}q_{x} - \\sum^{n-1}_{K_x = 0} a_{\\enclose{actuarial}{K_x}} \\cdot {}_{K_x|}q_{x} \\\\ &= \\sum^{\\infty}_{K_x = n} a_{\\enclose{actuarial}{K_x}} \\cdot {}_{K_x|}q_{x} \\\\ &= v^n {}_{n}p_{x} \\sum^{\\infty}_{K_x = n} a_{\\enclose{actuarial}{K_x-n}} \\cdot {}_{K_x-n|}q_{x+n} \\\\ &= v^n {}_{n}p_{x} \\sum^{\\infty}_{K_x = 0} a_{\\enclose{actuarial}{K_x}} \\cdot {}_{K_x|}q_{x+n} \\\\ &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\end{aligned} \\] This allows a TA to be expressed as the difference of two WL annuities issued at different times : \\[ \\begin{aligned} {}_{n|}a_{x} &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\\\ \\ddot{a}_x - \\ddot{a}_{x:\\enclose{actuarial}{n}} &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\\\ \\ddot{a}_{x:\\enclose{actuarial}{n}} &= \\ddot{a}_x - {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\end{aligned} \\]","title":"Deferred Life Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuity%20Functions/#guaranteed-life-annuities","text":"Guaranteed Life Annuities are whole life annuities with the benefits in the first \\(n\\) years being guaranteed - they will be paid out even if the insured dies during this period. Let GA be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{GA}_{\\text{Due}} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}},& K_x \\ge n \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{max(K_x+1,n)}} \\end{aligned} \\] Thus, the EPV is the expectation of the TA random variable: \\[ \\begin{aligned} E(\\text{GA}_{\\text{Due}}) &= hi \\\\ \\ddot{a}_{\\overline{x:\\enclose{actuarial}{n}}} &= \\ddot{a}_{\\enclose{actuarial}{n}} + {}_{n} \\ddot{a}_{x} \\end{aligned} \\]","title":"Guaranteed Life Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuity%20Functions/#assurances-and-annuities","text":"Consider the random variable for the PV Whole Life Assurance and Annuity Due. Notice that both RVs are related through the certain annuity formula: \\[ \\begin{aligned} \\text{WL} &= v^{K_x + 1} \\\\ \\text{WLA} &= \\ddot{a}_{\\enclose{actuarial}{K_x+1}} = \\frac{1-v^{K_x + 1}}{d} \\end{aligned} \\] This is why Annuity Due, rather Annuity Immediates, are provided in the SULT. Thus, given the EPV of a life annuity, the EPV of a life assurance can be determined, vice-versa: \\[ \\begin{aligned} \\text{WLA} &= \\frac{1-\\text{WL}}{d} \\\\ E(\\text{WLA}) &= \\frac{1-E(\\text{WL})}{d} \\\\ \\ddot{a}_x &= \\frac{1-A_x}{d} \\\\ d \\ddot{a}_x &= 1 - A_x \\\\ A_x &= 1 - d \\ddot{a}_x \\end{aligned} \\] The same logic can be applied for the Variance as well: \\[ \\begin{aligned} \\text{WLA} &= \\frac{1-\\text{WL}}{d} \\\\ Var(\\text{WLA}) &= Var \\left( \\frac{1-\\text{WL}}{d} \\right) \\\\ Var(\\text{WLA}) &= \\frac{1}{d^2} var(\\text{WL}) \\\\ Var(\\text{WLA}) &= \\frac{1}{d^2} \\left({}_{}^2 A_x - (A_x)^2 \\right) \\end{aligned} \\] The same relationship can be shown for a temporary annuity and an Endowment Assurance , NOT a term assurance: \\[ \\begin{aligned} A_{x:\\enclose{actuarial}{n}} &= 1 - d \\ddot{a}_{x:\\enclose{actuarial}{n}} \\\\ Var(\\text{TA}) &= \\frac{1}{d^2} \\left({}_{}^2 A_{x:\\enclose{actuarial}{n}} - (A_{x:\\enclose{actuarial}{n}})^2 \\right) \\end{aligned} \\]","title":"Assurances and Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuity%20Functions/#recursive-relationships","text":"\\[ \\ddot{a}_{x} = q_x(1) + p_x(1+v\\ddot{a}_{x+1}) \\] \\[ \\ddot{a}_{x:\\enclose{actuarial}{n}} = q_x(1) + p_x(1+v\\ddot{a}_{x+1:\\enclose{actuarial}{n-1}}) \\]","title":"Recursive Relationships"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuity%20Functions/#immediate-and-due","text":"The same values can be calculated for Annuity Immediates by relating them to Annuity Dues: \\[ \\begin{aligned} \\ddot{a}_{\\enclose{actuarial}{n}} &= 1 + a_{\\enclose{actuarial}{n-1}} WL_Due &= 1 + WL_Immediate E(WL_Due) &= 1 + E(WL_Immediate) Var(WL_Due) &= Var(WL_Immediate) \\end{aligned} \\]","title":"Immediate and Due"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuity%20Functions/#intuitions","text":"","title":"Intuitions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuity%20Functions/#payable-continuously","text":"","title":"Payable Continuously"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuity%20Functions/#woolhouse-approximation","text":"","title":"Woolhouse Approximation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Premium%20Calculations/","text":"","title":"Premium Calculations"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Reserve%20Caclulations/","text":"","title":"6. Reserve Caclulations"},{"location":"3.%20Predictive%20Analytics/Exam%20Overview/","text":"Overview of Predictive Analytics Predictive Analytics is the usage of statistical models to analyze historical or current data to make predictions about the future or unknown events. Due to the growing relevance of Predictive Analytics , the SOA has added a significant amount of material on the topic into the credentialling process: Exam Statistics for Risk Modelling (SRM) Exam Predictive Analytics (PA) Exam Advanced Topics for Predictive Analytics (ATPA) Exam ATPA was added midway through 2022 to replace Exam IFM. Credit for IFM also counts towards credit for ATPA, thus this set of notes will NOT be covering ATPA. All of them share the same theme of working with statistical models: Constructing statistical models Intepreting their outputs Evaluating their performance Exam SRM covers the theory about the various types of models, tested in the typical MCQ format . Unlike the other exams where most of the questions are quantitative, most of SRM's questions are qualitative . Exam PA tests the same concepts in an applied manner , providing real data and a business problem to navigate through in a written format . Exam ATPA builds on both of these exams, covering more advanced concepts in a business context.","title":"Overview"},{"location":"3.%20Predictive%20Analytics/Exam%20Overview/#overview-of-predictive-analytics","text":"Predictive Analytics is the usage of statistical models to analyze historical or current data to make predictions about the future or unknown events. Due to the growing relevance of Predictive Analytics , the SOA has added a significant amount of material on the topic into the credentialling process: Exam Statistics for Risk Modelling (SRM) Exam Predictive Analytics (PA) Exam Advanced Topics for Predictive Analytics (ATPA) Exam ATPA was added midway through 2022 to replace Exam IFM. Credit for IFM also counts towards credit for ATPA, thus this set of notes will NOT be covering ATPA. All of them share the same theme of working with statistical models: Constructing statistical models Intepreting their outputs Evaluating their performance Exam SRM covers the theory about the various types of models, tested in the typical MCQ format . Unlike the other exams where most of the questions are quantitative, most of SRM's questions are qualitative . Exam PA tests the same concepts in an applied manner , providing real data and a business problem to navigate through in a written format . Exam ATPA builds on both of these exams, covering more advanced concepts in a business context.","title":"Overview of Predictive Analytics"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/","text":"Linear Regression Population Regression Model Regression is a statistical model that relates a Dependent Variable (DV) to one or more Independent Variables (IV). The dependent variable is regressed on to the independent variable. They are fundamentally a function of the independent variables and several Regression Parameters , \\(\\beta\\) . The functional form of the regression is based on the relationship between the variables. The goal is to use a model that best captures the relationship between the variables. \\[ Y = f(X, \\beta) \\] Independent Variable(s) Dependent Variables Variable used to make predictions Variable being predicted Free to change the value Depends on the value of indepenent variable Deterministic Random Variable Denoted as \\(X\\) Denoted as \\(Y\\) To be precise, for every set of IVs, the DV has a Conditional Distribution dependent on the given IV. For instance, the the \\(Y\\) could take any possible value (Marginal Distribution), but given these set of \\(X\\) , the possible values can be narrowed down to a certain range (conditional distribution). \\[ \\displaylines{ Y \\sim Distribution \\\\ Y|X \\sim Conditional~Distribution} \\] Thus, the output of the regression model is actually the Expected Value of the conditional distribution, \\(E(Y|X)\\) , for every possible \\(X\\) . \\[ E(Y|X) = f(X, \\beta) \\] The actual observations are unlikely to be exactly equal to its expectation, thus there is a difference between an observation and the corresponding regression output. It known as the Random Error Term which accounts for all other factors that affect the DV that are not captured in the regression. This means that the relationship between the \\(Y\\) and \\(X\\) is only approximate , as the true relationship is probably different due to the possibility of unaccounted variables. Note that the sign of the errors are significant - positive implies the actual value lies above the regression output while negative implies it lies below. \\[ \\varepsilon_i = y_i - f(x_i, \\beta) \\] This means that \\(Y\\) (not its expectation!) can be expressed as a sum of the regression model and the error terms: \\[ y_i = f(x_i,\\beta) + \\varepsilon_i \\] The Regression is known as the Systematic component as it is shared among all observations The Error is known as the Non-Systematic component as it is unique to each observation Sample Regression Model In practice, the population is unobservable hence it is impossible to construct the population regression model. Instead, a regression model is constructed from a sample instead, which aims to estimate the population model. \\[ \\hat{y} = f(X,\\hat{\\beta}) \\] Similarly, the output of this model can be compared to the actual observations. However, the resulting difference is known as the Residual of the model, which like all the other components, is an estimate for the Error term. \\[ \\hat{\\varepsilon_i} = y_i - \\hat{y_i} \\] There are several different methods to estimate the regression parameters, but they usually involve minimizing the residuals of the model, such that the resulting model best fits the given sample, which is why it is also known as the Fitted Regression Model . Hypothesis Testing Once a regression model has been fit, the next step is to determine if the relationship found in the sample is indicative of a relationship in the population. This can be determined through the following two-sided hypothesis test : Null Hypothesis: \\(\\beta_1 = 0\\) Alternative Hypothesis: \\(\\beta_1 \\ne 0\\) Under the null, the regression parameters are assumed to be 0, implying that there is no relationship between \\(Y\\) and \\(X\\) . The test should reject the null , proving that there IS a relationship between the DV and IVs. Prediction Once the best model has been determined, it can be used to make Predictions about future unobserved values. Let these future values be denoted by the subscript \\(*\\) . These unobserved DVs come from the population, thus can be expressed as a function of the population model: \\[ y_* = f(x_*, \\beta) + \\varepsilon_* \\] The corresponding values from the sample regression model is an estimate for this unobserved value: \\[ \\hat{y}_* = f(x_*, \\hat{\\beta}) \\] Like before, the predicted value is unlikely to be exactly equal to the actual value. Thus, the difference between both values can be measured as the Prediction Error : \\[ y_* - \\hat{y_*} = \\varepsilon_* + [f(x_*, \\beta) - f(x_*, \\hat{\\beta})] \\] The prediction error is thus made up of two components : Inherent error present in the DV ( \\(\\varepsilon_*\\) ) Error in estimating the population model ( \\(f(x_*, \\beta) - f(x_*, \\hat{\\beta})\\) ) Based on the distribution of the prediction error, a Prediction Interval at a given confidence level can be calculated to accompany the regression estimate, which is essentially a confidence interval for the predicted value . Note that the prediction intervals will always be wider than confidence intervals . This is because CIs only takes into the account the error in estimating the population model/parameters while PIs take into account the inherent error of the DV as well.","title":"Regression Overview"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#linear-regression","text":"","title":"Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#population-regression-model","text":"Regression is a statistical model that relates a Dependent Variable (DV) to one or more Independent Variables (IV). The dependent variable is regressed on to the independent variable. They are fundamentally a function of the independent variables and several Regression Parameters , \\(\\beta\\) . The functional form of the regression is based on the relationship between the variables. The goal is to use a model that best captures the relationship between the variables. \\[ Y = f(X, \\beta) \\] Independent Variable(s) Dependent Variables Variable used to make predictions Variable being predicted Free to change the value Depends on the value of indepenent variable Deterministic Random Variable Denoted as \\(X\\) Denoted as \\(Y\\) To be precise, for every set of IVs, the DV has a Conditional Distribution dependent on the given IV. For instance, the the \\(Y\\) could take any possible value (Marginal Distribution), but given these set of \\(X\\) , the possible values can be narrowed down to a certain range (conditional distribution). \\[ \\displaylines{ Y \\sim Distribution \\\\ Y|X \\sim Conditional~Distribution} \\] Thus, the output of the regression model is actually the Expected Value of the conditional distribution, \\(E(Y|X)\\) , for every possible \\(X\\) . \\[ E(Y|X) = f(X, \\beta) \\] The actual observations are unlikely to be exactly equal to its expectation, thus there is a difference between an observation and the corresponding regression output. It known as the Random Error Term which accounts for all other factors that affect the DV that are not captured in the regression. This means that the relationship between the \\(Y\\) and \\(X\\) is only approximate , as the true relationship is probably different due to the possibility of unaccounted variables. Note that the sign of the errors are significant - positive implies the actual value lies above the regression output while negative implies it lies below. \\[ \\varepsilon_i = y_i - f(x_i, \\beta) \\] This means that \\(Y\\) (not its expectation!) can be expressed as a sum of the regression model and the error terms: \\[ y_i = f(x_i,\\beta) + \\varepsilon_i \\] The Regression is known as the Systematic component as it is shared among all observations The Error is known as the Non-Systematic component as it is unique to each observation","title":"Population Regression Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#sample-regression-model","text":"In practice, the population is unobservable hence it is impossible to construct the population regression model. Instead, a regression model is constructed from a sample instead, which aims to estimate the population model. \\[ \\hat{y} = f(X,\\hat{\\beta}) \\] Similarly, the output of this model can be compared to the actual observations. However, the resulting difference is known as the Residual of the model, which like all the other components, is an estimate for the Error term. \\[ \\hat{\\varepsilon_i} = y_i - \\hat{y_i} \\] There are several different methods to estimate the regression parameters, but they usually involve minimizing the residuals of the model, such that the resulting model best fits the given sample, which is why it is also known as the Fitted Regression Model .","title":"Sample Regression Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#hypothesis-testing","text":"Once a regression model has been fit, the next step is to determine if the relationship found in the sample is indicative of a relationship in the population. This can be determined through the following two-sided hypothesis test : Null Hypothesis: \\(\\beta_1 = 0\\) Alternative Hypothesis: \\(\\beta_1 \\ne 0\\) Under the null, the regression parameters are assumed to be 0, implying that there is no relationship between \\(Y\\) and \\(X\\) . The test should reject the null , proving that there IS a relationship between the DV and IVs.","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#prediction","text":"Once the best model has been determined, it can be used to make Predictions about future unobserved values. Let these future values be denoted by the subscript \\(*\\) . These unobserved DVs come from the population, thus can be expressed as a function of the population model: \\[ y_* = f(x_*, \\beta) + \\varepsilon_* \\] The corresponding values from the sample regression model is an estimate for this unobserved value: \\[ \\hat{y}_* = f(x_*, \\hat{\\beta}) \\] Like before, the predicted value is unlikely to be exactly equal to the actual value. Thus, the difference between both values can be measured as the Prediction Error : \\[ y_* - \\hat{y_*} = \\varepsilon_* + [f(x_*, \\beta) - f(x_*, \\hat{\\beta})] \\] The prediction error is thus made up of two components : Inherent error present in the DV ( \\(\\varepsilon_*\\) ) Error in estimating the population model ( \\(f(x_*, \\beta) - f(x_*, \\hat{\\beta})\\) ) Based on the distribution of the prediction error, a Prediction Interval at a given confidence level can be calculated to accompany the regression estimate, which is essentially a confidence interval for the predicted value . Note that the prediction intervals will always be wider than confidence intervals . This is because CIs only takes into the account the error in estimating the population model/parameters while PIs take into account the inherent error of the DV as well.","title":"Prediction"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/10.%20Clustering/","text":"","title":"10. Clustering"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/","text":"Simple Linear Regression Simple Linear Regression (SLR) assumes a Linear Relationship between a Numeric DV and single continuous quantitative IV. The model is considered to be simple because it only contains a single independent variable. \\[ E(Y|X) = \\beta_0 + \\beta_1 X \\] \\(\\beta_0\\) \\(\\beta_1\\) Expected value of \\(Y\\) when \\(X = 0\\) Change in the expected value of \\(Y\\) given a one unit increase in \\(X\\) Intercept Parameter Slope Parameter Each observation can also be expressed as sum of the regression and its error term: \\[ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\] Ordinary Least Squares SLR parameters are estimated using the Ordinary Least Squares method, which minimizes the Sum of Squared Residuals of the fitted model. It is commonly referred to as the Residual Sum Squared (RSS). \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 \\] The minimization is solved through calculus by setting the partial derivatives of the RSS to 0: For the intercept parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_0} RSS &= 0 \\\\ -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum y_i - \\sum \\hat{\\beta}_0 - \\sum \\hat{\\beta}_1 x &= 0 \\\\ n\\bar{y} -n\\hat{\\beta}_0 - n\\hat{\\beta}_1 \\bar{x} &= 0 \\\\ \\end{aligned} \\] \\[\\therefore \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\] For the slope parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_1} RSS &= 0 \\\\ -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum (y_i x_i) - \\hat{\\beta}_0 \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} + n\\hat{\\beta}_1 \\bar{x}^2 - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} &= \\hat{\\beta}_1 \\sum (x^2_i) - n\\bar{x}^2 \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 = \\frac{\\sum (x_i y_i) - n\\bar{x}\\bar{y}}{\\sum (x_i^2) - n\\bar{x}^2} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} = r * \\frac{s_y}{s_x} \\] This results in the following fitted regression model, which can be graphically expressed as a Regression Line : \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] Note that it should \\(\\hat{\\varepsilon}\\) in the above image, not \\(e_i\\) . OLS Properties By re-arranging the formula for \\(\\hat{\\beta}_0\\) , we can show that \\((\\bar{x}, \\bar{y})\\) always lies on the fitted regression model: \\[ \\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x} \\] Additionally, since the parameters are estimated through minimization, the resulting model must always fulfil the two first order conditions . The model thus has \\(n-2\\) degrees of freedom to reflect these \"constraints\". \\(\\beta_0\\) FOC \\(\\beta_1\\) FOC \\(\\frac{\\partial}{\\partial \\beta_0} = -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\frac{\\partial}{\\partial \\beta_1} = -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\sum \\hat{\\varepsilon_i} = 0\\) \\(\\sum x_i \\hat{\\varepsilon_i} = 0\\) Residuals are negatively correlated Residuals and Independendent variables are uncorrelated Using the above results, we can also show the following that the mean of the regression outputs is equal to the mean of the population: \\[ \\begin{aligned} \\hat{\\varepsilon_i} &= y_i - \\hat{y_i} \\\\ \\sum \\hat{\\varepsilon_i} &= \\sum y_i - \\hat{y_i} \\\\ 0 &= n\\bar{y} - n\\bar{\\hat{y}} \\\\ \\bar{\\hat{y}} &= \\bar{y} \\\\ \\end{aligned} \\] Goodness of Fit Ideally, the regression should fit the sample closely, having as small as residuals as possible . The size of all the residuals in the model can be summarized through the RSS. The lower the RSS, the better the fit of the model. Recall that the residuals naturally sum to 0 under OLS - the residuals are thus squared to remove the sign so that they can be summed together. \\[ RSS = \\sum (y_i - \\hat{y})^2 \\] However, the SSR on its own is hard to intepret as there is no indication of how low or high it actually is. Thus, the Total Sum of Squares (TSS) can be used as a benchmark for the RSS as it is at least equal to or higher than the RSS. The TSS represents the RSS for a Null Regression - a model with containing only the intercept parameter . The output of this regression is always the sample mean \\(\\bar{y}\\) , which is used for the computation of its residuals. It represents the worst possible model which thus has the highest possible RSS . The lower the RSS compared to the TSS, the better the fit of the model. \\[ TSS = \\sum (y_i - \\bar{y}) \\] Null Model Consider a regression with only the intercept; \\(\\beta_1 = 0\\) . It is known as the Null Model as there are no independent variables used. \\[ y = \\beta_0 \\] We can estimate \\(\\hat{\\beta_0}\\) using OLS, which results in the following result: \\[ \\begin{aligned} -2 \\sum (y_i - \\hat{\\beta_0}) &= 0 \\\\ n \\bar{y} - n \\hat{\\beta_0} &= 0 \\\\ \\hat{\\beta_0} &= \\bar{y} \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{y} = \\bar{y} \\] Sum of Squares The TSS can be further decomposed into two more parts for analysis: \\[ \\begin{aligned} TSS &= \\sum (y_i - \\bar{y})^2 \\\\ TSS &= \\sum[(y_i - \\hat{y}) + (\\hat{y}-\\bar{y})]^2 \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 2 \\sum((y_i - \\hat{y})(\\hat{y}-\\bar{y})) \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 0 \\\\ TSS &= RSS + RegSS \\end{aligned} \\] Residual SS (RSS) Regression SS (RegSS) \\(\\sum(\\hat{y}-\\bar{y})^2\\) \\(\\sum(y_i - \\hat{y})^2\\) Variation of the observed values about the regression Variation of the regression output about the sample mean Variation explained by the regression Variation unexplained by the regression Note that it can also be expressed in terms of the Slope Parameter : \\[ \\begin{align} RegSS &= \\sum(\\hat{y}-\\bar{y})^2 \\\\ &= \\sum(\\hat{\\beta}_0 + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum(\\bar{y} - \\beta_1 \\bar{x} + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum[\\hat{\\beta}_1 (x_i - \\bar{x})]^2 \\\\ &= \\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2 \\\\ \\end{align} \\] The Coefficient of Determination \\(R^2\\) can also be used to demonstrate goodness of fit. It measures the proportion of variation explained by the regression model: \\[ R^2 = \\frac{RegSS}{TSS} = 1 - \\frac{RSS}{TSS} \\] Building off the above expression, it can also be expressed in terms of the Sample Correlation : \\[ \\begin{align} R^2 &= \\frac{\\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\\right)^2 \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^4} \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x}) \\sum (y_i - \\bar{y})}\\right)^2 \\\\ &= r_{y,x}^2 \\end{align} \\] Degrees of Freedom The TSS is based on the naive model with only the intercept parameter, thus, it is subject to the single contraint of all residuals summing to 0. The TSS thus has \\(n-1\\) degrees of freedom . The RSS is based on the SLR with both the intercept and slope parameter, thus it is subject to an additional constraint of the sumproduct of all residuals and independent variables being 0. The RSS thus has \\(n-2\\) degrees of freedom. The sum of the RSS and RegSS is equal to the TSS, thus the sum of their degrees of freedom must also be equal to that of the TSS. By working backwards, the RegSS thus has only \\(1\\) degree of freedom . Mean Squared The division of any Sum of Square (TSS, RSS, RegSS) by its Degrees of Freedom is known as the Mean Squared (MS), which is a measure of its average variance . The MS of the TSS is the Unbiased Estimator for Population Variance , which is why this process is known as the Analysis of Variance , as it decomposes the variance of \\(Y\\) into its constituent components: \\[ s = \\frac{TSS}{n-1} \\] The MS of the RSS is known the Mean Squared Residuals , often also referred to as the Mean Squared Error , as it is an estimate for the population variance of the error \\(\\sigma^2\\) : \\[ MS_{\\text{Residuals}} = \\frac{RSS}{n-2} \\] The MS of the RegSS is known as the Mean Squared Regression , which represents the proportion of variance explained per \\(X\\) used. \\[ MS_{\\text{Regression}} = \\frac{RegSS}{1} \\] F Statistic The ANOVA parameters can be used to conduct a hypothesis test to determine if there is in fact a relationship between \\(X\\) and \\(Y\\) : \\(H_0\\) : \\(\\beta_1 = 0\\) \\(H_1\\) : \\(\\beta_1 \\ne 0\\) Under the null hypothesis, there should be no difference between the assumed model and a null model as both only contain the intercept parameter, thus \\(TSS = RSS\\) , where \\(RegSS = 0\\) . Thus, the F-statistic is testing for the equality of variance between the TSS and RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_1 \\ne 0\\) . The F-statistic can be constructed using the sum of squares: \\[ \\begin{aligned} F &= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &= \\frac{RegSS/1}{RSS/(n-2)} \\\\ &= \\frac{(TSS - RSS)/1}{RSS/(n-2)} \\\\ &= (n-2) * \\frac{R^2}{1-R^2}, \\text{divide both by TSS} \\end{aligned} \\] Similar to the variance test, it can be shown that under the null, this test-statistic follows an F distribution with \\(1\\) and \\(n-2\\) degrees of freedom: \\[ \\begin{aligned} T &= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &= \\frac{\\sigma^2_{RegSS} \\frac{MS_{Reg}}{\\sigma^2_{RegSS}}}{\\sigma^2_{RSS} \\frac{MS_{RSS}}{\\sigma^2_{RSS}}} \\\\ &= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\frac{1 * MS_{Reg}}{\\sigma^2_{RegSS}} * \\frac{1}{1}}{\\frac{(n-2) * MS_{RegSS}}{\\sigma^2_{RegSS}}* \\frac{1}{n-2}} \\\\ &= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\chi_1}{\\chi_{n-2}} * (n-2) \\\\ &= 1 * F_{1, n-2} * (n-2) \\\\ &= F_{1, n-2} * (n-2) \\end{aligned} \\] \\[ \\therefore F \\sim F_{1, n-2} \\] ANOVA Table All the above information is then summarized in a table for convenience, known as the ANOVA Table : Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(1\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-2\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) - Statistical Inference Sampling Distributions Since the errors are assumed to be normally distributed, then \\(Y\\) is assumed to be normally distributed as well. Since \\(Y\\) is a linear combination of the regression parameters, then the parameters (& their estimates) are normally distributed as well. Both estimates can be expressed in another form that makes it more convenient to find their expectation & variances. \\[ \\begin{aligned} \\hat{\\beta}_1 &= \\frac{\\sum [(x_i - \\bar{x})(y_i - \\bar{y})]}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})y_i}{\\sum (x^2_i - \\bar{x})} - \\frac{\\bar{y} \\sum (x_i - \\bar{x})}{\\sum (x^2_i - \\bar{x})} \\\\ &= \\sum \\frac{(x_i - \\bar{x})}{(x^2_i - \\bar{x})}* y_i - 0 \\\\ &= \\sum w_i * y_i \\\\ \\\\ \\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\ &= \\frac{1}{n} \\sum y_i - \\bar{x} \\sum w_i * y_i \\\\ &= \\sum y_i (\\frac{1}{n} - \\bar{x}w_i) \\end{aligned} \\] \\(w_i\\) is a sort of \"weight\" parameter of the sum of squares. It has three interesting properties that makes it useful: \\[ \\begin{aligned} \\sum w_i &= \\frac{\\sum (x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{n\\bar{x}-n\\bar{x}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{0}{\\sum (x^2_i - \\bar{x})} \\\\ &= 0 \\\\ \\\\ \\sum w_i x_i &= \\frac{\\sum x_i(x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x^2_i - \\bar{x} \\sum x_i)}{\\sum x^2_i - 2\\bar{x}\\sum x_i + \\sum \\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - \\bar{x}(n \\bar{x})}{\\sum x^2_i - 2\\bar{x}(n\\bar{x}) + n\\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - 2n\\bar{x}^2 + n\\bar{x}^2} \\\\ &=\\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - n\\bar{x}^2} \\\\ &= 1 \\\\ \\\\ \\sum w_i^2 &= \\frac{\\sum (x_i - \\bar{x})^2}{\\sum (x_i - \\bar{x})^4} \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i) &= n(\\frac{1}{n}) - \\bar{x} \\sum w_i \\\\ &= 1 - 0 \\\\ &= 1 \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i &= \\frac{1}{n} \\sum x_i - \\bar{x} \\sum w_i x_i \\\\ &= \\frac{1}{n} (n\\bar{x}) - \\bar{x} (1) \\\\ &= \\bar{x} - \\bar{x} \\\\ &= 0 \\end{aligned} \\] Using this, the Expectation & Variance can be determined: \\[ \\begin{aligned} E(\\hat{\\beta}_1) &= \\sum w_i E(y_i) \\\\ &= \\sum w_i E(\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum w_i + \\beta_1 \\sum w_i x_i \\\\ &= \\beta_0 (0) + \\beta_1 (1) \\\\ &= \\beta_1\\\\ \\\\ Var(\\hat{\\beta}_1) &= Var(\\sum w_i y_i) \\\\ &= \\sum w_i^2 Var (y_i) \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2} \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 \\sim N(\\beta_1, \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2}) \\] \\[ \\begin{aligned} E(\\hat{\\beta}_0) &= \\sum (\\frac{1}{n} - \\bar{x}w_i) E(y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i) (\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum (\\frac{1}{n} - \\bar{x}w_i) + \\beta_1 \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i \\\\ &= \\beta_0 (1) + \\beta_1 (0) \\\\ &= \\beta_0 \\\\ \\\\ Var(\\hat{\\beta}_0) &= Var(\\sum (\\frac{1}{n} - \\bar{x}w_i)y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i)^2 Var (y_i) \\\\ &= \\sigma^2 \\sum (\\frac{1}{n^2} -\\frac{2\\bar{x}w_i}{n} + \\bar{x}^2 w_i^2) \\\\ &= \\sigma^2 (\\sum \\frac{1}{n^2} - \\frac{2\\bar{x}}{n} \\sum w_i + \\bar{x}^2 \\sum w_i^2) \\\\ &= \\sigma^2 [n(\\frac{1}{n^2}) - \\frac{2\\bar{x}}{n} (0) + \\bar{x}^2 (\\frac{1}{\\sum (x_i - \\bar{x})^2})] \\\\ &= \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2}) \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_0 \\sim N(\\beta_0, \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2})) \\] Hypothesis Testing Since the regression parameters are normally distributed, a z-statistic can also be used to conduct the tests. However, since the population variance is not known, a t-statistic is used instead: \\[ \\begin{aligned} t &= \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\end{aligned} \\] Since the population variance is estimated by the MSE which has \\(n-2\\) degrees of freedom, the corresponding chi-squared and hence t-distribution has \\(n-2\\) degrees of freedom as well. \\[ \\begin{aligned} \\hat{Var}(\\hat{\\beta_1}) &= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} * \\frac{\\frac{1}{n-1}}{\\frac{1}{n-1}} \\\\ &= \\frac{MS_{RSS}}{(n-1) s^2} \\end{aligned} \\] \\[ t \\sim t_{n-2} \\] Since the square of the t-statistic is the F-statistic, both are equivalent ways of doing so and will always lead to the same conclusions. \\[ t^2 \\sim F_{1, n-2} \\] Confidence Intervals Since the distribution of t-statistic is known, it can be used to determine the confidence interval of the estimate: \\[ P\\left(\\text{Margin of Error} < \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} < \\text{Margin of Error}\\right) = 1 - \\alpha \\] \\[ \\text{Confidence Interval} = \\hat{\\beta}_1 \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_1}} \\] Prediction Intervals Consider the Prediction Error of the SLR model: \\[ y_* - \\hat{y_*} = \\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\] Since both \\(y_*\\) and \\(\\hat{y_*}\\) are normally distributed, the prediction errors are normally distributed as well: \\[ \\begin{aligned} E(y_* - \\hat{y_*}) &= E[\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)]] \\\\ &= 0 + \\beta_0 + \\beta_1 E(x_*) - \\beta_0 - \\beta_1 E(x_*) \\\\ &= 0 \\\\ \\\\ Var(y_* - \\hat{y_*}) &= Var(\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= Var(\\varepsilon_*) + Var[(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= ... \\\\ &= \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}] \\end{aligned} \\] \\[ \\therefore y_* - \\hat{y_*} \\sim N\\left(0, \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}]\\right) \\] Similar to before, since the population variance is unknown, it can be approximated using the MSE. Thus, a t-statistic constructed from this sample will have a t-distribution , allowing the following prediction interval to be calculated: \\[ \\text{Prediction Interval} = \\hat{y}_* \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\] Notice that the standard error of the prediction interval increases as \\(x_*\\) moves further away \\(\\bar{x}\\) , indicating that the predictions become less accurate for those values.","title":"Simple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#simple-linear-regression","text":"Simple Linear Regression (SLR) assumes a Linear Relationship between a Numeric DV and single continuous quantitative IV. The model is considered to be simple because it only contains a single independent variable. \\[ E(Y|X) = \\beta_0 + \\beta_1 X \\] \\(\\beta_0\\) \\(\\beta_1\\) Expected value of \\(Y\\) when \\(X = 0\\) Change in the expected value of \\(Y\\) given a one unit increase in \\(X\\) Intercept Parameter Slope Parameter Each observation can also be expressed as sum of the regression and its error term: \\[ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\]","title":"Simple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#ordinary-least-squares","text":"SLR parameters are estimated using the Ordinary Least Squares method, which minimizes the Sum of Squared Residuals of the fitted model. It is commonly referred to as the Residual Sum Squared (RSS). \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 \\] The minimization is solved through calculus by setting the partial derivatives of the RSS to 0: For the intercept parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_0} RSS &= 0 \\\\ -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum y_i - \\sum \\hat{\\beta}_0 - \\sum \\hat{\\beta}_1 x &= 0 \\\\ n\\bar{y} -n\\hat{\\beta}_0 - n\\hat{\\beta}_1 \\bar{x} &= 0 \\\\ \\end{aligned} \\] \\[\\therefore \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\] For the slope parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_1} RSS &= 0 \\\\ -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum (y_i x_i) - \\hat{\\beta}_0 \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} + n\\hat{\\beta}_1 \\bar{x}^2 - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} &= \\hat{\\beta}_1 \\sum (x^2_i) - n\\bar{x}^2 \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 = \\frac{\\sum (x_i y_i) - n\\bar{x}\\bar{y}}{\\sum (x_i^2) - n\\bar{x}^2} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} = r * \\frac{s_y}{s_x} \\] This results in the following fitted regression model, which can be graphically expressed as a Regression Line : \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] Note that it should \\(\\hat{\\varepsilon}\\) in the above image, not \\(e_i\\) .","title":"Ordinary Least Squares"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#ols-properties","text":"By re-arranging the formula for \\(\\hat{\\beta}_0\\) , we can show that \\((\\bar{x}, \\bar{y})\\) always lies on the fitted regression model: \\[ \\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x} \\] Additionally, since the parameters are estimated through minimization, the resulting model must always fulfil the two first order conditions . The model thus has \\(n-2\\) degrees of freedom to reflect these \"constraints\". \\(\\beta_0\\) FOC \\(\\beta_1\\) FOC \\(\\frac{\\partial}{\\partial \\beta_0} = -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\frac{\\partial}{\\partial \\beta_1} = -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\sum \\hat{\\varepsilon_i} = 0\\) \\(\\sum x_i \\hat{\\varepsilon_i} = 0\\) Residuals are negatively correlated Residuals and Independendent variables are uncorrelated Using the above results, we can also show the following that the mean of the regression outputs is equal to the mean of the population: \\[ \\begin{aligned} \\hat{\\varepsilon_i} &= y_i - \\hat{y_i} \\\\ \\sum \\hat{\\varepsilon_i} &= \\sum y_i - \\hat{y_i} \\\\ 0 &= n\\bar{y} - n\\bar{\\hat{y}} \\\\ \\bar{\\hat{y}} &= \\bar{y} \\\\ \\end{aligned} \\]","title":"OLS Properties"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#goodness-of-fit","text":"Ideally, the regression should fit the sample closely, having as small as residuals as possible . The size of all the residuals in the model can be summarized through the RSS. The lower the RSS, the better the fit of the model. Recall that the residuals naturally sum to 0 under OLS - the residuals are thus squared to remove the sign so that they can be summed together. \\[ RSS = \\sum (y_i - \\hat{y})^2 \\] However, the SSR on its own is hard to intepret as there is no indication of how low or high it actually is. Thus, the Total Sum of Squares (TSS) can be used as a benchmark for the RSS as it is at least equal to or higher than the RSS. The TSS represents the RSS for a Null Regression - a model with containing only the intercept parameter . The output of this regression is always the sample mean \\(\\bar{y}\\) , which is used for the computation of its residuals. It represents the worst possible model which thus has the highest possible RSS . The lower the RSS compared to the TSS, the better the fit of the model. \\[ TSS = \\sum (y_i - \\bar{y}) \\]","title":"Goodness of Fit"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#null-model","text":"Consider a regression with only the intercept; \\(\\beta_1 = 0\\) . It is known as the Null Model as there are no independent variables used. \\[ y = \\beta_0 \\] We can estimate \\(\\hat{\\beta_0}\\) using OLS, which results in the following result: \\[ \\begin{aligned} -2 \\sum (y_i - \\hat{\\beta_0}) &= 0 \\\\ n \\bar{y} - n \\hat{\\beta_0} &= 0 \\\\ \\hat{\\beta_0} &= \\bar{y} \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{y} = \\bar{y} \\]","title":"Null Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#sum-of-squares","text":"The TSS can be further decomposed into two more parts for analysis: \\[ \\begin{aligned} TSS &= \\sum (y_i - \\bar{y})^2 \\\\ TSS &= \\sum[(y_i - \\hat{y}) + (\\hat{y}-\\bar{y})]^2 \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 2 \\sum((y_i - \\hat{y})(\\hat{y}-\\bar{y})) \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 0 \\\\ TSS &= RSS + RegSS \\end{aligned} \\] Residual SS (RSS) Regression SS (RegSS) \\(\\sum(\\hat{y}-\\bar{y})^2\\) \\(\\sum(y_i - \\hat{y})^2\\) Variation of the observed values about the regression Variation of the regression output about the sample mean Variation explained by the regression Variation unexplained by the regression Note that it can also be expressed in terms of the Slope Parameter : \\[ \\begin{align} RegSS &= \\sum(\\hat{y}-\\bar{y})^2 \\\\ &= \\sum(\\hat{\\beta}_0 + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum(\\bar{y} - \\beta_1 \\bar{x} + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum[\\hat{\\beta}_1 (x_i - \\bar{x})]^2 \\\\ &= \\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2 \\\\ \\end{align} \\] The Coefficient of Determination \\(R^2\\) can also be used to demonstrate goodness of fit. It measures the proportion of variation explained by the regression model: \\[ R^2 = \\frac{RegSS}{TSS} = 1 - \\frac{RSS}{TSS} \\] Building off the above expression, it can also be expressed in terms of the Sample Correlation : \\[ \\begin{align} R^2 &= \\frac{\\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\\right)^2 \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^4} \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x}) \\sum (y_i - \\bar{y})}\\right)^2 \\\\ &= r_{y,x}^2 \\end{align} \\]","title":"Sum of Squares"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#degrees-of-freedom","text":"The TSS is based on the naive model with only the intercept parameter, thus, it is subject to the single contraint of all residuals summing to 0. The TSS thus has \\(n-1\\) degrees of freedom . The RSS is based on the SLR with both the intercept and slope parameter, thus it is subject to an additional constraint of the sumproduct of all residuals and independent variables being 0. The RSS thus has \\(n-2\\) degrees of freedom. The sum of the RSS and RegSS is equal to the TSS, thus the sum of their degrees of freedom must also be equal to that of the TSS. By working backwards, the RegSS thus has only \\(1\\) degree of freedom .","title":"Degrees of Freedom"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#mean-squared","text":"The division of any Sum of Square (TSS, RSS, RegSS) by its Degrees of Freedom is known as the Mean Squared (MS), which is a measure of its average variance . The MS of the TSS is the Unbiased Estimator for Population Variance , which is why this process is known as the Analysis of Variance , as it decomposes the variance of \\(Y\\) into its constituent components: \\[ s = \\frac{TSS}{n-1} \\] The MS of the RSS is known the Mean Squared Residuals , often also referred to as the Mean Squared Error , as it is an estimate for the population variance of the error \\(\\sigma^2\\) : \\[ MS_{\\text{Residuals}} = \\frac{RSS}{n-2} \\] The MS of the RegSS is known as the Mean Squared Regression , which represents the proportion of variance explained per \\(X\\) used. \\[ MS_{\\text{Regression}} = \\frac{RegSS}{1} \\]","title":"Mean Squared"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#f-statistic","text":"The ANOVA parameters can be used to conduct a hypothesis test to determine if there is in fact a relationship between \\(X\\) and \\(Y\\) : \\(H_0\\) : \\(\\beta_1 = 0\\) \\(H_1\\) : \\(\\beta_1 \\ne 0\\) Under the null hypothesis, there should be no difference between the assumed model and a null model as both only contain the intercept parameter, thus \\(TSS = RSS\\) , where \\(RegSS = 0\\) . Thus, the F-statistic is testing for the equality of variance between the TSS and RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_1 \\ne 0\\) . The F-statistic can be constructed using the sum of squares: \\[ \\begin{aligned} F &= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &= \\frac{RegSS/1}{RSS/(n-2)} \\\\ &= \\frac{(TSS - RSS)/1}{RSS/(n-2)} \\\\ &= (n-2) * \\frac{R^2}{1-R^2}, \\text{divide both by TSS} \\end{aligned} \\] Similar to the variance test, it can be shown that under the null, this test-statistic follows an F distribution with \\(1\\) and \\(n-2\\) degrees of freedom: \\[ \\begin{aligned} T &= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &= \\frac{\\sigma^2_{RegSS} \\frac{MS_{Reg}}{\\sigma^2_{RegSS}}}{\\sigma^2_{RSS} \\frac{MS_{RSS}}{\\sigma^2_{RSS}}} \\\\ &= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\frac{1 * MS_{Reg}}{\\sigma^2_{RegSS}} * \\frac{1}{1}}{\\frac{(n-2) * MS_{RegSS}}{\\sigma^2_{RegSS}}* \\frac{1}{n-2}} \\\\ &= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\chi_1}{\\chi_{n-2}} * (n-2) \\\\ &= 1 * F_{1, n-2} * (n-2) \\\\ &= F_{1, n-2} * (n-2) \\end{aligned} \\] \\[ \\therefore F \\sim F_{1, n-2} \\]","title":"F Statistic"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#anova-table","text":"All the above information is then summarized in a table for convenience, known as the ANOVA Table : Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(1\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-2\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) -","title":"ANOVA Table"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#statistical-inference","text":"","title":"Statistical Inference"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#sampling-distributions","text":"Since the errors are assumed to be normally distributed, then \\(Y\\) is assumed to be normally distributed as well. Since \\(Y\\) is a linear combination of the regression parameters, then the parameters (& their estimates) are normally distributed as well. Both estimates can be expressed in another form that makes it more convenient to find their expectation & variances. \\[ \\begin{aligned} \\hat{\\beta}_1 &= \\frac{\\sum [(x_i - \\bar{x})(y_i - \\bar{y})]}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})y_i}{\\sum (x^2_i - \\bar{x})} - \\frac{\\bar{y} \\sum (x_i - \\bar{x})}{\\sum (x^2_i - \\bar{x})} \\\\ &= \\sum \\frac{(x_i - \\bar{x})}{(x^2_i - \\bar{x})}* y_i - 0 \\\\ &= \\sum w_i * y_i \\\\ \\\\ \\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\ &= \\frac{1}{n} \\sum y_i - \\bar{x} \\sum w_i * y_i \\\\ &= \\sum y_i (\\frac{1}{n} - \\bar{x}w_i) \\end{aligned} \\] \\(w_i\\) is a sort of \"weight\" parameter of the sum of squares. It has three interesting properties that makes it useful: \\[ \\begin{aligned} \\sum w_i &= \\frac{\\sum (x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{n\\bar{x}-n\\bar{x}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{0}{\\sum (x^2_i - \\bar{x})} \\\\ &= 0 \\\\ \\\\ \\sum w_i x_i &= \\frac{\\sum x_i(x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x^2_i - \\bar{x} \\sum x_i)}{\\sum x^2_i - 2\\bar{x}\\sum x_i + \\sum \\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - \\bar{x}(n \\bar{x})}{\\sum x^2_i - 2\\bar{x}(n\\bar{x}) + n\\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - 2n\\bar{x}^2 + n\\bar{x}^2} \\\\ &=\\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - n\\bar{x}^2} \\\\ &= 1 \\\\ \\\\ \\sum w_i^2 &= \\frac{\\sum (x_i - \\bar{x})^2}{\\sum (x_i - \\bar{x})^4} \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i) &= n(\\frac{1}{n}) - \\bar{x} \\sum w_i \\\\ &= 1 - 0 \\\\ &= 1 \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i &= \\frac{1}{n} \\sum x_i - \\bar{x} \\sum w_i x_i \\\\ &= \\frac{1}{n} (n\\bar{x}) - \\bar{x} (1) \\\\ &= \\bar{x} - \\bar{x} \\\\ &= 0 \\end{aligned} \\] Using this, the Expectation & Variance can be determined: \\[ \\begin{aligned} E(\\hat{\\beta}_1) &= \\sum w_i E(y_i) \\\\ &= \\sum w_i E(\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum w_i + \\beta_1 \\sum w_i x_i \\\\ &= \\beta_0 (0) + \\beta_1 (1) \\\\ &= \\beta_1\\\\ \\\\ Var(\\hat{\\beta}_1) &= Var(\\sum w_i y_i) \\\\ &= \\sum w_i^2 Var (y_i) \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2} \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 \\sim N(\\beta_1, \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2}) \\] \\[ \\begin{aligned} E(\\hat{\\beta}_0) &= \\sum (\\frac{1}{n} - \\bar{x}w_i) E(y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i) (\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum (\\frac{1}{n} - \\bar{x}w_i) + \\beta_1 \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i \\\\ &= \\beta_0 (1) + \\beta_1 (0) \\\\ &= \\beta_0 \\\\ \\\\ Var(\\hat{\\beta}_0) &= Var(\\sum (\\frac{1}{n} - \\bar{x}w_i)y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i)^2 Var (y_i) \\\\ &= \\sigma^2 \\sum (\\frac{1}{n^2} -\\frac{2\\bar{x}w_i}{n} + \\bar{x}^2 w_i^2) \\\\ &= \\sigma^2 (\\sum \\frac{1}{n^2} - \\frac{2\\bar{x}}{n} \\sum w_i + \\bar{x}^2 \\sum w_i^2) \\\\ &= \\sigma^2 [n(\\frac{1}{n^2}) - \\frac{2\\bar{x}}{n} (0) + \\bar{x}^2 (\\frac{1}{\\sum (x_i - \\bar{x})^2})] \\\\ &= \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2}) \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_0 \\sim N(\\beta_0, \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2})) \\]","title":"Sampling Distributions"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#hypothesis-testing","text":"Since the regression parameters are normally distributed, a z-statistic can also be used to conduct the tests. However, since the population variance is not known, a t-statistic is used instead: \\[ \\begin{aligned} t &= \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\end{aligned} \\] Since the population variance is estimated by the MSE which has \\(n-2\\) degrees of freedom, the corresponding chi-squared and hence t-distribution has \\(n-2\\) degrees of freedom as well. \\[ \\begin{aligned} \\hat{Var}(\\hat{\\beta_1}) &= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} * \\frac{\\frac{1}{n-1}}{\\frac{1}{n-1}} \\\\ &= \\frac{MS_{RSS}}{(n-1) s^2} \\end{aligned} \\] \\[ t \\sim t_{n-2} \\] Since the square of the t-statistic is the F-statistic, both are equivalent ways of doing so and will always lead to the same conclusions. \\[ t^2 \\sim F_{1, n-2} \\]","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#confidence-intervals","text":"Since the distribution of t-statistic is known, it can be used to determine the confidence interval of the estimate: \\[ P\\left(\\text{Margin of Error} < \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} < \\text{Margin of Error}\\right) = 1 - \\alpha \\] \\[ \\text{Confidence Interval} = \\hat{\\beta}_1 \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_1}} \\]","title":"Confidence Intervals"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#prediction-intervals","text":"Consider the Prediction Error of the SLR model: \\[ y_* - \\hat{y_*} = \\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\] Since both \\(y_*\\) and \\(\\hat{y_*}\\) are normally distributed, the prediction errors are normally distributed as well: \\[ \\begin{aligned} E(y_* - \\hat{y_*}) &= E[\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)]] \\\\ &= 0 + \\beta_0 + \\beta_1 E(x_*) - \\beta_0 - \\beta_1 E(x_*) \\\\ &= 0 \\\\ \\\\ Var(y_* - \\hat{y_*}) &= Var(\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= Var(\\varepsilon_*) + Var[(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= ... \\\\ &= \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}] \\end{aligned} \\] \\[ \\therefore y_* - \\hat{y_*} \\sim N\\left(0, \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}]\\right) \\] Similar to before, since the population variance is unknown, it can be approximated using the MSE. Thus, a t-statistic constructed from this sample will have a t-distribution , allowing the following prediction interval to be calculated: \\[ \\text{Prediction Interval} = \\hat{y}_* \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\] Notice that the standard error of the prediction interval increases as \\(x_*\\) moves further away \\(\\bar{x}\\) , indicating that the predictions become less accurate for those values.","title":"Prediction Intervals"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/","text":"Multiple Linear Regression The natural extension of the SLR model is to include more than one independent variable , which thus results in the more generalized Multiple Linear Regression (MLR) model. \\[ E(Y|X_1, ... X_p) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_{p}X_{p} \\] Unlike the SLR which studies how each individual IV influences the DV, the goal of the MLR model is to study how all the IVs operate together to influence the DV. \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) when \\(X_1 = X_2 = ... = 0\\) Change in \\(E(Y)\\) given a one unit increase in \\(X_j\\) , holding all other \\(X\\) 's constant Intercept Parameter \"Slope\" Parameter For avoidance of doubt, the subscript \\(i\\) will be used to denote observations while \\(j\\) will be used to denote independent variables. Every observation can also be expressed as the sum of the regression and the error term. However, due to the multi-dimensional nature of the model, it is commonly expressed in matrix notation: \\[ \\begin{aligned} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} &= \\begin{pmatrix} 1 & x_{11} & x_{12} & ... & x_{1p} \\\\ 1 & x_{21} & x_{22} & ... & x_{2p} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & ... & x_{np} \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix} + \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix} \\\\ \\boldsymbol{y} &= \\boldsymbol{X\\beta + \\varepsilon} \\end{aligned} \\] Ordinary Least Squares Similar to the SLR model, the regression parameters can be found by minimizing the sum of squared residuals: \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_p x_ip)]^2 \\] There are \\(p+1\\) FOC equations to solve through the minimization, with the additional one reflecting the intercept parameter. It is difficult to algebraically solve this system of equations, thus there is no closed form tsolution for each individual paramater. Instead, here is a vector solution for all of the parameters: \\[ \\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\hat{\\beta_0} \\\\ \\hat{\\beta_1} \\\\ \\vdots \\\\ \\hat{\\beta_p} \\end{pmatrix} = (\\boldsymbol{X'X})^{-1}\\boldsymbol{X'y} \\] Note that since there are \\(p+1\\) equations that must be solved, the model has \\(n-p+1\\) degrees of freedom. Following the same logic, there must be at least \\(p+1\\) observations in order to solve the equations and hence construct the model. This results in the following fitted regression model, which can be graphically expressed as a Regression Plane : \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_P x_ip \\] Manual Computation The tricky part is that \\((\\boldsymbol{X'X})^{-1}\\) is hard to compute by manually , except for the special case where \\(p=1\\) (SLR). Thus, it is likely that the parameters will be provided by the question. If required to compute them manually, then \\((\\boldsymbol{X'X})^{-1}\\) is likely to be provided. The remaining \\(\\boldsymbol{X'y}\\) still needs to be computed and put together to obtain the regression parameters. However, if the model has \\(p=2\\) but no intercept , then \\((\\boldsymbol{X'X})^{-1}\\) is a 2 x 2 Matrix whose inverse can be easily calculated. Similarly, if \\((\\boldsymbol{X'X})^{-1}\\) is a Diagonal Matrix , its inverse can be easily calculated as well. Goodness of Fit The ANOVA for MLR follows the same intuition as the SLR version, adjusted for the new degrees of freedom: Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(p\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-(p+1)\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) - The coefficient of determination still represents the proportion of variance explained by the regression, but has a slightly different formula: \\[ R^2 = \\frac{RegSS}{TSS} = r_{y,\\hat{y}}^2 \\] The multiple IVs of the model are now captured through \\(\\hat{y}\\) instead of \\(x\\) directly. However, the hypothesis test under the MLR is vastly different from the SLR version. Instead of testing if an individual IV is useful, it tests if all the IVs are collectively useful in helping to explain the DV. \\[ \\begin{aligned} H_0 &: \\beta_1 = \\beta_2 = ... = \\beta_p = 0 \\\\ H_1 &: \\text{At least one } \\beta_j \\text{ is non-zero} \\end{aligned} \\] Thus, rejecting the null hypothesis implies that at least one of the IVs used is useful, but does not provide much insight into which of them are useful. Partial F Test A partial F-test can be used to precisely determine which of the IVs are useful in explaining the DV. A regular F test compares the null model with no IVs to the desired model with all the IVs. If the sum of squares are significantly different, then it implies that the additional IVs are jointly useful in explaining \\(Y\\) . The partial F test generalizes this idea. Instead of considering a null model with no IVs, a Reduced Model with a limited number IVs ( \\(q\\) ) is considered instead. Consequently, the desired model is known as the Full Model with all \\(p\\) IVs, where \\(q < p\\) . \\[ \\begin{aligned} H_0: \\beta_{p-q+1} = ... = \\beta_p = 0 \\\\ H_1: \\beta_{p-q+1} = ... = \\beta_p \\ne 0 \\end{aligned} \\] The test is also commonly referred to as the Generalized F test, where the models are referred to as the Restricted and Unrestricted Models. The difference in the RSS between the Full and Reduced Model is known as the Extra Sum of Squares (ExtraSS) . It represents the contribution of the missing variables in explaining the variance of \\(Y\\) . Under the null hypothesis, there should be no difference between the two RSS, and thus \\(ExtraSS = 0\\) . \\[ ExtraSS = RSS_{Reduced} - RSS_{Full} \\] Thus, the Partial F-statistic is testing for the equality of variance between the two RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_{p-q+1} = ... = \\beta_p \\ne 0\\) . \\[ \\begin{aligned} F &= \\frac{MS_{ExtraSS}}{MS_{RSS_{Full}}} \\\\ &= \\frac{ExtraSS/q}{RSS/(n-2)} \\\\ &= \\frac{(RSS_{Reduced} - RSS_{Full})/q}{RSS/(n-2)} \\\\ &= (n-2) * \\frac{(1- R_{Reduced}^2) - (1 - R_{Full}^2)}{1-RSS_{Full}^2}, \\text{divide both by TSS} \\\\ &= (n-2) * \\frac{R_{Full}^2 - R_{Reduced}^2}{1-RSS_{Full}^2} \\end{aligned} \\] Statistical Inference Sampling Distributions Similar to SLR, the regression parameters are normally distributed as well. However, since there are multiple regression parameters, they collectively follow a multivariate normal distribution . \\[ \\hat{\\beta} \\sim N_{p+1}(\\beta, \\sigma^2 (\\boldsymbol{X'X})^{-1}) \\] The variance of the distribution is known as the Variance Covariance Matrix , which provides the covariances between every possible pair of regression parameters. Since the covariance of a variable with itself is its variance, the diagonals are the respective variances of the parameters. Note that the first element of the diagonal is the intercept, thus the variance of the jth IV is the (j+1)th element of the diagonal . \\[ Var(\\hat{\\beta}) = \\begin{pmatrix} Var(\\hat{\\beta}_0) & Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) & ... & Cov(\\hat{\\beta}_0, \\hat{\\beta}_p) \\\\ Cov(\\hat{\\beta}_1, \\hat{\\beta}_0) & Var(\\hat{\\beta}_1) & ... & Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ Cov(\\hat{\\beta}_p, \\hat{\\beta}_0) & Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) & ... & Var(\\hat{\\beta}_p) \\end{pmatrix} \\] Note that the covariances are symmetrical about the diagonal - \\(Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = Cov(\\hat{\\beta}_1, \\hat{\\beta}_0)\\) . Hypothesis Testing Similar to SLR, the t-test can be used to test for the significance of an individual IV, but the intepretation of the test is different from the SLR case. It tests the usefulness of an individual IV in the presence of the other predictors . \\[ \\begin{aligned} H_0: \\beta_j = 0 \\\\ H_1: \\beta_j \\ne 0 \\end{aligned} \\] \\[ t(\\beta_j) = \\frac{\\hat{\\beta_j} - \\beta_j}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\] Recall that the variance of the jth IV si the (j+1)th element of the variance covariance matrix. \\[ t(\\beta_j) \\sim t_{n-p-1} \\] However, this leads to several odd results which needs to be accounted for: Predictor is not significant individually but significant when taken alone . Predictors are not significant individually but significant when taken together . There are now two possible ways to test for the significance of IVs: Conduct a single F-test to test for the joint significance of all IVs Conduct multiple t-tests to test for the joint significance of each IV The problem with the multiple t-test approach lies with the type I error of the tests. For \\(\\alpha = 0.05\\) , the probability of correctly rejecting the null is \\(0.95\\) . Assuming that all the tests are independent, the probability of correctly rejecting all the nulls is \\(0.95^p\\) , which drastically decreases with the number of tests conducted. Given enough predictors, this means that the probability drops to approximately 0, which means that there is bound to be a wrongly rejected null; a type I error is guaranteed even though it was supposed to be limited at a 0.05 chance. The Bonferroni Correction is a method of adjusting \\(\\alpha\\) of each hypothesis test such that the overall type I error is kept at its desired level. However, this has the consequence of increasing the probability of type II errors, which is why it is not popular. The F-test has the advantage of controlling the type I error regardless of the number of predictors , which is why it is preferred for hypothesis testing in the MLR. Confidence Intervals Similar to SLR, the confidence intervals can be constructed using the distribution of the test-statistic: \\[ \\text{Confidence Interval} = \\hat{\\beta}_j \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_j}} \\] Prediction Intervals Unlike in SLR, it is difficult to determine the distribution of the prediction error. Thus, the final result can be found below: \\[ \\begin{aligned} \\text{Prediction Interval} &= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\\\ &= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\sqrt{s^2 [1 + x'_* (\\boldsymbol{X'X})^-1 * x_*]} \\end{aligned} \\] Despite the result looking more complicated, the key takeaway remains the same - the further away \\(x_*\\) is from \\(\\bar{x}\\) , the greater Variations of MLR Qualitative IV The discussion so far has mostly focused on Quantitative IVs, thus this section will explore Qualitative IVs. They can only take values from a list of pre-defined values, known as the Levels of the variable. In a regression context, most qualitative IVs are represented in the form of a Dummy Variable which can only take two possible levels - Yes (1) or No (0). Note that there are other ways of encoding a dummy variable (-1/0/1 etc), but the principles stay large the same. \\[ \\text{Dummy Variable} = \\begin{cases} 1, & \\text{First Level} \\\\ 0, & \\text{Second Level} \\end{cases} \\] In general, \\(n-1\\) dummy variables are needed to represent a qualitative variable with \\(n\\) levels. This is because the status of the last level can be deduced from the other dummy variables . Thus, including a seperate dummy variable for this last level is redundant and will lead to the problem of Collinearity , which will be explored in a later section. For instance, consider four levels (North, East, South & West), represented by the three dummy variables ( \\(N, E , S\\) ). If any of the variables are 1, they represent their respective direction (North, East & South). If all of them are 0, then the direction is the remaining level (West). The last remaining level is often referred to as the Baseline Level as it is the default level of the variable when all other dummies are 0. Any level can be used as the baseline, but the parameters will differ across models with different baselines. \\[ E(Y|X) = \\beta_0 + \\beta_{1, North} x_{North} + \\beta_{2, East} * x_{East} + \\beta_{3, South} * x_{South} \\] \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) at the baseline level Change in \\(E(Y)\\) from the baseline to the chosen level \\(X_1 = X_2 = ... = X_j = 0\\) \\(X_1 = X_2 = ... = 0; X_j = 1\\) Dummy variables are usually used in conjunction with quantitative ones . This essentially creates a \"seperate\" regression model for each of the levels. For the simplest case of one quantiative and one dummy, \\(\\beta_2\\) is the difference in the intercept of the two resulting SLR models. \\[ E(Y|X) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\] \\[ E(Y|X) = \\begin{cases} (\\beta_0 + \\beta_2) + \\beta_1 x_1, & x_2 = 1 \\\\ \\beta_0 + \\beta_1 x_1, & x_2 = 0 \\end{cases} \\] Interaction Model So far, it was assumed that each IV had an independent effect on the DV. However, IVs may interact to produce a joint effect on the DV, where the effect of one IV depends on the value of another IV. For instance, the production of a factory may depend on the number of Machines and Workers. However, the more machines there are, the greater the effect of an additional worker . Thus, this interaction effect can be captured through an Interaction Variable , which is the product of both IVs: \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 \\\\ &= \\beta_0 + (\\beta_1 + \\beta_3 x_2)x_1 + \\beta_2 x_2 \\\\ &= \\beta_0 + \\beta_1 x_1 + (\\beta_2 + \\beta_3 x_1) x_2 \\\\ \\end{aligned} \\] A one unit increase in \\(x_1\\) will increase E(Y) by \\(\\beta_1 + \\beta_3 x_2\\) , which depends on the value of \\(x_2\\) as well, which is is why they interact with each another. Phrased another way, for every one unit increase in \\(x_2\\) , the change in E(Y) from a unit increase in \\(x_1\\) increases by \\(\\beta_3\\) . Something unusual to take note of is that the Interaction Variable tests significant but the constituent variables do not. In this case, it is common practice to retain both the interaction and the consistuent variables in the model. This is practice is known as the Hierarchical Principle . Models containing dummy variables can also have an interaction effect. Building off the example from the previous section, \\(\\beta_2\\) is still the difference in the intercept but with the new \\(\\beta_3\\) being the difference in slopes of the two resulting SLR models. \\[ E(Y|X) = \\begin{cases} (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3)x_1, & x_2 = 1 \\\\ \\beta_0 + \\beta_1 x_1, & x_2 = 0 \\end{cases} \\] Piecewise Model If the DV has an abrupt change in behaviour over different values of the IVs, it can be accounted for the through the use of a Piecewise Regression . The first method of creating a piecewise regression involves the use of an Indicator Function . It is essentially a dummy variable which depends on the value of the other IVs. \\[ z_{\\{x>=c\\}} = \\begin{cases} 0, & x < c \\\\ 1, & x \\ge c \\end{cases} \\] \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 z(x-c) \\\\ &= \\begin{cases} \\beta_0 + \\beta_1 x_1, & x < c \\\\ (\\beta_0 - \\beta_2c) + (\\beta_1 + \\beta_2) x_1, & x \\ge c \\end{cases} \\end{aligned} \\] Note that \\(z(x-c)\\) is treated as a distinct IV and hence can be equivalently expressed as \\(x_2\\) ; the full notation is used here for clarity. \\(c\\) is the value at which the DV abruptly changes in behaviour, known as a Kink in the graph, which continuously connects the two regression lines. The other method is to use an interaction variable instead. Similar to how the interaction variables resulted in the model to \"split\", the model now splits at \\(x = c\\) , resulting in a non-continuous gap. \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2z + \\beta_3 zx \\\\ &= \\begin{cases} \\beta_0 + \\beta_1 x_1, & x < c \\\\ (\\beta_0 - \\beta_2) + (\\beta_1 + \\beta_3) x_1, & x \\ge c \\end{cases} \\end{aligned} \\] Polynomial Model If the relationship between the DV and IV is complex (non-linear), then a Polynomial Regression can be used to better model the relationship between the two. \\[ E(Y|X) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 \\] Note that it is the same IV used in the regression, just with additional powers. Although a polynomial regression may capture the true relationship better, the regression parameters become hard to intepret. The partial derivatives can no longer be intepreted as holding other IVs constant as each IV is dependent on the same quantity, just to different powers. \\[ \\frac{\\partial E(Y|X)}{\\partial x} = \\beta_1 + 2\\beta_2 x + ... + m \\beta_m x^{m-1} \\]","title":"Multiple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#multiple-linear-regression","text":"The natural extension of the SLR model is to include more than one independent variable , which thus results in the more generalized Multiple Linear Regression (MLR) model. \\[ E(Y|X_1, ... X_p) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_{p}X_{p} \\] Unlike the SLR which studies how each individual IV influences the DV, the goal of the MLR model is to study how all the IVs operate together to influence the DV. \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) when \\(X_1 = X_2 = ... = 0\\) Change in \\(E(Y)\\) given a one unit increase in \\(X_j\\) , holding all other \\(X\\) 's constant Intercept Parameter \"Slope\" Parameter For avoidance of doubt, the subscript \\(i\\) will be used to denote observations while \\(j\\) will be used to denote independent variables. Every observation can also be expressed as the sum of the regression and the error term. However, due to the multi-dimensional nature of the model, it is commonly expressed in matrix notation: \\[ \\begin{aligned} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} &= \\begin{pmatrix} 1 & x_{11} & x_{12} & ... & x_{1p} \\\\ 1 & x_{21} & x_{22} & ... & x_{2p} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & ... & x_{np} \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix} + \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix} \\\\ \\boldsymbol{y} &= \\boldsymbol{X\\beta + \\varepsilon} \\end{aligned} \\]","title":"Multiple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#ordinary-least-squares","text":"Similar to the SLR model, the regression parameters can be found by minimizing the sum of squared residuals: \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_p x_ip)]^2 \\] There are \\(p+1\\) FOC equations to solve through the minimization, with the additional one reflecting the intercept parameter. It is difficult to algebraically solve this system of equations, thus there is no closed form tsolution for each individual paramater. Instead, here is a vector solution for all of the parameters: \\[ \\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\hat{\\beta_0} \\\\ \\hat{\\beta_1} \\\\ \\vdots \\\\ \\hat{\\beta_p} \\end{pmatrix} = (\\boldsymbol{X'X})^{-1}\\boldsymbol{X'y} \\] Note that since there are \\(p+1\\) equations that must be solved, the model has \\(n-p+1\\) degrees of freedom. Following the same logic, there must be at least \\(p+1\\) observations in order to solve the equations and hence construct the model. This results in the following fitted regression model, which can be graphically expressed as a Regression Plane : \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_P x_ip \\]","title":"Ordinary Least Squares"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#manual-computation","text":"The tricky part is that \\((\\boldsymbol{X'X})^{-1}\\) is hard to compute by manually , except for the special case where \\(p=1\\) (SLR). Thus, it is likely that the parameters will be provided by the question. If required to compute them manually, then \\((\\boldsymbol{X'X})^{-1}\\) is likely to be provided. The remaining \\(\\boldsymbol{X'y}\\) still needs to be computed and put together to obtain the regression parameters. However, if the model has \\(p=2\\) but no intercept , then \\((\\boldsymbol{X'X})^{-1}\\) is a 2 x 2 Matrix whose inverse can be easily calculated. Similarly, if \\((\\boldsymbol{X'X})^{-1}\\) is a Diagonal Matrix , its inverse can be easily calculated as well.","title":"Manual Computation"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#goodness-of-fit","text":"The ANOVA for MLR follows the same intuition as the SLR version, adjusted for the new degrees of freedom: Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(p\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-(p+1)\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) - The coefficient of determination still represents the proportion of variance explained by the regression, but has a slightly different formula: \\[ R^2 = \\frac{RegSS}{TSS} = r_{y,\\hat{y}}^2 \\] The multiple IVs of the model are now captured through \\(\\hat{y}\\) instead of \\(x\\) directly. However, the hypothesis test under the MLR is vastly different from the SLR version. Instead of testing if an individual IV is useful, it tests if all the IVs are collectively useful in helping to explain the DV. \\[ \\begin{aligned} H_0 &: \\beta_1 = \\beta_2 = ... = \\beta_p = 0 \\\\ H_1 &: \\text{At least one } \\beta_j \\text{ is non-zero} \\end{aligned} \\] Thus, rejecting the null hypothesis implies that at least one of the IVs used is useful, but does not provide much insight into which of them are useful.","title":"Goodness of Fit"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#partial-f-test","text":"A partial F-test can be used to precisely determine which of the IVs are useful in explaining the DV. A regular F test compares the null model with no IVs to the desired model with all the IVs. If the sum of squares are significantly different, then it implies that the additional IVs are jointly useful in explaining \\(Y\\) . The partial F test generalizes this idea. Instead of considering a null model with no IVs, a Reduced Model with a limited number IVs ( \\(q\\) ) is considered instead. Consequently, the desired model is known as the Full Model with all \\(p\\) IVs, where \\(q < p\\) . \\[ \\begin{aligned} H_0: \\beta_{p-q+1} = ... = \\beta_p = 0 \\\\ H_1: \\beta_{p-q+1} = ... = \\beta_p \\ne 0 \\end{aligned} \\] The test is also commonly referred to as the Generalized F test, where the models are referred to as the Restricted and Unrestricted Models. The difference in the RSS between the Full and Reduced Model is known as the Extra Sum of Squares (ExtraSS) . It represents the contribution of the missing variables in explaining the variance of \\(Y\\) . Under the null hypothesis, there should be no difference between the two RSS, and thus \\(ExtraSS = 0\\) . \\[ ExtraSS = RSS_{Reduced} - RSS_{Full} \\] Thus, the Partial F-statistic is testing for the equality of variance between the two RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_{p-q+1} = ... = \\beta_p \\ne 0\\) . \\[ \\begin{aligned} F &= \\frac{MS_{ExtraSS}}{MS_{RSS_{Full}}} \\\\ &= \\frac{ExtraSS/q}{RSS/(n-2)} \\\\ &= \\frac{(RSS_{Reduced} - RSS_{Full})/q}{RSS/(n-2)} \\\\ &= (n-2) * \\frac{(1- R_{Reduced}^2) - (1 - R_{Full}^2)}{1-RSS_{Full}^2}, \\text{divide both by TSS} \\\\ &= (n-2) * \\frac{R_{Full}^2 - R_{Reduced}^2}{1-RSS_{Full}^2} \\end{aligned} \\]","title":"Partial F Test"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#statistical-inference","text":"","title":"Statistical Inference"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#sampling-distributions","text":"Similar to SLR, the regression parameters are normally distributed as well. However, since there are multiple regression parameters, they collectively follow a multivariate normal distribution . \\[ \\hat{\\beta} \\sim N_{p+1}(\\beta, \\sigma^2 (\\boldsymbol{X'X})^{-1}) \\] The variance of the distribution is known as the Variance Covariance Matrix , which provides the covariances between every possible pair of regression parameters. Since the covariance of a variable with itself is its variance, the diagonals are the respective variances of the parameters. Note that the first element of the diagonal is the intercept, thus the variance of the jth IV is the (j+1)th element of the diagonal . \\[ Var(\\hat{\\beta}) = \\begin{pmatrix} Var(\\hat{\\beta}_0) & Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) & ... & Cov(\\hat{\\beta}_0, \\hat{\\beta}_p) \\\\ Cov(\\hat{\\beta}_1, \\hat{\\beta}_0) & Var(\\hat{\\beta}_1) & ... & Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ Cov(\\hat{\\beta}_p, \\hat{\\beta}_0) & Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) & ... & Var(\\hat{\\beta}_p) \\end{pmatrix} \\] Note that the covariances are symmetrical about the diagonal - \\(Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = Cov(\\hat{\\beta}_1, \\hat{\\beta}_0)\\) .","title":"Sampling Distributions"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#hypothesis-testing","text":"Similar to SLR, the t-test can be used to test for the significance of an individual IV, but the intepretation of the test is different from the SLR case. It tests the usefulness of an individual IV in the presence of the other predictors . \\[ \\begin{aligned} H_0: \\beta_j = 0 \\\\ H_1: \\beta_j \\ne 0 \\end{aligned} \\] \\[ t(\\beta_j) = \\frac{\\hat{\\beta_j} - \\beta_j}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\] Recall that the variance of the jth IV si the (j+1)th element of the variance covariance matrix. \\[ t(\\beta_j) \\sim t_{n-p-1} \\] However, this leads to several odd results which needs to be accounted for: Predictor is not significant individually but significant when taken alone . Predictors are not significant individually but significant when taken together . There are now two possible ways to test for the significance of IVs: Conduct a single F-test to test for the joint significance of all IVs Conduct multiple t-tests to test for the joint significance of each IV The problem with the multiple t-test approach lies with the type I error of the tests. For \\(\\alpha = 0.05\\) , the probability of correctly rejecting the null is \\(0.95\\) . Assuming that all the tests are independent, the probability of correctly rejecting all the nulls is \\(0.95^p\\) , which drastically decreases with the number of tests conducted. Given enough predictors, this means that the probability drops to approximately 0, which means that there is bound to be a wrongly rejected null; a type I error is guaranteed even though it was supposed to be limited at a 0.05 chance. The Bonferroni Correction is a method of adjusting \\(\\alpha\\) of each hypothesis test such that the overall type I error is kept at its desired level. However, this has the consequence of increasing the probability of type II errors, which is why it is not popular. The F-test has the advantage of controlling the type I error regardless of the number of predictors , which is why it is preferred for hypothesis testing in the MLR.","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#confidence-intervals","text":"Similar to SLR, the confidence intervals can be constructed using the distribution of the test-statistic: \\[ \\text{Confidence Interval} = \\hat{\\beta}_j \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_j}} \\]","title":"Confidence Intervals"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#prediction-intervals","text":"Unlike in SLR, it is difficult to determine the distribution of the prediction error. Thus, the final result can be found below: \\[ \\begin{aligned} \\text{Prediction Interval} &= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\\\ &= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\sqrt{s^2 [1 + x'_* (\\boldsymbol{X'X})^-1 * x_*]} \\end{aligned} \\] Despite the result looking more complicated, the key takeaway remains the same - the further away \\(x_*\\) is from \\(\\bar{x}\\) , the greater","title":"Prediction Intervals"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#variations-of-mlr","text":"","title":"Variations of MLR"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#qualitative-iv","text":"The discussion so far has mostly focused on Quantitative IVs, thus this section will explore Qualitative IVs. They can only take values from a list of pre-defined values, known as the Levels of the variable. In a regression context, most qualitative IVs are represented in the form of a Dummy Variable which can only take two possible levels - Yes (1) or No (0). Note that there are other ways of encoding a dummy variable (-1/0/1 etc), but the principles stay large the same. \\[ \\text{Dummy Variable} = \\begin{cases} 1, & \\text{First Level} \\\\ 0, & \\text{Second Level} \\end{cases} \\] In general, \\(n-1\\) dummy variables are needed to represent a qualitative variable with \\(n\\) levels. This is because the status of the last level can be deduced from the other dummy variables . Thus, including a seperate dummy variable for this last level is redundant and will lead to the problem of Collinearity , which will be explored in a later section. For instance, consider four levels (North, East, South & West), represented by the three dummy variables ( \\(N, E , S\\) ). If any of the variables are 1, they represent their respective direction (North, East & South). If all of them are 0, then the direction is the remaining level (West). The last remaining level is often referred to as the Baseline Level as it is the default level of the variable when all other dummies are 0. Any level can be used as the baseline, but the parameters will differ across models with different baselines. \\[ E(Y|X) = \\beta_0 + \\beta_{1, North} x_{North} + \\beta_{2, East} * x_{East} + \\beta_{3, South} * x_{South} \\] \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) at the baseline level Change in \\(E(Y)\\) from the baseline to the chosen level \\(X_1 = X_2 = ... = X_j = 0\\) \\(X_1 = X_2 = ... = 0; X_j = 1\\) Dummy variables are usually used in conjunction with quantitative ones . This essentially creates a \"seperate\" regression model for each of the levels. For the simplest case of one quantiative and one dummy, \\(\\beta_2\\) is the difference in the intercept of the two resulting SLR models. \\[ E(Y|X) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\] \\[ E(Y|X) = \\begin{cases} (\\beta_0 + \\beta_2) + \\beta_1 x_1, & x_2 = 1 \\\\ \\beta_0 + \\beta_1 x_1, & x_2 = 0 \\end{cases} \\]","title":"Qualitative IV"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#interaction-model","text":"So far, it was assumed that each IV had an independent effect on the DV. However, IVs may interact to produce a joint effect on the DV, where the effect of one IV depends on the value of another IV. For instance, the production of a factory may depend on the number of Machines and Workers. However, the more machines there are, the greater the effect of an additional worker . Thus, this interaction effect can be captured through an Interaction Variable , which is the product of both IVs: \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 \\\\ &= \\beta_0 + (\\beta_1 + \\beta_3 x_2)x_1 + \\beta_2 x_2 \\\\ &= \\beta_0 + \\beta_1 x_1 + (\\beta_2 + \\beta_3 x_1) x_2 \\\\ \\end{aligned} \\] A one unit increase in \\(x_1\\) will increase E(Y) by \\(\\beta_1 + \\beta_3 x_2\\) , which depends on the value of \\(x_2\\) as well, which is is why they interact with each another. Phrased another way, for every one unit increase in \\(x_2\\) , the change in E(Y) from a unit increase in \\(x_1\\) increases by \\(\\beta_3\\) . Something unusual to take note of is that the Interaction Variable tests significant but the constituent variables do not. In this case, it is common practice to retain both the interaction and the consistuent variables in the model. This is practice is known as the Hierarchical Principle . Models containing dummy variables can also have an interaction effect. Building off the example from the previous section, \\(\\beta_2\\) is still the difference in the intercept but with the new \\(\\beta_3\\) being the difference in slopes of the two resulting SLR models. \\[ E(Y|X) = \\begin{cases} (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3)x_1, & x_2 = 1 \\\\ \\beta_0 + \\beta_1 x_1, & x_2 = 0 \\end{cases} \\]","title":"Interaction Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#piecewise-model","text":"If the DV has an abrupt change in behaviour over different values of the IVs, it can be accounted for the through the use of a Piecewise Regression . The first method of creating a piecewise regression involves the use of an Indicator Function . It is essentially a dummy variable which depends on the value of the other IVs. \\[ z_{\\{x>=c\\}} = \\begin{cases} 0, & x < c \\\\ 1, & x \\ge c \\end{cases} \\] \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 z(x-c) \\\\ &= \\begin{cases} \\beta_0 + \\beta_1 x_1, & x < c \\\\ (\\beta_0 - \\beta_2c) + (\\beta_1 + \\beta_2) x_1, & x \\ge c \\end{cases} \\end{aligned} \\] Note that \\(z(x-c)\\) is treated as a distinct IV and hence can be equivalently expressed as \\(x_2\\) ; the full notation is used here for clarity. \\(c\\) is the value at which the DV abruptly changes in behaviour, known as a Kink in the graph, which continuously connects the two regression lines. The other method is to use an interaction variable instead. Similar to how the interaction variables resulted in the model to \"split\", the model now splits at \\(x = c\\) , resulting in a non-continuous gap. \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2z + \\beta_3 zx \\\\ &= \\begin{cases} \\beta_0 + \\beta_1 x_1, & x < c \\\\ (\\beta_0 - \\beta_2) + (\\beta_1 + \\beta_3) x_1, & x \\ge c \\end{cases} \\end{aligned} \\]","title":"Piecewise Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#polynomial-model","text":"If the relationship between the DV and IV is complex (non-linear), then a Polynomial Regression can be used to better model the relationship between the two. \\[ E(Y|X) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 \\] Note that it is the same IV used in the regression, just with additional powers. Although a polynomial regression may capture the true relationship better, the regression parameters become hard to intepret. The partial derivatives can no longer be intepreted as holding other IVs constant as each IV is dependent on the same quantity, just to different powers. \\[ \\frac{\\partial E(Y|X)}{\\partial x} = \\beta_1 + 2\\beta_2 x + ... + m \\beta_m x^{m-1} \\]","title":"Polynomial Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/","text":"Gauss Markov Theorem Using OLS, the estimated regression parameters will always be unbiased under certain assumptions. The Gauss Markov Theorem extends this, which states that under certain assumptions, the OLS estimators will have the lowest variance among all possible linear unbiased estimators. In a statistics context, they are said to be the most efficient among all other linear unbiased estimators. In a regression context, the OLS estimators are said to be the Best Linear Unbiased Estimators (BLUE). This section will go over the various assumptions for both OLS and Gauss Markov Theorem. It will also cover the diagnostics to determine if the assumptions have been violated. The assumptions needed for OLS and the Gauss Markov theorem are often mixed up with each other as the assumptions needed for OLS are also needed for the theorem. This set of notes makes a clear distinction between the two. OLS Assumptions #1: Linearity Linear regression is a model where the relationship between the DV and IVs are linear. Thus, the regression parameters must be linear , but NOT the DV or IV. This means that the model is still considered a \"Linear Regression\" even after a transformation of the DV and/or IV. \\[ \\begin{aligned} y_i &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\\\ y_i &= \\beta_0 + \\beta_1 x^2 + \\beta_2 x^3 \\\\ \\ln y_i &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\end{aligned} \\] #2: Exogenity Exogenity refers to how a variable comes from outside the model and is thus independent of any other variables within the model . In a regression context, this comes in the form of the errors having a conditional mean of 0 , ensuring that the errors are random and thus not related to the IVs. \\[ E(\\varepsilon_i | x_{ij}) = 0 \\\\ \\] \"Endo\" and \"Exo\" in Greek means \"In\" and \"Out\" respectively, which is how the meaning of the words were derived. There are two key implications of Exogenity: By the Law of Total Expectations, the unconditional expectation of the error is also 0. By the Linearity of Conditional Expectations, the expectation of the product of the Error and IVs is 0. \\[ \\begin{aligned} E(\\varepsilon_i) = 0 \\\\ E(\\varepsilon_i x_{ij}) = 0 \\end{aligned} \\] Following these two implications, it can be shown that the Covariance between the Error and IVs are also 0, which is another consequence of independence (NOT the other way around). \\[ \\begin{aligned} Cov (\\varepsilon_i, x_{ij}) &= E(\\varepsilon_i x_{ij}) - E(\\varepsilon_i) * E(x_{ij}) \\\\ &= 0 - 0 * E(x_{ij}) \\\\ &= 0 \\end{aligned} \\] Without exogenity, the regression parameters would reflect the effect of both the IV and the unmodelled variable within the error term. This causes the OLS estimate to be biased , known as the Omitted Variable Bias . Since the unmodelled variable confounds the results of the regression, it is known as a Confounding Variable . Residual Analysis Since the errors are unobservable, the residuals are used to estimate the errors. If the fitted model is adequate - all relavent IVs are included in the right form, then the residuals should closely resemble the errors and therefore be structureless (random). However, if there are patterns in the residuals , it indicates that there is additional information that can be used to improve the model and thus should be included. Due to the OLS, the correlation between residuals and existing IVs will always be 0 indicating no linear relationship . To check for unmodelled non-linear relationships , a Residual Plot of the IVs against the Residuals can be used. For instance, if the residual plot shows a quadractic pattern (curve), then a quadractic IV should be added into the model. Mathematically, it can be expressed as a function of the existing estimates: \\[ \\begin{aligned} \\hat{y_i} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\varepsilon}_i \\\\ \\hat{\\varepsilon_i} &= \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\text{ (From residual plot)} \\\\ \\hat{y_i} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &= (\\hat{\\beta}_0 + \\hat{\\gamma}_0) + (\\hat{\\beta}_1 + \\hat{\\gamma}_1)x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &= \\hat{\\beta}_0' + \\hat{\\beta}_1'x + \\hat{\\beta}_2' x^2 \\end{aligned} \\] #3: No Perfect Collinearity Collinearity refers to when an IV can be expressed as a linear combination of one or more other IVs . Perfect collinearity is an extreme case where an IV can be perfectly expressed as a combination of another. Technically speaking, Collinearity refers to one to one variable relationship, while Multicollinearity refers to one to many variable relationship, hence \"Multi\". Variable is a multiple of another: \\(x_1 = cx_2\\) Variable differs by a constant from another: \\(x_1 = x_2 \\pm c\\) Variable is an affine transformation of another: Sum of several variables is fixed: Dummy Variable Trap The issue with perfect collinearity is that it affects the linear algebra used to solve for the coefficients (EG. Two equations to solve for three unknowns). There will be no unique solutions - many different values for the coefficients could work equally well. Imperfect Collinearity Imperfect Collinearity is a less extreme case where an IV is highly (but not perfectly) correlated with one or more other IVs. Recall that correlation refers to the extent of a Linear relationship. Non-linear relationships between variables are fine (EG. Polynomial Regression). This means that including the IV does not bring much additional predctive power into the model as its effects are already captured through related predictors and thus can be removed from the model. Unlike in the perfect case, high collinearity does not prevent OLS from finding a solution. However, the intepretation of the variables become complicated: The original intepretation of coefficients \"holding other variables constant\" is no longer true as highly correlated variables tend to move together. Thus, it is hard to seperate the effects of an individual variable . Consequently, OLS has difficulty estimating these coefficients, which could result in weird meaningless estimates ; EG. Large positive coefficient but large negative for its correlated counterpart. This also results in higher standard errors for the coefficients of correlated variables. This reduces the magnitude of t-statistic , which results in more false negatives , failing to reject the null when it should. This results in important variables being omitted from the regression. Technically speaking, there is nothing wrong with collinearity if the purpose of the model is solely for prediction. However, if the purpose of the model was to establish causality, then collinearity poses a problem as it interferes with statistical inference. Detecting Collinearity The simplest way to detect collinearity is through a Scatterplot or Correlation Matrix , which shows the correlations between pairs of variables . A correlation of 0.8 and higher is typically considered high enough where the collinearity becomes problematic. However, the issue is that this method can only detect collinearity between pairs of variable at a time. In order to detect collinearity among three or more variables ( multicollinearity ), then the Variance Inflation Factor (VIF) should be used instead. \\[ VIF = \\frac{1}{1-R^2_j} \\] The VIF is derived from the variance of the regression coefficient. As mentioned previously, under the presence of collinearity, the standard error and hence variance of the coefficient increases (\"inflated\"). The extent of the increase is known as the VIF. \\[ \\hat{Var}(\\hat{\\beta_1}) = \\frac{MS_{RSS}}{(n-1) s^2} * \\frac{1}{1-R^2_j} \\] The \\(R^2_j\\) in the VIF is the coefficient of determination of a model where the jth IV is regressed against all other IVs . A high \\(R^2_j\\) means that the IV is well explained by the other IVs (high correlation), which indicates the presence of collinearity. Generally, a \\(VIF > 10\\) is deemed to have severe collinearity . #4: No Extreme Outliers Outliers are observations with unusual values of the DV relative to fitted regression model. The last OLS assumption is that there are no extreme outliers in the dataset used to create the regression model. Generally, as long as the DV and IVs have a positive and finite Kurtosis , then the probability of such observations occuring are low. Outliers are problematic as OLS is sensitive to outliers . Extreme outliers have large residuals which receive more weight in the optimization process, which causes the resulting model to accomodate it (when it should not), causing the resulting coefficients to be biased. Identifying Outliers By definition, Outliers have unusually large residuals . In order to gauge what is considered a \"large residual\", the residuals are standardized for comparison. Standardization requires knowledge of the sampling distribution of the residuals . Given that the errors have a constant variance of \\(\\sigma^2\\) , the variance of the residuals can be shown to be: \\[ var(\\hat{\\varepsilon}_i) = \\sigma^2 (1-h_{ii}) \\] \\(h_ii\\) is known as the Leverage of the observation, which will be covered in the following section. The sampling distribution can then be determined: \\[ \\hat{\\varepsilon}_i ~ N(0, \\sigma^2 (1-h_{ii})) \\] Thus, the standardized residuals are the raw residuals scaled by their standard error: \\[ \\hat{\\varepsilon}_i^{\\text{standardized}} = \\frac{\\hat{\\varepsilon}_i}{\\sqrt{\\sigma^2 (1-h_{ii})}} \\] In practice, the variance of the errors are unknown, thus it is estimated using the Sample Variance of the residuals instead. Residuals with standardized values of larger than 2 or 3 are considered large and thus can be considered as an outlier. High Leverage Points While outliers are unusual points of the DV, High Leverage observations have unusual values of the IV relative to the majority of the values. It is easy to identify high leverage points when there is only one IV through a scatterplot - simply find the observation that is away from the rest. It becomes much more complicated when there are multiple IVs. The observation's values for each of the IVs could be in the common range of each IV, but unusual when taken collectively : The leverage of the observation can be determined from the MLR: \\[ \\begin{aligned} \\hat{\\boldsymbol{y}} &= \\boldsymbol{X}\\boldsymbol{\\beta} \\\\ &= \\boldsymbol{X}[(X'X)^{-1}y] \\\\ &= \\boldsymbol{H}\\boldsymbol{y} \\end{aligned} \\] \\(\\boldsymbol{H}\\) is known as the Hat Matrix as it puts a hat on y in the notation. The leverage of the \\(ith\\) observation is the \\(ith\\) diagonal element of the matrix, \\(h_{ii}\\) . An observation is considered to have high leverage if its leverage is greater than three times the average leverage: \\[ h_{ii} \\gt 3 \\left(\\frac{p+1}{n}\\right) \\] Influential Points The effect of Outliers and High leverage points can both be summarized into a concept known as Influence . An observation is influential if the exclusion of the observation from the regression leads to significantly differently results. In general the process involves three steps: Fit the original model with all \\(n\\) observations; determine the \\(j-th\\) fitted value \\(\\hat{y}_j\\) Fit an adjusted model with omitting the \\(i-th\\) observation, determine the \\(j-th\\) fitted value \\(\\hat{y}_{j(i)}\\) Calculate the change in the \\(j-th\\) fitted value This process has to be repeated for all fitted values for all observations . Cook's Distance summarizes the effect of the \\(i-th\\) observation on the whole model: \\[ D_i = \\frac{\\sum^n_{j=1} (\\hat{y}_j - \\hat{y}_{j(i)})^2}{(p+1) \\cdot MS_{Residuals}} \\] This method of computation requires \\(n+1\\) datasets - 1 dataset with all the observation and \\(n\\) datasets with the \\(i-th\\) observation omitted. It is also extremely time consuming to have to fit a model to each dataset. An alternative method of determining Cook's Distance is to make use of both the Outliers and Leverage: \\[ D_i = \\frac{1}{p+1} \\cdot (e^{\\text{standardized}})^2 \\cdot \\frac{h_{ii}}{1-h_{ii}} \\] Thus, an observation must be unusual in BOTH the DV and IV in order to be considered influential. Outliers and High leverage points are necessary but not sufficient conditions to be influential. Gauss Markov Assumptions #1: Conditional Homoscedasticity Homoscedasticity refers to the error terms having constant variance while Heteroscedasticity refers to having non-constant variance. \\[ var(\\varepsilon_i | x_i) = \\sigma^2 \\] Under homoscedasticity, the sampling distribution of the estimates are easily derived and thus it can be shown that they are the most efficient estimators. The same cannot be proven under heteroscedasticity. Note that that the OLS estimates are still unbiased; they are just not the most efficient. Identifying Heteroscedasticity Similar to before, if the model is adequate, then the residuals should resemble the errors and have constant variance . Thus, this can be easily determined through a residual plot of the Residuals against the fitted values. If the points are equally spread out about the mean (0) and show no pattern , then homoscedasticity is present. However, if the points show an increasing or decreasing variance (typically in the shape of a funnel ), then heteroscedasticity is present. Alternatively, a hypothesis test can be conducted to determine if heteroscedasticity is present, known as the Bresuch Pagan Test . \\[ \\begin{aligned} H_0 &: \\sigma^2 \\\\ H_1 &: \\sigma^2 + \\boldsymbol{Z\\gamma} \\end{aligned} \\] The test-statistic is computed as follows: Compute the squared standardized residuals from the original model Regress them onto the variables in Z (LOL need to change this part) Compute the RegSS of the new regression \\[ T = \\frac{RegSS}{2} \\] The test-statistic follows a chi-square distribution with \\(q\\) degrees of freedom, where \\(q\\) is the number of variables in \\(\\boldsymbol{Z}\\) . \\[ T \\sim \\chi^2_q \\] Dealing with Heteroscedasticity If prior information is known about the structure of the data, then the most intuitive method would be to incorporate that information into the data. \\[ Var(\\varepsilon_i) = \\frac{\\sigma^2}{w_i} \\] If no prior information is known, then the Heteroscedasticity can be reduced by using a Variance Stabilizing Transformation , such as the Log or Squareroot . Note that since they require positive data, a constant can be added to each term before the transformation to ensure that the values are positive. It is out of the scope for this set of notes to show why these help to stabilize variance. Alternatively, if there is only mild heteroscedasticity in the data, then OLS can be used but with an adjustment to the standard errors of the coefficients, known as heteroscedastic-robust standard errors . Due to complexity of the computations, it will not be covered in this set of notes. However, the general idea is that an weighted estimate of the variance covariance matrix is computed and the standard errors are computed from there. #2: No Serial Correlation If errors are correlated with one another, it is known as Serial Correlation or Autocorrelation . It implies that there are other unmodelled factors that can be used for prediction, which would imply the current model specification to be wrong. Thus, for the SLR model to be true, the errors must be independent of one another . \\[ Cov(\\varepsilon_i,\\varepsilon_j) = 0 \\] Confidence Intervals and PI are narrower than it should be > 95% PI is actually < 95%> P values lower > Appear statisticlaly significant when they shld not be Time series tends to have errors that are positively correlated, which is why it has its own dedicated section No Serial Correlation > Outcome of zero conditional mean, but most likely in time series data Error Distribution Although not needed for OLS estimation or Guass Markov, the errors of the regression are usually assumed to be normally distributed . \\[ \\varepsilon \\sim N(0, \\sigma^2) \\] If the errors are normally distributed, then it follows that \\(\\beta\\) is normally distributed as well since they are linear and additive. This greatly eases the computation needed to determine the sampling distribution for statistical inference. Q-Q Plots Since the errors are normally distributed, the residuals should be normally distributed as well. This can be verified using a Quantile-Quantile Plot (QQ Plot) , which compares the quantiles of two distributions. The first distribution is plotted on the x-axis while the second on the y-axis. If the quantiles are the same (same distribution), then the points should lie on \\(y = x\\) , the 45 degree line.","title":"Gauss Markov Theorem"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#gauss-markov-theorem","text":"Using OLS, the estimated regression parameters will always be unbiased under certain assumptions. The Gauss Markov Theorem extends this, which states that under certain assumptions, the OLS estimators will have the lowest variance among all possible linear unbiased estimators. In a statistics context, they are said to be the most efficient among all other linear unbiased estimators. In a regression context, the OLS estimators are said to be the Best Linear Unbiased Estimators (BLUE). This section will go over the various assumptions for both OLS and Gauss Markov Theorem. It will also cover the diagnostics to determine if the assumptions have been violated. The assumptions needed for OLS and the Gauss Markov theorem are often mixed up with each other as the assumptions needed for OLS are also needed for the theorem. This set of notes makes a clear distinction between the two.","title":"Gauss Markov Theorem"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#ols-assumptions","text":"","title":"OLS Assumptions"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#1-linearity","text":"Linear regression is a model where the relationship between the DV and IVs are linear. Thus, the regression parameters must be linear , but NOT the DV or IV. This means that the model is still considered a \"Linear Regression\" even after a transformation of the DV and/or IV. \\[ \\begin{aligned} y_i &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\\\ y_i &= \\beta_0 + \\beta_1 x^2 + \\beta_2 x^3 \\\\ \\ln y_i &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\end{aligned} \\]","title":"#1: Linearity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#2-exogenity","text":"Exogenity refers to how a variable comes from outside the model and is thus independent of any other variables within the model . In a regression context, this comes in the form of the errors having a conditional mean of 0 , ensuring that the errors are random and thus not related to the IVs. \\[ E(\\varepsilon_i | x_{ij}) = 0 \\\\ \\] \"Endo\" and \"Exo\" in Greek means \"In\" and \"Out\" respectively, which is how the meaning of the words were derived. There are two key implications of Exogenity: By the Law of Total Expectations, the unconditional expectation of the error is also 0. By the Linearity of Conditional Expectations, the expectation of the product of the Error and IVs is 0. \\[ \\begin{aligned} E(\\varepsilon_i) = 0 \\\\ E(\\varepsilon_i x_{ij}) = 0 \\end{aligned} \\] Following these two implications, it can be shown that the Covariance between the Error and IVs are also 0, which is another consequence of independence (NOT the other way around). \\[ \\begin{aligned} Cov (\\varepsilon_i, x_{ij}) &= E(\\varepsilon_i x_{ij}) - E(\\varepsilon_i) * E(x_{ij}) \\\\ &= 0 - 0 * E(x_{ij}) \\\\ &= 0 \\end{aligned} \\] Without exogenity, the regression parameters would reflect the effect of both the IV and the unmodelled variable within the error term. This causes the OLS estimate to be biased , known as the Omitted Variable Bias . Since the unmodelled variable confounds the results of the regression, it is known as a Confounding Variable .","title":"#2: Exogenity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#residual-analysis","text":"Since the errors are unobservable, the residuals are used to estimate the errors. If the fitted model is adequate - all relavent IVs are included in the right form, then the residuals should closely resemble the errors and therefore be structureless (random). However, if there are patterns in the residuals , it indicates that there is additional information that can be used to improve the model and thus should be included. Due to the OLS, the correlation between residuals and existing IVs will always be 0 indicating no linear relationship . To check for unmodelled non-linear relationships , a Residual Plot of the IVs against the Residuals can be used. For instance, if the residual plot shows a quadractic pattern (curve), then a quadractic IV should be added into the model. Mathematically, it can be expressed as a function of the existing estimates: \\[ \\begin{aligned} \\hat{y_i} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\varepsilon}_i \\\\ \\hat{\\varepsilon_i} &= \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\text{ (From residual plot)} \\\\ \\hat{y_i} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &= (\\hat{\\beta}_0 + \\hat{\\gamma}_0) + (\\hat{\\beta}_1 + \\hat{\\gamma}_1)x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &= \\hat{\\beta}_0' + \\hat{\\beta}_1'x + \\hat{\\beta}_2' x^2 \\end{aligned} \\]","title":"Residual Analysis"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#3-no-perfect-collinearity","text":"Collinearity refers to when an IV can be expressed as a linear combination of one or more other IVs . Perfect collinearity is an extreme case where an IV can be perfectly expressed as a combination of another. Technically speaking, Collinearity refers to one to one variable relationship, while Multicollinearity refers to one to many variable relationship, hence \"Multi\". Variable is a multiple of another: \\(x_1 = cx_2\\) Variable differs by a constant from another: \\(x_1 = x_2 \\pm c\\) Variable is an affine transformation of another: Sum of several variables is fixed: Dummy Variable Trap The issue with perfect collinearity is that it affects the linear algebra used to solve for the coefficients (EG. Two equations to solve for three unknowns). There will be no unique solutions - many different values for the coefficients could work equally well.","title":"#3: No Perfect Collinearity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#imperfect-collinearity","text":"Imperfect Collinearity is a less extreme case where an IV is highly (but not perfectly) correlated with one or more other IVs. Recall that correlation refers to the extent of a Linear relationship. Non-linear relationships between variables are fine (EG. Polynomial Regression). This means that including the IV does not bring much additional predctive power into the model as its effects are already captured through related predictors and thus can be removed from the model. Unlike in the perfect case, high collinearity does not prevent OLS from finding a solution. However, the intepretation of the variables become complicated: The original intepretation of coefficients \"holding other variables constant\" is no longer true as highly correlated variables tend to move together. Thus, it is hard to seperate the effects of an individual variable . Consequently, OLS has difficulty estimating these coefficients, which could result in weird meaningless estimates ; EG. Large positive coefficient but large negative for its correlated counterpart. This also results in higher standard errors for the coefficients of correlated variables. This reduces the magnitude of t-statistic , which results in more false negatives , failing to reject the null when it should. This results in important variables being omitted from the regression. Technically speaking, there is nothing wrong with collinearity if the purpose of the model is solely for prediction. However, if the purpose of the model was to establish causality, then collinearity poses a problem as it interferes with statistical inference.","title":"Imperfect Collinearity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#detecting-collinearity","text":"The simplest way to detect collinearity is through a Scatterplot or Correlation Matrix , which shows the correlations between pairs of variables . A correlation of 0.8 and higher is typically considered high enough where the collinearity becomes problematic. However, the issue is that this method can only detect collinearity between pairs of variable at a time. In order to detect collinearity among three or more variables ( multicollinearity ), then the Variance Inflation Factor (VIF) should be used instead. \\[ VIF = \\frac{1}{1-R^2_j} \\] The VIF is derived from the variance of the regression coefficient. As mentioned previously, under the presence of collinearity, the standard error and hence variance of the coefficient increases (\"inflated\"). The extent of the increase is known as the VIF. \\[ \\hat{Var}(\\hat{\\beta_1}) = \\frac{MS_{RSS}}{(n-1) s^2} * \\frac{1}{1-R^2_j} \\] The \\(R^2_j\\) in the VIF is the coefficient of determination of a model where the jth IV is regressed against all other IVs . A high \\(R^2_j\\) means that the IV is well explained by the other IVs (high correlation), which indicates the presence of collinearity. Generally, a \\(VIF > 10\\) is deemed to have severe collinearity .","title":"Detecting Collinearity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#4-no-extreme-outliers","text":"Outliers are observations with unusual values of the DV relative to fitted regression model. The last OLS assumption is that there are no extreme outliers in the dataset used to create the regression model. Generally, as long as the DV and IVs have a positive and finite Kurtosis , then the probability of such observations occuring are low. Outliers are problematic as OLS is sensitive to outliers . Extreme outliers have large residuals which receive more weight in the optimization process, which causes the resulting model to accomodate it (when it should not), causing the resulting coefficients to be biased.","title":"#4: No Extreme Outliers"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#identifying-outliers","text":"By definition, Outliers have unusually large residuals . In order to gauge what is considered a \"large residual\", the residuals are standardized for comparison. Standardization requires knowledge of the sampling distribution of the residuals . Given that the errors have a constant variance of \\(\\sigma^2\\) , the variance of the residuals can be shown to be: \\[ var(\\hat{\\varepsilon}_i) = \\sigma^2 (1-h_{ii}) \\] \\(h_ii\\) is known as the Leverage of the observation, which will be covered in the following section. The sampling distribution can then be determined: \\[ \\hat{\\varepsilon}_i ~ N(0, \\sigma^2 (1-h_{ii})) \\] Thus, the standardized residuals are the raw residuals scaled by their standard error: \\[ \\hat{\\varepsilon}_i^{\\text{standardized}} = \\frac{\\hat{\\varepsilon}_i}{\\sqrt{\\sigma^2 (1-h_{ii})}} \\] In practice, the variance of the errors are unknown, thus it is estimated using the Sample Variance of the residuals instead. Residuals with standardized values of larger than 2 or 3 are considered large and thus can be considered as an outlier.","title":"Identifying Outliers"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#high-leverage-points","text":"While outliers are unusual points of the DV, High Leverage observations have unusual values of the IV relative to the majority of the values. It is easy to identify high leverage points when there is only one IV through a scatterplot - simply find the observation that is away from the rest. It becomes much more complicated when there are multiple IVs. The observation's values for each of the IVs could be in the common range of each IV, but unusual when taken collectively : The leverage of the observation can be determined from the MLR: \\[ \\begin{aligned} \\hat{\\boldsymbol{y}} &= \\boldsymbol{X}\\boldsymbol{\\beta} \\\\ &= \\boldsymbol{X}[(X'X)^{-1}y] \\\\ &= \\boldsymbol{H}\\boldsymbol{y} \\end{aligned} \\] \\(\\boldsymbol{H}\\) is known as the Hat Matrix as it puts a hat on y in the notation. The leverage of the \\(ith\\) observation is the \\(ith\\) diagonal element of the matrix, \\(h_{ii}\\) . An observation is considered to have high leverage if its leverage is greater than three times the average leverage: \\[ h_{ii} \\gt 3 \\left(\\frac{p+1}{n}\\right) \\]","title":"High Leverage Points"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#influential-points","text":"The effect of Outliers and High leverage points can both be summarized into a concept known as Influence . An observation is influential if the exclusion of the observation from the regression leads to significantly differently results. In general the process involves three steps: Fit the original model with all \\(n\\) observations; determine the \\(j-th\\) fitted value \\(\\hat{y}_j\\) Fit an adjusted model with omitting the \\(i-th\\) observation, determine the \\(j-th\\) fitted value \\(\\hat{y}_{j(i)}\\) Calculate the change in the \\(j-th\\) fitted value This process has to be repeated for all fitted values for all observations . Cook's Distance summarizes the effect of the \\(i-th\\) observation on the whole model: \\[ D_i = \\frac{\\sum^n_{j=1} (\\hat{y}_j - \\hat{y}_{j(i)})^2}{(p+1) \\cdot MS_{Residuals}} \\] This method of computation requires \\(n+1\\) datasets - 1 dataset with all the observation and \\(n\\) datasets with the \\(i-th\\) observation omitted. It is also extremely time consuming to have to fit a model to each dataset. An alternative method of determining Cook's Distance is to make use of both the Outliers and Leverage: \\[ D_i = \\frac{1}{p+1} \\cdot (e^{\\text{standardized}})^2 \\cdot \\frac{h_{ii}}{1-h_{ii}} \\] Thus, an observation must be unusual in BOTH the DV and IV in order to be considered influential. Outliers and High leverage points are necessary but not sufficient conditions to be influential.","title":"Influential Points"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#gauss-markov-assumptions","text":"","title":"Gauss Markov Assumptions"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#1-conditional-homoscedasticity","text":"Homoscedasticity refers to the error terms having constant variance while Heteroscedasticity refers to having non-constant variance. \\[ var(\\varepsilon_i | x_i) = \\sigma^2 \\] Under homoscedasticity, the sampling distribution of the estimates are easily derived and thus it can be shown that they are the most efficient estimators. The same cannot be proven under heteroscedasticity. Note that that the OLS estimates are still unbiased; they are just not the most efficient.","title":"#1: Conditional Homoscedasticity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#identifying-heteroscedasticity","text":"Similar to before, if the model is adequate, then the residuals should resemble the errors and have constant variance . Thus, this can be easily determined through a residual plot of the Residuals against the fitted values. If the points are equally spread out about the mean (0) and show no pattern , then homoscedasticity is present. However, if the points show an increasing or decreasing variance (typically in the shape of a funnel ), then heteroscedasticity is present. Alternatively, a hypothesis test can be conducted to determine if heteroscedasticity is present, known as the Bresuch Pagan Test . \\[ \\begin{aligned} H_0 &: \\sigma^2 \\\\ H_1 &: \\sigma^2 + \\boldsymbol{Z\\gamma} \\end{aligned} \\] The test-statistic is computed as follows: Compute the squared standardized residuals from the original model Regress them onto the variables in Z (LOL need to change this part) Compute the RegSS of the new regression \\[ T = \\frac{RegSS}{2} \\] The test-statistic follows a chi-square distribution with \\(q\\) degrees of freedom, where \\(q\\) is the number of variables in \\(\\boldsymbol{Z}\\) . \\[ T \\sim \\chi^2_q \\]","title":"Identifying Heteroscedasticity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#dealing-with-heteroscedasticity","text":"If prior information is known about the structure of the data, then the most intuitive method would be to incorporate that information into the data. \\[ Var(\\varepsilon_i) = \\frac{\\sigma^2}{w_i} \\] If no prior information is known, then the Heteroscedasticity can be reduced by using a Variance Stabilizing Transformation , such as the Log or Squareroot . Note that since they require positive data, a constant can be added to each term before the transformation to ensure that the values are positive. It is out of the scope for this set of notes to show why these help to stabilize variance. Alternatively, if there is only mild heteroscedasticity in the data, then OLS can be used but with an adjustment to the standard errors of the coefficients, known as heteroscedastic-robust standard errors . Due to complexity of the computations, it will not be covered in this set of notes. However, the general idea is that an weighted estimate of the variance covariance matrix is computed and the standard errors are computed from there.","title":"Dealing with Heteroscedasticity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#2-no-serial-correlation","text":"If errors are correlated with one another, it is known as Serial Correlation or Autocorrelation . It implies that there are other unmodelled factors that can be used for prediction, which would imply the current model specification to be wrong. Thus, for the SLR model to be true, the errors must be independent of one another . \\[ Cov(\\varepsilon_i,\\varepsilon_j) = 0 \\] Confidence Intervals and PI are narrower than it should be > 95% PI is actually < 95%> P values lower > Appear statisticlaly significant when they shld not be Time series tends to have errors that are positively correlated, which is why it has its own dedicated section No Serial Correlation > Outcome of zero conditional mean, but most likely in time series data","title":"#2: No Serial Correlation"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#error-distribution","text":"Although not needed for OLS estimation or Guass Markov, the errors of the regression are usually assumed to be normally distributed . \\[ \\varepsilon \\sim N(0, \\sigma^2) \\] If the errors are normally distributed, then it follows that \\(\\beta\\) is normally distributed as well since they are linear and additive. This greatly eases the computation needed to determine the sampling distribution for statistical inference.","title":"Error Distribution"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#q-q-plots","text":"Since the errors are normally distributed, the residuals should be normally distributed as well. This can be verified using a Quantile-Quantile Plot (QQ Plot) , which compares the quantiles of two distributions. The first distribution is plotted on the x-axis while the second on the y-axis. If the quantiles are the same (same distribution), then the points should lie on \\(y = x\\) , the 45 degree line.","title":"Q-Q Plots"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/","text":"Statistical Learning The regression concepts covered in the previous sections are widely considered to be traditional applied statistics . In a contemporary context, regression is just one of the many methods that fall under Statistical Learning . It is a framework of harnessing data to gain an understanding of how the data is related to one another and/or how a group of variables can be used to accurately predict another. Similar to regression, the relationship between variables can be expressed as a combination of a Signal Function and a Noise term: \\[ y = f(x) + \\varepsilon \\] The two terms originate from Engineering, where Signal refers to the meaningful component of the data while Noise refers to the random variation that inteferes with the signal. Statistical learning models are distinguished based on their signal function. There are two main kinds of models: Parametric Models Non-Parametric Assumes DV follows a specific functional form Does not assume any functional form Function parameters determined from the data - Does not require a large amount of data Requires a large amount of data to work well EG. Linear Regression EG. Clustering | May not fit the data well | Fits the data well | | Described by parameters | No parameters used | | Simple to implement | Requires large amount of data to implement | | Risk that assumed function is wrong | Does not make any assumption about data | They can also be further split according to: Supervised Learning Unsupervised Learning Specified DV to supervise the learning No specific variable chosen Inference/Prediction with respect to the DV Inference/Prediction for all variables EG. Linear Regression EG. Clustering Regression vs classification classification - classifying the observations to a certian level Model Accuracy Since the end goal of the model is to make predictions on new unobserved data, the quality of the model should be evaluated against its performance on unobserved data as well. The observed data used to create the model is known as the Training Data as it helps train the signal function to identify relationships between variables, while the unobserved data used to evaluate the model is known as the Test Data . The quality of the model can then be quantified by the extent to which the model predictions match the data . Similar to regression, this quantity is known as the Error of the model and is summarized through the Mean Square Error (MSE) statistic. The MSE is the average of the sum of squared errors and can be calculated for both the training and test data: \\[ \\begin{aligned} \\text{Training MSE} &= \\frac{[y_i - \\hat{f}(x_0)]^2}{n_{training}} \\\\ \\text{Test MSE} &= \\frac{[y_0 - \\hat{f}(x_0)]^2}{n_{test}} \\end{aligned} \\] Note that this is different from the MSE defined in ANOVA, where the MS is divded by its degrees of freedom rather than the number of observations. The MSE defined in this section is a general concept while the ANOVA MSE is a purely regression concept. The Training MSE reflects the goodness of fit of the model while the Test MSE reflects its prediction accuracy. As alluded to earlier, the goal of statistical learning is to choose the model with the lowest Test MSE . In general, the training MSE should always be smaller than the testing MSE, which is why they are not interchangeable. This is because all models are trained to match the training data to various extents, thus they should naturally have relatively smaller errors. On the flipside, test MSE should always be higher because the model is likely to have mistakenly captured some of the noise in the training data that do not generalize to the test data, resulting in a higher testing error. The extent of the difference is dependent on how well the model fits the training data; the extent to which it learns from it: High Flexibility/Complexity - Tends to overfit the training data; matches data too much; learns too much Low Flexibility/Complexity - Tends to underfit the training data; matches data too little; learns too little This is not to say that low flexibility models are better. In fact, some level of flexibility is needed for the model to pick up most of the signals in the training data, but not too much such that the noise is captured as well. Thus, the test MSE generally decreases with flexibility up till a certain point, following which it increases, forming a U shaped curve : Bias Variance Tradeoff The test MSE can be better understood by decomposing it into its consistuent commponents: \\[ \\text{MSE} = \\text{Bias}[\\hat{f}(x_0)]^2 + \\text{Variance}[\\hat{f}(x_0)] \\] The Bias of the model (also known as the Accuracy ) is the difference in expected value of the estimated signal function and the true signal function. More complex models are better able to capture the signal in the data, thus tends to have a lower bias. The Variance of the model (also known as Precision ) is the change in the estimated signal function across different datasets. Ideally, the model should have low variance such that it would be relatively stable across different training data. While more complex models are better able to capture signals, this makes them prone to overfitting and hence more sensitive to differences in the training data , leading to higher variance. Note that although Precision & Accuracy are both synonyms in English, they have distinct meanings in statistics. Ideally, a model should have both low bias and low variance. However, as explained above, there is an inherent tension between Bias and Variance due to the complexity of the model, known as the Bias-Variance Tradeoff . A relatively simple model (underfitted) tends to have a high bias but low variance . As the complexity increases, the bias initially decreases more than the variance increases , causing the test MSE to fall. At some point, the model becomes too complex (overfitted), where the increase in variance outweighs the fall in bias , resulting in the U-shaped curve as seen previously. Thus, the goal is to find an optimal balance in between Bias and Variance where the test MSE is minimized. Resampling Methods Validation Set LOO Cross Validation K fold Cross Validation Model Selection Feature Selection Forward Stepwise Selection Backward Stepwise Selection Stepwise Selection Shrinkage Methods","title":"Statistical Learning"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#statistical-learning","text":"The regression concepts covered in the previous sections are widely considered to be traditional applied statistics . In a contemporary context, regression is just one of the many methods that fall under Statistical Learning . It is a framework of harnessing data to gain an understanding of how the data is related to one another and/or how a group of variables can be used to accurately predict another. Similar to regression, the relationship between variables can be expressed as a combination of a Signal Function and a Noise term: \\[ y = f(x) + \\varepsilon \\] The two terms originate from Engineering, where Signal refers to the meaningful component of the data while Noise refers to the random variation that inteferes with the signal. Statistical learning models are distinguished based on their signal function. There are two main kinds of models: Parametric Models Non-Parametric Assumes DV follows a specific functional form Does not assume any functional form Function parameters determined from the data - Does not require a large amount of data Requires a large amount of data to work well EG. Linear Regression EG. Clustering | May not fit the data well | Fits the data well | | Described by parameters | No parameters used | | Simple to implement | Requires large amount of data to implement | | Risk that assumed function is wrong | Does not make any assumption about data | They can also be further split according to: Supervised Learning Unsupervised Learning Specified DV to supervise the learning No specific variable chosen Inference/Prediction with respect to the DV Inference/Prediction for all variables EG. Linear Regression EG. Clustering Regression vs classification classification - classifying the observations to a certian level","title":"Statistical Learning"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#model-accuracy","text":"Since the end goal of the model is to make predictions on new unobserved data, the quality of the model should be evaluated against its performance on unobserved data as well. The observed data used to create the model is known as the Training Data as it helps train the signal function to identify relationships between variables, while the unobserved data used to evaluate the model is known as the Test Data . The quality of the model can then be quantified by the extent to which the model predictions match the data . Similar to regression, this quantity is known as the Error of the model and is summarized through the Mean Square Error (MSE) statistic. The MSE is the average of the sum of squared errors and can be calculated for both the training and test data: \\[ \\begin{aligned} \\text{Training MSE} &= \\frac{[y_i - \\hat{f}(x_0)]^2}{n_{training}} \\\\ \\text{Test MSE} &= \\frac{[y_0 - \\hat{f}(x_0)]^2}{n_{test}} \\end{aligned} \\] Note that this is different from the MSE defined in ANOVA, where the MS is divded by its degrees of freedom rather than the number of observations. The MSE defined in this section is a general concept while the ANOVA MSE is a purely regression concept. The Training MSE reflects the goodness of fit of the model while the Test MSE reflects its prediction accuracy. As alluded to earlier, the goal of statistical learning is to choose the model with the lowest Test MSE . In general, the training MSE should always be smaller than the testing MSE, which is why they are not interchangeable. This is because all models are trained to match the training data to various extents, thus they should naturally have relatively smaller errors. On the flipside, test MSE should always be higher because the model is likely to have mistakenly captured some of the noise in the training data that do not generalize to the test data, resulting in a higher testing error. The extent of the difference is dependent on how well the model fits the training data; the extent to which it learns from it: High Flexibility/Complexity - Tends to overfit the training data; matches data too much; learns too much Low Flexibility/Complexity - Tends to underfit the training data; matches data too little; learns too little This is not to say that low flexibility models are better. In fact, some level of flexibility is needed for the model to pick up most of the signals in the training data, but not too much such that the noise is captured as well. Thus, the test MSE generally decreases with flexibility up till a certain point, following which it increases, forming a U shaped curve :","title":"Model Accuracy"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#bias-variance-tradeoff","text":"The test MSE can be better understood by decomposing it into its consistuent commponents: \\[ \\text{MSE} = \\text{Bias}[\\hat{f}(x_0)]^2 + \\text{Variance}[\\hat{f}(x_0)] \\] The Bias of the model (also known as the Accuracy ) is the difference in expected value of the estimated signal function and the true signal function. More complex models are better able to capture the signal in the data, thus tends to have a lower bias. The Variance of the model (also known as Precision ) is the change in the estimated signal function across different datasets. Ideally, the model should have low variance such that it would be relatively stable across different training data. While more complex models are better able to capture signals, this makes them prone to overfitting and hence more sensitive to differences in the training data , leading to higher variance. Note that although Precision & Accuracy are both synonyms in English, they have distinct meanings in statistics. Ideally, a model should have both low bias and low variance. However, as explained above, there is an inherent tension between Bias and Variance due to the complexity of the model, known as the Bias-Variance Tradeoff . A relatively simple model (underfitted) tends to have a high bias but low variance . As the complexity increases, the bias initially decreases more than the variance increases , causing the test MSE to fall. At some point, the model becomes too complex (overfitted), where the increase in variance outweighs the fall in bias , resulting in the U-shaped curve as seen previously. Thus, the goal is to find an optimal balance in between Bias and Variance where the test MSE is minimized.","title":"Bias Variance Tradeoff"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#resampling-methods","text":"","title":"Resampling Methods"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#validation-set","text":"","title":"Validation Set"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#loo-cross-validation","text":"","title":"LOO Cross Validation"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#k-fold-cross-validation","text":"","title":"K fold Cross Validation"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#model-selection","text":"","title":"Model Selection"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#feature-selection","text":"","title":"Feature Selection"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#forward-stepwise-selection","text":"","title":"Forward Stepwise Selection"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#backward-stepwise-selection","text":"","title":"Backward Stepwise Selection"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#stepwise-selection","text":"","title":"Stepwise Selection"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#shrinkage-methods","text":"","title":"Shrinkage Methods"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/6.%20Generalized%20Linear%20Models/","text":"","title":"6. Generalized Linear Models"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/7.%20Time%20Series%20Models/","text":"","title":"7. Time Series Models"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/8.%20Tree%20Models/","text":"","title":"8. Tree Models"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/9.%20Principal%20Component%20Analysis/","text":"","title":"9. Principal Component Analysis"}]}