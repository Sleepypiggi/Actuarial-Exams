{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Actuarial Exam Notes Test","title":"**Actuarial Exam Notes**"},{"location":"#actuarial-exam-notes","text":"Test","title":"Actuarial Exam Notes"},{"location":"Placeholder/","text":"Work in progress!","title":"FSA-ALM"},{"location":"Placeholder/#work-in-progress","text":"","title":"Work in progress!"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Profit%20Testing/","text":"","title":"Profit Testing"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Special%20Products/","text":"Special Products","title":"Special"},{"location":"2.%20Actuarial%20Mathematics/ASA-ALTAM/Special%20Products/#special-products","text":"","title":"Special Products"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/","text":"Survival Models Survival Models are probability distributions that measure the time to failure of an entity, or phrased another way, the future lifetime of an entity. In an Actuarial context, it measures the time to death of a person. Since survival models measure time , they are denoted by the continuous random variable \\(T_i\\) , where the subscript represents the age of the person being studied. Newborn Lifetime The base survival model measures the future lifetime of a newborn ( aged 0 ), denoted by the random variable \\(T_0\\) . The CDF thus represents the probability that the newborn dies before age \\(t\\) : \\[ F_0(t) = P(T_0 \\le t) \\] The Survival Function is the complement of the CDF and is the probability that the newborn survives till a certain age \\(t\\) . From a different perspective, it can also be seen as the probability of the newborn dying AFTER age \\(t\\) : \\[ \\begin{aligned} S_0(t) &= P(T_0 \\ge t) \\\\ &= 1 - P(T_0 \\le t) \\\\ &= 1 - F_0(t) \\end{aligned} \\] Note that all humans will inevitably die : \\[ \\begin{aligned} \\lim_{t \\to \\infty} S_0 (t) = 0 \\iff \\lim_{t \\to \\infty} F_0 (t) = 1 \\end{aligned} \\] Continuous Lifetime The above can be generalized for a person aged \\(x\\) , denoted by \\(T_x\\) . However, the intepretation of the two functions changes slightly. The CDF instead represents the probability that the person dies within \\(t\\) years from age \\(x\\) : \\[ \\begin{aligned} F_x(t) &= P(T_x \\le t) \\\\ T_x &= 1, 2, \\dots \\end{aligned} \\] It can also be written in terms of the newborn lifetime . The key is to understand that the newborn is assumed to survive till age \\(x\\) and then die within the next \\(t\\) years: \\[ \\begin{aligned} P(T_x \\le t) &= P(T_0 \\le x + t | T_0 > x) \\\\ &= \\frac{P(x < T_0 < x +t)}{P(T_0 > x)} \\\\ &= \\frac{P(T_0 < x +t)-P(T_0 < x)}{P(T_0 > x)} \\\\ &= \\frac{F_0(x+t) - F_0(x)}{S_0(x)} \\end{aligned} \\] Conversely, the survival function is the probability that the person survives another \\(t\\) years or dies AFTER \\(t\\) years : \\[ S_x(t) = P(T_x \\ge t) \\] It can also be written in terms of the newborn lifetime following the same logic as above. The newborn is assumed to survive till age \\(x\\) and then survive another \\(t\\) years: \\[ \\begin{aligned} P(T_x \\ge t) &= P(T_0 \\ge x + t | T_0 > x) \\\\ &= \\frac{P(T_0 > x + t)}{P(T_0 > x)} \\\\ &= \\frac{S_0(x+t)}{S_0(x)} \\end{aligned} \\] Note that since the surviving till age \\(x\\) is a subset of surviving till age \\(x+t\\) , the numerator can be simplified to just surviving till \\(x+t\\) . Note the survival function can thus be re-written as the following: \\[ \\begin{aligned} S_x(t) &= \\frac{S_0(x+t)}{S_0(x)} \\\\ S_0(x+t) &= S_0(x)S_x(t) \\\\ \\end{aligned} \\] The probability of the survival of a newborn till age \\(x+t\\) is the product of: The probability of survival of a newborn till age \\(x\\) Given that the newborn survived till age \\(x\\) , that they will survive another \\(t\\) years This can be generalized for people of any age to obtain: \\[ S_x(t+u) = S_x(t) S_{x+t}(u) \\] It is important to remember to \"update\" the starting age of the second term to the \"new age\". The opposite of the above is when an individual surives another \\(t\\) years and then dies within the following \\(u\\) years . This is known as the probability of Deferred Death : \\[ \\begin{aligned} P(t < T_x < t+u) &= P(T_x < t+u) - P(T_x < t) \\\\ &= F_x(t+u) - F_x(t) \\\\ &= 1 - S_x(t+u) - (1 - S_x(t)) \\\\ &= 1 - S_x(t)S_{x+t}(u) - 1 + S_x(t) \\\\ &= S_x(t) - S_x(t)S_{x+t}(u) \\\\ &= S_x(t)(1-S_{x+t}(u)) \\\\ &= S_x(t)F_{x+t}(u) \\end{aligned} \\] Probability Tree Perspective Since the probabilites of death and survival are conditional on the age of the individual, they can be better expressed in the form of a probability tree: Thus, it can be shown through recursion that the probability of surviving \\(t\\) years is equal to the sum of the probabilities of deferred deaths for every year after: \\[ \\begin{aligned} S_x(1) &= S_x(1)F_{x+1}(1) + S_x(1)S_{x+1}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(1) [S_{x+1}(1)F_{x+2}(1) + S_{x+1}(1)S_{x+2}(1)] \\\\ &= S_x(1)F_{x+1}(1) + S_x(1) S_{x+1}(1)F_{x+2}(1) + S_x(1) S_{x+1}(1)S_{x+2}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(2)F_{x+2}(1) + S_x(2)S_{x+2}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(2)F_{x+2}(1) + \\dots \\\\ \\\\ \\therefore S_x(t) &= \\sum S_x(t)F{x+t}(1) \\end{aligned} \\] Recall that the probability of surviving till a certain age is simply the probability that the individual will die sometime after that age. The above expression solidifies this, where the RHS is the probability of dying in every possible age after . Force of Mortality The Force of Mortality is the probability of dying instantly ; in an infinitely small unit of time (normalized by unit time): \\[ \\begin{aligned} \\mu_{0+x} &= \\lim_{h \\to 0} \\frac{P(T_x < h)}{h} \\\\ &= \\lim_{h \\to 0} \\frac{1 - S_x(h)}{h} \\\\ &= \\lim_{h \\to 0} \\frac{1 - (\\frac{S_0(x+h)}{S_0(x)})}{h} \\\\ &= \\lim_{h \\to 0} \\frac{S_0(x) - S_0(x+h)}{S_0(x)h} \\\\ &= \\frac{1}{S_0(x)} \\lim_{h \\to 0} \\frac{S_0(x) - S_0(x+h)}{h} \\\\ &= - \\frac{1}{S_0(x)} \\lim_{h \\to 0} \\frac{S_0(x+h) - S_0(x)}{h} \\\\ &= - \\frac{1}{S_0(x)} S'_0(x) \\\\ &= - \\frac{S'_0(x)}{S_0(x)} \\end{aligned} \\] Since \\(S_0(x)\\) is the complement of \\(F_0(x)\\) , the derivative of the survival function can be written in terms of the PDF of the future lifetime: \\[ \\begin{aligned} f_0(x) &= \\frac{d}{dx} F_0(x) \\\\ &= \\frac{d}{dx} (1-S_0(x)) \\\\ &= -S'_0(x) \\end{aligned} \\] The force of mortality can be rewritten as the following: \\[ \\mu_{0+x} = \\frac{f_0(x)}{S_0(x)} \\] Thus, the force of mortality can also be intepreted as the CONDITIONAL probability distribution of death for a person aged \\(x\\) , assuming that the person survives till age \\(x\\) . \\(f_0(x)\\) is UNCONDITIONAL distribution of the future lifetime as it does not assume the individual lives to any age. Alternative Death Function More generally, the force of mortality can be rewritten as: \\[ \\begin{aligned} \\mu_{0+x} &= \\frac{f_0(x)}{S_0(x)} \\\\ \\mu_{x+t} &= \\frac{f_x(t)}{S_x(t)} \\\\ f_x(t) &= S_x(t)\\mu_{x+t} \\\\ F_x(t) &= \\int_0^t S_x(t)\\mu_{x+t} \\end{aligned} \\] The probability of death in an interval is the SUM of the product of the probability of surviving till a given age and then dying in an infinitely small time afterwards. The sum of all the infinitely small time periods is equal to the given time interval. Following this definition, for extremely small time intervals , the force of mortality can be used to approximate the probability of death in that interval: \\[ P(T_x < h) \\approx h * \\mu_{x} \\] Alternative Survival Function More generally, the force of mortality can be rewritten as: \\[ \\begin{aligned} \\mu_{x} &= - \\frac{S'_0(x)}{S_0(x)} \\\\ \\mu_{x + t} &= \\frac{-S'_x(t)}{S_x(t)} \\\\ \\mu_{x + t} &= - \\frac{d}{dt} (\\ln S_x(t)) \\\\ \\ln S_x(t) &= - \\int_{0}^{t} \\mu_{x + t} \\\\ S_x(t) &= e ^ {- \\int_{0}^{t} \\mu_{x + t}} \\\\ \\end{aligned} \\] Note that most expressions give \\(\\mu_x\\) , but it is \\(\\mu_{x+t}\\) that is usually of interest - simply adjust the parameters accordingly. Thus, if \\(\\mu_x\\) is known for all ages, then the survival probabilites for any age can be calculated from it. In other words, the distribution of \\(T_x\\) can be determined from the force of mortality . Actuarial Notation Given how often these expressions are used, they are often abbreviated using the International Actuarial Notation : \\[ \\begin{aligned} {}_{t}p_{x} &= S_x(t) \\\\ {}_{t}q_{x} &= F_x(t) \\\\ {}_{t}q_{x} + {}_{t}p_{x} &= 1 \\end{aligned} \\] If \\(t=1\\) , it is omitted; EG. \\({}_{1}q_{x} = q_{x}\\) . Deferred Deaths are expressed using the pipe symbol: \\[ \\begin{aligned} {}_{t|u}q_{x} &= S_x(t)F_{x+t}(u) \\\\ &= {}_{t}p_{x} * {}_{u}q_{x+t} \\end{aligned} \\] These notations are typically used when \\(t\\) is an integer, but not necessarily so, as will be seen in the next section. Continuous Expectation The Expectation of the distribution of future lifetime is the Life Expectancy of the individual. It is commonly calculated for newborns as a measure of the general health of the population. In an insurance context, it is known as the Complete Expectation of Life and is denoted by \\(\\mathring{e}_{x}\\) : \\[ \\begin{aligned} E(T_x) &= \\int_{0}^{\\infty} t f_x(t) \\\\ e_{x} &= \\int_{0}^{\\infty} t S_x(t)\\mu_{x+t} \\end{aligned} \\] Through integration by parts, \\[ \\begin{aligned} \\mathring{e}_{x} &= \\left(\\Bigl[-tS_x(t)\\Bigr]_0^\\infty + \\int_0^\\infty S_x(t)\\right) \\\\ &= \\int_0^\\infty {}_{t}p_x \\end{aligned} \\] Note that \\([-t * S_x(t)]^{\\infty}_0\\) is 0 as the probability of living forever is 0. If the future lifetime variable is artificially limited to \\(n\\) years, then the expectation of the future lifetime is known as the Term Expectation of Life . It is similar denoted as \\(\\mathring{e}_{x:\\enclose{actuarial}{n}}\\) : \\[ \\begin{aligned} E(min(T_x, n)) &= \\int^{n}_0 {}_{t}p_x \\\\ \\mathring{e}_{x:\\enclose{actuarial}{n}} &= \\int^{n}_0 {}_{t}p_x \\end{aligned} \\] Thus, the complete expectation can be decomposed into two components: Term Expectation at the current age representing the \"early\" years Complete Expectation at a future age represenging the \"later\" years \\[ \\mathring{e}_x = \\mathring{e}_{x:n} + \\mathring{e}_{x+n} \\] However, the above makes an implicit assumption that the individual will survive the first n years. Thus, the second term needs to account for the probability of surviving another n years: \\[ \\mathring{e}_x = \\mathring{e}_{x:n} + {}_{n}p_x \\cdot \\mathring{e}_{x+n} \\] Continuous Variance The second moment can be calculated in a similar fashion through integration by parts: \\[ \\begin{aligned} E(T^2_x) &= \\int_{0}^{\\infty} t^2 f_x(t) \\\\ &= -\\left(\\Bigl[t^2 S_x(t)\\Bigr]_0^\\infty - \\int_{0}^{\\infty} 2tS_x(t) \\right) \\\\ &= 2 \\int_{0}^{\\infty} t \\cdot {}_{t}p_x \\end{aligned} \\] Thus, the Variance of the future lifetime about the Expectation is the following: \\[ \\begin{aligned} Var(T_x) &= E(T^2_x) - [E(T_x)]^2 \\\\ &= 2 \\int_{0}^{\\infty} t \\cdot {}_{t}p_x - \\left(\\int_0^\\infty {}_{t}p_x \\right)^2 \\end{aligned} \\] Discrete Lifetime If only the integer components of the future lifetime are considered, then it becomes a discrete distribution known as the Curtate Future Lifetime of the person. It is denoted by the random variable \\(K_x\\) , which is a truncated version of \\(T_x\\) . Formally, it is known as a Floor Function : \\[ \\begin{aligned} K_x &= \\lfloor T_x \\rfloor \\\\ K_x &= 0, 1, \\dots \\end{aligned} \\] It measures the number of full years that the individual is expected to live for. If \\(K_x = k\\) , then it means the individual will another \\(k\\) full years but not live past \\(k+1\\) years full years. In other words, they will die between \\(k\\) (inclusive) and \\(k+1\\) (Exclusive) years . Note that the distribution starts from 0 . If \\(K_x = 0\\) , then they will not live another full year; they will die within the current year. \\[ \\begin{aligned} P(K_x = k) &= P(x+k \\le T_0 < x+k+1) \\\\ &= P(k \\le T_x < k+1) \\\\ &= P(T_x < k+1) - P(T_x \\le k) \\\\ &= F_x(k+1) - F_x(k) \\\\ &= S_x(k) - S_x(k+1) \\\\ &= S_x(k) - S_x(k) * S_{x+k}(1) \\\\ &= S_x(k) (1 - S_{x+k}(1)) \\\\ &= S_x(k) F_{x+k}(1) \\\\ &= {} {}_{k}p_{x} {}_{}q_{x + k} \\\\ &= {}_{k|}q_{x} \\end{aligned} \\] Discrete Expectation Similarly, the Expected Curtate Lifetime is the expectation of \\(K_x\\) : \\[ \\begin{aligned} e_x &=(K_x) \\\\ &= \\sum_{k=0}^{\\infty} k ({}_{k}p_{x} - {}_{k+1}p_{x}) \\\\ &= 1 ({}_{1}p_{x} - {}_{2}p_{x}) + 2 ({}_{2}p_{x} - {}_{3}p_{x}) + 3 ({}_{3}p_{x} - {}_{4}p_{x}) + ... \\\\ &= {}_{1}p_{x} - {}_{2}p_{x} + 2{}_{2}p_{x} - 2{}_{3}p_{x} + 3 {}_{3}p_{x} - 3 {}_{4}p_{x} \\\\ &= {}_{1}p_{x} + 2{}_{2}p_{x} + 3 {}_{3}p_{x} + ... \\\\ &= \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\end{aligned} \\] The expectation can also recursively calculated: If the individual dies within the year with probability \\(q_x\\) , then the future lifetime is 0 If the individual survives the year with probability \\(p_x\\) , then the future lifetime is 1 (for surviving the current year) AND the expected lifetime past that \\[ \\begin{aligned} e_x &= \\begin{cases} 0,& q_x \\\\ 1+e_{x+1},& p_x \\end{cases} \\\\ \\\\ \\therefore e_x &= p_x (1+e_{x+1}) \\end{aligned} \\] Discrete Variance The second moment can be calculated in a similar fashion: \\[ \\begin{aligned} E(K^2_x) &= \\sum_{k=0}^{\\infty} k^2 ({}_{k}p_{x} - {}_{k+1}p_{x}) \\\\ &= 1 ({}_{1}p_{x} - {}_{2}p_{x}) + 4 ({}_{2}p_{x} - {}_{3}p_{x}) + 9 ({}_{3}p_{x} - {}_{4}p_{x}) + ... \\\\ &= {}_{1}p_{x} - {}_{2}p_{x} + 4{}_{2}p_{x} - 4{}_{3}p_{x} + 9 {}_{3}p_{x} - 9 {}_{4}p_{x} \\\\ &= {}_{1}p_{x} + 3{}_{2}p_{x} + 5 {}_{3}p_{x} + ... \\\\ &= \\sum_{k=1}^{\\infty} (2k-1) {}_{k}p_{x} \\\\ &= \\sum_{k=1}^{\\infty} 2k{}_{k}p_{x} - {}_{k}p_{x} \\\\ &= \\sum_{k=1}^{\\infty} 2k{}_{k}p_{x} - \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\\\ &= 2 \\sum_{k=1}^{\\infty} k{}_{k}p_{x} - e_x \\end{aligned} \\] Thus, the variance can be calculated as: \\[ \\begin{aligned} Var(K_x) &= E(K_x^2) - [E(K_x)]^2 \\\\ &= 2 \\sum_{k=1}^{\\infty} k{}_{k}p_{x} - e_x - (e_x)^2 \\end{aligned} \\] Trapezoidal Rule Note that the two expectations are similar to one another: Continuous Expectation - Area under survival function Discrete Expectation - Right Riemann Sum of the area under the survival function A Right Riemann Sum is when the curve passes through the top right of the rectangles use to approximate it. Recall from Calculus that the area under a curve can be estimated through the Trapezium Rule , which states that the area is approximately equal to the sum of the area of discrete trapeziums formed under the curve. \\[ \\begin{aligned} \\int^b_a f(x) & \\approx \\sum \\text{Area of Trapezium} \\\\ & \\approx \\sum \\frac{1}{2} h \\left[f(a+kh) + f(a+(k+1)h) \\right] \\\\ & \\approx \\frac{h}{2}[f(a) + f(a+h)] + \\frac{h}{2}[f(a+h) + f(a+2h)] + \\dots + \\frac{h}{2}[f(b-h) + f(b)] \\\\ & \\approx \\frac{h}{2} f(a) + h[f(a+h) + f(a+2h) + \\dots + f(b-h)] + \\frac{h}{2} f(b) \\\\ & \\approx h[f(a+h) + f(a+2h) + \\dots + f(b-h)] + \\frac{h}{2} [f(a)+f(b)] \\\\ & \\approx h[f(a) + f(a+h) + \\dots + f(b)] - \\frac{h}{2} [f(a)+f(b)] \\\\ & \\approx h[f(a) + f(a+h) + \\dots + f(b-h)] - \\frac{h}{2} [f(a)-f(b)] \\\\ & \\approx h[f(a+h) + f(a+2h) + \\dots + f(b)] + \\frac{h}{2} [f(a)-f(b)] \\end{aligned} \\] The proof relies on the fact that other than \\(f(a)\\) and \\(f(b)\\) , all other terms are repeated twice. Note that the last four lines of the proof are all variations of the same thing: Both \\(f(a)\\) and \\(f(b)\\) excluded from main expression Both \\(f(a)\\) and \\(f(b)\\) included in main expression Only \\(f(a)\\) included in main expression; Main expression is a Left Riemman Sum Only \\(f(b)\\) included in main expression; Main expression is a Right Riemman Sum Since the discrete expectation is a right riemann sum, the last expression of the trapezoidal rule should be used. This allows the continuous expectation to be expressed using the discrete expectation, assuming \\(h=1\\) : \\[ \\begin{aligned} \\mathring{e}_x &= \\int^{\\infty}_{0} S_x(t) \\\\ & \\approx h[f(a+h) + f(a+2h) + \\dots + f(b)] + \\frac{h}{2} [f(a)-f(b)] \\\\ & \\approx (1) [S_x(1) + S_x(2) + \\dots + S_x(\\infty)] + \\frac{1}{2} [S_x(0)-S_x(\\infty)] \\\\ & \\approx [S_x(1) + S_x(2) + \\dots + S_x(\\infty)] + \\frac{1}{2} [1-0] \\\\ & \\approx e_x + \\frac{1}{2} \\end{aligned} \\] An alternative way to view the above is that the area of the trapezoid is the sum of the riemann rectangles and a triangle . Euler Maclaurin Formula Note that the Trapezoidal Rule is not perfect - there is an inherent error in trying to approximate a curve using a line. \\[ \\begin{aligned} \\int^b_a f(x) & \\approx \\sum (\\text{Area of Trapezium} + \\text{Error})\\\\ \\end{aligned} \\] The error can be positive or negative, depending on the shape of the curve in that interval. The error term is calculated by taking the difference between the integral and the trapezium, and is generalized using the taylor series. The final result is known as the Euler Maclaurin Formula : \\[ \\sum \\text{Error} = \\frac{h^2}{12} [f'(a)-f'(b)] \\] Given that a taylor series was used, the error can be expressed as the sum of many different terms, but higher powers are ignored . Thus, the trapezoidal approximation for the expectation can be made more precise: \\[ \\begin{aligned} \\mathring{e}_x & \\approx e_x + \\frac{1}{2} + \\frac{1^2}{12} [S'(0)-S'(\\infty)] \\\\ & \\approx e_x + \\frac{1}{2} + \\frac{1}{12} [S'(0)] \\end{aligned} \\] Note that the above two concepts are usually covered in the Life Annuities section in a more complicated scenario, known as the Woolhouse Approximation . Although the above will rarely be used in this manner, it provides a simple example to understand the key concepts behind it. Parametric Survival Models Given the importance of \\(\\mu_x\\) , several mathematical functions have been made to describe the force of mortality, known as a Parametric Survival Model . De Moivre's Model Gompertz Model The most commonly used model is Gompertz Model , which suggests that mortality increases exponentially with age . The is highly intuitive, as older people have a higher probability of death due to health issues related to age. It is interesting to note that the ratio of death probabilities year over year after the age of 30 is approximately constant - which means that the death probability increases geometrically . Let \\(\\lambda\\) be the geometric mean of the death probabilities: \\[ \\begin{aligned} q_{71} &= \\lambda \\cdot q_{70} \\\\ \\\\ q_{72} &= \\lambda \\cdot q_{71} \\\\ &= \\lambda^2 \\cdot q_{70} \\\\ \\\\ \\therefore q_{x+t} &= \\lambda^t q_{x} \\end{aligned} \\] Let \\(\\lambda = e^b\\) . Thus, the model can be expressed continuously as the following: \\[ \\mu_{x+t} = Bc^{x+t} \\] Makeham Gompertz Model Makeham added a constant \\(A\\) into the Gompertz model, resulting in the Makeham Gompertz Model : \\[ \\mu_{x+t} = A + Bc^{x+t} \\] \\(A\\) represents the age independent mortality - dying from reasons unrelated to age, such as from accidents or natural catastrophes. \\[ \\begin{aligned} S_x(t) &= - \\int^{t}_{0} A + Bc^{x+t} \\\\ &= \\int^{t}_{0} -A - Bc^{x+t} \\\\ &= \\left[-At - \\frac{B}{\\ln c} c^{x+t} \\right]^{t}_{0} \\\\ &= \\left[-At - \\frac{B}{\\ln c} c^{x+t} \\right] - \\left[-\\frac{B}{\\ln c} c^{x} \\right] \\\\ &= -At - \\frac{B}{\\ln c} c^{x+t} + \\frac{B}{\\ln c} c^{x} \\\\ &= -At - \\frac{B}{\\ln c} c^x [c^t - 1] \\end{aligned} \\]","title":"Survival Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#survival-models","text":"Survival Models are probability distributions that measure the time to failure of an entity, or phrased another way, the future lifetime of an entity. In an Actuarial context, it measures the time to death of a person. Since survival models measure time , they are denoted by the continuous random variable \\(T_i\\) , where the subscript represents the age of the person being studied.","title":"Survival Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#newborn-lifetime","text":"The base survival model measures the future lifetime of a newborn ( aged 0 ), denoted by the random variable \\(T_0\\) . The CDF thus represents the probability that the newborn dies before age \\(t\\) : \\[ F_0(t) = P(T_0 \\le t) \\] The Survival Function is the complement of the CDF and is the probability that the newborn survives till a certain age \\(t\\) . From a different perspective, it can also be seen as the probability of the newborn dying AFTER age \\(t\\) : \\[ \\begin{aligned} S_0(t) &= P(T_0 \\ge t) \\\\ &= 1 - P(T_0 \\le t) \\\\ &= 1 - F_0(t) \\end{aligned} \\] Note that all humans will inevitably die : \\[ \\begin{aligned} \\lim_{t \\to \\infty} S_0 (t) = 0 \\iff \\lim_{t \\to \\infty} F_0 (t) = 1 \\end{aligned} \\]","title":"Newborn Lifetime"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#continuous-lifetime","text":"The above can be generalized for a person aged \\(x\\) , denoted by \\(T_x\\) . However, the intepretation of the two functions changes slightly. The CDF instead represents the probability that the person dies within \\(t\\) years from age \\(x\\) : \\[ \\begin{aligned} F_x(t) &= P(T_x \\le t) \\\\ T_x &= 1, 2, \\dots \\end{aligned} \\] It can also be written in terms of the newborn lifetime . The key is to understand that the newborn is assumed to survive till age \\(x\\) and then die within the next \\(t\\) years: \\[ \\begin{aligned} P(T_x \\le t) &= P(T_0 \\le x + t | T_0 > x) \\\\ &= \\frac{P(x < T_0 < x +t)}{P(T_0 > x)} \\\\ &= \\frac{P(T_0 < x +t)-P(T_0 < x)}{P(T_0 > x)} \\\\ &= \\frac{F_0(x+t) - F_0(x)}{S_0(x)} \\end{aligned} \\] Conversely, the survival function is the probability that the person survives another \\(t\\) years or dies AFTER \\(t\\) years : \\[ S_x(t) = P(T_x \\ge t) \\] It can also be written in terms of the newborn lifetime following the same logic as above. The newborn is assumed to survive till age \\(x\\) and then survive another \\(t\\) years: \\[ \\begin{aligned} P(T_x \\ge t) &= P(T_0 \\ge x + t | T_0 > x) \\\\ &= \\frac{P(T_0 > x + t)}{P(T_0 > x)} \\\\ &= \\frac{S_0(x+t)}{S_0(x)} \\end{aligned} \\] Note that since the surviving till age \\(x\\) is a subset of surviving till age \\(x+t\\) , the numerator can be simplified to just surviving till \\(x+t\\) . Note the survival function can thus be re-written as the following: \\[ \\begin{aligned} S_x(t) &= \\frac{S_0(x+t)}{S_0(x)} \\\\ S_0(x+t) &= S_0(x)S_x(t) \\\\ \\end{aligned} \\] The probability of the survival of a newborn till age \\(x+t\\) is the product of: The probability of survival of a newborn till age \\(x\\) Given that the newborn survived till age \\(x\\) , that they will survive another \\(t\\) years This can be generalized for people of any age to obtain: \\[ S_x(t+u) = S_x(t) S_{x+t}(u) \\] It is important to remember to \"update\" the starting age of the second term to the \"new age\". The opposite of the above is when an individual surives another \\(t\\) years and then dies within the following \\(u\\) years . This is known as the probability of Deferred Death : \\[ \\begin{aligned} P(t < T_x < t+u) &= P(T_x < t+u) - P(T_x < t) \\\\ &= F_x(t+u) - F_x(t) \\\\ &= 1 - S_x(t+u) - (1 - S_x(t)) \\\\ &= 1 - S_x(t)S_{x+t}(u) - 1 + S_x(t) \\\\ &= S_x(t) - S_x(t)S_{x+t}(u) \\\\ &= S_x(t)(1-S_{x+t}(u)) \\\\ &= S_x(t)F_{x+t}(u) \\end{aligned} \\]","title":"Continuous Lifetime"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#probability-tree-perspective","text":"Since the probabilites of death and survival are conditional on the age of the individual, they can be better expressed in the form of a probability tree: Thus, it can be shown through recursion that the probability of surviving \\(t\\) years is equal to the sum of the probabilities of deferred deaths for every year after: \\[ \\begin{aligned} S_x(1) &= S_x(1)F_{x+1}(1) + S_x(1)S_{x+1}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(1) [S_{x+1}(1)F_{x+2}(1) + S_{x+1}(1)S_{x+2}(1)] \\\\ &= S_x(1)F_{x+1}(1) + S_x(1) S_{x+1}(1)F_{x+2}(1) + S_x(1) S_{x+1}(1)S_{x+2}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(2)F_{x+2}(1) + S_x(2)S_{x+2}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(2)F_{x+2}(1) + \\dots \\\\ \\\\ \\therefore S_x(t) &= \\sum S_x(t)F{x+t}(1) \\end{aligned} \\] Recall that the probability of surviving till a certain age is simply the probability that the individual will die sometime after that age. The above expression solidifies this, where the RHS is the probability of dying in every possible age after .","title":"Probability Tree Perspective"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#force-of-mortality","text":"The Force of Mortality is the probability of dying instantly ; in an infinitely small unit of time (normalized by unit time): \\[ \\begin{aligned} \\mu_{0+x} &= \\lim_{h \\to 0} \\frac{P(T_x < h)}{h} \\\\ &= \\lim_{h \\to 0} \\frac{1 - S_x(h)}{h} \\\\ &= \\lim_{h \\to 0} \\frac{1 - (\\frac{S_0(x+h)}{S_0(x)})}{h} \\\\ &= \\lim_{h \\to 0} \\frac{S_0(x) - S_0(x+h)}{S_0(x)h} \\\\ &= \\frac{1}{S_0(x)} \\lim_{h \\to 0} \\frac{S_0(x) - S_0(x+h)}{h} \\\\ &= - \\frac{1}{S_0(x)} \\lim_{h \\to 0} \\frac{S_0(x+h) - S_0(x)}{h} \\\\ &= - \\frac{1}{S_0(x)} S'_0(x) \\\\ &= - \\frac{S'_0(x)}{S_0(x)} \\end{aligned} \\] Since \\(S_0(x)\\) is the complement of \\(F_0(x)\\) , the derivative of the survival function can be written in terms of the PDF of the future lifetime: \\[ \\begin{aligned} f_0(x) &= \\frac{d}{dx} F_0(x) \\\\ &= \\frac{d}{dx} (1-S_0(x)) \\\\ &= -S'_0(x) \\end{aligned} \\] The force of mortality can be rewritten as the following: \\[ \\mu_{0+x} = \\frac{f_0(x)}{S_0(x)} \\] Thus, the force of mortality can also be intepreted as the CONDITIONAL probability distribution of death for a person aged \\(x\\) , assuming that the person survives till age \\(x\\) . \\(f_0(x)\\) is UNCONDITIONAL distribution of the future lifetime as it does not assume the individual lives to any age.","title":"Force of Mortality"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#alternative-death-function","text":"More generally, the force of mortality can be rewritten as: \\[ \\begin{aligned} \\mu_{0+x} &= \\frac{f_0(x)}{S_0(x)} \\\\ \\mu_{x+t} &= \\frac{f_x(t)}{S_x(t)} \\\\ f_x(t) &= S_x(t)\\mu_{x+t} \\\\ F_x(t) &= \\int_0^t S_x(t)\\mu_{x+t} \\end{aligned} \\] The probability of death in an interval is the SUM of the product of the probability of surviving till a given age and then dying in an infinitely small time afterwards. The sum of all the infinitely small time periods is equal to the given time interval. Following this definition, for extremely small time intervals , the force of mortality can be used to approximate the probability of death in that interval: \\[ P(T_x < h) \\approx h * \\mu_{x} \\]","title":"Alternative Death Function"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#alternative-survival-function","text":"More generally, the force of mortality can be rewritten as: \\[ \\begin{aligned} \\mu_{x} &= - \\frac{S'_0(x)}{S_0(x)} \\\\ \\mu_{x + t} &= \\frac{-S'_x(t)}{S_x(t)} \\\\ \\mu_{x + t} &= - \\frac{d}{dt} (\\ln S_x(t)) \\\\ \\ln S_x(t) &= - \\int_{0}^{t} \\mu_{x + t} \\\\ S_x(t) &= e ^ {- \\int_{0}^{t} \\mu_{x + t}} \\\\ \\end{aligned} \\] Note that most expressions give \\(\\mu_x\\) , but it is \\(\\mu_{x+t}\\) that is usually of interest - simply adjust the parameters accordingly. Thus, if \\(\\mu_x\\) is known for all ages, then the survival probabilites for any age can be calculated from it. In other words, the distribution of \\(T_x\\) can be determined from the force of mortality .","title":"Alternative Survival Function"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#actuarial-notation","text":"Given how often these expressions are used, they are often abbreviated using the International Actuarial Notation : \\[ \\begin{aligned} {}_{t}p_{x} &= S_x(t) \\\\ {}_{t}q_{x} &= F_x(t) \\\\ {}_{t}q_{x} + {}_{t}p_{x} &= 1 \\end{aligned} \\] If \\(t=1\\) , it is omitted; EG. \\({}_{1}q_{x} = q_{x}\\) . Deferred Deaths are expressed using the pipe symbol: \\[ \\begin{aligned} {}_{t|u}q_{x} &= S_x(t)F_{x+t}(u) \\\\ &= {}_{t}p_{x} * {}_{u}q_{x+t} \\end{aligned} \\] These notations are typically used when \\(t\\) is an integer, but not necessarily so, as will be seen in the next section.","title":"Actuarial Notation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#continuous-expectation","text":"The Expectation of the distribution of future lifetime is the Life Expectancy of the individual. It is commonly calculated for newborns as a measure of the general health of the population. In an insurance context, it is known as the Complete Expectation of Life and is denoted by \\(\\mathring{e}_{x}\\) : \\[ \\begin{aligned} E(T_x) &= \\int_{0}^{\\infty} t f_x(t) \\\\ e_{x} &= \\int_{0}^{\\infty} t S_x(t)\\mu_{x+t} \\end{aligned} \\] Through integration by parts, \\[ \\begin{aligned} \\mathring{e}_{x} &= \\left(\\Bigl[-tS_x(t)\\Bigr]_0^\\infty + \\int_0^\\infty S_x(t)\\right) \\\\ &= \\int_0^\\infty {}_{t}p_x \\end{aligned} \\] Note that \\([-t * S_x(t)]^{\\infty}_0\\) is 0 as the probability of living forever is 0. If the future lifetime variable is artificially limited to \\(n\\) years, then the expectation of the future lifetime is known as the Term Expectation of Life . It is similar denoted as \\(\\mathring{e}_{x:\\enclose{actuarial}{n}}\\) : \\[ \\begin{aligned} E(min(T_x, n)) &= \\int^{n}_0 {}_{t}p_x \\\\ \\mathring{e}_{x:\\enclose{actuarial}{n}} &= \\int^{n}_0 {}_{t}p_x \\end{aligned} \\] Thus, the complete expectation can be decomposed into two components: Term Expectation at the current age representing the \"early\" years Complete Expectation at a future age represenging the \"later\" years \\[ \\mathring{e}_x = \\mathring{e}_{x:n} + \\mathring{e}_{x+n} \\] However, the above makes an implicit assumption that the individual will survive the first n years. Thus, the second term needs to account for the probability of surviving another n years: \\[ \\mathring{e}_x = \\mathring{e}_{x:n} + {}_{n}p_x \\cdot \\mathring{e}_{x+n} \\]","title":"Continuous Expectation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#continuous-variance","text":"The second moment can be calculated in a similar fashion through integration by parts: \\[ \\begin{aligned} E(T^2_x) &= \\int_{0}^{\\infty} t^2 f_x(t) \\\\ &= -\\left(\\Bigl[t^2 S_x(t)\\Bigr]_0^\\infty - \\int_{0}^{\\infty} 2tS_x(t) \\right) \\\\ &= 2 \\int_{0}^{\\infty} t \\cdot {}_{t}p_x \\end{aligned} \\] Thus, the Variance of the future lifetime about the Expectation is the following: \\[ \\begin{aligned} Var(T_x) &= E(T^2_x) - [E(T_x)]^2 \\\\ &= 2 \\int_{0}^{\\infty} t \\cdot {}_{t}p_x - \\left(\\int_0^\\infty {}_{t}p_x \\right)^2 \\end{aligned} \\]","title":"Continuous Variance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#discrete-lifetime","text":"If only the integer components of the future lifetime are considered, then it becomes a discrete distribution known as the Curtate Future Lifetime of the person. It is denoted by the random variable \\(K_x\\) , which is a truncated version of \\(T_x\\) . Formally, it is known as a Floor Function : \\[ \\begin{aligned} K_x &= \\lfloor T_x \\rfloor \\\\ K_x &= 0, 1, \\dots \\end{aligned} \\] It measures the number of full years that the individual is expected to live for. If \\(K_x = k\\) , then it means the individual will another \\(k\\) full years but not live past \\(k+1\\) years full years. In other words, they will die between \\(k\\) (inclusive) and \\(k+1\\) (Exclusive) years . Note that the distribution starts from 0 . If \\(K_x = 0\\) , then they will not live another full year; they will die within the current year. \\[ \\begin{aligned} P(K_x = k) &= P(x+k \\le T_0 < x+k+1) \\\\ &= P(k \\le T_x < k+1) \\\\ &= P(T_x < k+1) - P(T_x \\le k) \\\\ &= F_x(k+1) - F_x(k) \\\\ &= S_x(k) - S_x(k+1) \\\\ &= S_x(k) - S_x(k) * S_{x+k}(1) \\\\ &= S_x(k) (1 - S_{x+k}(1)) \\\\ &= S_x(k) F_{x+k}(1) \\\\ &= {} {}_{k}p_{x} {}_{}q_{x + k} \\\\ &= {}_{k|}q_{x} \\end{aligned} \\]","title":"Discrete Lifetime"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#discrete-expectation","text":"Similarly, the Expected Curtate Lifetime is the expectation of \\(K_x\\) : \\[ \\begin{aligned} e_x &=(K_x) \\\\ &= \\sum_{k=0}^{\\infty} k ({}_{k}p_{x} - {}_{k+1}p_{x}) \\\\ &= 1 ({}_{1}p_{x} - {}_{2}p_{x}) + 2 ({}_{2}p_{x} - {}_{3}p_{x}) + 3 ({}_{3}p_{x} - {}_{4}p_{x}) + ... \\\\ &= {}_{1}p_{x} - {}_{2}p_{x} + 2{}_{2}p_{x} - 2{}_{3}p_{x} + 3 {}_{3}p_{x} - 3 {}_{4}p_{x} \\\\ &= {}_{1}p_{x} + 2{}_{2}p_{x} + 3 {}_{3}p_{x} + ... \\\\ &= \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\end{aligned} \\] The expectation can also recursively calculated: If the individual dies within the year with probability \\(q_x\\) , then the future lifetime is 0 If the individual survives the year with probability \\(p_x\\) , then the future lifetime is 1 (for surviving the current year) AND the expected lifetime past that \\[ \\begin{aligned} e_x &= \\begin{cases} 0,& q_x \\\\ 1+e_{x+1},& p_x \\end{cases} \\\\ \\\\ \\therefore e_x &= p_x (1+e_{x+1}) \\end{aligned} \\]","title":"Discrete Expectation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#discrete-variance","text":"The second moment can be calculated in a similar fashion: \\[ \\begin{aligned} E(K^2_x) &= \\sum_{k=0}^{\\infty} k^2 ({}_{k}p_{x} - {}_{k+1}p_{x}) \\\\ &= 1 ({}_{1}p_{x} - {}_{2}p_{x}) + 4 ({}_{2}p_{x} - {}_{3}p_{x}) + 9 ({}_{3}p_{x} - {}_{4}p_{x}) + ... \\\\ &= {}_{1}p_{x} - {}_{2}p_{x} + 4{}_{2}p_{x} - 4{}_{3}p_{x} + 9 {}_{3}p_{x} - 9 {}_{4}p_{x} \\\\ &= {}_{1}p_{x} + 3{}_{2}p_{x} + 5 {}_{3}p_{x} + ... \\\\ &= \\sum_{k=1}^{\\infty} (2k-1) {}_{k}p_{x} \\\\ &= \\sum_{k=1}^{\\infty} 2k{}_{k}p_{x} - {}_{k}p_{x} \\\\ &= \\sum_{k=1}^{\\infty} 2k{}_{k}p_{x} - \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\\\ &= 2 \\sum_{k=1}^{\\infty} k{}_{k}p_{x} - e_x \\end{aligned} \\] Thus, the variance can be calculated as: \\[ \\begin{aligned} Var(K_x) &= E(K_x^2) - [E(K_x)]^2 \\\\ &= 2 \\sum_{k=1}^{\\infty} k{}_{k}p_{x} - e_x - (e_x)^2 \\end{aligned} \\]","title":"Discrete Variance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#trapezoidal-rule","text":"Note that the two expectations are similar to one another: Continuous Expectation - Area under survival function Discrete Expectation - Right Riemann Sum of the area under the survival function A Right Riemann Sum is when the curve passes through the top right of the rectangles use to approximate it. Recall from Calculus that the area under a curve can be estimated through the Trapezium Rule , which states that the area is approximately equal to the sum of the area of discrete trapeziums formed under the curve. \\[ \\begin{aligned} \\int^b_a f(x) & \\approx \\sum \\text{Area of Trapezium} \\\\ & \\approx \\sum \\frac{1}{2} h \\left[f(a+kh) + f(a+(k+1)h) \\right] \\\\ & \\approx \\frac{h}{2}[f(a) + f(a+h)] + \\frac{h}{2}[f(a+h) + f(a+2h)] + \\dots + \\frac{h}{2}[f(b-h) + f(b)] \\\\ & \\approx \\frac{h}{2} f(a) + h[f(a+h) + f(a+2h) + \\dots + f(b-h)] + \\frac{h}{2} f(b) \\\\ & \\approx h[f(a+h) + f(a+2h) + \\dots + f(b-h)] + \\frac{h}{2} [f(a)+f(b)] \\\\ & \\approx h[f(a) + f(a+h) + \\dots + f(b)] - \\frac{h}{2} [f(a)+f(b)] \\\\ & \\approx h[f(a) + f(a+h) + \\dots + f(b-h)] - \\frac{h}{2} [f(a)-f(b)] \\\\ & \\approx h[f(a+h) + f(a+2h) + \\dots + f(b)] + \\frac{h}{2} [f(a)-f(b)] \\end{aligned} \\] The proof relies on the fact that other than \\(f(a)\\) and \\(f(b)\\) , all other terms are repeated twice. Note that the last four lines of the proof are all variations of the same thing: Both \\(f(a)\\) and \\(f(b)\\) excluded from main expression Both \\(f(a)\\) and \\(f(b)\\) included in main expression Only \\(f(a)\\) included in main expression; Main expression is a Left Riemman Sum Only \\(f(b)\\) included in main expression; Main expression is a Right Riemman Sum Since the discrete expectation is a right riemann sum, the last expression of the trapezoidal rule should be used. This allows the continuous expectation to be expressed using the discrete expectation, assuming \\(h=1\\) : \\[ \\begin{aligned} \\mathring{e}_x &= \\int^{\\infty}_{0} S_x(t) \\\\ & \\approx h[f(a+h) + f(a+2h) + \\dots + f(b)] + \\frac{h}{2} [f(a)-f(b)] \\\\ & \\approx (1) [S_x(1) + S_x(2) + \\dots + S_x(\\infty)] + \\frac{1}{2} [S_x(0)-S_x(\\infty)] \\\\ & \\approx [S_x(1) + S_x(2) + \\dots + S_x(\\infty)] + \\frac{1}{2} [1-0] \\\\ & \\approx e_x + \\frac{1}{2} \\end{aligned} \\] An alternative way to view the above is that the area of the trapezoid is the sum of the riemann rectangles and a triangle .","title":"Trapezoidal Rule"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#euler-maclaurin-formula","text":"Note that the Trapezoidal Rule is not perfect - there is an inherent error in trying to approximate a curve using a line. \\[ \\begin{aligned} \\int^b_a f(x) & \\approx \\sum (\\text{Area of Trapezium} + \\text{Error})\\\\ \\end{aligned} \\] The error can be positive or negative, depending on the shape of the curve in that interval. The error term is calculated by taking the difference between the integral and the trapezium, and is generalized using the taylor series. The final result is known as the Euler Maclaurin Formula : \\[ \\sum \\text{Error} = \\frac{h^2}{12} [f'(a)-f'(b)] \\] Given that a taylor series was used, the error can be expressed as the sum of many different terms, but higher powers are ignored . Thus, the trapezoidal approximation for the expectation can be made more precise: \\[ \\begin{aligned} \\mathring{e}_x & \\approx e_x + \\frac{1}{2} + \\frac{1^2}{12} [S'(0)-S'(\\infty)] \\\\ & \\approx e_x + \\frac{1}{2} + \\frac{1}{12} [S'(0)] \\end{aligned} \\] Note that the above two concepts are usually covered in the Life Annuities section in a more complicated scenario, known as the Woolhouse Approximation . Although the above will rarely be used in this manner, it provides a simple example to understand the key concepts behind it.","title":"Euler Maclaurin Formula"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#parametric-survival-models","text":"Given the importance of \\(\\mu_x\\) , several mathematical functions have been made to describe the force of mortality, known as a Parametric Survival Model .","title":"Parametric Survival Models"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#de-moivres-model","text":"","title":"De Moivre's Model"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#gompertz-model","text":"The most commonly used model is Gompertz Model , which suggests that mortality increases exponentially with age . The is highly intuitive, as older people have a higher probability of death due to health issues related to age. It is interesting to note that the ratio of death probabilities year over year after the age of 30 is approximately constant - which means that the death probability increases geometrically . Let \\(\\lambda\\) be the geometric mean of the death probabilities: \\[ \\begin{aligned} q_{71} &= \\lambda \\cdot q_{70} \\\\ \\\\ q_{72} &= \\lambda \\cdot q_{71} \\\\ &= \\lambda^2 \\cdot q_{70} \\\\ \\\\ \\therefore q_{x+t} &= \\lambda^t q_{x} \\end{aligned} \\] Let \\(\\lambda = e^b\\) . Thus, the model can be expressed continuously as the following: \\[ \\mu_{x+t} = Bc^{x+t} \\]","title":"Gompertz Model"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#makeham-gompertz-model","text":"Makeham added a constant \\(A\\) into the Gompertz model, resulting in the Makeham Gompertz Model : \\[ \\mu_{x+t} = A + Bc^{x+t} \\] \\(A\\) represents the age independent mortality - dying from reasons unrelated to age, such as from accidents or natural catastrophes. \\[ \\begin{aligned} S_x(t) &= - \\int^{t}_{0} A + Bc^{x+t} \\\\ &= \\int^{t}_{0} -A - Bc^{x+t} \\\\ &= \\left[-At - \\frac{B}{\\ln c} c^{x+t} \\right]^{t}_{0} \\\\ &= \\left[-At - \\frac{B}{\\ln c} c^{x+t} \\right] - \\left[-\\frac{B}{\\ln c} c^{x} \\right] \\\\ &= -At - \\frac{B}{\\ln c} c^{x+t} + \\frac{B}{\\ln c} c^{x} \\\\ &= -At - \\frac{B}{\\ln c} c^x [c^t - 1] \\end{aligned} \\]","title":"Makeham Gompertz Model"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/","text":"Life Tables Life Tables existed long before the survival models in the previous section. The mortality functions were presented in tabular form, where the probabilities and expectations were then calculated from. Both the survival model and life table are equivalent ways of obtaining the same results. Basic Life Table The life table is constructed based on the mortality of a group of people known as the Cohort . The initial age of the group people is known as the Starting Age , denoted by \\(\\alpha\\) . The initial number of people in the cohort is known as the Radix , which is usually a large round number (EG. 1000) Conversely, the maximum age is known as the Terminal Age , denoted by \\(\\omega\\) . Everybody in the cohort is expected to gradually die by the terminal age . The number of people alive from the cohort at age \\(x\\) is denoted as \\(\\ell_x\\) : \\[ \\begin{aligned} \\ell_0 &= 1000 \\\\ \\ell_{\\omega} &= 0 \\\\ \\\\ \\therefore \\ell_{x+t} &< \\ell_{x} \\end{aligned} \\] From this basic life table, several important values can be calculated: The expected number of deaths over a period, \\({}_{n}d_{x}\\) The probability of survival past a period, \\({}_{n}p_{x}\\) The probability of death within a period, \\({}_{n}q_{x}\\) The deferred probability of death within a period, \\({}_{s|t}q_{x}\\) \\[ \\begin{aligned} {}_{n}d_{x} &= \\ell_{x} - \\ell_{x+n} \\\\ {}_{n}p_{x} &= \\frac{\\ell_{x+t}}{\\ell_{x}} \\\\ {}_{n}q_{x} &= \\frac{{}_{n}d_{x}}{\\ell_{x}} \\\\ {}_{s|t}q_{x} &= \\frac{{}_{t}d_{x+s}}{l_{x}} \\\\ \\end{aligned} \\] Thus, it can be seen that the probabilities are simply the proportion of people who died/survived over the a given period. Fractional Age Assumptions One limitation of the life table is that it is computed at discrete ages while many problems require probabilities for non-discrete ages. Thus, several assumptions about the life table that allows non-discrete values to be interpolated from the discrete ones, known as Fractional Age Assumptions . Uniform Distribution of Deaths The Uniform Distribution of Deaths (UDD) is an assumption that allows for Linear Interpolation between discrete ages. It assumes that there is a uniform distribution of deaths between ages such that the probability of survival decreases linearly over the year. The UDD assumption is less appropriate for older individuals as the probability of death begins to increase exponentially, even from month to month. Let the fractional age be \\(s\\) . Thus, the probability using fractional ages is simply the weighted average of the discrete points: \\[ \\begin{aligned} {}_{s}p_{x} &= (1-s) {}_{0}p_{x} + s {}_{1}p_{x} \\\\ &= (1-s)\\frac{\\ell_{x}}{\\ell_{x}} + s \\frac{\\ell_{x+1}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x}-s\\ell_{x} + s\\ell_{x+1}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x} - s(\\ell_{x}-\\ell_{x+1})}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x} - s d_{x}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x}}{\\ell_{x}} - s\\frac{d_{x}}{\\ell_{x}} \\\\ &= 1 - s q_{x} \\\\ \\\\ \\therefore {}_{s}q_{x} &= s \\cdot q_x \\end{aligned} \\] Note that \\({}_{0}p_{x}=1\\) as it is the probability of surviving in that moment; which should be guaranteed. Based on this, the PDF and Force of Mortality can also be determined: \\[ \\begin{aligned} F_x(s) &= s \\cdot q_x \\\\ \\\\ f_x(s) &= \\frac{d}{ds} (s \\cdot q_x) \\\\ &= q_x \\\\ \\\\ \\mu_{x+t} &= \\frac{f_x(s)}{S_x(s)} \\\\ &= \\frac{q_x}{1-sq_x} \\end{aligned} \\] Constant Force of Mortality The Constant Force of Mortality is an assumption that allows for Exponential Interpolation between discrete ages. It assumes that there is a constant force of mortality between ages, such that the survival probability decreases exponentially over the year. \\[ \\begin{aligned} {}_{1}p_{x} &= e^{-\\int^{x+1}_{x} \\mu_c} \\\\ &= e^{-[t\\mu_c]^{x+1}_{x}} \\\\ &= e^{-[(x+1)\\mu_c - (x)\\mu_c ]} \\\\ &= e^{-[\\mu_c (x+1-x)]} \\\\ &= e^{-\\mu_c} \\\\ \\mu_c &= - \\ln p_{x} \\\\ \\\\ \\therefore {}_{s}p_{x} &= e^{-\\int^{x+s}_{x} \\mu_c} \\\\ &= e^{-s\\mu_c} \\\\ &= e^{s\\ln p_{x}} \\\\ &= e^{\\ln (p_{x})^{s}} \\\\ &= (p_{x})^{s} \\\\ \\\\ \\therefore {}_{s}q_{x} &= 1 - {}_{s}p_{x} \\\\ &= 1 - (p_{x})^{s} \\\\ &= 1 - [(1-q_{x})]^{s} \\end{aligned} \\] Select & Ultimate Mortality Mortality rates for the general population versus individuals who buy life insurance tend to be different. Generally speaking, people who purchase insurance tend to be richer and thus have better mortality rates than the general population . Within the individuals who purchase life insurance, those who have recently purchased a policy tend to have better mortality . This is because these individuals would have gone through Medical Underwriting and thus is expected to be in better health. The extent of the better mortality decreases over time and after a few years, they should experience the same mortality as the rest of the individuals who purchased life insurance. The duration of time is dependent on the rigorousness of the underwriting process . Formally speaking, the individuals who purchased life insurance were selected by the underwriting process and thus the better mortality experienced is known as Select Mortality . Similarly, the time whereby the select mortality is better is known as the Select Period . The select mortality ultimately converges with the non-select group, known as the Ultimate Mortality . In actuarial notation, subscript \\([x]\\) is used to distinguish select mortality from ultimate mortality, where the select period is denoted as \\(d\\) : \\[ \\begin{aligned} \\begin{cases} q_{[x]+t} < q_{x+t},& t < d \\\\ q_{[x]+t} = q_{x+t},& t \\ge d \\end{cases} \\end{aligned} \\] Note that the definition of \\(x\\) is dependent on the questions: \\(x\\) is the select age - \\([x], [x]+1, [x]+2, ...\\) \\(x\\) is the ultimate age - \\([x], [x-1], [x-2], ...\\) if given table, high chance the rate is not 5%","title":"Life Tables"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#life-tables","text":"Life Tables existed long before the survival models in the previous section. The mortality functions were presented in tabular form, where the probabilities and expectations were then calculated from. Both the survival model and life table are equivalent ways of obtaining the same results.","title":"Life Tables"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#basic-life-table","text":"The life table is constructed based on the mortality of a group of people known as the Cohort . The initial age of the group people is known as the Starting Age , denoted by \\(\\alpha\\) . The initial number of people in the cohort is known as the Radix , which is usually a large round number (EG. 1000) Conversely, the maximum age is known as the Terminal Age , denoted by \\(\\omega\\) . Everybody in the cohort is expected to gradually die by the terminal age . The number of people alive from the cohort at age \\(x\\) is denoted as \\(\\ell_x\\) : \\[ \\begin{aligned} \\ell_0 &= 1000 \\\\ \\ell_{\\omega} &= 0 \\\\ \\\\ \\therefore \\ell_{x+t} &< \\ell_{x} \\end{aligned} \\] From this basic life table, several important values can be calculated: The expected number of deaths over a period, \\({}_{n}d_{x}\\) The probability of survival past a period, \\({}_{n}p_{x}\\) The probability of death within a period, \\({}_{n}q_{x}\\) The deferred probability of death within a period, \\({}_{s|t}q_{x}\\) \\[ \\begin{aligned} {}_{n}d_{x} &= \\ell_{x} - \\ell_{x+n} \\\\ {}_{n}p_{x} &= \\frac{\\ell_{x+t}}{\\ell_{x}} \\\\ {}_{n}q_{x} &= \\frac{{}_{n}d_{x}}{\\ell_{x}} \\\\ {}_{s|t}q_{x} &= \\frac{{}_{t}d_{x+s}}{l_{x}} \\\\ \\end{aligned} \\] Thus, it can be seen that the probabilities are simply the proportion of people who died/survived over the a given period.","title":"Basic Life Table"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#fractional-age-assumptions","text":"One limitation of the life table is that it is computed at discrete ages while many problems require probabilities for non-discrete ages. Thus, several assumptions about the life table that allows non-discrete values to be interpolated from the discrete ones, known as Fractional Age Assumptions .","title":"Fractional Age Assumptions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#uniform-distribution-of-deaths","text":"The Uniform Distribution of Deaths (UDD) is an assumption that allows for Linear Interpolation between discrete ages. It assumes that there is a uniform distribution of deaths between ages such that the probability of survival decreases linearly over the year. The UDD assumption is less appropriate for older individuals as the probability of death begins to increase exponentially, even from month to month. Let the fractional age be \\(s\\) . Thus, the probability using fractional ages is simply the weighted average of the discrete points: \\[ \\begin{aligned} {}_{s}p_{x} &= (1-s) {}_{0}p_{x} + s {}_{1}p_{x} \\\\ &= (1-s)\\frac{\\ell_{x}}{\\ell_{x}} + s \\frac{\\ell_{x+1}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x}-s\\ell_{x} + s\\ell_{x+1}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x} - s(\\ell_{x}-\\ell_{x+1})}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x} - s d_{x}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x}}{\\ell_{x}} - s\\frac{d_{x}}{\\ell_{x}} \\\\ &= 1 - s q_{x} \\\\ \\\\ \\therefore {}_{s}q_{x} &= s \\cdot q_x \\end{aligned} \\] Note that \\({}_{0}p_{x}=1\\) as it is the probability of surviving in that moment; which should be guaranteed. Based on this, the PDF and Force of Mortality can also be determined: \\[ \\begin{aligned} F_x(s) &= s \\cdot q_x \\\\ \\\\ f_x(s) &= \\frac{d}{ds} (s \\cdot q_x) \\\\ &= q_x \\\\ \\\\ \\mu_{x+t} &= \\frac{f_x(s)}{S_x(s)} \\\\ &= \\frac{q_x}{1-sq_x} \\end{aligned} \\]","title":"Uniform Distribution of Deaths"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#constant-force-of-mortality","text":"The Constant Force of Mortality is an assumption that allows for Exponential Interpolation between discrete ages. It assumes that there is a constant force of mortality between ages, such that the survival probability decreases exponentially over the year. \\[ \\begin{aligned} {}_{1}p_{x} &= e^{-\\int^{x+1}_{x} \\mu_c} \\\\ &= e^{-[t\\mu_c]^{x+1}_{x}} \\\\ &= e^{-[(x+1)\\mu_c - (x)\\mu_c ]} \\\\ &= e^{-[\\mu_c (x+1-x)]} \\\\ &= e^{-\\mu_c} \\\\ \\mu_c &= - \\ln p_{x} \\\\ \\\\ \\therefore {}_{s}p_{x} &= e^{-\\int^{x+s}_{x} \\mu_c} \\\\ &= e^{-s\\mu_c} \\\\ &= e^{s\\ln p_{x}} \\\\ &= e^{\\ln (p_{x})^{s}} \\\\ &= (p_{x})^{s} \\\\ \\\\ \\therefore {}_{s}q_{x} &= 1 - {}_{s}p_{x} \\\\ &= 1 - (p_{x})^{s} \\\\ &= 1 - [(1-q_{x})]^{s} \\end{aligned} \\]","title":"Constant Force of Mortality"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/2.%20Life%20Tables/#select-ultimate-mortality","text":"Mortality rates for the general population versus individuals who buy life insurance tend to be different. Generally speaking, people who purchase insurance tend to be richer and thus have better mortality rates than the general population . Within the individuals who purchase life insurance, those who have recently purchased a policy tend to have better mortality . This is because these individuals would have gone through Medical Underwriting and thus is expected to be in better health. The extent of the better mortality decreases over time and after a few years, they should experience the same mortality as the rest of the individuals who purchased life insurance. The duration of time is dependent on the rigorousness of the underwriting process . Formally speaking, the individuals who purchased life insurance were selected by the underwriting process and thus the better mortality experienced is known as Select Mortality . Similarly, the time whereby the select mortality is better is known as the Select Period . The select mortality ultimately converges with the non-select group, known as the Ultimate Mortality . In actuarial notation, subscript \\([x]\\) is used to distinguish select mortality from ultimate mortality, where the select period is denoted as \\(d\\) : \\[ \\begin{aligned} \\begin{cases} q_{[x]+t} < q_{x+t},& t < d \\\\ q_{[x]+t} = q_{x+t},& t \\ge d \\end{cases} \\end{aligned} \\] Note that the definition of \\(x\\) is dependent on the questions: \\(x\\) is the select age - \\([x], [x]+1, [x]+2, ...\\) \\(x\\) is the ultimate age - \\([x], [x-1], [x-2], ...\\) if given table, high chance the rate is not 5%","title":"Select &amp; Ultimate Mortality"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/","text":"Life Assurances Life Assurances are contracts that promise to pay out a benefit if the insured event occurs in the future . The value of a life assurance must reflect these two aspects: Uncertainty of cashflows - Expected Value , based on Survival Models Time value of money - Present Value , based on Interest Theory Thus, the value of a life assurance is the Expected Present Value (EPV) of the promised cashflows. Payable Discretely The simplest form of life assurance pays the benefits at the end of the year that the insured event occurs. While not common in practice, it provides a simple framework to understand the core concepts. For an assurance payable discretely, \\(K_x\\) is used as the survival model . Thus, the present value of the benefit payable in each period is \\(v^{K_x + 1}\\) . The EPV is the Triple Product Summation of the various components: Amount of benefit, \\(B\\) Discounting of the benefit, \\(v^{K_x + 1}\\) Probability of paying the benefit, \\({}_{K_x}p_{x} {}_{}q_{x + K_x}\\) \\[ \\begin{aligned} EPV &= \\sum B \\cdot v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\\\ &= \\sum B \\cdot v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] For simplicity, all of the proofs in this section will use \\(B=1\\) . This also has the benefit of allowing the EPVs to be easily scaled for any level of \\(B\\) . Actuarial Notation Similar to the survival model, given how often these values are calculated, they are abbreviated using the International Actuarial Notation as well. \\(A\\) represents the first moment (expectation) of the present value of a contract where a benefit of 1 is payable discretely when the status fails . The subscript \\(x\\) is the age of the policyholder, known as the Life Status . It fails when the policyholder dies . An additional subscript \\(:\\enclose{actuarial}{n}\\) is the duration that the assurance remains valid, known as the Duration Status . It fails if the policyholder survives past the policyterm. A \\(1\\) above the status indicates that the assurance pays only pays if that particular status fails first . If nothing is indicated, then the assurance pays whichever status fails first. If a particular status is omitted, then the assurance is not dependent on that status. For instance, omitting the duration status means that the contract is only dependent on the age of the policyholder. An assurance that begins \\(n\\) years later is known as a Deferred Assurance , which is denoted by \\({}_{n|}A\\) , following the same notation as deferred probabilities. Thus, putting everything together, the EPV of each assurance can be denoted as follows: Whole Life Assurance - Payable whenever policyholder dies; \\(A_x\\) Term Assurance - Payable only if policyholder dies during assurance period; \\(A^{1}_{x:\\enclose{actuarial}{n}}\\) Pure Endowment - Payable only if policyholder survives past assurance period; \\(A^{\\>\\>\\> 1}_{x:\\enclose{actuarial}{n}}\\) Endowment Assurance - Payable whichever of the above two occur first; \\(A_{x:\\enclose{actuarial}{n}}\\) Deferred Whole Life Assurance - Payable only if policyholder dies after \\(n\\) years; \\({}_{n|}A_x\\) Pure Endowments can also be expressed as \\({}_{n}E_x\\) for easier typesetting. Whole Life Assurance Whole Life Assurances cover the insured indefinitely and thus will pay out whenever the insured dies. Let WL be the random variable denoting the PV of the death benefit: \\[ \\begin{aligned} \\text{WL} &= v^{K_x + 1} \\\\ K_x &= 0, 1, \\dots, \\infty \\end{aligned} \\] Technically speaking, the upper limit of \\(K_x\\) should \\(\\omega-x\\) as it represents the maximum attainable age in the discrete survival model. However, since the \\({}_{k}p_{x} = 0\\) for all \\(k \\ge \\omega-x\\) , it does not matter what the upper limit is as long as it is larger than \\(\\omega-x\\) . Thus, \\(\\infty\\) is used for conciseness instead. The EPV is the Expectation/First Moment of the WL random variable: \\[ \\begin{aligned} E(\\text{WL}) &= \\sum^\\infty_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ A_{x} &= \\sum^\\infty_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Another commonly used metric is the Variance of the contract. In order to get it, the Second Moment must first be determined: \\[ \\begin{aligned} E({\\text{WL}}^2) &= \\sum^\\infty_{K_x = 0} \\left(v^{K_x + 1}\\right)^2 \\cdot {}_{K_x|}q_{x} \\\\ &= \\sum^\\infty_{K_x = 0} \\left((v^2)^{K_x + 1}\\right) \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Notice that the expression can be simplified to a form that looks almost identical to the first moment - with the only difference being that is uses \\(v^2\\) instead of \\(v\\) . Thus, the second moment is simply the first moment evaluated at a higher interest rate \\(i^*=(1+i)^2-1\\) , such that \\(v^* = v^2\\) . Generally, the k-th moment is denoted as \\({}^{k}A_x\\) , where the \\(k\\) is the multiplier on the interest rate used to evaluate the moment, \\(i^*=(1+i)^k-1\\) . It can also be written as \\(A_x |_{i = (1+i)^2-1}\\) using non-actuarial notation, which will come in handy for more complicated expressions. The Second Moment and hence Variance can be shown as: \\[ \\begin{aligned} E({\\text{WL}}^2) &= \\sum^\\infty_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ {}^{2} A_{x} &= \\sum^\\infty_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ \\\\ \\therefore Var(\\text{WL}) &= {}^{2} A_{x} - (A_{x})^2 \\end{aligned} \\] Note that the actual benefit must also be squared - this is likely to result in a relatively large value for the second moment and variance. Term Assurance Term Assurance covers the insured for a specified period \\(n\\) and only pays out if the insured dies within that period. NOTHING is paid out if the insured survives beyond that. A WL Assurance can be thought of as a special Term Assurance with infinite coverage. Let TA be the random variable denoting the PV of the death benefit: \\[ \\begin{aligned} \\text{TA} &= \\begin{cases} v^{K_x + 1} ,& K_x = 0, 1, \\dots, n-1 \\\\ 0 ,& K_x \\ge n \\end{cases} \\\\ &= {}_{\\{K_x \\le n-1\\}} v^{K_x+1} \\end{aligned} \\] \\(\\{K_x \\le n-1\\}\\) is known as an Indicator Function , which is a binary variable that takes 1 if the condition is true and 0 if the condition if false . It provides a concise way to express a piecewise function in a single expression. The EPV is the expectation of the TA random variable: \\[ \\begin{aligned} E(\\text{TA}) &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + 0 \\cdot {}_{n}p_{x} \\\\ A^1_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Similar as before, the second moment and thus Variance can be calculated as the following: \\[ \\begin{aligned} E({\\text{TA}}^2) &= \\sum^{n-1}_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} + 0^2 \\cdot {}_{n}p_{x} \\\\ {}^{2} A^1_{x:\\enclose{actuarial}{n}} &= \\sum^\\infty_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ \\\\ \\therefore Var(\\text{TA}) &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} - (A^1_{x:\\enclose{actuarial}{n}})^2 \\end{aligned} \\] Pure Endowment Pure Endowments are a special kind of contract that instead only pays out if the insured survives past a specified period \\(n\\) . NOTHING is paid out if the insured dies before that. Let PE be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{PE} &= \\begin{cases} 0 ,& K_x = 0, 1, 2 \\dots, n \\\\ v^n ,& K_x \\ge n \\end{cases} \\end{aligned} \\] The EPV is the expectation of the PE random variable. However, note that the probability of surviving past the period is given by a single probability \\({}_{n}p_{x}\\) : \\[ \\begin{aligned} E(\\text{PE}) &= 0 \\cdot {}_{n}q_{x} + v^n {}_{n}p_{x} \\\\ {}_{n}E_x &= v^n {}_{n}p_{x} \\end{aligned} \\] Similar as before, the second moment and thus Variance can be calculated: \\[ \\begin{aligned} E({\\text{PE}}^2) &= 0^2 \\cdot {}_{n}q_{x} + (v^2)^n {}_{n}p_{x} \\\\ {}^{2}_{n}E_x &= (v^*)^n {}_{n}p_{x} \\\\ \\\\ \\therefore Var(\\text{PE}) &= {}^{2}_{n}E_x - ({}_{n}E_x)^2 \\end{aligned} \\] Endowment Assurance Endowment Assurances are a combination of term assurances and pure endowments: Term Assurance - Pays out if the insured dies within the period Pure Endowment - Pays out if the insured survives past the period Thus, endowment assurances WILL pay out no matter what Let EA be the random variable denoting the PV of the benefits: \\[ \\begin{aligned} \\text{EA} &= \\begin{cases} v^{K_x + 1} ,& K_x = 0, 1, \\dots, n-1 \\\\ v^n ,& K_x \\ge n \\end{cases} \\\\ &= v^{min(K_x + 1, n)} \\end{aligned} \\] The EPV is the expectation of the EA random variable: \\[ \\begin{aligned} E(\\text{EA}) &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n {}_{n}p_{x} \\\\ A_{x:\\enclose{actuarial}{n}} &= A^1_{x:\\enclose{actuarial}{n}} + {}_{n}E_{x} \\end{aligned} \\] Note that the in the final year of the contract, the assurance will pay \\(v^n\\) regardless of the outcome - TA pays if they die while PE pays if they survive. Thus, a simplification can be made to the EPV: \\[ \\begin{aligned} A_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n {}_{n}p_{x} \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} + v^n {}_{n-1}p_{x} {}_{}q_{x + n - 1} + v^n {}_{n}p_{x} \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n ({}_{n-1}p_{x} {}_{}q_{x + n - 1} + {}_{n}p_{x}) \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n {}_{n-1}p_{x} \\end{aligned} \\] Similar as before, the second moment and thus Variance can be calculated: \\[ \\begin{aligned} E(\\text{EA}^2) &= \\sum^{n-1}_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} + (v^2)^n {}_{n}p_{x} \\\\ {}^{2} A_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} + (v^*)^n {}_{n}p_{x} \\\\ &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\\\ \\\\ \\therefore Var(\\text{EA}) &= {}^{2} A_{x:\\enclose{actuarial}{n}} - \\left(A_{x:\\enclose{actuarial}{n}}\\right)^2 \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left(A^1_{x:\\enclose{actuarial}{n}} - ({}_{n}E_x) \\right)^2 \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left[(A^1_{x:\\enclose{actuarial}{n}})^2 + 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) + ({}_{n}E_x)^2 \\right] \\end{aligned} \\] The same outcome can be reached using a slightly different approach: \\[ \\begin{aligned} Var(EA) &= Var(\\text{TA} + \\text{PE}) \\\\ &= Var(\\text{TA}) + Var(\\text{PE}) + 2 Cov(\\text{TA}, \\text{PE}) \\\\ &= Var(\\text{TA}) + Var(\\text{PE}) + 2 [E(\\text{TA} \\cdot \\text{PE}) - E(\\text{TA}) \\cdot E(\\text{PE})] \\end{aligned} \\] Consider the distribution of TA and PE: \\[ \\begin{aligned} TA \\cdot PE &= \\begin{cases} v^{K_x + 1} \\cdot 0, K_x \\lt n \\\\ 0 \\cdot v^n, K_x \\ge n \\end{cases} \\\\ &= \\begin{cases} 0, K_x \\lt n \\\\ 0, K_x \\ge n \\end{cases} \\\\ \\\\ \\therefore E(\\text{TA} \\cdot \\text{PE}) &= 0 \\cdot {}_{n}q_x + 0 \\cdot {}_{n}p_x \\\\ &= 0 \\end{aligned} \\] This results in the same variance as before: \\[ \\begin{aligned} Var(EA) &= Var(\\text{TA}) + Var(\\text{PE}) + 2 [E(\\text{TA} \\cdot \\text{PE}) - E(\\text{TA}) \\cdot E(\\text{PE})] \\\\ &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} - (A^1_{x:\\enclose{actuarial}{n}})^2 + {}^{2}_{n}E_x - ({}_{n}E_x)^2 - 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left[(A^1_{x:\\enclose{actuarial}{n}})^2 + 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) + ({}_{n}E_x)^2 \\right] \\\\ \\end{aligned} \\] While this process might seem redundant, it provides an easy to understand example of how the variance of a combination of assurances is derived by considering the covariance. Deferred Assurances Deferred Assurances are variations of any of the above assurances, where the assurance starts \\(n\\) years later rather than immediately. While any assurance can be deferred, the most common is the Deferred Whole Life . Deferred WL Let DWL be the random variable denoting the PV of the benefits of a deferred whole life assurance: \\[ \\begin{aligned} \\text{DWL} &= \\begin{cases} 0,& K_x = 0, 1, 2, \\dots, n-1 \\\\ v^{K_x + 1} ,& K_x \\ge n \\end{cases} \\\\ &= {}_{\\{K_x \\ge n\\}} v^{K_x + 1} \\end{aligned} \\] The EPV is the expectation of the DWL random variable: \\[ \\begin{aligned} E(\\text{DWL}) &= 0 \\cdot {}_{n}p_{x} + \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ {}_{n|} A_{x} &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} - \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ &= A_x - A^1_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] Alternatively, since a DWL is simply a WL assurance issued \\(n\\) years later, the EPV of the DWL is equivalent to the EPV of a WL issued at age \\(x+n\\) after adjusting for interest and survival : \\[ \\begin{aligned} {}_{n|} A_{x} &= \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ &= \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x}p_{x} \\cdot q_{x+K_x} \\\\ &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1 + n} \\cdot {}_{K_x+n}p_{x} \\cdot q_{x+K_x+n} \\\\ &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} v^{n} \\cdot {}_{n}p_{x} {}_{K_x}p_{x+n} \\cdot q_{x+K_x+n} \\\\ &= v^n {}_{n} p_{x} \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x+n} \\\\ &= {}_{n}E_{x} \\cdot A_{x+n} \\\\ \\end{aligned} \\] PEs can be use as a discount factor for functions that takes mortality into consideration . If provided by the life table, this reduces the need for computation. When going \"back\" in time, it must reflect that the policyholder will eventually survive till the current age, which is why the probability of surviving the period must be multiplied. This allows a TA to be expressed as the difference of two WL assurances issued at different times : \\[ \\begin{aligned} {}_{n|} A_{x} &= {}_{n}E_{x} * A_{x+n} \\\\ A_x - A^1_{x:\\enclose{actuarial}{n}} &= {}_{n}E_{x} * A_{x+n} \\\\ A^1_{x:\\enclose{actuarial}{n}} &= A_{x} - {}_{n}E_{x} * A_{x+n} \\\\ \\end{aligned} \\] This result is extremely important as this is the main method of calculating the EPV of a TA as the values for the WL can be easily found in the SULT. If the interest is not 0.05 or if mortality does NOT follow the SULT, then the EPV of a TA must be calculated manually. However, there is usually a catch that allows the EPV to be easily calculated manually: Different Interest : Term of the contract is short (EG. 3 Years) Different Mortality : Mortality can be simplified (EG. Becomes a constant) It is rare to have problems that have both different interest and mortality. In such cases, it is likely that an adjusted mortality table is provided. The second moment of a TA is still an expectation, thus the above method applies to it as well: \\[ \\begin{aligned} {}^{2} A^1_{x:\\enclose{actuarial}{n}} &= {}^{2} A_{x} - {}^{2}_{n} E_{x} \\cdot {}^{2} A_{x+n} \\end{aligned} \\] The key intuition is understanding that the discounting factor should be squared as well, which is why \\({}^{2}_{n} E_{x}\\) is used instead. Finally, the variance of a DWL can be easily calculated by applying previous results: \\[ \\begin{aligned} Var (DWL) &= {}^{2}_{n|}A_x - ({}_{n|}A_x)^2 \\\\ &= \\left({}^{2}A_x - {}^{2}A^1_{x:\\enclose{actuarial}{n}}\\right) - \\left(A_x - A^1_{x:\\enclose{actuarial}{n}}\\right)^2 \\\\ &= \\left({}^{2}A_x - {}^{2}A^1_{x:\\enclose{actuarial}{n}}\\right) + \\left[(A_x)^2 - 2(A_x)(A^1_{x:\\enclose{actuarial}{n}}) + (A^1_{x:\\enclose{actuarial}{n}})^2 \\right] \\end{aligned} \\] Deferred TA Another less commonly used deferred assurance is the Deferred Term Assurance . As both a deferred and term assurance, both results apply to it: \\[ \\begin{aligned} {}_{k|}A^1_{x:\\enclose{actuarial}{n}} &= {}_{k}E_{x} \\cdot A^1_{x+k:\\enclose{actuarial}{n}} \\\\ &= {}_{k}E_{x} \\cdot (A_{x+k} - {}_{n}E_{x} \\cdot A_{x+k+n}) \\end{aligned} \\] Although rarely used, this result can be tricky due to the different PEs used to discount - one is for the deferred assurance while the other is for the term. Alternatively, it can be used as a building block to decompose a regular assurance. An \\(n\\) year term assurance can be thought of as the sum of \\(n\\) deferred TAs , each with a one year term: \\[ \\begin{aligned} A^1_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{k=0} {}_{k|}A^1_{x:\\enclose{actuarial}{1}} \\\\ A_{x} &= \\sum^{\\infty}_{k=0} {}_{k|}A_{x} \\end{aligned} \\] Since WLs are just term assurances with infinite coverage, it can be extended to WLs as well if needed. This result will come in handy in later sections. Recursions The EPV of each contract can also be expressed through backwards recursion , where it is calculated as a function of itself. Consider the WL random variable: If the policyholder dies in the year, then a benefit of 1 is paid at the end of the year. If the policyholder survives past the year, then the policyholder will die in some future year. The PV of the benefit at the end of the year is given is the EPV of the same contract but at that future time ; \\(A_{x+1}\\) . \\[ \\begin{aligned} WL &= \\begin{cases} v \\cdot 1 ,& {}_{}q_{x} \\\\ v \\cdot A_{x+1} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore A_x = v{}_{}q_{x} + v{}_{}p_{x}A_{x+1} \\] The same exercise can be shown for the TA random variable. The main difference is understanding how the age & duration changes: \\(x+1\\) reflects the new age of the policyholder (same as WL) \\(n-1\\) reflects that one year of coverage has passed (not applicable for WL) \\[ \\begin{aligned} TA &= \\begin{cases} v \\cdot 1 ,& {}_{}q_{x} \\\\ v \\cdot A^1_{x+1:\\enclose{actuarial}{n-1}} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore A^1_{x:\\enclose{actuarial}{n}} = v{}_{}q_{x} + v{}_{}p_{x}A^1_{x+1:\\enclose{actuarial}{n-1}} \\] Note that since it requires a term policy with a reduced term , this recursion is not particularly useful as it is difficult to obtain it. The PE variable is similar, with the main difference being that the policyholder will receive nothing if the policyholder dies . Thus, only the second component of the recursion remains: \\[ \\begin{aligned} PE &= \\begin{cases} 0 ,& {}_{}q_{x} \\\\ v \\cdot {}_{n-1}E_{x+1} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore {}_{n}E_{x} = v{}_{}p_{x} \\cdot {}_{n-1}E_{x+1} \\] EA is omitted from this section as it is simply the combination of a TA and PE. These identities are most useful in a spreadsheet setting where the calculations can be easily repeated to fill up an entire life table. However, even then it is necessary to have a starting point for the recusions to occur. The most common starting point is the terminal age as the EPVs can be intuitively determined since the policyholder will inevitably die at the end of the year: \\[ \\begin{aligned} A_{\\omega-1} &= v \\\\ A^{\\> \\> 1}_{\\omega-1:\\enclose{actuarial}{n}} &= v \\\\ {}_{n}E_{\\omega-1} &= 0 \\\\ A^{1}_{\\omega-1:\\enclose{actuarial}{n}} &= v \\end{aligned} \\] Intuitions Although the exam questions are mostly computational, it is good to have an understanding of how the different EPVs compare against one another to serve as a sort of \"sense check\" against the SULT provided to determine if the answer is in the right direction. Same Assurance Recall that the probability of death is an increasing function with age. The death benefit is more likely to be paid out to an older policyholder - in other words, they receive the death benefit \"sooner\" than a younger policyholder. Thus, an older policyholder has larger expected cashflows that are discounted less (due to receiving it sooner), which results in a higher EPV than a younger policyholder, all else equal: \\[ \\begin{aligned} A_{x+k} & \\gt A_{x} \\\\ A^{\\> \\> 1} _{x+k:\\enclose{actuarial}{n}} & \\gt A^{\\> \\> 1}_{x+k:\\enclose{actuarial}{n}} \\end{aligned} \\] Conversely, the probability of survival is a decreasing function with age. The survival benefit is less likely to be paid out to an older policyholder - smaller expected cashflows. Regardless of the age of the policyholder, the survival benefit is paid at the same time ( same discounting ). Thus, since an older policyholder has smaller expected cashflows , it has a lower EPV than a younger policyholder: \\[ {}_{n}E_{x+k} \\le {}_{n}E_{x} \\] Endowment Assurances have both a death and survival component , thus the comparison is a combination of the two: A younger policyholder is more likely to survive and receive the survival benefit at the end of the term (discounted more) An older policyholder is more likely to die and receive the death benefit during the term (discounted less) Assuming that the difference in expected cashflows are negligible , then an older policyholder would have an higher EPV due to the lower discounting : \\[ A_{x+k:\\enclose{actuarial}{n}} \\gt A_{x:\\enclose{actuarial}{n}} \\] Naturally, all else equal, assurances with a lower interest rate are discounted less and thus have a higher EPV . Different Assurances At a young age where the probability of death is low, all else equal (where applicable), the EPVs of each assurance rank as follows: TA will have the smallest EPV . Although their benefits are paid out sooner, the expected benefits are small as the probability of death is small. WL has the next largest EPV . They have the same benefits as term in the short run, but have large expected benefits in the future . However, these large benefits are heavily discounted , still resuling in a small EPV. PE has the next largest EPV . Given the high probability of survival, the expected benefits are large . EA has the largest EPV . Since it is a combination of TA and PE, it is naturally the highest. \\[ \\begin{aligned} A^{1}_{30:\\enclose{actuarial}{n}} < A_{30} < {}_{n}E_{30} < A_{30:\\enclose{actuarial}{n}} \\\\ \\end{aligned} \\] At an old age where the probability of death is high, all else equal (where applicable), the EPVs of each assurance rank as follows: PE will have the smallest EPV . Given the low probability of survival, the expected benefits are are small . TA will have the next largest EPV . Given the high probability of death, the expected benefits are high . EA will have the next largest EPV . Since it is combination of TA and PE, it is naturally higher than both of them. WL has the largest EPV . Given the inevitable death of the policyholder, the expected benefits are the highest . \\[ \\begin{aligned} {}_{n}E_{100} < A^{1}_{100:\\enclose{actuarial}{n}} < A_{100:\\enclose{actuarial}{n}} < A_{100} \\\\ \\end{aligned} \\] As the policyholder approaches the terminal age, the EPVs tend to one another: \\[ \\begin{aligned} x &\\to \\omega \\\\ E(\\text{PE}) &\\to 0 \\\\ E(\\text{EA}) &\\to E(\\text{TA}) \\\\ E(\\text{TA}) &\\to E(\\text{WL}) \\end{aligned} \\] TA tends to WL whenever the end of the term exceeds the terminal age - thus the cashflows and hence EPV for both assurances become identical. Probabilities and Percentiles Apart from just calculating the Expectation and Variance, the probabilities and hence percentiles of the contract benefits can be calculated as well. The former refers to calculating the probability that the random variable is at most some value \\(u\\) . In other words, that the PV ( NOT the EPV! ) is at most \\(u\\) . \\[ \\begin{aligned} \\text{WL} &\\le u \\\\ v^{K_x+1} &\\le u \\\\ (K_x+1) &\\ln v \\le \\ln u \\\\ K_x+1 &\\ge \\frac{\\ln u}{\\ln v} \\\\ K_x & \\ge \\frac{\\ln u}{\\ln v} - 1 \\\\ \\end{aligned} \\] Note that it is a common mistake to forget to flip the inequality sign as \\(\\ln v \\lt 0\\) . To avoid this error, it is advised to plot a graph to remember that \\(K_x\\) should be larger than the calculated value: The RHS of the expression is unlikely to be an integer. However, \\(K_x\\) can only take integer values. Thus, the values can rounded up to the nearest whole number (EG. 5): \\[ \\begin{aligned} P(K_x \\ge \\frac{\\ln u}{\\ln v} - 1) &= P(K_x \\ge 5) \\\\ &= P(T_x \\ge 5) \\\\ &= {}_{5}p_x \\end{aligned} \\] The latter refers to calculating the percentile of the random variable , which is the smallest value of the RV that results in the specified probability. This process is the opposite of the previous one - it involves solving for \\(T_x\\) , converting it to \\(K_x\\) and then subsituting it back into the RV, which results in the associated percentile. Payable Continuously In practice, life assurances pay benefits as soon as the insured event occurs , thus is akin to paying out continuously throughout the year. Notice that paying out continuously is actually a special case of a contract that pays out \\(m\\) times a year , where \\(m\\) tends to infinity. \\(m=4\\) ; Quarterly Payments \\(m=12\\) ; Monthly Payments \\(m=\\infty\\) ; Continuous Payments Thus, despite the header stating \"payable continously\", this section will instead cover paying out \\(m\\) times a year , which can be used to derive the continuous case. Note that questions with Assurances that pay out \\(m\\) times a year are rare - if anything, the continuous case will be directly tested instead. However, the \\(m\\) times a year case is still covered because the ideas can be extended to Annuities, where such questions are common. The survival model used must now reflect the probability of death at fractional ages . Intuitively, it can be thought of as the probability living till a discrete age and then dying within a sub-period within that year: \\[ \\begin{aligned} K_x + \\frac{j+1}{m}, \\>\\> j = 0, 1, 2, ..., m-1 \\end{aligned} \\] Thus, the corresponding random variable representing the PV of the benefits: \\[ \\text{PV} = v^{K_x + \\frac{j+1}{m}} \\] The EPV of an assurance payable \\(m\\) times a year can be denoted with the \\((m)\\) superscript with the same notations as before: \\[ A^{(m)}_{x} = \\sum^{\\infty}_{K_x=0} v^{K_x} {}_{K_x}p_x \\cdot \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}} {}_{\\frac{j}{m}}p_{x+K_x} \\cdot {}_{\\frac{1}{m}} q_{x+K_x+\\frac{j}{m}} \\\\ \\] Note that the benefit used is still the ANNUAL BENEFIT . To obtain the actual benefit payable per period, then it should be divided by the number of periods; \\(\\frac{B}{m}\\) . Unfortunately, most life tables do not naturally provide probabilities for death at fractional ages, thus the EPVs must be approximated from the discrete case , which will be covered later. Continuous Case As \\(m \\to \\infty\\) , then the survival model becomes \\(T_x\\) and hence random variable is: \\[ \\begin{aligned} \\text{PV} &= v^{T_x} \\\\ &= e^{-\\delta \\cdot T_x} \\end{aligned} \\] The EPV of an assurance payable continously is denoted with the \\(\\bar{}\\) accent with the same notations as before: \\[ \\begin{aligned} \\bar{A}_x &= \\int^{\\infty}_{0} v^{T_x} \\cdot f_x(t) \\\\ &= \\int^{\\infty}_{0} e^{-\\delta \\cdot T_x} \\cdot {}_{t}p_{x} \\mu_{x+t} \\end{aligned} \\] Recall that the second term represents the probability of living to a certain age \\(x+t\\) and then dying in an infinitely small time after that (definition of force of interest). Thus, although the EPV looks very different, it is still fundamentally a Triple Product Summation of the same components: Amount of Benefit, 1 Discounting of the Benefit, \\(e^{-\\delta \\cdot T_x}\\) Probability of paying the Benefit, \\({}_{t}p_{x} \\mu_{x+t}\\) Unlike the payable \\(m\\) times case, the EPVs can be calculated directly if the survival distribution is provided. However, since they are special cases of the payable \\(m\\) times case, it can also be calculated through approximation from the discrete case. The same logic can be applied to all other assurances, EXCEPT for PEs . This is because they pay out at a fixed time , thus there is NO such thing as a payable \\(m\\) times PE or a continuously payable PE. Uniform Distribution of Deaths Recall that the UDD assumption can be used to approximate survival probabilities of fractional ages from discrete probabilities. Using this approach, the Continuous EPVs can be approximated from the Discrete ones. Assuming UDD between integer ages, \\[ {}_{\\frac{j}{m}}p_{x+K_x} \\cdot {}_{\\frac{1}{m}} q_{x+K_x+\\frac{j}{m}} \\approx \\frac{1}{m} \\cdot q_{x+K_x} \\] Thus, the EPV of a continuous contract can be expressed as a function of the discrete case: \\[ \\begin{aligned} A^{(m)}_{x} &= \\sum^{\\infty}_{K_x=0} v^{K_x} {}_{K_x}p_x \\cdot \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}} {}_{\\frac{j}{m}}p_{x+K_x} \\cdot {}_{\\frac{1}{m}} q_{x+K_x+\\frac{j}{m}} \\\\ & \\approx \\sum^{\\infty}_{K_x=0} v^{K_x} {}_{K_x}p_x \\cdot \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}} \\frac{1}{m} \\cdot q_{x+K_x} \\\\ & \\approx \\sum^{\\infty}_{K_x=0} v^{K_x+1} {}_{K_x}p_x q_{x+K_x} \\cdot \\frac{1}{m} \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}-1} \\\\ & \\approx A_x \\cdot \\frac{v^{-1}}{m} \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}} \\\\ & \\approx A_x \\cdot \\frac{1+i}{m} \\cdot \\frac{v^{\\frac{1}{m}}(1-v)}{1-v^{\\frac{1}{m}}} \\\\ & \\approx A_x \\cdot \\frac{1+i}{m} \\cdot \\frac{v^{\\frac{1}{m}}(1-v)}{v^{\\frac{1}{m}}[(1+i)^{m}-1]} \\\\ & \\approx A_x \\cdot \\frac{1+i}{m} \\cdot \\frac{1-v}{(1+i)^{m}-1} \\\\ & \\approx A_x \\cdot \\frac{1+i-1}{m[(1+i)^{m}-1]} \\\\ & \\approx \\frac{i}{i^{(m)}} \\cdot A_x \\end{aligned} \\] As \\(m \\to \\infty\\) , \\[ \\begin{aligned} i^{(m)} &\\to \\delta, \\\\ A^{(m)}_{x} &\\to \\bar{A}_x, \\\\ \\therefore \\bar{A}_x &= \\frac{i}{\\delta} A_x \\end{aligned} \\] Claims Acceleration Approach Given that claims occur every \\(\\frac{1}{m}\\) period, then on average , the claims within a year occur at \\(\\frac{m+1}{2m}\\) : \\[ \\begin{aligned} \\text{Average Death} &= \\frac{\\frac{1}{m} + \\frac{m}{m}}{2} \\\\ &= \\frac{\\frac{1+m}{m}}{2} \\\\ &= \\frac{m+1}{2m} \\end{aligned} \\] Thus, the EPV of a continuous contract can be expressed as a function of the discrete case: \\[ \\begin{aligned} A^{(m)}_{x} & \\approx \\sum^{\\infty}_{K_x = 0} v^{K_x + \\frac{m+1}{2m}} {}_{K_x|} q_{x} \\\\ & \\approx v^{\\frac{m+1}{2m}-1} \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} {}_{K_x|} q_{x} \\\\ & \\approx v^{\\frac{m+1-2m}{2m}} A_x \\\\ & \\approx v^{\\frac{-m+1}{2m}} A_x \\\\ & \\approx v^{-\\frac{m-1}{2m}} A_x \\\\ & \\approx (1+i)^{\\frac{m-1}{2m}} A_x \\end{aligned} \\] As \\(m \\to \\infty\\) , \\[ \\begin{aligned} m &\\to \\infty, \\\\ \\frac{m-1}{2m} &\\to \\frac{1}{2}, \\\\ A^{(m)}_{x} &\\to \\bar{A}_x, \\\\ \\therefore \\bar{A}_x &= (1+i)^{2} A_x \\end{aligned} \\] This is known as the Claims Acceleration Approach as the claims are paid out earlier on in the year as compared to the end. Common Pitfalls Recall that there are no continuous PEs - thus, be very careful when approximating an EA as ONLY the TA portion needs to be approximated ! This means that we cannot directly apply an approximation to the EA values provided in the SULT - they must be broken down into the TA and EA components first. If calculating manually, it is always advised to calculate the TA and EA components seperately before combining into an EA to prevent falling into the trap of directly approximating the EA. \\[ \\begin{aligned} \\bar{A}_{x:\\enclose{actuarial}{n}} & \\ne \\frac{i}{\\delta} A_{x:\\enclose{actuarial}{n}} \\\\ \\bar{A}_{x:\\enclose{actuarial}{n}} & = \\frac{i}{\\delta} A^{1}_{x:\\enclose{actuarial}{n}} + {}_{n}E_x \\end{aligned} \\] Similarly, if approximating the second moment for variance, note that the NEW interest rate must be used in the approximation term as well: \\[ \\begin{aligned} {}^{2} \\bar{A}_x &= \\frac{i^*}{\\delta^*} \\cdot {}^{2} A_x \\end{aligned} \\] Although the above two points were illustrated with UDD, they apply to the claims acceleration approach as well. Lastly, the two approaches should produce similar results which differ by a few decimal places. However, these differences are enlarged when dealing with large benefits as these decimal places will be brought forward, resulting in seemingly large differences - do not be alarmed!","title":"Life Assurances"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#life-assurances","text":"Life Assurances are contracts that promise to pay out a benefit if the insured event occurs in the future . The value of a life assurance must reflect these two aspects: Uncertainty of cashflows - Expected Value , based on Survival Models Time value of money - Present Value , based on Interest Theory Thus, the value of a life assurance is the Expected Present Value (EPV) of the promised cashflows.","title":"Life Assurances"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#payable-discretely","text":"The simplest form of life assurance pays the benefits at the end of the year that the insured event occurs. While not common in practice, it provides a simple framework to understand the core concepts. For an assurance payable discretely, \\(K_x\\) is used as the survival model . Thus, the present value of the benefit payable in each period is \\(v^{K_x + 1}\\) . The EPV is the Triple Product Summation of the various components: Amount of benefit, \\(B\\) Discounting of the benefit, \\(v^{K_x + 1}\\) Probability of paying the benefit, \\({}_{K_x}p_{x} {}_{}q_{x + K_x}\\) \\[ \\begin{aligned} EPV &= \\sum B \\cdot v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} \\\\ &= \\sum B \\cdot v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] For simplicity, all of the proofs in this section will use \\(B=1\\) . This also has the benefit of allowing the EPVs to be easily scaled for any level of \\(B\\) .","title":"Payable Discretely"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#actuarial-notation","text":"Similar to the survival model, given how often these values are calculated, they are abbreviated using the International Actuarial Notation as well. \\(A\\) represents the first moment (expectation) of the present value of a contract where a benefit of 1 is payable discretely when the status fails . The subscript \\(x\\) is the age of the policyholder, known as the Life Status . It fails when the policyholder dies . An additional subscript \\(:\\enclose{actuarial}{n}\\) is the duration that the assurance remains valid, known as the Duration Status . It fails if the policyholder survives past the policyterm. A \\(1\\) above the status indicates that the assurance pays only pays if that particular status fails first . If nothing is indicated, then the assurance pays whichever status fails first. If a particular status is omitted, then the assurance is not dependent on that status. For instance, omitting the duration status means that the contract is only dependent on the age of the policyholder. An assurance that begins \\(n\\) years later is known as a Deferred Assurance , which is denoted by \\({}_{n|}A\\) , following the same notation as deferred probabilities. Thus, putting everything together, the EPV of each assurance can be denoted as follows: Whole Life Assurance - Payable whenever policyholder dies; \\(A_x\\) Term Assurance - Payable only if policyholder dies during assurance period; \\(A^{1}_{x:\\enclose{actuarial}{n}}\\) Pure Endowment - Payable only if policyholder survives past assurance period; \\(A^{\\>\\>\\> 1}_{x:\\enclose{actuarial}{n}}\\) Endowment Assurance - Payable whichever of the above two occur first; \\(A_{x:\\enclose{actuarial}{n}}\\) Deferred Whole Life Assurance - Payable only if policyholder dies after \\(n\\) years; \\({}_{n|}A_x\\) Pure Endowments can also be expressed as \\({}_{n}E_x\\) for easier typesetting.","title":"Actuarial Notation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#whole-life-assurance","text":"Whole Life Assurances cover the insured indefinitely and thus will pay out whenever the insured dies. Let WL be the random variable denoting the PV of the death benefit: \\[ \\begin{aligned} \\text{WL} &= v^{K_x + 1} \\\\ K_x &= 0, 1, \\dots, \\infty \\end{aligned} \\] Technically speaking, the upper limit of \\(K_x\\) should \\(\\omega-x\\) as it represents the maximum attainable age in the discrete survival model. However, since the \\({}_{k}p_{x} = 0\\) for all \\(k \\ge \\omega-x\\) , it does not matter what the upper limit is as long as it is larger than \\(\\omega-x\\) . Thus, \\(\\infty\\) is used for conciseness instead. The EPV is the Expectation/First Moment of the WL random variable: \\[ \\begin{aligned} E(\\text{WL}) &= \\sum^\\infty_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ A_{x} &= \\sum^\\infty_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Another commonly used metric is the Variance of the contract. In order to get it, the Second Moment must first be determined: \\[ \\begin{aligned} E({\\text{WL}}^2) &= \\sum^\\infty_{K_x = 0} \\left(v^{K_x + 1}\\right)^2 \\cdot {}_{K_x|}q_{x} \\\\ &= \\sum^\\infty_{K_x = 0} \\left((v^2)^{K_x + 1}\\right) \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Notice that the expression can be simplified to a form that looks almost identical to the first moment - with the only difference being that is uses \\(v^2\\) instead of \\(v\\) . Thus, the second moment is simply the first moment evaluated at a higher interest rate \\(i^*=(1+i)^2-1\\) , such that \\(v^* = v^2\\) . Generally, the k-th moment is denoted as \\({}^{k}A_x\\) , where the \\(k\\) is the multiplier on the interest rate used to evaluate the moment, \\(i^*=(1+i)^k-1\\) . It can also be written as \\(A_x |_{i = (1+i)^2-1}\\) using non-actuarial notation, which will come in handy for more complicated expressions. The Second Moment and hence Variance can be shown as: \\[ \\begin{aligned} E({\\text{WL}}^2) &= \\sum^\\infty_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ {}^{2} A_{x} &= \\sum^\\infty_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ \\\\ \\therefore Var(\\text{WL}) &= {}^{2} A_{x} - (A_{x})^2 \\end{aligned} \\] Note that the actual benefit must also be squared - this is likely to result in a relatively large value for the second moment and variance.","title":"Whole Life Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#term-assurance","text":"Term Assurance covers the insured for a specified period \\(n\\) and only pays out if the insured dies within that period. NOTHING is paid out if the insured survives beyond that. A WL Assurance can be thought of as a special Term Assurance with infinite coverage. Let TA be the random variable denoting the PV of the death benefit: \\[ \\begin{aligned} \\text{TA} &= \\begin{cases} v^{K_x + 1} ,& K_x = 0, 1, \\dots, n-1 \\\\ 0 ,& K_x \\ge n \\end{cases} \\\\ &= {}_{\\{K_x \\le n-1\\}} v^{K_x+1} \\end{aligned} \\] \\(\\{K_x \\le n-1\\}\\) is known as an Indicator Function , which is a binary variable that takes 1 if the condition is true and 0 if the condition if false . It provides a concise way to express a piecewise function in a single expression. The EPV is the expectation of the TA random variable: \\[ \\begin{aligned} E(\\text{TA}) &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + 0 \\cdot {}_{n}p_{x} \\\\ A^1_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] Similar as before, the second moment and thus Variance can be calculated as the following: \\[ \\begin{aligned} E({\\text{TA}}^2) &= \\sum^{n-1}_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} + 0^2 \\cdot {}_{n}p_{x} \\\\ {}^{2} A^1_{x:\\enclose{actuarial}{n}} &= \\sum^\\infty_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ \\\\ \\therefore Var(\\text{TA}) &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} - (A^1_{x:\\enclose{actuarial}{n}})^2 \\end{aligned} \\]","title":"Term Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#pure-endowment","text":"Pure Endowments are a special kind of contract that instead only pays out if the insured survives past a specified period \\(n\\) . NOTHING is paid out if the insured dies before that. Let PE be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{PE} &= \\begin{cases} 0 ,& K_x = 0, 1, 2 \\dots, n \\\\ v^n ,& K_x \\ge n \\end{cases} \\end{aligned} \\] The EPV is the expectation of the PE random variable. However, note that the probability of surviving past the period is given by a single probability \\({}_{n}p_{x}\\) : \\[ \\begin{aligned} E(\\text{PE}) &= 0 \\cdot {}_{n}q_{x} + v^n {}_{n}p_{x} \\\\ {}_{n}E_x &= v^n {}_{n}p_{x} \\end{aligned} \\] Similar as before, the second moment and thus Variance can be calculated: \\[ \\begin{aligned} E({\\text{PE}}^2) &= 0^2 \\cdot {}_{n}q_{x} + (v^2)^n {}_{n}p_{x} \\\\ {}^{2}_{n}E_x &= (v^*)^n {}_{n}p_{x} \\\\ \\\\ \\therefore Var(\\text{PE}) &= {}^{2}_{n}E_x - ({}_{n}E_x)^2 \\end{aligned} \\]","title":"Pure Endowment"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#endowment-assurance","text":"Endowment Assurances are a combination of term assurances and pure endowments: Term Assurance - Pays out if the insured dies within the period Pure Endowment - Pays out if the insured survives past the period Thus, endowment assurances WILL pay out no matter what Let EA be the random variable denoting the PV of the benefits: \\[ \\begin{aligned} \\text{EA} &= \\begin{cases} v^{K_x + 1} ,& K_x = 0, 1, \\dots, n-1 \\\\ v^n ,& K_x \\ge n \\end{cases} \\\\ &= v^{min(K_x + 1, n)} \\end{aligned} \\] The EPV is the expectation of the EA random variable: \\[ \\begin{aligned} E(\\text{EA}) &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n {}_{n}p_{x} \\\\ A_{x:\\enclose{actuarial}{n}} &= A^1_{x:\\enclose{actuarial}{n}} + {}_{n}E_{x} \\end{aligned} \\] Note that the in the final year of the contract, the assurance will pay \\(v^n\\) regardless of the outcome - TA pays if they die while PE pays if they survive. Thus, a simplification can be made to the EPV: \\[ \\begin{aligned} A_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n {}_{n}p_{x} \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x}p_{x} {}_{}q_{x + K_x} + v^n {}_{n-1}p_{x} {}_{}q_{x + n - 1} + v^n {}_{n}p_{x} \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n ({}_{n-1}p_{x} {}_{}q_{x + n - 1} + {}_{n}p_{x}) \\\\ &= \\sum^{n-2}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} + v^n {}_{n-1}p_{x} \\end{aligned} \\] Similar as before, the second moment and thus Variance can be calculated: \\[ \\begin{aligned} E(\\text{EA}^2) &= \\sum^{n-1}_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} + (v^2)^n {}_{n}p_{x} \\\\ {}^{2} A_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} + (v^*)^n {}_{n}p_{x} \\\\ &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\\\ \\\\ \\therefore Var(\\text{EA}) &= {}^{2} A_{x:\\enclose{actuarial}{n}} - \\left(A_{x:\\enclose{actuarial}{n}}\\right)^2 \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left(A^1_{x:\\enclose{actuarial}{n}} - ({}_{n}E_x) \\right)^2 \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left[(A^1_{x:\\enclose{actuarial}{n}})^2 + 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) + ({}_{n}E_x)^2 \\right] \\end{aligned} \\] The same outcome can be reached using a slightly different approach: \\[ \\begin{aligned} Var(EA) &= Var(\\text{TA} + \\text{PE}) \\\\ &= Var(\\text{TA}) + Var(\\text{PE}) + 2 Cov(\\text{TA}, \\text{PE}) \\\\ &= Var(\\text{TA}) + Var(\\text{PE}) + 2 [E(\\text{TA} \\cdot \\text{PE}) - E(\\text{TA}) \\cdot E(\\text{PE})] \\end{aligned} \\] Consider the distribution of TA and PE: \\[ \\begin{aligned} TA \\cdot PE &= \\begin{cases} v^{K_x + 1} \\cdot 0, K_x \\lt n \\\\ 0 \\cdot v^n, K_x \\ge n \\end{cases} \\\\ &= \\begin{cases} 0, K_x \\lt n \\\\ 0, K_x \\ge n \\end{cases} \\\\ \\\\ \\therefore E(\\text{TA} \\cdot \\text{PE}) &= 0 \\cdot {}_{n}q_x + 0 \\cdot {}_{n}p_x \\\\ &= 0 \\end{aligned} \\] This results in the same variance as before: \\[ \\begin{aligned} Var(EA) &= Var(\\text{TA}) + Var(\\text{PE}) + 2 [E(\\text{TA} \\cdot \\text{PE}) - E(\\text{TA}) \\cdot E(\\text{PE})] \\\\ &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} - (A^1_{x:\\enclose{actuarial}{n}})^2 + {}^{2}_{n}E_x - ({}_{n}E_x)^2 - 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left[(A^1_{x:\\enclose{actuarial}{n}})^2 + 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) + ({}_{n}E_x)^2 \\right] \\\\ \\end{aligned} \\] While this process might seem redundant, it provides an easy to understand example of how the variance of a combination of assurances is derived by considering the covariance.","title":"Endowment Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#deferred-assurances","text":"Deferred Assurances are variations of any of the above assurances, where the assurance starts \\(n\\) years later rather than immediately. While any assurance can be deferred, the most common is the Deferred Whole Life .","title":"Deferred Assurances"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#deferred-wl","text":"Let DWL be the random variable denoting the PV of the benefits of a deferred whole life assurance: \\[ \\begin{aligned} \\text{DWL} &= \\begin{cases} 0,& K_x = 0, 1, 2, \\dots, n-1 \\\\ v^{K_x + 1} ,& K_x \\ge n \\end{cases} \\\\ &= {}_{\\{K_x \\ge n\\}} v^{K_x + 1} \\end{aligned} \\] The EPV is the expectation of the DWL random variable: \\[ \\begin{aligned} E(\\text{DWL}) &= 0 \\cdot {}_{n}p_{x} + \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ {}_{n|} A_{x} &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} - \\sum^{n-1}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ &= A_x - A^1_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] Alternatively, since a DWL is simply a WL assurance issued \\(n\\) years later, the EPV of the DWL is equivalent to the EPV of a WL issued at age \\(x+n\\) after adjusting for interest and survival : \\[ \\begin{aligned} {}_{n|} A_{x} &= \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ &= \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x}p_{x} \\cdot q_{x+K_x} \\\\ &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1 + n} \\cdot {}_{K_x+n}p_{x} \\cdot q_{x+K_x+n} \\\\ &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} v^{n} \\cdot {}_{n}p_{x} {}_{K_x}p_{x+n} \\cdot q_{x+K_x+n} \\\\ &= v^n {}_{n} p_{x} \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x+n} \\\\ &= {}_{n}E_{x} \\cdot A_{x+n} \\\\ \\end{aligned} \\] PEs can be use as a discount factor for functions that takes mortality into consideration . If provided by the life table, this reduces the need for computation. When going \"back\" in time, it must reflect that the policyholder will eventually survive till the current age, which is why the probability of surviving the period must be multiplied. This allows a TA to be expressed as the difference of two WL assurances issued at different times : \\[ \\begin{aligned} {}_{n|} A_{x} &= {}_{n}E_{x} * A_{x+n} \\\\ A_x - A^1_{x:\\enclose{actuarial}{n}} &= {}_{n}E_{x} * A_{x+n} \\\\ A^1_{x:\\enclose{actuarial}{n}} &= A_{x} - {}_{n}E_{x} * A_{x+n} \\\\ \\end{aligned} \\] This result is extremely important as this is the main method of calculating the EPV of a TA as the values for the WL can be easily found in the SULT. If the interest is not 0.05 or if mortality does NOT follow the SULT, then the EPV of a TA must be calculated manually. However, there is usually a catch that allows the EPV to be easily calculated manually: Different Interest : Term of the contract is short (EG. 3 Years) Different Mortality : Mortality can be simplified (EG. Becomes a constant) It is rare to have problems that have both different interest and mortality. In such cases, it is likely that an adjusted mortality table is provided. The second moment of a TA is still an expectation, thus the above method applies to it as well: \\[ \\begin{aligned} {}^{2} A^1_{x:\\enclose{actuarial}{n}} &= {}^{2} A_{x} - {}^{2}_{n} E_{x} \\cdot {}^{2} A_{x+n} \\end{aligned} \\] The key intuition is understanding that the discounting factor should be squared as well, which is why \\({}^{2}_{n} E_{x}\\) is used instead. Finally, the variance of a DWL can be easily calculated by applying previous results: \\[ \\begin{aligned} Var (DWL) &= {}^{2}_{n|}A_x - ({}_{n|}A_x)^2 \\\\ &= \\left({}^{2}A_x - {}^{2}A^1_{x:\\enclose{actuarial}{n}}\\right) - \\left(A_x - A^1_{x:\\enclose{actuarial}{n}}\\right)^2 \\\\ &= \\left({}^{2}A_x - {}^{2}A^1_{x:\\enclose{actuarial}{n}}\\right) + \\left[(A_x)^2 - 2(A_x)(A^1_{x:\\enclose{actuarial}{n}}) + (A^1_{x:\\enclose{actuarial}{n}})^2 \\right] \\end{aligned} \\]","title":"Deferred WL"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#deferred-ta","text":"Another less commonly used deferred assurance is the Deferred Term Assurance . As both a deferred and term assurance, both results apply to it: \\[ \\begin{aligned} {}_{k|}A^1_{x:\\enclose{actuarial}{n}} &= {}_{k}E_{x} \\cdot A^1_{x+k:\\enclose{actuarial}{n}} \\\\ &= {}_{k}E_{x} \\cdot (A_{x+k} - {}_{n}E_{x} \\cdot A_{x+k+n}) \\end{aligned} \\] Although rarely used, this result can be tricky due to the different PEs used to discount - one is for the deferred assurance while the other is for the term. Alternatively, it can be used as a building block to decompose a regular assurance. An \\(n\\) year term assurance can be thought of as the sum of \\(n\\) deferred TAs , each with a one year term: \\[ \\begin{aligned} A^1_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{k=0} {}_{k|}A^1_{x:\\enclose{actuarial}{1}} \\\\ A_{x} &= \\sum^{\\infty}_{k=0} {}_{k|}A_{x} \\end{aligned} \\] Since WLs are just term assurances with infinite coverage, it can be extended to WLs as well if needed. This result will come in handy in later sections.","title":"Deferred TA"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#recursions","text":"The EPV of each contract can also be expressed through backwards recursion , where it is calculated as a function of itself. Consider the WL random variable: If the policyholder dies in the year, then a benefit of 1 is paid at the end of the year. If the policyholder survives past the year, then the policyholder will die in some future year. The PV of the benefit at the end of the year is given is the EPV of the same contract but at that future time ; \\(A_{x+1}\\) . \\[ \\begin{aligned} WL &= \\begin{cases} v \\cdot 1 ,& {}_{}q_{x} \\\\ v \\cdot A_{x+1} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore A_x = v{}_{}q_{x} + v{}_{}p_{x}A_{x+1} \\] The same exercise can be shown for the TA random variable. The main difference is understanding how the age & duration changes: \\(x+1\\) reflects the new age of the policyholder (same as WL) \\(n-1\\) reflects that one year of coverage has passed (not applicable for WL) \\[ \\begin{aligned} TA &= \\begin{cases} v \\cdot 1 ,& {}_{}q_{x} \\\\ v \\cdot A^1_{x+1:\\enclose{actuarial}{n-1}} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore A^1_{x:\\enclose{actuarial}{n}} = v{}_{}q_{x} + v{}_{}p_{x}A^1_{x+1:\\enclose{actuarial}{n-1}} \\] Note that since it requires a term policy with a reduced term , this recursion is not particularly useful as it is difficult to obtain it. The PE variable is similar, with the main difference being that the policyholder will receive nothing if the policyholder dies . Thus, only the second component of the recursion remains: \\[ \\begin{aligned} PE &= \\begin{cases} 0 ,& {}_{}q_{x} \\\\ v \\cdot {}_{n-1}E_{x+1} ,& {}_{}p_{x} \\end{cases} \\end{aligned} \\] \\[ \\therefore {}_{n}E_{x} = v{}_{}p_{x} \\cdot {}_{n-1}E_{x+1} \\] EA is omitted from this section as it is simply the combination of a TA and PE. These identities are most useful in a spreadsheet setting where the calculations can be easily repeated to fill up an entire life table. However, even then it is necessary to have a starting point for the recusions to occur. The most common starting point is the terminal age as the EPVs can be intuitively determined since the policyholder will inevitably die at the end of the year: \\[ \\begin{aligned} A_{\\omega-1} &= v \\\\ A^{\\> \\> 1}_{\\omega-1:\\enclose{actuarial}{n}} &= v \\\\ {}_{n}E_{\\omega-1} &= 0 \\\\ A^{1}_{\\omega-1:\\enclose{actuarial}{n}} &= v \\end{aligned} \\]","title":"Recursions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#intuitions","text":"Although the exam questions are mostly computational, it is good to have an understanding of how the different EPVs compare against one another to serve as a sort of \"sense check\" against the SULT provided to determine if the answer is in the right direction.","title":"Intuitions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#same-assurance","text":"Recall that the probability of death is an increasing function with age. The death benefit is more likely to be paid out to an older policyholder - in other words, they receive the death benefit \"sooner\" than a younger policyholder. Thus, an older policyholder has larger expected cashflows that are discounted less (due to receiving it sooner), which results in a higher EPV than a younger policyholder, all else equal: \\[ \\begin{aligned} A_{x+k} & \\gt A_{x} \\\\ A^{\\> \\> 1} _{x+k:\\enclose{actuarial}{n}} & \\gt A^{\\> \\> 1}_{x+k:\\enclose{actuarial}{n}} \\end{aligned} \\] Conversely, the probability of survival is a decreasing function with age. The survival benefit is less likely to be paid out to an older policyholder - smaller expected cashflows. Regardless of the age of the policyholder, the survival benefit is paid at the same time ( same discounting ). Thus, since an older policyholder has smaller expected cashflows , it has a lower EPV than a younger policyholder: \\[ {}_{n}E_{x+k} \\le {}_{n}E_{x} \\] Endowment Assurances have both a death and survival component , thus the comparison is a combination of the two: A younger policyholder is more likely to survive and receive the survival benefit at the end of the term (discounted more) An older policyholder is more likely to die and receive the death benefit during the term (discounted less) Assuming that the difference in expected cashflows are negligible , then an older policyholder would have an higher EPV due to the lower discounting : \\[ A_{x+k:\\enclose{actuarial}{n}} \\gt A_{x:\\enclose{actuarial}{n}} \\] Naturally, all else equal, assurances with a lower interest rate are discounted less and thus have a higher EPV .","title":"Same Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#different-assurances","text":"At a young age where the probability of death is low, all else equal (where applicable), the EPVs of each assurance rank as follows: TA will have the smallest EPV . Although their benefits are paid out sooner, the expected benefits are small as the probability of death is small. WL has the next largest EPV . They have the same benefits as term in the short run, but have large expected benefits in the future . However, these large benefits are heavily discounted , still resuling in a small EPV. PE has the next largest EPV . Given the high probability of survival, the expected benefits are large . EA has the largest EPV . Since it is a combination of TA and PE, it is naturally the highest. \\[ \\begin{aligned} A^{1}_{30:\\enclose{actuarial}{n}} < A_{30} < {}_{n}E_{30} < A_{30:\\enclose{actuarial}{n}} \\\\ \\end{aligned} \\] At an old age where the probability of death is high, all else equal (where applicable), the EPVs of each assurance rank as follows: PE will have the smallest EPV . Given the low probability of survival, the expected benefits are are small . TA will have the next largest EPV . Given the high probability of death, the expected benefits are high . EA will have the next largest EPV . Since it is combination of TA and PE, it is naturally higher than both of them. WL has the largest EPV . Given the inevitable death of the policyholder, the expected benefits are the highest . \\[ \\begin{aligned} {}_{n}E_{100} < A^{1}_{100:\\enclose{actuarial}{n}} < A_{100:\\enclose{actuarial}{n}} < A_{100} \\\\ \\end{aligned} \\] As the policyholder approaches the terminal age, the EPVs tend to one another: \\[ \\begin{aligned} x &\\to \\omega \\\\ E(\\text{PE}) &\\to 0 \\\\ E(\\text{EA}) &\\to E(\\text{TA}) \\\\ E(\\text{TA}) &\\to E(\\text{WL}) \\end{aligned} \\] TA tends to WL whenever the end of the term exceeds the terminal age - thus the cashflows and hence EPV for both assurances become identical.","title":"Different Assurances"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#probabilities-and-percentiles","text":"Apart from just calculating the Expectation and Variance, the probabilities and hence percentiles of the contract benefits can be calculated as well. The former refers to calculating the probability that the random variable is at most some value \\(u\\) . In other words, that the PV ( NOT the EPV! ) is at most \\(u\\) . \\[ \\begin{aligned} \\text{WL} &\\le u \\\\ v^{K_x+1} &\\le u \\\\ (K_x+1) &\\ln v \\le \\ln u \\\\ K_x+1 &\\ge \\frac{\\ln u}{\\ln v} \\\\ K_x & \\ge \\frac{\\ln u}{\\ln v} - 1 \\\\ \\end{aligned} \\] Note that it is a common mistake to forget to flip the inequality sign as \\(\\ln v \\lt 0\\) . To avoid this error, it is advised to plot a graph to remember that \\(K_x\\) should be larger than the calculated value: The RHS of the expression is unlikely to be an integer. However, \\(K_x\\) can only take integer values. Thus, the values can rounded up to the nearest whole number (EG. 5): \\[ \\begin{aligned} P(K_x \\ge \\frac{\\ln u}{\\ln v} - 1) &= P(K_x \\ge 5) \\\\ &= P(T_x \\ge 5) \\\\ &= {}_{5}p_x \\end{aligned} \\] The latter refers to calculating the percentile of the random variable , which is the smallest value of the RV that results in the specified probability. This process is the opposite of the previous one - it involves solving for \\(T_x\\) , converting it to \\(K_x\\) and then subsituting it back into the RV, which results in the associated percentile.","title":"Probabilities and Percentiles"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#payable-continuously","text":"In practice, life assurances pay benefits as soon as the insured event occurs , thus is akin to paying out continuously throughout the year. Notice that paying out continuously is actually a special case of a contract that pays out \\(m\\) times a year , where \\(m\\) tends to infinity. \\(m=4\\) ; Quarterly Payments \\(m=12\\) ; Monthly Payments \\(m=\\infty\\) ; Continuous Payments Thus, despite the header stating \"payable continously\", this section will instead cover paying out \\(m\\) times a year , which can be used to derive the continuous case. Note that questions with Assurances that pay out \\(m\\) times a year are rare - if anything, the continuous case will be directly tested instead. However, the \\(m\\) times a year case is still covered because the ideas can be extended to Annuities, where such questions are common. The survival model used must now reflect the probability of death at fractional ages . Intuitively, it can be thought of as the probability living till a discrete age and then dying within a sub-period within that year: \\[ \\begin{aligned} K_x + \\frac{j+1}{m}, \\>\\> j = 0, 1, 2, ..., m-1 \\end{aligned} \\] Thus, the corresponding random variable representing the PV of the benefits: \\[ \\text{PV} = v^{K_x + \\frac{j+1}{m}} \\] The EPV of an assurance payable \\(m\\) times a year can be denoted with the \\((m)\\) superscript with the same notations as before: \\[ A^{(m)}_{x} = \\sum^{\\infty}_{K_x=0} v^{K_x} {}_{K_x}p_x \\cdot \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}} {}_{\\frac{j}{m}}p_{x+K_x} \\cdot {}_{\\frac{1}{m}} q_{x+K_x+\\frac{j}{m}} \\\\ \\] Note that the benefit used is still the ANNUAL BENEFIT . To obtain the actual benefit payable per period, then it should be divided by the number of periods; \\(\\frac{B}{m}\\) . Unfortunately, most life tables do not naturally provide probabilities for death at fractional ages, thus the EPVs must be approximated from the discrete case , which will be covered later.","title":"Payable Continuously"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#continuous-case","text":"As \\(m \\to \\infty\\) , then the survival model becomes \\(T_x\\) and hence random variable is: \\[ \\begin{aligned} \\text{PV} &= v^{T_x} \\\\ &= e^{-\\delta \\cdot T_x} \\end{aligned} \\] The EPV of an assurance payable continously is denoted with the \\(\\bar{}\\) accent with the same notations as before: \\[ \\begin{aligned} \\bar{A}_x &= \\int^{\\infty}_{0} v^{T_x} \\cdot f_x(t) \\\\ &= \\int^{\\infty}_{0} e^{-\\delta \\cdot T_x} \\cdot {}_{t}p_{x} \\mu_{x+t} \\end{aligned} \\] Recall that the second term represents the probability of living to a certain age \\(x+t\\) and then dying in an infinitely small time after that (definition of force of interest). Thus, although the EPV looks very different, it is still fundamentally a Triple Product Summation of the same components: Amount of Benefit, 1 Discounting of the Benefit, \\(e^{-\\delta \\cdot T_x}\\) Probability of paying the Benefit, \\({}_{t}p_{x} \\mu_{x+t}\\) Unlike the payable \\(m\\) times case, the EPVs can be calculated directly if the survival distribution is provided. However, since they are special cases of the payable \\(m\\) times case, it can also be calculated through approximation from the discrete case. The same logic can be applied to all other assurances, EXCEPT for PEs . This is because they pay out at a fixed time , thus there is NO such thing as a payable \\(m\\) times PE or a continuously payable PE.","title":"Continuous Case"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#uniform-distribution-of-deaths","text":"Recall that the UDD assumption can be used to approximate survival probabilities of fractional ages from discrete probabilities. Using this approach, the Continuous EPVs can be approximated from the Discrete ones. Assuming UDD between integer ages, \\[ {}_{\\frac{j}{m}}p_{x+K_x} \\cdot {}_{\\frac{1}{m}} q_{x+K_x+\\frac{j}{m}} \\approx \\frac{1}{m} \\cdot q_{x+K_x} \\] Thus, the EPV of a continuous contract can be expressed as a function of the discrete case: \\[ \\begin{aligned} A^{(m)}_{x} &= \\sum^{\\infty}_{K_x=0} v^{K_x} {}_{K_x}p_x \\cdot \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}} {}_{\\frac{j}{m}}p_{x+K_x} \\cdot {}_{\\frac{1}{m}} q_{x+K_x+\\frac{j}{m}} \\\\ & \\approx \\sum^{\\infty}_{K_x=0} v^{K_x} {}_{K_x}p_x \\cdot \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}} \\frac{1}{m} \\cdot q_{x+K_x} \\\\ & \\approx \\sum^{\\infty}_{K_x=0} v^{K_x+1} {}_{K_x}p_x q_{x+K_x} \\cdot \\frac{1}{m} \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}-1} \\\\ & \\approx A_x \\cdot \\frac{v^{-1}}{m} \\sum^{m-1}_{j=0} v^{\\frac{j+1}{m}} \\\\ & \\approx A_x \\cdot \\frac{1+i}{m} \\cdot \\frac{v^{\\frac{1}{m}}(1-v)}{1-v^{\\frac{1}{m}}} \\\\ & \\approx A_x \\cdot \\frac{1+i}{m} \\cdot \\frac{v^{\\frac{1}{m}}(1-v)}{v^{\\frac{1}{m}}[(1+i)^{m}-1]} \\\\ & \\approx A_x \\cdot \\frac{1+i}{m} \\cdot \\frac{1-v}{(1+i)^{m}-1} \\\\ & \\approx A_x \\cdot \\frac{1+i-1}{m[(1+i)^{m}-1]} \\\\ & \\approx \\frac{i}{i^{(m)}} \\cdot A_x \\end{aligned} \\] As \\(m \\to \\infty\\) , \\[ \\begin{aligned} i^{(m)} &\\to \\delta, \\\\ A^{(m)}_{x} &\\to \\bar{A}_x, \\\\ \\therefore \\bar{A}_x &= \\frac{i}{\\delta} A_x \\end{aligned} \\]","title":"Uniform Distribution of Deaths"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#claims-acceleration-approach","text":"Given that claims occur every \\(\\frac{1}{m}\\) period, then on average , the claims within a year occur at \\(\\frac{m+1}{2m}\\) : \\[ \\begin{aligned} \\text{Average Death} &= \\frac{\\frac{1}{m} + \\frac{m}{m}}{2} \\\\ &= \\frac{\\frac{1+m}{m}}{2} \\\\ &= \\frac{m+1}{2m} \\end{aligned} \\] Thus, the EPV of a continuous contract can be expressed as a function of the discrete case: \\[ \\begin{aligned} A^{(m)}_{x} & \\approx \\sum^{\\infty}_{K_x = 0} v^{K_x + \\frac{m+1}{2m}} {}_{K_x|} q_{x} \\\\ & \\approx v^{\\frac{m+1}{2m}-1} \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} {}_{K_x|} q_{x} \\\\ & \\approx v^{\\frac{m+1-2m}{2m}} A_x \\\\ & \\approx v^{\\frac{-m+1}{2m}} A_x \\\\ & \\approx v^{-\\frac{m-1}{2m}} A_x \\\\ & \\approx (1+i)^{\\frac{m-1}{2m}} A_x \\end{aligned} \\] As \\(m \\to \\infty\\) , \\[ \\begin{aligned} m &\\to \\infty, \\\\ \\frac{m-1}{2m} &\\to \\frac{1}{2}, \\\\ A^{(m)}_{x} &\\to \\bar{A}_x, \\\\ \\therefore \\bar{A}_x &= (1+i)^{2} A_x \\end{aligned} \\] This is known as the Claims Acceleration Approach as the claims are paid out earlier on in the year as compared to the end.","title":"Claims Acceleration Approach"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/3.%20Life%20Assurances/#common-pitfalls","text":"Recall that there are no continuous PEs - thus, be very careful when approximating an EA as ONLY the TA portion needs to be approximated ! This means that we cannot directly apply an approximation to the EA values provided in the SULT - they must be broken down into the TA and EA components first. If calculating manually, it is always advised to calculate the TA and EA components seperately before combining into an EA to prevent falling into the trap of directly approximating the EA. \\[ \\begin{aligned} \\bar{A}_{x:\\enclose{actuarial}{n}} & \\ne \\frac{i}{\\delta} A_{x:\\enclose{actuarial}{n}} \\\\ \\bar{A}_{x:\\enclose{actuarial}{n}} & = \\frac{i}{\\delta} A^{1}_{x:\\enclose{actuarial}{n}} + {}_{n}E_x \\end{aligned} \\] Similarly, if approximating the second moment for variance, note that the NEW interest rate must be used in the approximation term as well: \\[ \\begin{aligned} {}^{2} \\bar{A}_x &= \\frac{i^*}{\\delta^*} \\cdot {}^{2} A_x \\end{aligned} \\] Although the above two points were illustrated with UDD, they apply to the claims acceleration approach as well. Lastly, the two approaches should produce similar results which differ by a few decimal places. However, these differences are enlarged when dealing with large benefits as these decimal places will be brought forward, resulting in seemingly large differences - do not be alarmed!","title":"Common Pitfalls"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/","text":"Life Annuities Life Annuity contracts promise to pay out a stream of benefits in the future for as long the policyholder remains alive. Similar to life insurance, the benefits can be paid discretely or continuously and can be valued through their EPV. Note that these are different from the annuities covered in Exam FM. The payments for those annuities are made regardless of the life of the policyholder, known as Annuity Certain . They serve as the foundation to understanding Life Annuities. Review: Annuities Certain There are two types of payment structures: Period Start Period End Paid in Advance Paid in Arrears Annuity Due Annuity Immediate \\(\\ddot{a}_{\\enclose{actuarial}{n}}\\) \\(a_{\\enclose{actuarial}{n}}\\) The overall PV of the annuity is the sum of the PV of the stream of payments: \\[ \\begin{aligned} a_{\\enclose{actuarial}{n}} &= v + v^2 + v^3 + \\dots + v^n \\\\ &= \\frac{v-v^{n+1}}{1-v} \\\\ &= \\sum^n_{k=1} v^k \\\\ &= \\frac{v(1-v^n)}{1-v} \\\\ &= \\frac{1-v^n}{i} \\\\ \\\\ \\ddot{a}_{\\enclose{actuarial}{n}} &= 1 + v + v^2 + \\dots + v^n \\\\ &= \\sum^{n-1}_{k=0} v^k \\\\ &= \\frac{1- v^{n}}{1-v} \\\\ &= \\frac{1- v^{n}}{d} \\end{aligned} \\] Notice that the payments simply differ by one period and hence one discounting factor: \\[ \\begin{aligned} \\ddot{a}_{\\enclose{actuarial}{n}} &= 1 + v + v^2 + v^3 + \\dots + v^n \\\\ &= (1+i) (v + v^2 + v^3 + \\dots + v^n) \\\\ &= (1+i) \\sum^{n}_{k=1} v^k \\\\ &= (1+i) a_{\\enclose{actuarial}{n}} \\end{aligned} \\] From another perspective, only the payments at the end points \\(t=0\\) and \\(t=n\\) are different: \\[ \\begin{aligned} \\ddot{a}_{\\enclose{actuarial}{n}} &= \\sum^{n-1}_0 v^k \\\\ &= v^0 + \\sum^{n-1}_{k=1} v^k \\\\ &= 1 + a_{\\enclose{actuarial}{n-1}} \\\\ &= 1 + a_{\\enclose{actuarial}{n}} - v^n \\end{aligned} \\] Payable Discretely If the contract pays out benefits discretely, then \\(K_x\\) is used as the survival model. Since a life annuity pays out a stream of benefits, the PV of the contract at every time period can be slightly confusing. It is important to remember the following: The PV is calculated with respect to time 0 The PV is the sum of the PV of all the payments the individual is expected to live \\[ \\begin{aligned} PV &= \\begin{cases} a_{\\enclose{actuarial}{K_x}}, & \\text{Life Annuity Immediate} \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x + 1}}, & \\text{Life Annuity Due} \\end{cases} \\end{aligned} \\] Thus, the EPV of the contract is the sum product of the PV of the benefit in each period and the probability of death in that period: \\[ \\begin{aligned} EPV &= \\begin{cases} \\text{Life Annuity Immediate} &=\\ \\sum a_{\\enclose{actuarial}{K_x}} \\cdot {}_{K_x|}q_{x} \\\\ \\text{Life Annuity Due} &= \\sum \\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\cdot {}_{K_x|}q_{x} \\end{cases} \\end{aligned} \\] For the remainder of the section, to be concise, only the formulas for Annuity Due will be given. The corresponding formulas for Annuity Immediates can be easily calculated from it. Actuarial Notation Most of the notation for Life Insurance applies to Life Annuities as well. The key difference is that \\({}^{k}a\\) is used to represent the k-th moment of the present value of a contract where a benefit of 1 is paid discretely for as long as the status does NOT fail . \\(A\\) stands for Assurance while \\(a\\) stands for Annuity. Whole Life Annuity Whole Life Annuities covers the insured indefinitely and thus will pay out for as long as the insured survives . Let WL be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{WL}_{\\text{Due}} &=\\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\end{aligned} \\] Thus, the EPV is the Expectation of the WLA random variable: \\[ \\begin{aligned} E(\\text{WLA}_{\\text{Due}}) &= \\sum^{\\infty}_{K_x = 0} \\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\cdot {}_{K_x|}q_{x} \\\\ \\ddot{a}_{x} &= \\sum^{\\infty}_{K_x = 0} \\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] The EPV can be furthered simplified, allowing a life annuity to be viewed as the sum of a series of pure endowments : \\[ \\begin{aligned} \\ddot{a}_{x} &= \\sum^{\\infty}_{K_x = 0} \\ddot{a}_{\\enclose{actuarial}{K_x}} \\cdot {}_{K_x|}q_{x} \\\\ &=\\sum^{\\infty}_{K_x = 0} \\left(\\sum^{n-1}_{j=0} v^j \\right) \\cdot {}_{K_x|}q_{x} \\\\ &= v^0 \\cdot {}_{0|}q_{x} + (v^0 + v^1) \\cdot {}_{1|}q_{x} + (v^0 + v^1 + v^2) \\cdot {}_{2|}q_{x} + \\dots \\\\ &= v^0 ({}_{0|}q_{x} + {}_{1|}q_{x} + \\dots) + v^1 ({}_{1|}q_{x} + {}_{2|}q_{x} + \\dots) + v^2 ({}_{2|}q_{x} + {}_{3|}q_{x} + \\dots) + \\dots \\\\ &= \\sum^{\\infty}_{j=0} v^j \\cdot \\left(\\sum^{\\infty}_{K_x = j} {}_{K_x|}q_{x} \\right) \\\\ &= \\sum^{\\infty}_{j=0} v^j \\cdot {}_{j}p_{x} \\end{aligned} \\] Note that the above formula can be used to easily prove the relationships between different kinds of annuities, but is NOT a proper definition of an annuity - it hence cannot be adjusted to compute the variance. The second moment and variance for all life annuities will be covered in a later section. Temporary Annuity Temporary Life Annuities covers the insured for a specified period \\(n\\) and thus pays out for as long as the insured survives during that period only . Let TA be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{TA}_\\text{Due} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{K_x+1}},& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{n}},& K_x \\ge n \\\\ \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{min(K_x + 1, n)}} \\end{aligned} \\] Thus, the EPV is the expectation of the TA random variable, which can be simplified using the same approach as before: \\[ \\begin{aligned} E(\\text{TA}_\\text{Due}) &= \\sum^{n-1}_{K_x = 0} \\ddot{a}_{\\enclose{actuarial}{K_x}} \\cdot {}_{K_x|}q_x + \\ddot{a}_{\\enclose{actuarial}{n}} \\cdot {}_{n}p_x \\\\ \\ddot{a}_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{j=0} v^j \\cdot {}_{j}p_{x} \\end{aligned} \\] Note the change in limits for the simplified approach - an annuity is the combination of pure endowments for as long as the annuity lasts . This gives rise to a very simple sense check - the EPV of an \\(n\\) year temporary annuity should NOT be larger than \\(n\\) as there are \\(n\\) payments of 1 which are discounted. Deferred Annuities Deferred Annuities are variations of the above contracts where the coverage is deferred by \\(n\\) years . While any contract can be deferred, the most useful is the Deferred Whole Life Annuity . Let DWL be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{DWL}_{\\text{Due}} &= \\begin{cases} 0, & K_x = 0, 1, 2, \\dots, n-1 \\\\ v^n \\cdot \\ddot{a}_{\\enclose{actuarial}{K_x+1-n}}, & K_x \\ge n \\end{cases} \\end{aligned} \\] Intuitively, the RV can also be expressed as the difference between two certain annuities: \\[ \\begin{aligned} \\text{DWL}_{\\text{Due}} &= v^n \\cdot \\ddot{a}_{\\enclose{actuarial}{K_x+1-n}} \\\\ &= v^n \\cdot \\frac{1-v^{K_x+1-n}}{d} \\\\ &= \\frac{v^{n}-v^{K_x+1}}{d} \\\\ &= \\frac{(1-v^{K_x+1}) - (1-v^{n})}{d} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{K_x+1}} -\\ddot{a}_{\\enclose{actuarial}{n}} \\\\ \\\\ \\therefore \\text{DWL}_{\\text{Due}} &= \\begin{cases} 0, & K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}} -\\ddot{a}_{\\enclose{actuarial}{n}} , & K_x \\ge n \\end{cases} \\end{aligned} \\] Thus, the EPV is the expectation of the DWL random variable: \\[ \\begin{aligned} E(\\text{DWL}_{\\text{Due}}) &= \\sum^{\\infty}_{K_x = n} (\\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{K_x|}q_{x} \\\\ {}_{n|}\\ddot{a}_{x} &= \\sum^{\\infty}_{K_x = 0} (\\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{K_x|}q_{x} - \\sum^{n-1}_{K_x = 0} (\\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{K_x|}q_{x} \\\\ &= \\sum^{\\infty}_{K_x = 0} \\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\cdot {}_{K_x|}q_{x} - \\sum^{\\infty}_{K_x = 0} \\ddot{a}_{\\enclose{actuarial}{n}} \\cdot {}_{K_x|}q_{x} - \\sum^{n-1}_{K_x = 0} \\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\cdot {}_{K_x|}q_{x} + \\sum^{n-1}_{K_x = 0} \\ddot{a}_{\\enclose{actuarial}{n}} \\cdot {}_{K_x|}q_{x} \\\\ &= \\sum^{\\infty}_{K_x = 0} \\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\cdot {}_{K_x|}q_{x} - \\sum^{n-1}_{K_x = 0} \\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\cdot {}_{K_x|}q_{x} \\\\ &= \\ddot{a}_x - \\ddot{a}_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] Similar to before, the EPV can be shown to be a sum of pure endowments: \\[ {}_{n|}\\ddot{a}_{x} = \\sum^{\\infty}_{j=n} v^j \\cdot {}_{j}p_{x} \\] Since a DWL is effectively a WL that is issued \\(n\\) years later, the EPV of a DWL is the EPV of the WL, adjusted for both interest and survival: \\[ \\begin{aligned} {}_{n|}a_{x} &= \\sum^{\\infty}_{j=n} v^j \\cdot {}_{j}p_{x} \\\\ &= \\sum^{\\infty}_{j=0} v^{j+n} \\cdot {}_{j+n}p_{x} \\\\ &= \\sum^{\\infty}_{j=0} v^{j} v^{n} \\cdot {}_{n}p_{x} {}_{j}p_{x+n} \\\\ &= v^{n} \\cdot {}_{n}p_{x} \\sum^{\\infty}_{j=0} v^{j} \\cdot {}_{j}p_{x+n} \\\\ &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\end{aligned} \\] This allows a TA to be expressed as the difference of two WL annuities issued at different times : \\[ \\begin{aligned} {}_{n|}\\ddot{a}_{x} &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\\\ \\ddot{a}_x - \\ddot{a}_{x:\\enclose{actuarial}{n}} &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\\\ \\ddot{a}_{x:\\enclose{actuarial}{n}} &= \\ddot{a}_x - {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\end{aligned} \\] Guaranteed Annuities Guaranteed Life Annuities are whole life annuities with the benefits in the first \\(n\\) years being guaranteed - they will be paid out even if the insured dies during this period. Let GA be the random variable denoting the PV of the survival benefit. Since the payments are guaranteed, they can be represented using a certain annuity : \\[ \\begin{aligned} \\text{GA}_{\\text{Due}} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}},& K_x \\ge n \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{max(K_x+1,n)}} \\end{aligned} \\] The random variable can be manipulated to be easier to work with: \\[ \\begin{aligned} \\text{GA}_{\\text{Due}} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}},& K_x \\ge n \\end{cases} \\\\ &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}} + \\ddot{a}_{\\enclose{actuarial}{n}},& K_x \\ge n \\end{cases} \\\\ &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}} \\\\ \\ddot{a}_{\\enclose{actuarial}{n}} \\end{cases} + \\begin{cases} 0,& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}},& K_x \\ge n \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{n}} + \\begin{cases} 0,& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}},& K_x \\ge n \\end{cases} \\end{aligned} \\] Thus, the EPV can be more easily calculated: \\[ \\begin{aligned} E(\\text{GA}_{\\text{Due}}) &= \\ddot{a}_{\\enclose{actuarial}{n}} + 0 \\cdot {}_{n}q_x + \\sum^{\\infty}_{K_x = n} (\\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{K_x|}q_{x}\\\\ \\ddot{a}_{\\overline{x:\\enclose{actuarial}{n}}} &= \\ddot{a}_{\\enclose{actuarial}{n}} + {}_{n} \\ddot{a}_{x} \\end{aligned} \\] Note that a Guaranteed Annuity is denoted via an additional bar above the status. Assurances and Annuities Consider the random variable for the PV Whole Life Assurance and Annuity Due. Notice that both RVs are related through the certain annuity formula: \\[ \\begin{aligned} \\text{WL} &= v^{K_x + 1} \\\\ \\text{WLA}_{Due} &= \\ddot{a}_{\\enclose{actuarial}{K_x+1}} = \\frac{1-v^{K_x + 1}}{d} \\end{aligned} \\] This is why Annuity Due, rather Annuity Immediates, are provided in the SULT. Thus, given the EPV of a life annuity, the EPV of a life assurance can be determined, vice-versa: \\[ \\begin{aligned} \\text{WLA} &= \\frac{1-\\text{WL}}{d} \\\\ E(\\text{WLA}_{Due}) &= \\frac{1-E(\\text{WL})}{d} \\\\ \\ddot{a}_x &= \\frac{1-A_x}{d} \\\\ d \\ddot{a}_x &= 1 - A_x \\\\ A_x &= 1 - d \\ddot{a}_x \\end{aligned} \\] The same relationship can be shown for a temporary annuity and an Endowment Assurance, NOT a term assurance : \\[ \\begin{aligned} A_{x:\\enclose{actuarial}{n}} &= 1 - d \\ddot{a}_{x:\\enclose{actuarial}{n}} \\\\ \\end{aligned} \\] It is a very common mistake to confuse the two as both are denoted by TA. However, it is the endowment assurance that shares the same random variable as the temporary annuity. It also makes sense when comparing the actuarial notation - both share the same subscript. Variance However, the second moment is slightly different as it must reflect the new interest rate used. The same logic can be applied for the Variance as well: \\[ \\begin{aligned} \\text{WLA}_{Due} &= \\frac{1-\\text{WL}}{d} \\\\ Var(\\text{WLA}_{Due}) &= Var \\left( \\frac{1-\\text{WL}}{d} \\right) \\\\ Var(\\text{WLA}_{Due}) &= \\frac{1}{d^2} var(\\text{WL}) \\\\ Var(\\text{WLA}_{Due}) &= \\frac{1}{d^2} \\left({}_{}^2 A_x - (A_x)^2 \\right) \\end{aligned} \\] Var(\\text{TA}_{Due}) &= \\frac{1}{d^2} \\left({}_{}^2 A_{x:\\enclose{actuarial}{n}} - (A_{x:\\enclose{actuarial}{n}})^2 \\right) WLDue and WL Immediate same variance but DWLDue and DWL immediate different variance because it is dependent on survival GA = Certain + Deferred Var (GA) = Var (Deferred) Problem is Var(Deferred) Immediate and Due Recall that for Certain Annuities, the only difference between Due and Immediate ones are the payments at the start and end times. This relationship can be extended for Life Annuities as well. For WL annuities, since the end point is infinity , the difference in the end points can be ignored. Thus, the only difference is the first payment of 1, which can be easily accounted for: \\[ a_x = \\ddot{a}_x - 1 \\] This section uses \\(a_x\\) as the focus, as it is assumed that if needed, the due versions will be calculated and then converted to immediate , rather than calculating immediate annuities directly. Unfortunately, the difference at the end cannot be ignored for TA annuities: \\[ a_x = \\ddot{a_x} - 1 + {}_{n}E_x \\] For variance, the random variables (which are certain annuities) are used instead: \\[ \\begin{aligned} Var(\\text{WL}_{Immediate}) &= Var(a_{\\enclose{actuarial}{K_x}}) \\\\ &= Var(\\ddot{a}_{\\enclose{actuarial}{K_x+1}}-1) \\\\ &= Var(\\ddot{a}_{\\enclose{actuarial}{K_x+1}}) \\\\ &= Var(\\text{WL}_{Due}) \\\\ \\\\ Var(\\text{TA}_{Immediate}) &= Var(a_{\\enclose{actuarial}{min(K_x,n)}}) \\\\ &= Var(\\ddot{a}_{\\enclose{actuarial}{min(K_x+1,n+1)}}-1) \\\\ &= Var(\\ddot{a}_{\\enclose{actuarial}{min(K_x+1,n+1)}}) \\\\ &= Var \\left( \\frac{1-v^{min(K_x+1,n+1)}}{d} \\right) \\\\ &= \\frac{1}{d^2} Var(v^{min(K_x+1,n+1)}) \\\\ &= \\frac{1}{d^2} \\left({}_{2}A_{x:\\enclose{actuarial}{n+1}} - (A_{x:\\enclose{actuarial}{n+1}})^2 \\right) \\end{aligned} \\] Thus, the variance of a WL annuity is the same for both Immediate and Due but NOT for a TA annuity. Recursions Following the same logic as assurances, Annuities can also be recursively expressed as a function of itself: If the policyholder dies, they would have only received the benefit of 1 at the start of the year If the policyholder survives, they would receive the additional benefits of the future periods The benefit of 1 is received REGARDLESS of death or survival as it was paid at the start of the year \\[ \\begin{aligned} \\text{WL}_\\text{Due} &= \\begin{cases} 1,& q_x \\\\ 1 + v\\ddot{a}_{x+1} ,& p_x \\end{cases} \\\\ &= 1 + \\begin{cases} 0,& q_x \\\\ v\\ddot{a}_{x+1} ,& p_x \\end{cases} \\\\ \\\\ \\therefore \\ddot{a}_{x} &= 1 + vp_x\\ddot{a}_{x+1} \\end{aligned} \\] The same can be shown for TAs, but remember that the remaining duration of the policy must decrease as well: \\[ \\ddot{a}_{x:\\enclose{actuarial}{n}} = 1 + vp_x \\ddot{a}_{x+1:\\enclose{actuarial}{n-1}} \\] Intuitions Similar to assurances, several intuitions can be made about the EPV of various annuities to serve as a sort of \"sense check\" against the SULT provided to determine if the answer is in the right direction. Same Annuity Since annuities are all contingent on the survival of the policyholder, only one case needs to be considered. The probability of survival is a decreasing function with age. The benefits are less likely to be paid out to an older policyholder, resulting in smaller expected cashflows . Since the cashflows are discounted the same amount, an older policyholder will have a lower EPV than a younger one: \\[ \\ddot{a}_{x+n} \\lt \\ddot{a}_{x} \\] As shown previously, Annuity Dues are always smaller than Immediates as the cashflows occur earlier and are hence discounted less . \\[ a_x < \\ddot{a}_x \\] Naturally, all else equal, annuities with a lower interest rate are discounted less and thus have a higher EPV . Different Annuities Since annuities are all contingent on the survival of the policyholder, the age of the policyholder for comparison does not matter. TA has the smallest EPV as it can only pay for a maximum of \\(n\\) years, while WLs and GA can pay indefinitely. WL is always smaller than GA as its payments in the first \\(n\\) years are not guaranteed; the payments after that are identical. \\[ \\ddot{a}_{x:\\enclose{actuarial}{n}} < \\ddot{a}_x < \\ddot{a}_{\\bar{x:\\enclose{actuarial}{n}}} \\] Probabilities and Percentiles Similar to assurances, apart from just calculating the Expectation and Variance, the probabilities and hence percentiles of the contract benefits can be calculated as well. \\[ \\begin{aligned} \\text{WL}_\\text{Due} &\\le u \\\\ \\frac{1-v^{K_x+1}}{d} &\\le u \\\\ 1-v^{K_x+1} &\\le ud \\\\ v^{K_x+1} &\\ge 1-ud \\\\ (K_x+1) \\ln v &\\ge \\ln(1-ud) \\\\ K_x+1 &\\le \\frac{\\ln(1-ud)}{\\ln v} \\\\ K_x &\\le \\frac{\\ln(1-ud)}{\\ln v} - 1 \\\\ \\end{aligned} \\] The common mistake of not flipping the inequality sign is relevant in annuities as well. To avoid this error, it is advised to plot a graph to remember that \\(K_x\\) should be smaller than the calculated value: The RHS of the expression is unlikely to be an integer. However, \\(K_x\\) can only take integer values. Thus, the values can rounded DOWN to the nearest whole number (EG. 4): \\[ \\begin{aligned} P(K_x \\le \\frac{\\ln(1-ud)}{\\ln v} - 1) &= P(K_x \\le 4) \\\\ &= P(T_x \\le 4) \\\\ &= {}_{4}p_x \\end{aligned} \\] The latter refers to calculating the percentile of the random variable , which is the smallest value of the RV that results in the specified probability. This process is the opposite of the previous one - it involves solving for \\(T_x\\) , converting it to \\(K_x\\) and then subsituting it back into the RV, which results in the associated percentile. Payable Continuously Similar to assurances, annuities can also be payable \\(m\\) times a year or payable continuously. In fact, most of the key ideas carry over from the assurances, thus this section will only focus on the approximations from discrete annuities. Uniform Distribution of Deaths Using the relationship between Assurances and Annuities, a continuous WL annuity can also be expressed in the form of a continuous assurance: \\[ \\begin{aligned} \\ddot{a}^{(m)}_x &= \\frac{1 - A^{(m)}_x}{d^{(m)}} \\end{aligned} \\] Assuming UDD, the continuous assurance and hence the overarching continuous annuity can be simplified: \\[ \\begin{aligned} \\ddot{a}^{(m)}_x &= \\frac{1 - A^{(m)}_x}{d^{(m)}} \\\\ &= \\frac{1 - \\frac{i}{i^{(m)}}A_x}{d^{(m)}} \\\\ &= \\frac{\\frac{i^{(m)}-iA_x}{i^{(m)}}}{d^{(m)}} \\\\ &= \\frac{i^{(m)}-iA_x}{i^{(m)}d^{(m)}} \\\\ &= \\frac{i^{(m)}-i(1-d\\ddot{a}_x)}{i^{(m)}d^{(m)}} \\\\ &= \\frac{i^{(m)}-i+id\\ddot{a}_x}{i^{(m)}d^{(m)}} \\\\ &= \\frac{id}{i^{(m)}d^{(m)}} \\ddot{a}_x - \\frac{i-i^{(m)}}{i^{(m)}d^{(m)}} \\\\ &= \\alpha(m)\\ddot{a}_x - \\beta(m) \\end{aligned} \\] An adjustment can be made for temporary annuities: \\[ \\begin{aligned} \\ddot{a}^{(m)}_{x:\\enclose{actuarial}{n}} &= \\ddot{a}^{(m)}_{x} - {}_{n}E_x \\ddot{a}^{(m)}_{x+n} \\\\ &= \\alpha(m)\\ddot{a}_x - \\beta(m) - {}_{n}E_x [\\alpha(m)\\ddot{a}_{x:\\enclose{actuarial}{n}} - \\beta(m)] \\\\ &= \\alpha(m)\\ddot{a}_x - \\beta(m) - {}_{n}E_x \\alpha(m)\\ddot{a}_{x:\\enclose{actuarial}{n}} - {}_{n}E_x \\beta(m) \\\\ &= \\alpha(m) [\\ddot{a}_x - {}_{n}E_x \\ddot{a}_{x:\\enclose{actuarial}{n}}] - \\beta(m) [1-{}_{n}E_x] \\\\ &= \\alpha(m) \\ddot{a}_{x:\\enclose{actuarial}{n}} - \\beta(m) [1-{}_{n}E_x] \\end{aligned} \\] Woolhouse Approximation Since a continuous annuity can be expressed as a sum of discrete pure endowments , the trapezoidal rule and euler-macluarin correction can be used to approximate its EPV. The key difference from before is understanding that the EPV of an annuity due is a LEFT riemann sum as there is a payment at time 0. \\[ \\begin{aligned} \\bar{a}_x &= \\int^{\\infty}_0 v^t {}_{t}p_x \\\\ & \\approx h[f(a) + f(a+h) + \\dots + f(b-h)] - \\frac{h}{2} [f(a)-f(b)] + \\frac{h^2}{12} [f'(a)-f'(b)] \\end{aligned} \\] The first derivatives can be found below: \\[ \\begin{aligned} f'(n) &= \\frac{d}{dt} (e^{-\\delta n} {}_{n}p_x) \\\\ &= {}_{n}p_x \\delta e^{-\\delta n} - e^{-\\delta n} {}_{n}p_x \\mu_{x+n} \\\\ &= e^{-\\delta n} {}_{n}p_x (\\delta + \\mu_{x+n}) \\\\ &= {}_{n}E_x (\\delta + \\mu_{x+n}) \\\\ \\\\ \\therefore f'(0) &= -(\\delta + \\mu_{x}) \\\\ \\therefore f'(\\infty) &= 0 \\end{aligned} \\] Assuming \\(h=1\\) , \\[ \\begin{aligned} \\bar{a}_x & \\approx (1) [{}_{0}E_x + {}_{1}E_x + \\dots + {}_{\\infty-1}E_x] - \\frac{1}{2} [{}_{0}E_x-{}_{\\infty-1}E_x] + \\frac{1}{12} [-(\\delta + \\mu_{x}) - 0] \\\\ & \\approx [1 + {}_{n}E_1 + \\dots] - \\frac{1}{2} [1-0] - \\frac{1}{12} (\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12} (\\delta + \\mu_{x}) \\end{aligned} \\] Assuming \\(h=\\frac{1}{m}\\) instead, \\[ \\begin{aligned} \\bar{a}_x & \\approx \\left(\\frac{1}{m} \\right) [{}_{0}E_x + {}_{\\frac{1}{m}}E_x + \\dots + {}_{\\infty-\\frac{1}{m}}E_x] - \\frac{1}{2m} [{}_{0}E_x-{}_{\\infty-\\frac{1}{m}}E_x] + \\frac{1}{12m^2} [-(\\delta + \\mu_{x}) - 0] \\\\ & \\approx \\left(\\frac{1}{m} \\right) [1 + {}_{\\frac{1}{m}}E_x + \\dots] - \\frac{1}{2m} [1-0] - \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}^{(m)}_x - \\frac{1}{2m} - \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\end{aligned} \\] Combining both together, \\[ \\begin{aligned} \\bar{a}_x &= \\bar{a}_x \\\\ \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12} (\\delta + \\mu_{x}) & \\approx \\ddot{a}^{(m)}_x - \\frac{1}{2m} - \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\\\ \\ddot{a}^{(m)}_x & \\approx \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12} (\\delta + \\mu_{x}) + \\frac{1}{2m} + \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}_x - (\\frac{1}{2}-\\frac{1}{2m}) - (\\frac{1}{12} - \\frac{1}{12m^2})(\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}_x - \\frac{m-1}{2m} - \\frac{m^2-1}{12m^2}(\\delta + \\mu_{x}) \\end{aligned} \\] This process is known as the Woolhouse Approximation . If the error term is omitted, then it is known as the 2-term Woolhouse Approximation else it is known as the 3-term Woolhouse Approximation . The 3-term woolhouse approximation is extremely accurate due to its mathematical roots. It so much more accurate than UDD such that when calculating a continuous assurance , it is advisable to calculate the corresponding annuity using the 3 term approximation first and then convert it, for the best results. Unfortunately, the drawback is that it requires knowledge about the force of mortality, which is not commonly provided in life tables. Thus, although less accurate, the 2 term approach can be easily used in any situation. Term Approximations Unlike assurances, the approximations for annuities have multiple components. Thus, the approximations for term annuities are slightly different from the ones used for WL. However, there is NO need to remember a seperate expression for it, it can be calculated by converting the WL to a TA: \\[ \\begin{aligned} a^{(m)}_{x:\\enclose{actuarial}{n}} &= a^{(m)}_{x} - {}_{n}E_{x} a^{(m)}_{x+n} \\\\ &= \\alpha(m) \\ddot{a}_{x} - \\beta(m) - {}_{n}E_{x} (\\alpha(m) \\ddot{a}_{x+n} - \\beta(m)) \\\\ &= \\alpha(m) \\ddot{a}_{x} - \\beta(m) - {}_{n}E_{x} \\alpha(m) \\ddot{a}_{x+n} + {}_{n}E_{x} \\beta(m) \\\\ &= \\alpha(m) (\\ddot{a}_{x} - {}_{n}E_{x} \\ddot{a}_{x+n}) - \\beta(m) (1-{}_{n}E_{x}) \\\\ &= \\alpha(m) (\\ddot{a}_{x+n}) - \\beta(m) (1-{}_{n}E_{x}) \\end{aligned} \\] A similar approach can be taken for the woolhouse approximation, resulting in the following: \\[ a^{(m)}_{x:\\enclose{actuarial}{n}} = \\ddot{a}_{x+n} - \\frac{m-1}{2m} (1-{}_{n}E_{x}) \\] For simplicity, the three term approach is not shown.","title":"Life Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#life-annuities","text":"Life Annuity contracts promise to pay out a stream of benefits in the future for as long the policyholder remains alive. Similar to life insurance, the benefits can be paid discretely or continuously and can be valued through their EPV. Note that these are different from the annuities covered in Exam FM. The payments for those annuities are made regardless of the life of the policyholder, known as Annuity Certain . They serve as the foundation to understanding Life Annuities.","title":"Life Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#review-annuities-certain","text":"There are two types of payment structures: Period Start Period End Paid in Advance Paid in Arrears Annuity Due Annuity Immediate \\(\\ddot{a}_{\\enclose{actuarial}{n}}\\) \\(a_{\\enclose{actuarial}{n}}\\) The overall PV of the annuity is the sum of the PV of the stream of payments: \\[ \\begin{aligned} a_{\\enclose{actuarial}{n}} &= v + v^2 + v^3 + \\dots + v^n \\\\ &= \\frac{v-v^{n+1}}{1-v} \\\\ &= \\sum^n_{k=1} v^k \\\\ &= \\frac{v(1-v^n)}{1-v} \\\\ &= \\frac{1-v^n}{i} \\\\ \\\\ \\ddot{a}_{\\enclose{actuarial}{n}} &= 1 + v + v^2 + \\dots + v^n \\\\ &= \\sum^{n-1}_{k=0} v^k \\\\ &= \\frac{1- v^{n}}{1-v} \\\\ &= \\frac{1- v^{n}}{d} \\end{aligned} \\] Notice that the payments simply differ by one period and hence one discounting factor: \\[ \\begin{aligned} \\ddot{a}_{\\enclose{actuarial}{n}} &= 1 + v + v^2 + v^3 + \\dots + v^n \\\\ &= (1+i) (v + v^2 + v^3 + \\dots + v^n) \\\\ &= (1+i) \\sum^{n}_{k=1} v^k \\\\ &= (1+i) a_{\\enclose{actuarial}{n}} \\end{aligned} \\] From another perspective, only the payments at the end points \\(t=0\\) and \\(t=n\\) are different: \\[ \\begin{aligned} \\ddot{a}_{\\enclose{actuarial}{n}} &= \\sum^{n-1}_0 v^k \\\\ &= v^0 + \\sum^{n-1}_{k=1} v^k \\\\ &= 1 + a_{\\enclose{actuarial}{n-1}} \\\\ &= 1 + a_{\\enclose{actuarial}{n}} - v^n \\end{aligned} \\]","title":"Review: Annuities Certain"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#payable-discretely","text":"If the contract pays out benefits discretely, then \\(K_x\\) is used as the survival model. Since a life annuity pays out a stream of benefits, the PV of the contract at every time period can be slightly confusing. It is important to remember the following: The PV is calculated with respect to time 0 The PV is the sum of the PV of all the payments the individual is expected to live \\[ \\begin{aligned} PV &= \\begin{cases} a_{\\enclose{actuarial}{K_x}}, & \\text{Life Annuity Immediate} \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x + 1}}, & \\text{Life Annuity Due} \\end{cases} \\end{aligned} \\] Thus, the EPV of the contract is the sum product of the PV of the benefit in each period and the probability of death in that period: \\[ \\begin{aligned} EPV &= \\begin{cases} \\text{Life Annuity Immediate} &=\\ \\sum a_{\\enclose{actuarial}{K_x}} \\cdot {}_{K_x|}q_{x} \\\\ \\text{Life Annuity Due} &= \\sum \\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\cdot {}_{K_x|}q_{x} \\end{cases} \\end{aligned} \\] For the remainder of the section, to be concise, only the formulas for Annuity Due will be given. The corresponding formulas for Annuity Immediates can be easily calculated from it.","title":"Payable Discretely"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#actuarial-notation","text":"Most of the notation for Life Insurance applies to Life Annuities as well. The key difference is that \\({}^{k}a\\) is used to represent the k-th moment of the present value of a contract where a benefit of 1 is paid discretely for as long as the status does NOT fail . \\(A\\) stands for Assurance while \\(a\\) stands for Annuity.","title":"Actuarial Notation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#whole-life-annuity","text":"Whole Life Annuities covers the insured indefinitely and thus will pay out for as long as the insured survives . Let WL be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{WL}_{\\text{Due}} &=\\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\end{aligned} \\] Thus, the EPV is the Expectation of the WLA random variable: \\[ \\begin{aligned} E(\\text{WLA}_{\\text{Due}}) &= \\sum^{\\infty}_{K_x = 0} \\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\cdot {}_{K_x|}q_{x} \\\\ \\ddot{a}_{x} &= \\sum^{\\infty}_{K_x = 0} \\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\cdot {}_{K_x|}q_{x} \\end{aligned} \\] The EPV can be furthered simplified, allowing a life annuity to be viewed as the sum of a series of pure endowments : \\[ \\begin{aligned} \\ddot{a}_{x} &= \\sum^{\\infty}_{K_x = 0} \\ddot{a}_{\\enclose{actuarial}{K_x}} \\cdot {}_{K_x|}q_{x} \\\\ &=\\sum^{\\infty}_{K_x = 0} \\left(\\sum^{n-1}_{j=0} v^j \\right) \\cdot {}_{K_x|}q_{x} \\\\ &= v^0 \\cdot {}_{0|}q_{x} + (v^0 + v^1) \\cdot {}_{1|}q_{x} + (v^0 + v^1 + v^2) \\cdot {}_{2|}q_{x} + \\dots \\\\ &= v^0 ({}_{0|}q_{x} + {}_{1|}q_{x} + \\dots) + v^1 ({}_{1|}q_{x} + {}_{2|}q_{x} + \\dots) + v^2 ({}_{2|}q_{x} + {}_{3|}q_{x} + \\dots) + \\dots \\\\ &= \\sum^{\\infty}_{j=0} v^j \\cdot \\left(\\sum^{\\infty}_{K_x = j} {}_{K_x|}q_{x} \\right) \\\\ &= \\sum^{\\infty}_{j=0} v^j \\cdot {}_{j}p_{x} \\end{aligned} \\] Note that the above formula can be used to easily prove the relationships between different kinds of annuities, but is NOT a proper definition of an annuity - it hence cannot be adjusted to compute the variance. The second moment and variance for all life annuities will be covered in a later section.","title":"Whole Life Annuity"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#temporary-annuity","text":"Temporary Life Annuities covers the insured for a specified period \\(n\\) and thus pays out for as long as the insured survives during that period only . Let TA be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{TA}_\\text{Due} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{K_x+1}},& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{n}},& K_x \\ge n \\\\ \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{min(K_x + 1, n)}} \\end{aligned} \\] Thus, the EPV is the expectation of the TA random variable, which can be simplified using the same approach as before: \\[ \\begin{aligned} E(\\text{TA}_\\text{Due}) &= \\sum^{n-1}_{K_x = 0} \\ddot{a}_{\\enclose{actuarial}{K_x}} \\cdot {}_{K_x|}q_x + \\ddot{a}_{\\enclose{actuarial}{n}} \\cdot {}_{n}p_x \\\\ \\ddot{a}_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{j=0} v^j \\cdot {}_{j}p_{x} \\end{aligned} \\] Note the change in limits for the simplified approach - an annuity is the combination of pure endowments for as long as the annuity lasts . This gives rise to a very simple sense check - the EPV of an \\(n\\) year temporary annuity should NOT be larger than \\(n\\) as there are \\(n\\) payments of 1 which are discounted.","title":"Temporary Annuity"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#deferred-annuities","text":"Deferred Annuities are variations of the above contracts where the coverage is deferred by \\(n\\) years . While any contract can be deferred, the most useful is the Deferred Whole Life Annuity . Let DWL be the random variable denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{DWL}_{\\text{Due}} &= \\begin{cases} 0, & K_x = 0, 1, 2, \\dots, n-1 \\\\ v^n \\cdot \\ddot{a}_{\\enclose{actuarial}{K_x+1-n}}, & K_x \\ge n \\end{cases} \\end{aligned} \\] Intuitively, the RV can also be expressed as the difference between two certain annuities: \\[ \\begin{aligned} \\text{DWL}_{\\text{Due}} &= v^n \\cdot \\ddot{a}_{\\enclose{actuarial}{K_x+1-n}} \\\\ &= v^n \\cdot \\frac{1-v^{K_x+1-n}}{d} \\\\ &= \\frac{v^{n}-v^{K_x+1}}{d} \\\\ &= \\frac{(1-v^{K_x+1}) - (1-v^{n})}{d} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{K_x+1}} -\\ddot{a}_{\\enclose{actuarial}{n}} \\\\ \\\\ \\therefore \\text{DWL}_{\\text{Due}} &= \\begin{cases} 0, & K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}} -\\ddot{a}_{\\enclose{actuarial}{n}} , & K_x \\ge n \\end{cases} \\end{aligned} \\] Thus, the EPV is the expectation of the DWL random variable: \\[ \\begin{aligned} E(\\text{DWL}_{\\text{Due}}) &= \\sum^{\\infty}_{K_x = n} (\\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{K_x|}q_{x} \\\\ {}_{n|}\\ddot{a}_{x} &= \\sum^{\\infty}_{K_x = 0} (\\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{K_x|}q_{x} - \\sum^{n-1}_{K_x = 0} (\\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{K_x|}q_{x} \\\\ &= \\sum^{\\infty}_{K_x = 0} \\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\cdot {}_{K_x|}q_{x} - \\sum^{\\infty}_{K_x = 0} \\ddot{a}_{\\enclose{actuarial}{n}} \\cdot {}_{K_x|}q_{x} - \\sum^{n-1}_{K_x = 0} \\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\cdot {}_{K_x|}q_{x} + \\sum^{n-1}_{K_x = 0} \\ddot{a}_{\\enclose{actuarial}{n}} \\cdot {}_{K_x|}q_{x} \\\\ &= \\sum^{\\infty}_{K_x = 0} \\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\cdot {}_{K_x|}q_{x} - \\sum^{n-1}_{K_x = 0} \\ddot{a}_{\\enclose{actuarial}{K_x+1}} \\cdot {}_{K_x|}q_{x} \\\\ &= \\ddot{a}_x - \\ddot{a}_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] Similar to before, the EPV can be shown to be a sum of pure endowments: \\[ {}_{n|}\\ddot{a}_{x} = \\sum^{\\infty}_{j=n} v^j \\cdot {}_{j}p_{x} \\] Since a DWL is effectively a WL that is issued \\(n\\) years later, the EPV of a DWL is the EPV of the WL, adjusted for both interest and survival: \\[ \\begin{aligned} {}_{n|}a_{x} &= \\sum^{\\infty}_{j=n} v^j \\cdot {}_{j}p_{x} \\\\ &= \\sum^{\\infty}_{j=0} v^{j+n} \\cdot {}_{j+n}p_{x} \\\\ &= \\sum^{\\infty}_{j=0} v^{j} v^{n} \\cdot {}_{n}p_{x} {}_{j}p_{x+n} \\\\ &= v^{n} \\cdot {}_{n}p_{x} \\sum^{\\infty}_{j=0} v^{j} \\cdot {}_{j}p_{x+n} \\\\ &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\end{aligned} \\] This allows a TA to be expressed as the difference of two WL annuities issued at different times : \\[ \\begin{aligned} {}_{n|}\\ddot{a}_{x} &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\\\ \\ddot{a}_x - \\ddot{a}_{x:\\enclose{actuarial}{n}} &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\\\ \\ddot{a}_{x:\\enclose{actuarial}{n}} &= \\ddot{a}_x - {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\end{aligned} \\]","title":"Deferred Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#guaranteed-annuities","text":"Guaranteed Life Annuities are whole life annuities with the benefits in the first \\(n\\) years being guaranteed - they will be paid out even if the insured dies during this period. Let GA be the random variable denoting the PV of the survival benefit. Since the payments are guaranteed, they can be represented using a certain annuity : \\[ \\begin{aligned} \\text{GA}_{\\text{Due}} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}},& K_x \\ge n \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{max(K_x+1,n)}} \\end{aligned} \\] The random variable can be manipulated to be easier to work with: \\[ \\begin{aligned} \\text{GA}_{\\text{Due}} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}},& K_x \\ge n \\end{cases} \\\\ &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}} + \\ddot{a}_{\\enclose{actuarial}{n}},& K_x \\ge n \\end{cases} \\\\ &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}} \\\\ \\ddot{a}_{\\enclose{actuarial}{n}} \\end{cases} + \\begin{cases} 0,& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}},& K_x \\ge n \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{n}} + \\begin{cases} 0,& K_x = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}},& K_x \\ge n \\end{cases} \\end{aligned} \\] Thus, the EPV can be more easily calculated: \\[ \\begin{aligned} E(\\text{GA}_{\\text{Due}}) &= \\ddot{a}_{\\enclose{actuarial}{n}} + 0 \\cdot {}_{n}q_x + \\sum^{\\infty}_{K_x = n} (\\ddot{a}_{\\enclose{actuarial}{K_x+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{K_x|}q_{x}\\\\ \\ddot{a}_{\\overline{x:\\enclose{actuarial}{n}}} &= \\ddot{a}_{\\enclose{actuarial}{n}} + {}_{n} \\ddot{a}_{x} \\end{aligned} \\] Note that a Guaranteed Annuity is denoted via an additional bar above the status.","title":"Guaranteed Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#assurances-and-annuities","text":"Consider the random variable for the PV Whole Life Assurance and Annuity Due. Notice that both RVs are related through the certain annuity formula: \\[ \\begin{aligned} \\text{WL} &= v^{K_x + 1} \\\\ \\text{WLA}_{Due} &= \\ddot{a}_{\\enclose{actuarial}{K_x+1}} = \\frac{1-v^{K_x + 1}}{d} \\end{aligned} \\] This is why Annuity Due, rather Annuity Immediates, are provided in the SULT. Thus, given the EPV of a life annuity, the EPV of a life assurance can be determined, vice-versa: \\[ \\begin{aligned} \\text{WLA} &= \\frac{1-\\text{WL}}{d} \\\\ E(\\text{WLA}_{Due}) &= \\frac{1-E(\\text{WL})}{d} \\\\ \\ddot{a}_x &= \\frac{1-A_x}{d} \\\\ d \\ddot{a}_x &= 1 - A_x \\\\ A_x &= 1 - d \\ddot{a}_x \\end{aligned} \\] The same relationship can be shown for a temporary annuity and an Endowment Assurance, NOT a term assurance : \\[ \\begin{aligned} A_{x:\\enclose{actuarial}{n}} &= 1 - d \\ddot{a}_{x:\\enclose{actuarial}{n}} \\\\ \\end{aligned} \\] It is a very common mistake to confuse the two as both are denoted by TA. However, it is the endowment assurance that shares the same random variable as the temporary annuity. It also makes sense when comparing the actuarial notation - both share the same subscript.","title":"Assurances and Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#variance","text":"However, the second moment is slightly different as it must reflect the new interest rate used. The same logic can be applied for the Variance as well: \\[ \\begin{aligned} \\text{WLA}_{Due} &= \\frac{1-\\text{WL}}{d} \\\\ Var(\\text{WLA}_{Due}) &= Var \\left( \\frac{1-\\text{WL}}{d} \\right) \\\\ Var(\\text{WLA}_{Due}) &= \\frac{1}{d^2} var(\\text{WL}) \\\\ Var(\\text{WLA}_{Due}) &= \\frac{1}{d^2} \\left({}_{}^2 A_x - (A_x)^2 \\right) \\end{aligned} \\] Var(\\text{TA}_{Due}) &= \\frac{1}{d^2} \\left({}_{}^2 A_{x:\\enclose{actuarial}{n}} - (A_{x:\\enclose{actuarial}{n}})^2 \\right) WLDue and WL Immediate same variance but DWLDue and DWL immediate different variance because it is dependent on survival GA = Certain + Deferred Var (GA) = Var (Deferred) Problem is Var(Deferred)","title":"Variance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#immediate-and-due","text":"Recall that for Certain Annuities, the only difference between Due and Immediate ones are the payments at the start and end times. This relationship can be extended for Life Annuities as well. For WL annuities, since the end point is infinity , the difference in the end points can be ignored. Thus, the only difference is the first payment of 1, which can be easily accounted for: \\[ a_x = \\ddot{a}_x - 1 \\] This section uses \\(a_x\\) as the focus, as it is assumed that if needed, the due versions will be calculated and then converted to immediate , rather than calculating immediate annuities directly. Unfortunately, the difference at the end cannot be ignored for TA annuities: \\[ a_x = \\ddot{a_x} - 1 + {}_{n}E_x \\] For variance, the random variables (which are certain annuities) are used instead: \\[ \\begin{aligned} Var(\\text{WL}_{Immediate}) &= Var(a_{\\enclose{actuarial}{K_x}}) \\\\ &= Var(\\ddot{a}_{\\enclose{actuarial}{K_x+1}}-1) \\\\ &= Var(\\ddot{a}_{\\enclose{actuarial}{K_x+1}}) \\\\ &= Var(\\text{WL}_{Due}) \\\\ \\\\ Var(\\text{TA}_{Immediate}) &= Var(a_{\\enclose{actuarial}{min(K_x,n)}}) \\\\ &= Var(\\ddot{a}_{\\enclose{actuarial}{min(K_x+1,n+1)}}-1) \\\\ &= Var(\\ddot{a}_{\\enclose{actuarial}{min(K_x+1,n+1)}}) \\\\ &= Var \\left( \\frac{1-v^{min(K_x+1,n+1)}}{d} \\right) \\\\ &= \\frac{1}{d^2} Var(v^{min(K_x+1,n+1)}) \\\\ &= \\frac{1}{d^2} \\left({}_{2}A_{x:\\enclose{actuarial}{n+1}} - (A_{x:\\enclose{actuarial}{n+1}})^2 \\right) \\end{aligned} \\] Thus, the variance of a WL annuity is the same for both Immediate and Due but NOT for a TA annuity.","title":"Immediate and Due"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#recursions","text":"Following the same logic as assurances, Annuities can also be recursively expressed as a function of itself: If the policyholder dies, they would have only received the benefit of 1 at the start of the year If the policyholder survives, they would receive the additional benefits of the future periods The benefit of 1 is received REGARDLESS of death or survival as it was paid at the start of the year \\[ \\begin{aligned} \\text{WL}_\\text{Due} &= \\begin{cases} 1,& q_x \\\\ 1 + v\\ddot{a}_{x+1} ,& p_x \\end{cases} \\\\ &= 1 + \\begin{cases} 0,& q_x \\\\ v\\ddot{a}_{x+1} ,& p_x \\end{cases} \\\\ \\\\ \\therefore \\ddot{a}_{x} &= 1 + vp_x\\ddot{a}_{x+1} \\end{aligned} \\] The same can be shown for TAs, but remember that the remaining duration of the policy must decrease as well: \\[ \\ddot{a}_{x:\\enclose{actuarial}{n}} = 1 + vp_x \\ddot{a}_{x+1:\\enclose{actuarial}{n-1}} \\]","title":"Recursions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#intuitions","text":"Similar to assurances, several intuitions can be made about the EPV of various annuities to serve as a sort of \"sense check\" against the SULT provided to determine if the answer is in the right direction.","title":"Intuitions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#same-annuity","text":"Since annuities are all contingent on the survival of the policyholder, only one case needs to be considered. The probability of survival is a decreasing function with age. The benefits are less likely to be paid out to an older policyholder, resulting in smaller expected cashflows . Since the cashflows are discounted the same amount, an older policyholder will have a lower EPV than a younger one: \\[ \\ddot{a}_{x+n} \\lt \\ddot{a}_{x} \\] As shown previously, Annuity Dues are always smaller than Immediates as the cashflows occur earlier and are hence discounted less . \\[ a_x < \\ddot{a}_x \\] Naturally, all else equal, annuities with a lower interest rate are discounted less and thus have a higher EPV .","title":"Same Annuity"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#different-annuities","text":"Since annuities are all contingent on the survival of the policyholder, the age of the policyholder for comparison does not matter. TA has the smallest EPV as it can only pay for a maximum of \\(n\\) years, while WLs and GA can pay indefinitely. WL is always smaller than GA as its payments in the first \\(n\\) years are not guaranteed; the payments after that are identical. \\[ \\ddot{a}_{x:\\enclose{actuarial}{n}} < \\ddot{a}_x < \\ddot{a}_{\\bar{x:\\enclose{actuarial}{n}}} \\]","title":"Different Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#probabilities-and-percentiles","text":"Similar to assurances, apart from just calculating the Expectation and Variance, the probabilities and hence percentiles of the contract benefits can be calculated as well. \\[ \\begin{aligned} \\text{WL}_\\text{Due} &\\le u \\\\ \\frac{1-v^{K_x+1}}{d} &\\le u \\\\ 1-v^{K_x+1} &\\le ud \\\\ v^{K_x+1} &\\ge 1-ud \\\\ (K_x+1) \\ln v &\\ge \\ln(1-ud) \\\\ K_x+1 &\\le \\frac{\\ln(1-ud)}{\\ln v} \\\\ K_x &\\le \\frac{\\ln(1-ud)}{\\ln v} - 1 \\\\ \\end{aligned} \\] The common mistake of not flipping the inequality sign is relevant in annuities as well. To avoid this error, it is advised to plot a graph to remember that \\(K_x\\) should be smaller than the calculated value: The RHS of the expression is unlikely to be an integer. However, \\(K_x\\) can only take integer values. Thus, the values can rounded DOWN to the nearest whole number (EG. 4): \\[ \\begin{aligned} P(K_x \\le \\frac{\\ln(1-ud)}{\\ln v} - 1) &= P(K_x \\le 4) \\\\ &= P(T_x \\le 4) \\\\ &= {}_{4}p_x \\end{aligned} \\] The latter refers to calculating the percentile of the random variable , which is the smallest value of the RV that results in the specified probability. This process is the opposite of the previous one - it involves solving for \\(T_x\\) , converting it to \\(K_x\\) and then subsituting it back into the RV, which results in the associated percentile.","title":"Probabilities and Percentiles"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#payable-continuously","text":"Similar to assurances, annuities can also be payable \\(m\\) times a year or payable continuously. In fact, most of the key ideas carry over from the assurances, thus this section will only focus on the approximations from discrete annuities.","title":"Payable Continuously"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#uniform-distribution-of-deaths","text":"Using the relationship between Assurances and Annuities, a continuous WL annuity can also be expressed in the form of a continuous assurance: \\[ \\begin{aligned} \\ddot{a}^{(m)}_x &= \\frac{1 - A^{(m)}_x}{d^{(m)}} \\end{aligned} \\] Assuming UDD, the continuous assurance and hence the overarching continuous annuity can be simplified: \\[ \\begin{aligned} \\ddot{a}^{(m)}_x &= \\frac{1 - A^{(m)}_x}{d^{(m)}} \\\\ &= \\frac{1 - \\frac{i}{i^{(m)}}A_x}{d^{(m)}} \\\\ &= \\frac{\\frac{i^{(m)}-iA_x}{i^{(m)}}}{d^{(m)}} \\\\ &= \\frac{i^{(m)}-iA_x}{i^{(m)}d^{(m)}} \\\\ &= \\frac{i^{(m)}-i(1-d\\ddot{a}_x)}{i^{(m)}d^{(m)}} \\\\ &= \\frac{i^{(m)}-i+id\\ddot{a}_x}{i^{(m)}d^{(m)}} \\\\ &= \\frac{id}{i^{(m)}d^{(m)}} \\ddot{a}_x - \\frac{i-i^{(m)}}{i^{(m)}d^{(m)}} \\\\ &= \\alpha(m)\\ddot{a}_x - \\beta(m) \\end{aligned} \\] An adjustment can be made for temporary annuities: \\[ \\begin{aligned} \\ddot{a}^{(m)}_{x:\\enclose{actuarial}{n}} &= \\ddot{a}^{(m)}_{x} - {}_{n}E_x \\ddot{a}^{(m)}_{x+n} \\\\ &= \\alpha(m)\\ddot{a}_x - \\beta(m) - {}_{n}E_x [\\alpha(m)\\ddot{a}_{x:\\enclose{actuarial}{n}} - \\beta(m)] \\\\ &= \\alpha(m)\\ddot{a}_x - \\beta(m) - {}_{n}E_x \\alpha(m)\\ddot{a}_{x:\\enclose{actuarial}{n}} - {}_{n}E_x \\beta(m) \\\\ &= \\alpha(m) [\\ddot{a}_x - {}_{n}E_x \\ddot{a}_{x:\\enclose{actuarial}{n}}] - \\beta(m) [1-{}_{n}E_x] \\\\ &= \\alpha(m) \\ddot{a}_{x:\\enclose{actuarial}{n}} - \\beta(m) [1-{}_{n}E_x] \\end{aligned} \\]","title":"Uniform Distribution of Deaths"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#woolhouse-approximation","text":"Since a continuous annuity can be expressed as a sum of discrete pure endowments , the trapezoidal rule and euler-macluarin correction can be used to approximate its EPV. The key difference from before is understanding that the EPV of an annuity due is a LEFT riemann sum as there is a payment at time 0. \\[ \\begin{aligned} \\bar{a}_x &= \\int^{\\infty}_0 v^t {}_{t}p_x \\\\ & \\approx h[f(a) + f(a+h) + \\dots + f(b-h)] - \\frac{h}{2} [f(a)-f(b)] + \\frac{h^2}{12} [f'(a)-f'(b)] \\end{aligned} \\] The first derivatives can be found below: \\[ \\begin{aligned} f'(n) &= \\frac{d}{dt} (e^{-\\delta n} {}_{n}p_x) \\\\ &= {}_{n}p_x \\delta e^{-\\delta n} - e^{-\\delta n} {}_{n}p_x \\mu_{x+n} \\\\ &= e^{-\\delta n} {}_{n}p_x (\\delta + \\mu_{x+n}) \\\\ &= {}_{n}E_x (\\delta + \\mu_{x+n}) \\\\ \\\\ \\therefore f'(0) &= -(\\delta + \\mu_{x}) \\\\ \\therefore f'(\\infty) &= 0 \\end{aligned} \\] Assuming \\(h=1\\) , \\[ \\begin{aligned} \\bar{a}_x & \\approx (1) [{}_{0}E_x + {}_{1}E_x + \\dots + {}_{\\infty-1}E_x] - \\frac{1}{2} [{}_{0}E_x-{}_{\\infty-1}E_x] + \\frac{1}{12} [-(\\delta + \\mu_{x}) - 0] \\\\ & \\approx [1 + {}_{n}E_1 + \\dots] - \\frac{1}{2} [1-0] - \\frac{1}{12} (\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12} (\\delta + \\mu_{x}) \\end{aligned} \\] Assuming \\(h=\\frac{1}{m}\\) instead, \\[ \\begin{aligned} \\bar{a}_x & \\approx \\left(\\frac{1}{m} \\right) [{}_{0}E_x + {}_{\\frac{1}{m}}E_x + \\dots + {}_{\\infty-\\frac{1}{m}}E_x] - \\frac{1}{2m} [{}_{0}E_x-{}_{\\infty-\\frac{1}{m}}E_x] + \\frac{1}{12m^2} [-(\\delta + \\mu_{x}) - 0] \\\\ & \\approx \\left(\\frac{1}{m} \\right) [1 + {}_{\\frac{1}{m}}E_x + \\dots] - \\frac{1}{2m} [1-0] - \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}^{(m)}_x - \\frac{1}{2m} - \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\end{aligned} \\] Combining both together, \\[ \\begin{aligned} \\bar{a}_x &= \\bar{a}_x \\\\ \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12} (\\delta + \\mu_{x}) & \\approx \\ddot{a}^{(m)}_x - \\frac{1}{2m} - \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\\\ \\ddot{a}^{(m)}_x & \\approx \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12} (\\delta + \\mu_{x}) + \\frac{1}{2m} + \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}_x - (\\frac{1}{2}-\\frac{1}{2m}) - (\\frac{1}{12} - \\frac{1}{12m^2})(\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}_x - \\frac{m-1}{2m} - \\frac{m^2-1}{12m^2}(\\delta + \\mu_{x}) \\end{aligned} \\] This process is known as the Woolhouse Approximation . If the error term is omitted, then it is known as the 2-term Woolhouse Approximation else it is known as the 3-term Woolhouse Approximation . The 3-term woolhouse approximation is extremely accurate due to its mathematical roots. It so much more accurate than UDD such that when calculating a continuous assurance , it is advisable to calculate the corresponding annuity using the 3 term approximation first and then convert it, for the best results. Unfortunately, the drawback is that it requires knowledge about the force of mortality, which is not commonly provided in life tables. Thus, although less accurate, the 2 term approach can be easily used in any situation.","title":"Woolhouse Approximation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/4.%20Life%20Annuities/#term-approximations","text":"Unlike assurances, the approximations for annuities have multiple components. Thus, the approximations for term annuities are slightly different from the ones used for WL. However, there is NO need to remember a seperate expression for it, it can be calculated by converting the WL to a TA: \\[ \\begin{aligned} a^{(m)}_{x:\\enclose{actuarial}{n}} &= a^{(m)}_{x} - {}_{n}E_{x} a^{(m)}_{x+n} \\\\ &= \\alpha(m) \\ddot{a}_{x} - \\beta(m) - {}_{n}E_{x} (\\alpha(m) \\ddot{a}_{x+n} - \\beta(m)) \\\\ &= \\alpha(m) \\ddot{a}_{x} - \\beta(m) - {}_{n}E_{x} \\alpha(m) \\ddot{a}_{x+n} + {}_{n}E_{x} \\beta(m) \\\\ &= \\alpha(m) (\\ddot{a}_{x} - {}_{n}E_{x} \\ddot{a}_{x+n}) - \\beta(m) (1-{}_{n}E_{x}) \\\\ &= \\alpha(m) (\\ddot{a}_{x+n}) - \\beta(m) (1-{}_{n}E_{x}) \\end{aligned} \\] A similar approach can be taken for the woolhouse approximation, resulting in the following: \\[ a^{(m)}_{x:\\enclose{actuarial}{n}} = \\ddot{a}_{x+n} - \\frac{m-1}{2m} (1-{}_{n}E_{x}) \\] For simplicity, the three term approach is not shown.","title":"Term Approximations"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/","text":"Variable Benefits Previously, the benefit of the assurances were assumed to be fixed at 1 for simplicity. In practice, it is common to have assurances where the benefits change over time . Geometric Contracts If the benefits change by a common factor each period, then it is known as a Geometric contract. The benefit of the policy starts at 1 in the first year and changes by \\((1+b)\\) each period: If \\(b>0\\) , then the benefits are increasing If \\(b<0\\) , then the benefits are decreasing \\[ \\text{Geometric Benefit} = (1+b)^{K_x} \\] Note that the power is \\(K_x\\) and NOT \\(K_x+1\\) to reflect that the benefit starts at 1 when \\(K_x=0\\) . Geometric Assurance Let \\(\\text{Geom WL}\\) be the random variable denoting the PV of a Geometric Assurance. \\[ \\text{Geom WL} = (1+b)^{K_x} v^{K_x+1} \\] Thus, the EPV of a WL Geometric Assurance can be calculated as: \\[ \\begin{aligned} \\text{EPV Geom WL} &= \\sum^{\\infty}_{k=0} (1+b)^{k} v^{k+1} {}_{k|}q_x \\\\ &= (1+b)^{-1} \\sum^{\\infty}_{k=0} (1+b)^{k+1} v^{k+1} {}_{k|}q_x \\\\ &= \\frac{1}{1+b} \\sum^{\\infty}_{k=0} \\left(\\frac{1+b}{1+i}\\right)^{k+1} {}_{k|}q_x \\\\ &= \\frac{1}{1+b} \\sum^{\\infty}_{k=0} (v')^{k+1} {}_{k|}q_x \\\\ &= \\frac{1}{1+b} A_x |_{i=\\frac{1+i}{1+b}-1} \\end{aligned} \\] Note that this means that the term \\(A_x |_{i=\\frac{1+i}{1+b}-1}\\) by itself is a Geometric Assurance that starts with payments of \\((1+b)\\) , which is why \\(\\frac{1}{1+b}\\) is required to scale it down by one factor such that it starts at one. The expression follows similar intuition to the second moment - it is simply a regular EPV expression evaluated at a different interest rate . Unfortunately, there is no specified actuarial notation for this niche case. In practice, most questions will provide a value of \\(b\\) such that \\((i=\\frac{1+i}{1+b}-1)\\) simplifies to a nice percentage . However, this usually means that the value of \\(b\\) provided is some complicated number, so do not be taken aback! Geometric Annuity Let \\(\\text{EPV Geom WL}_\\text{Due}\\) be the random variable denoting the PV of a Geometric Annuity Due. Thus, the EPV of a Geometric Annuity Due can be calculated as: \\[ \\begin{aligned} \\text{EPV Geom WL}_\\text{Due} &= \\sum^{\\infty}_{k=0} (1+c)^k v^k {}_{k}p_x \\\\ &= \\sum^{\\infty}_{k=0} \\left(\\frac{1+c}{1+i}\\right)^k {}_{k}p_x \\\\ &= \\ddot{a}_x|_{i = \\frac{1+i}{1+c}-1} \\end{aligned} \\] Continuous Geometric The EPV of continuously payable geometric contracts can be calculated using the usual UDD, Claims Acceleration or Woolhouse Approximations. However, note that the \"new\" interest rate must be used in the approximations, similar to how the new interest rate is used for approximating the second moments. Arithmetic Contracts If the benefits change by a fixed constant each period, then it is instead known as an Arithmetic contract. For simplicity, the change each period is assumed to be 1: Arithmetically Increasing - Increases by 1 each period Arithmetically Decreasing - Decreased by 1 each period Since a WL contract lasts forever, the benefits of a decreasing WL contract would inevitably become negative , which do not make sense. Thus, WLs can only be arithmetically increasing contracts . \\[ \\text{Arithmetically Increasing Benefit} = K_x + 1 \\] Arithmetic Assurance Let \\(\\text{Arith WL}\\) be the random variable denoting the PV of an arithmetically increasing WL Assurance: \\[ \\text{Arith WL} = (K_x + 1) v^{K_x+1} \\] Thus, the EPV of a WL Arithmetic Assurance can be shown to be: \\[ \\begin{aligned} \\text{EPV Arith WL} &= \\sum^{\\infty}_{k=0} (k+1) v^{k+1} {}_{k|}q_x \\\\ (IA)_x &= \\sum^{\\infty}_{k=0} (k+1) v^{k+1} {}_{k|}q_x \\end{aligned} \\] \\((IA)_x\\) is the actuarial notation for the EPV of a contract with arithmetically increasing benefits starting at 1 and increasing by 1 each period. Intuitively, it can also be shown to be a sum of deferred WLs , each deferred by one period from the previous. This creates a step-like function which creates the increasing effect of the assurance: \\[ \\begin{aligned} (IA)_x &= {}_{0|}A_x + {}_{1|}A_x + {}_{2|}A_x + ... \\\\ &= \\sum^{\\infty}_{K_x = 0} {}_{K_x|}A_x \\end{aligned} \\] In practice, the magnitude of the change is unlikely to be one. Thus, the EPV must be scaled to match the actual change. This can be problematic if the starting benefit is NOT the same as the change . For instance, consider a contract with starting benefit of 100k which increases by 1k each period. Since the change is 1000, the EPV must be scaled accordingly, resulting in \\(1000(IA)_x\\) . However, this is the EPV for a contract with a starting benefit of 1000 which increases by 1000 each period. Thus, there must be an additional term to add the remaining 99k back inside: \\[ \\text{EPV 100k increasing by 1k} = 99k \\cdot A_x + 1k \\cdot (IA)_x \\] Arithmetic Annuities Let \\(\\text{Arith WL}_\\text{Due}\\) be the random variable denoting the PV of an arithmetically increasing annuity. The PV can be denoted by an arithmetically increasing annuity certain : \\[ \\begin{aligned} \\text{Arith WL}_\\text{Due} &= (I\\ddot{a})_{\\enclose{actuarial}{n}} \\\\ &= \\sum^{n-1}_{k=0} v^k \\cdot \\ddot{a}_{\\enclose{actuarial}{n-k}} \\\\ &= \\sum^{n-1}_{k=0} v^k \\cdot \\frac{1-v^{n-k}}{d} \\\\ &= \\sum^{n-1}_{k=0} \\frac{v^k - v^n}{d} \\\\ &= \\frac{1}{d} \\left(\\sum^{n-1}_{k=0} v^k - \\sum^{n-1}_{k=0} v^n \\right) \\\\ &= \\frac{1}{d} (\\ddot{a}_{\\enclose{actuarial}{n}} - nv^n) \\\\ &= \\frac{\\ddot{a}_{\\enclose{actuarial}{n}} - nv^n}{d} \\end{aligned} \\] Thus, the EPV of an arithmetically increasing WL annuity Due can be calculated as: \\[ \\begin{aligned} \\text{EPV Arith WL}_\\text{Due} &= \\sum^{\\infty}_{k=0} (I\\ddot{a})_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_x \\\\ (I\\ddot{a})_x &= \\sum^{\\infty}_{k=0} (k+1) v^k {}_{k}p_x \\\\ \\end{aligned} \\] Alternatively, it can be calculated from an assurance instead: \\[ \\begin{aligned} \\text{Arith WL}_\\text{Due} &= \\frac{\\ddot{a}_{\\enclose{actuarial}{k+1}} - (k+1)v^{k+1}}{d} \\\\ E(\\text{Arith WL}_\\text{Due}) &= \\frac{E(\\ddot{a}_{\\enclose{actuarial}{k+1}}) - E((k+1)v^{k+1})}{d} \\\\ (I\\ddot{a})_x &= \\frac{\\ddot{a}_x - (IA)_x}{d} \\end{aligned} \\] Notice that the main difference between an increasing annuity due and immediate is that the immediate case now lags the due by 1 every period . This difference can be accounted for using a level annuity due: \\[ \\begin{aligned} (I\\ddot{a})_x &= \\sum^{\\infty}_{k=0} (k+1) v^k {}_{k}p_{x} \\\\ &= v^{0} {}_{0}p_{x} + 2v^{1} {}_{1}p_{x} + 3v^{2} {}_{2}p_{x} + \\dots \\\\ &= (v^{1} {}_{1}p_{x} + 2v^{2} {}_{2}p_{x} + \\dots) + (v^{0} {}_{0}p_{x} + v^{1} {}_{1}p_{x} + v^{2} {}_{2}p_{x} + \\dots) \\\\ &= (Ia)_x + \\ddot{a}_x \\\\ (Ia)_x &= (I\\ddot{a})_x - \\ddot{a}_x \\end{aligned} \\] Arithmetic Term/Temporary For contract with level benefits , term/temporary contracts can be expressed as a difference of two WLs issued at different times because the benefits for the time after the specified term would cancel out . For contracts with variable benefits, this is slightly more complicated as the benefits after the specified term do NOT cancel out : The \"earlier\" WL would have increased significantly from its starting value The \"later\" WL would only be at its starting value Thus, an additional expression needs to be subtracted in order to remove the remaining benefit past the specified term. This can be done using a level benefit contract with the a benefit equal to the remaining amount . Thus, the EPV of an Increasing Term Assurance can be calculated as the following: \\[ \\begin{aligned} \\text{EPV Arith TA} &= \\sum^{n-1}_{K_x=0} (k+1) v^{K_x+1} {}_{k|}q_x \\\\ (IA)^1_{x:\\enclose{actuarial}{n}} &= (IA)_x - {}_{n}E_x [(IA)_{x+n} + nA_{x+n}] \\end{aligned} \\] Thus, the EPV of an Increasing Term Annuity can be calculated as the following: \\[ \\begin{aligned} \\text{EPV Arith TA}_\\text{Due} &= \\sum^{n-1}_{k=0} (I\\ddot{a})_{\\enclose{actuarial}{n}} \\cdot {}_{k|}q_x \\\\ (I\\ddot{a})_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{k=0} (k+1) v^k {}_{k}p_x \\\\ &= (I\\ddot{a})_x - {}_{n}E_x[(I\\ddot{a})_{x+n}+n\\ddot{a}_{x+n}] \\end{aligned} \\] Decreasing Benefits As mentioned previously, another key feature of term/temporary contracts is that they can have decreasing benefits . This is because the benefit of the policy can be set such that it would exactly decrease to 0 by the end of the policy term. This is done by setting a starting benefit of \\(n\\) and decreasing by \\(1\\) each period : \\[ \\text{Arithmetically Decreasing Benefit} = n-k \\] Thus, the EPV of a Decreasing Term Assurance can be calculated as the following: \\[ \\begin{aligned} \\text{EPV Arith Decreasing TA} &= \\sum^{\\infty}_{k=0} (n-k) v^{k+1} {}_{k|}q_x \\\\ (D_{\\enclose{actuarial}{n}}A)_{x:\\enclose{actuarial}{n}} &= \\sum^{\\infty}_{k=0} (n-k+1-1) v^{k+1} {}_{k|}q_x \\\\ &= \\sum^{\\infty}_{k=0} (n+1) v^{k+1} {}_{k|}q_x - \\sum^{\\infty}_{k=0} (k+1) v^{k+1} {}_{k|}q_x \\\\ &= (n+1)A^1_{x:\\enclose{actuarial}{n}} - (IA)^1_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] \\((DA)\\) is the actuarial notation for an Assurance with Arithmetically decreasing benefits. Thus, the EPV of a decreasing temporary annuity can be calculated as the following: Continuous Arithmetic Similar to Geometric Contracts, the EPV of a continuously payable Arithmetic Contracts can be calculated using the usual approximations. However, continuous arithmetic contracts could refer to a contract that not only payable continuously, but changes continuously as well. The benefits change at a constant rate throughout the period , such that the total change is still equal to 1. This can also be approximated from the discrete case, but the intuition is best understood graphically : Thus, for both Assurances and Annuities, the EPV can be calculated as: \\[ \\begin{aligned} (\\bar{I}\\bar{A})_{x} &= (I\\bar{A})_{x} - \\frac{1}{2} \\bar{A}_x \\\\ (\\bar{I}\\bar{a}) &= (I\\bar{a}) - \\frac{1}{2} \\bar{a}_x \\end{aligned} \\] Variable Recursions Since the benefit payable at the end of the first year is still 1 , the recursion for variable contracts are similar to the level ones. Variable Assurances Consider the two scenarios: If the policyholder dies in the current year with probability \\(q_x\\) , then a benefit of 1 is paid at the end of the year If the policyholder lives past the current year with probability \\(p_x\\) , then the they will receive benefits that are larger than 1 at some future time For the second half, the term multiplied must be the EPV of an assurance that STARTS with an increased benefit of \\((1+b)\\) or \\(2\\) . The issue is that the random variable is currently defined as a contract with benefits starting at 1 , which is why the recursive formula is different from the level case. For Geometric Assurances, \\(A_{x+1}|_{i=\\frac{1+i}{1+b}-1}\\) is already an Geometric Assurance that starts at \\((1+b)\\) , thus the recursive formula is simply: \\[ \\text{EPV Geom WL}_{x} = vq_x + vp_x A_{x+1}|_{i=\\frac{1+i}{1+b}-1} \\] For Arithmetic Assurances, an EPV representing an additional benefit of 1 each period must be added that the expression starts from 2 and increases by an additional 1 each period. \\[ (IA)_{x} = vq_x + vp_x[(IA)_{x+1} + A_{x+1}] \\] Variable Annuities Recall that a benefit of 1 is received at the beginning of the year regardless of whether the policyholder lives or dies. If the policyholder dies with probability \\(q_x\\) , then they receive nothing extra If the policyholder lives past the current year with probability \\(p_x\\) , then they will receive benefits that are larger than 1 at some future time For Geometric Annuities, TBC. \\[ \\begin{aligned} \\text{EPV Geom WL}_\\text{Due}_{x} = 1 + vp_x \\cdot (1+c) \\left(\\ddot{a}_{x+1}|_{i = \\frac{1+i}{1+c}-1}\\right) \\end{aligned} \\] For Arithmetic Annuities, the same adjustment must be made: \\[ (I\\ddot{a})_x = 1 + vp_x[(I\\ddot{a})_{x+1} + \\ddot{a}_{x+1}] \\] The same can be shown for Variable Term/Temporary Assurances/Annuities, but remember that the remaining duration of the policy must decrease as well. Variance and Second Moment Consider the second moment of both types of variable assurances: \\[ \\begin{aligned} E\\left[(\\text{Arith WL})^2\\right] &= \\sum^{\\infty}_{K_x=0} (k+1)^2 (v^2)^{K_x+1} {}_{k|}q_x \\\\ \\\\ E\\left[(\\text{Geom WL})^2\\right] &= \\sum^{\\infty}_{K_x=0} (1+b)^{2K_x} (v^2)^{K_x+1} {}_{k|}q_x \\end{aligned} \\] Notice that the usual approximation of \\(v=v^2\\) is insufficient to solve for the second moment due to the squared benefit. Thus, there are no \\({}^{2}A\\) defined for variable assurances.","title":"Variable Benefits"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#variable-benefits","text":"Previously, the benefit of the assurances were assumed to be fixed at 1 for simplicity. In practice, it is common to have assurances where the benefits change over time .","title":"Variable Benefits"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#geometric-contracts","text":"If the benefits change by a common factor each period, then it is known as a Geometric contract. The benefit of the policy starts at 1 in the first year and changes by \\((1+b)\\) each period: If \\(b>0\\) , then the benefits are increasing If \\(b<0\\) , then the benefits are decreasing \\[ \\text{Geometric Benefit} = (1+b)^{K_x} \\] Note that the power is \\(K_x\\) and NOT \\(K_x+1\\) to reflect that the benefit starts at 1 when \\(K_x=0\\) .","title":"Geometric Contracts"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#geometric-assurance","text":"Let \\(\\text{Geom WL}\\) be the random variable denoting the PV of a Geometric Assurance. \\[ \\text{Geom WL} = (1+b)^{K_x} v^{K_x+1} \\] Thus, the EPV of a WL Geometric Assurance can be calculated as: \\[ \\begin{aligned} \\text{EPV Geom WL} &= \\sum^{\\infty}_{k=0} (1+b)^{k} v^{k+1} {}_{k|}q_x \\\\ &= (1+b)^{-1} \\sum^{\\infty}_{k=0} (1+b)^{k+1} v^{k+1} {}_{k|}q_x \\\\ &= \\frac{1}{1+b} \\sum^{\\infty}_{k=0} \\left(\\frac{1+b}{1+i}\\right)^{k+1} {}_{k|}q_x \\\\ &= \\frac{1}{1+b} \\sum^{\\infty}_{k=0} (v')^{k+1} {}_{k|}q_x \\\\ &= \\frac{1}{1+b} A_x |_{i=\\frac{1+i}{1+b}-1} \\end{aligned} \\] Note that this means that the term \\(A_x |_{i=\\frac{1+i}{1+b}-1}\\) by itself is a Geometric Assurance that starts with payments of \\((1+b)\\) , which is why \\(\\frac{1}{1+b}\\) is required to scale it down by one factor such that it starts at one. The expression follows similar intuition to the second moment - it is simply a regular EPV expression evaluated at a different interest rate . Unfortunately, there is no specified actuarial notation for this niche case. In practice, most questions will provide a value of \\(b\\) such that \\((i=\\frac{1+i}{1+b}-1)\\) simplifies to a nice percentage . However, this usually means that the value of \\(b\\) provided is some complicated number, so do not be taken aback!","title":"Geometric Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#geometric-annuity","text":"Let \\(\\text{EPV Geom WL}_\\text{Due}\\) be the random variable denoting the PV of a Geometric Annuity Due. Thus, the EPV of a Geometric Annuity Due can be calculated as: \\[ \\begin{aligned} \\text{EPV Geom WL}_\\text{Due} &= \\sum^{\\infty}_{k=0} (1+c)^k v^k {}_{k}p_x \\\\ &= \\sum^{\\infty}_{k=0} \\left(\\frac{1+c}{1+i}\\right)^k {}_{k}p_x \\\\ &= \\ddot{a}_x|_{i = \\frac{1+i}{1+c}-1} \\end{aligned} \\]","title":"Geometric Annuity"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#continuous-geometric","text":"The EPV of continuously payable geometric contracts can be calculated using the usual UDD, Claims Acceleration or Woolhouse Approximations. However, note that the \"new\" interest rate must be used in the approximations, similar to how the new interest rate is used for approximating the second moments.","title":"Continuous Geometric"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#arithmetic-contracts","text":"If the benefits change by a fixed constant each period, then it is instead known as an Arithmetic contract. For simplicity, the change each period is assumed to be 1: Arithmetically Increasing - Increases by 1 each period Arithmetically Decreasing - Decreased by 1 each period Since a WL contract lasts forever, the benefits of a decreasing WL contract would inevitably become negative , which do not make sense. Thus, WLs can only be arithmetically increasing contracts . \\[ \\text{Arithmetically Increasing Benefit} = K_x + 1 \\]","title":"Arithmetic Contracts"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#arithmetic-assurance","text":"Let \\(\\text{Arith WL}\\) be the random variable denoting the PV of an arithmetically increasing WL Assurance: \\[ \\text{Arith WL} = (K_x + 1) v^{K_x+1} \\] Thus, the EPV of a WL Arithmetic Assurance can be shown to be: \\[ \\begin{aligned} \\text{EPV Arith WL} &= \\sum^{\\infty}_{k=0} (k+1) v^{k+1} {}_{k|}q_x \\\\ (IA)_x &= \\sum^{\\infty}_{k=0} (k+1) v^{k+1} {}_{k|}q_x \\end{aligned} \\] \\((IA)_x\\) is the actuarial notation for the EPV of a contract with arithmetically increasing benefits starting at 1 and increasing by 1 each period. Intuitively, it can also be shown to be a sum of deferred WLs , each deferred by one period from the previous. This creates a step-like function which creates the increasing effect of the assurance: \\[ \\begin{aligned} (IA)_x &= {}_{0|}A_x + {}_{1|}A_x + {}_{2|}A_x + ... \\\\ &= \\sum^{\\infty}_{K_x = 0} {}_{K_x|}A_x \\end{aligned} \\] In practice, the magnitude of the change is unlikely to be one. Thus, the EPV must be scaled to match the actual change. This can be problematic if the starting benefit is NOT the same as the change . For instance, consider a contract with starting benefit of 100k which increases by 1k each period. Since the change is 1000, the EPV must be scaled accordingly, resulting in \\(1000(IA)_x\\) . However, this is the EPV for a contract with a starting benefit of 1000 which increases by 1000 each period. Thus, there must be an additional term to add the remaining 99k back inside: \\[ \\text{EPV 100k increasing by 1k} = 99k \\cdot A_x + 1k \\cdot (IA)_x \\]","title":"Arithmetic Assurance"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#arithmetic-annuities","text":"Let \\(\\text{Arith WL}_\\text{Due}\\) be the random variable denoting the PV of an arithmetically increasing annuity. The PV can be denoted by an arithmetically increasing annuity certain : \\[ \\begin{aligned} \\text{Arith WL}_\\text{Due} &= (I\\ddot{a})_{\\enclose{actuarial}{n}} \\\\ &= \\sum^{n-1}_{k=0} v^k \\cdot \\ddot{a}_{\\enclose{actuarial}{n-k}} \\\\ &= \\sum^{n-1}_{k=0} v^k \\cdot \\frac{1-v^{n-k}}{d} \\\\ &= \\sum^{n-1}_{k=0} \\frac{v^k - v^n}{d} \\\\ &= \\frac{1}{d} \\left(\\sum^{n-1}_{k=0} v^k - \\sum^{n-1}_{k=0} v^n \\right) \\\\ &= \\frac{1}{d} (\\ddot{a}_{\\enclose{actuarial}{n}} - nv^n) \\\\ &= \\frac{\\ddot{a}_{\\enclose{actuarial}{n}} - nv^n}{d} \\end{aligned} \\] Thus, the EPV of an arithmetically increasing WL annuity Due can be calculated as: \\[ \\begin{aligned} \\text{EPV Arith WL}_\\text{Due} &= \\sum^{\\infty}_{k=0} (I\\ddot{a})_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_x \\\\ (I\\ddot{a})_x &= \\sum^{\\infty}_{k=0} (k+1) v^k {}_{k}p_x \\\\ \\end{aligned} \\] Alternatively, it can be calculated from an assurance instead: \\[ \\begin{aligned} \\text{Arith WL}_\\text{Due} &= \\frac{\\ddot{a}_{\\enclose{actuarial}{k+1}} - (k+1)v^{k+1}}{d} \\\\ E(\\text{Arith WL}_\\text{Due}) &= \\frac{E(\\ddot{a}_{\\enclose{actuarial}{k+1}}) - E((k+1)v^{k+1})}{d} \\\\ (I\\ddot{a})_x &= \\frac{\\ddot{a}_x - (IA)_x}{d} \\end{aligned} \\] Notice that the main difference between an increasing annuity due and immediate is that the immediate case now lags the due by 1 every period . This difference can be accounted for using a level annuity due: \\[ \\begin{aligned} (I\\ddot{a})_x &= \\sum^{\\infty}_{k=0} (k+1) v^k {}_{k}p_{x} \\\\ &= v^{0} {}_{0}p_{x} + 2v^{1} {}_{1}p_{x} + 3v^{2} {}_{2}p_{x} + \\dots \\\\ &= (v^{1} {}_{1}p_{x} + 2v^{2} {}_{2}p_{x} + \\dots) + (v^{0} {}_{0}p_{x} + v^{1} {}_{1}p_{x} + v^{2} {}_{2}p_{x} + \\dots) \\\\ &= (Ia)_x + \\ddot{a}_x \\\\ (Ia)_x &= (I\\ddot{a})_x - \\ddot{a}_x \\end{aligned} \\]","title":"Arithmetic Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#arithmetic-termtemporary","text":"For contract with level benefits , term/temporary contracts can be expressed as a difference of two WLs issued at different times because the benefits for the time after the specified term would cancel out . For contracts with variable benefits, this is slightly more complicated as the benefits after the specified term do NOT cancel out : The \"earlier\" WL would have increased significantly from its starting value The \"later\" WL would only be at its starting value Thus, an additional expression needs to be subtracted in order to remove the remaining benefit past the specified term. This can be done using a level benefit contract with the a benefit equal to the remaining amount . Thus, the EPV of an Increasing Term Assurance can be calculated as the following: \\[ \\begin{aligned} \\text{EPV Arith TA} &= \\sum^{n-1}_{K_x=0} (k+1) v^{K_x+1} {}_{k|}q_x \\\\ (IA)^1_{x:\\enclose{actuarial}{n}} &= (IA)_x - {}_{n}E_x [(IA)_{x+n} + nA_{x+n}] \\end{aligned} \\] Thus, the EPV of an Increasing Term Annuity can be calculated as the following: \\[ \\begin{aligned} \\text{EPV Arith TA}_\\text{Due} &= \\sum^{n-1}_{k=0} (I\\ddot{a})_{\\enclose{actuarial}{n}} \\cdot {}_{k|}q_x \\\\ (I\\ddot{a})_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{k=0} (k+1) v^k {}_{k}p_x \\\\ &= (I\\ddot{a})_x - {}_{n}E_x[(I\\ddot{a})_{x+n}+n\\ddot{a}_{x+n}] \\end{aligned} \\]","title":"Arithmetic Term/Temporary"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#decreasing-benefits","text":"As mentioned previously, another key feature of term/temporary contracts is that they can have decreasing benefits . This is because the benefit of the policy can be set such that it would exactly decrease to 0 by the end of the policy term. This is done by setting a starting benefit of \\(n\\) and decreasing by \\(1\\) each period : \\[ \\text{Arithmetically Decreasing Benefit} = n-k \\] Thus, the EPV of a Decreasing Term Assurance can be calculated as the following: \\[ \\begin{aligned} \\text{EPV Arith Decreasing TA} &= \\sum^{\\infty}_{k=0} (n-k) v^{k+1} {}_{k|}q_x \\\\ (D_{\\enclose{actuarial}{n}}A)_{x:\\enclose{actuarial}{n}} &= \\sum^{\\infty}_{k=0} (n-k+1-1) v^{k+1} {}_{k|}q_x \\\\ &= \\sum^{\\infty}_{k=0} (n+1) v^{k+1} {}_{k|}q_x - \\sum^{\\infty}_{k=0} (k+1) v^{k+1} {}_{k|}q_x \\\\ &= (n+1)A^1_{x:\\enclose{actuarial}{n}} - (IA)^1_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] \\((DA)\\) is the actuarial notation for an Assurance with Arithmetically decreasing benefits. Thus, the EPV of a decreasing temporary annuity can be calculated as the following:","title":"Decreasing Benefits"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#continuous-arithmetic","text":"Similar to Geometric Contracts, the EPV of a continuously payable Arithmetic Contracts can be calculated using the usual approximations. However, continuous arithmetic contracts could refer to a contract that not only payable continuously, but changes continuously as well. The benefits change at a constant rate throughout the period , such that the total change is still equal to 1. This can also be approximated from the discrete case, but the intuition is best understood graphically : Thus, for both Assurances and Annuities, the EPV can be calculated as: \\[ \\begin{aligned} (\\bar{I}\\bar{A})_{x} &= (I\\bar{A})_{x} - \\frac{1}{2} \\bar{A}_x \\\\ (\\bar{I}\\bar{a}) &= (I\\bar{a}) - \\frac{1}{2} \\bar{a}_x \\end{aligned} \\]","title":"Continuous Arithmetic"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#variable-recursions","text":"Since the benefit payable at the end of the first year is still 1 , the recursion for variable contracts are similar to the level ones.","title":"Variable Recursions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#variable-assurances","text":"Consider the two scenarios: If the policyholder dies in the current year with probability \\(q_x\\) , then a benefit of 1 is paid at the end of the year If the policyholder lives past the current year with probability \\(p_x\\) , then the they will receive benefits that are larger than 1 at some future time For the second half, the term multiplied must be the EPV of an assurance that STARTS with an increased benefit of \\((1+b)\\) or \\(2\\) . The issue is that the random variable is currently defined as a contract with benefits starting at 1 , which is why the recursive formula is different from the level case. For Geometric Assurances, \\(A_{x+1}|_{i=\\frac{1+i}{1+b}-1}\\) is already an Geometric Assurance that starts at \\((1+b)\\) , thus the recursive formula is simply: \\[ \\text{EPV Geom WL}_{x} = vq_x + vp_x A_{x+1}|_{i=\\frac{1+i}{1+b}-1} \\] For Arithmetic Assurances, an EPV representing an additional benefit of 1 each period must be added that the expression starts from 2 and increases by an additional 1 each period. \\[ (IA)_{x} = vq_x + vp_x[(IA)_{x+1} + A_{x+1}] \\]","title":"Variable Assurances"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#variable-annuities","text":"Recall that a benefit of 1 is received at the beginning of the year regardless of whether the policyholder lives or dies. If the policyholder dies with probability \\(q_x\\) , then they receive nothing extra If the policyholder lives past the current year with probability \\(p_x\\) , then they will receive benefits that are larger than 1 at some future time For Geometric Annuities, TBC. \\[ \\begin{aligned} \\text{EPV Geom WL}_\\text{Due}_{x} = 1 + vp_x \\cdot (1+c) \\left(\\ddot{a}_{x+1}|_{i = \\frac{1+i}{1+c}-1}\\right) \\end{aligned} \\] For Arithmetic Annuities, the same adjustment must be made: \\[ (I\\ddot{a})_x = 1 + vp_x[(I\\ddot{a})_{x+1} + \\ddot{a}_{x+1}] \\] The same can be shown for Variable Term/Temporary Assurances/Annuities, but remember that the remaining duration of the policy must decrease as well.","title":"Variable Annuities"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/5.%20Variable%20Benefits/#variance-and-second-moment","text":"Consider the second moment of both types of variable assurances: \\[ \\begin{aligned} E\\left[(\\text{Arith WL})^2\\right] &= \\sum^{\\infty}_{K_x=0} (k+1)^2 (v^2)^{K_x+1} {}_{k|}q_x \\\\ \\\\ E\\left[(\\text{Geom WL})^2\\right] &= \\sum^{\\infty}_{K_x=0} (1+b)^{2K_x} (v^2)^{K_x+1} {}_{k|}q_x \\end{aligned} \\] Notice that the usual approximation of \\(v=v^2\\) is insufficient to solve for the second moment due to the squared benefit. Thus, there are no \\({}^{2}A\\) defined for variable assurances.","title":"Variance and Second Moment"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Premiums/","text":"Premiums Unlike other products, the cost of a Life Assurance or Annuity are not known when it is issued, as it is impossible to precisely predict when an individual will die or how long they will live for. Premiums are the amount that the insurance company charges for a Life Assurance or Annuity. They aim to charge a sufficiently high premium such that they expect to at least break even on the sale of the policy on an EPV basis . Premiums are always paid in advance , but the frequency of payment depends on the type of premium: Single Premium Limited Premium Regular Premium Single Lump Sum Recurring for a fixed period Recurring as long as the contract is valid Use dollar amount Modelled using TA Annuities Modelled using WL Annuities If premiums are paid in arrears instead, the policyholder may refuse to pay premiums on the flawed grounds that since they did not die in the period, they did not utilize the coverage and hence should not pay. There are two types of premiums that can be calculated: Net Premiums, \\(P\\) - Excluding expenses; based on benefits only Gross Premiums, \\(G\\) - Including expenses; based on BOTH benefits, expenses and other cashflows The key thing to notice is that the components of net premiums are fixed while gross premiums differ from insurer to insurer. Thus, this section will only show formulas for net premiums . Equivalence Principle Approach To determine the breakeven point of the insurer, the expected amount of loss must first be defined. Thus, let \\(L_x\\) be the random variable denoting the loss of the policy: If \\(L_x\\) is positive, then the policy is losing money If \\(L_x\\) is negative, then the policy is making money The intepretation is reversed since \\(L\\) represents an outflow rather than an inflow! \\[ \\begin{aligned} L_x &= \\text{PV Outflow} - \\text{PV Inflows} \\end{aligned} \\] The goal is to solve for the premiums such that there is no expected loss . This is known as the Equivalence Principle approach as it sets the EPVs to be equal: \\[ \\begin{aligned} E(L_x) &= 0 \\\\ \\text{EPV Outflow} &= \\text{EPV Inflow} \\end{aligned} \\] The loss variable for Net and Gross can be differentiated using superscripts: Net Premium : \\(L^{B}_{x}\\) , where \\(B\\) represents the B enefits only Gross Premium : \\(L^{B+E}_{x}\\) , where \\(B+E\\) represents B enefits and E xpenses For Net Premiums only, \\[ \\begin{aligned} E \\left(L^{B}_{x} \\right) &= 0 \\\\ \\text{EPV Benefits} &= \\text{EPV Premiums} \\\\ B \\cdot A_x &= P \\cdot \\ddot{a}_x \\\\ P &= B \\cdot \\frac{A_x}{\\ddot{a}_x} \\end{aligned} \\] Portfolio Percentile Approach If there are a large number of policies within the insurer's portfolio, then the insurer can consider the breakeven on a portfolio of policies rather than a single policy. Let \\(Y\\) be the random variable denoting the aggregate loss on a portfolio of \\(n\\) iid policies: \\[ \\begin{aligned} Y & \\sim iid \\\\ E(Y) &= n \\cdot E(L) \\\\ Var(Y) &= n \\cdot Var(L) \\\\ \\end{aligned} \\] If the insurer has a large portfolio of policies ( \\(n\\) is sufficiently large), then through Central Limit Theorem , \\[ Y \\sim N(E(Y), Var(Y)) \\] The premium is then set such the probability of profit is set a pre-determined level close to 1: \\[ \\begin{aligned} P(Y < 0) &= \\alpha \\\\ P\\left(\\frac{Y - E(Y)}{\\sqrt{Var (Y)}} \\le \\frac{0 - E(Y)}{\\sqrt{Var (Y)}}\\right) &= \\alpha \\\\ P\\left(Z \\le \\frac{- E(Y)}{\\sqrt{Var (Y)}}\\right) &= \\alpha \\\\ P\\left(Z \\le \\frac{- n \\cdot E(L)}{n \\cdot \\sqrt{Var (L)}}\\right) &= \\alpha \\\\ P\\left(Z \\le \\frac{- E(L)}{\\sqrt{Var (L)}}\\right) &= \\alpha \\end{aligned} \\] Thus, only the expectation and variance of an individual policy is required. For net premiums only, \\[ \\begin{aligned} L_x &= B \\cdot \\text{WL} - P \\cdot \\text{WL}_{Due} \\\\ &= B \\cdot \\text{WL} - P \\cdot \\frac{1- \\text{WL}}{d} \\\\ &= B \\cdot \\text{WL} - P \\left(\\frac{1}{d}-\\frac{\\text{WL}}{d} \\right) \\\\ &= B \\cdot \\text{WL} - P \\left[\\left(\\frac{1}{d}\\right) + P\\left(\\frac{\\text{WL}}{d}\\right) \\right] \\\\ &= \\text{WL} \\left(B + \\frac{P}{d} \\right) - \\frac{P}{d} \\\\ \\\\ \\therefore E(L_x) &= E \\left[\\text{WL} \\left(B + \\frac{P}{d} \\right) - \\frac{P}{d} \\right] \\\\ &= A_x \\left(B + \\frac{P}{d} \\right) - \\frac{P}{d} \\\\ \\\\ \\therefore Var(L_x) &= Var\\left[\\text{WL} \\left(B + \\frac{P}{d} \\right) - \\frac{P}{d} \\right] \\\\ &= Var \\left[\\text{WL} \\left(B + \\frac{P}{d} \\right) \\right] \\\\ &= \\left(B + \\frac{P}{d} \\right)^2 \\cdot Var(\\text{WL}) \\\\ &= \\left(B + \\frac{P}{d} \\right)^2 \\cdot \\left[{}^{2}A_x - (A_x)^2 \\right] \\end{aligned} \\] The premiums determined this way will always be larger than the premiums determined through the equivalence principle: \\[ \\begin{aligned} P_{\\text{Portfolio Percentile}} &\\ge P_{\\text{Equivalence Principle}} \\\\ \\\\ \\text{As n} &\\to \\infty \\\\ P_{\\text{Portfolio Percentile}} &\\to P_{\\text{Equivalence Principle}} \\end{aligned} \\] Gross Premiums What makes gross premiums so much trickier than net premiums is that there are many different types of expenses , where every question can ask for a unique combination of them. Expenses can be differentiated in terms of when they are charged: Acquisition Expenses Renewal Expenses Termination Expense At policy inception During policy lifetime At policy termination Single payment at time 0 Recurring payments from time 1 onwards Single payment at unknown time No transformation needed Corresponding Annuity Functions Corresponding Assurance Function EG. Commissions expenses EG. Administrative Expenses EG. Claims expenses Renewal Expenses can also inflate over time, where they increase by a factpr of \\((1+r)\\) each year. In this case, they are modelled using a geometrically increasing annuity instead. IMPORTANT : Acquisition expenses are usually higher than renewal ones as it is costly to set up a policy. They can also be differentiated in terms of how much they charge: Overhead Expenses Direct Expenses Shared among all policies Borne by a specific policy Fixed amount Percentage of policy premium or benefit Spread out among all policies Borne only by that policy EG. Office Rental EG. Sales Commission or Underwriting Expenses Note that gross premiums can also include other cashflows, such as a pre-determined profit margin . Since these are rare and highly dependent on the situation, they will not be covered in this section. Expense Calculations Consider a policy that only has acquisition and renewal expenses , denoted by A and R respectively: The PV of the expenses can be easily calculated by using an annuity immediate: \\[ \\text{EPV Expenses} = A + R \\cdot a_x \\] While this is not challenging per se, it is slightly inconvenient as there is a need to use an annuity immediate while premiums use annuity dues - this would make it harder to simplify or more computationally intensive. This can be worked around by splitting the acquisition expense , such that one part of it forms the first payment for the renewal expenses, allowing an annuity due to be used instead: \\[ \\begin{aligned} \\text{EPV Expenses} &= A + R \\cdot a_x \\\\ &= A - R + R + R \\cdot a_x \\\\ &= A - R + R \\cdot \\ddot{a}_x \\\\ &= A + R \\cdot (\\ddot{a}_x - 1) \\end{aligned} \\] However, note that the simplification does NOT hold true if expense inflation is taken into account - the level annuity immediate CANNOT simply be replaced with the geometric annuity. This is because the expenses only start to inflate from the third payment onwards: New Business Strain If only expenses are considered for premium calculations (ignoring benefits), then the resulting premium is known as the Expense Premium , \\(E\\) . If the gross premiums are calculated using only benefits and expenses (no profit), then the expense premium is the difference between the gross and net premium: \\[ \\begin{aligned} G &= P + E \\\\ E &= G - P \\end{aligned} \\] The expense premium is always smaller than the acquisition expenses . The insurer does not collect enough premiums to set-up the policy, thus makes an initial loss on the policy, known as the New Business Strain . However, the expense premium is always higher than the renewal expenses , reflecting that the insurer recovers the loss over time .","title":"Premiums"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Premiums/#premiums","text":"Unlike other products, the cost of a Life Assurance or Annuity are not known when it is issued, as it is impossible to precisely predict when an individual will die or how long they will live for. Premiums are the amount that the insurance company charges for a Life Assurance or Annuity. They aim to charge a sufficiently high premium such that they expect to at least break even on the sale of the policy on an EPV basis . Premiums are always paid in advance , but the frequency of payment depends on the type of premium: Single Premium Limited Premium Regular Premium Single Lump Sum Recurring for a fixed period Recurring as long as the contract is valid Use dollar amount Modelled using TA Annuities Modelled using WL Annuities If premiums are paid in arrears instead, the policyholder may refuse to pay premiums on the flawed grounds that since they did not die in the period, they did not utilize the coverage and hence should not pay. There are two types of premiums that can be calculated: Net Premiums, \\(P\\) - Excluding expenses; based on benefits only Gross Premiums, \\(G\\) - Including expenses; based on BOTH benefits, expenses and other cashflows The key thing to notice is that the components of net premiums are fixed while gross premiums differ from insurer to insurer. Thus, this section will only show formulas for net premiums .","title":"Premiums"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Premiums/#equivalence-principle-approach","text":"To determine the breakeven point of the insurer, the expected amount of loss must first be defined. Thus, let \\(L_x\\) be the random variable denoting the loss of the policy: If \\(L_x\\) is positive, then the policy is losing money If \\(L_x\\) is negative, then the policy is making money The intepretation is reversed since \\(L\\) represents an outflow rather than an inflow! \\[ \\begin{aligned} L_x &= \\text{PV Outflow} - \\text{PV Inflows} \\end{aligned} \\] The goal is to solve for the premiums such that there is no expected loss . This is known as the Equivalence Principle approach as it sets the EPVs to be equal: \\[ \\begin{aligned} E(L_x) &= 0 \\\\ \\text{EPV Outflow} &= \\text{EPV Inflow} \\end{aligned} \\] The loss variable for Net and Gross can be differentiated using superscripts: Net Premium : \\(L^{B}_{x}\\) , where \\(B\\) represents the B enefits only Gross Premium : \\(L^{B+E}_{x}\\) , where \\(B+E\\) represents B enefits and E xpenses For Net Premiums only, \\[ \\begin{aligned} E \\left(L^{B}_{x} \\right) &= 0 \\\\ \\text{EPV Benefits} &= \\text{EPV Premiums} \\\\ B \\cdot A_x &= P \\cdot \\ddot{a}_x \\\\ P &= B \\cdot \\frac{A_x}{\\ddot{a}_x} \\end{aligned} \\]","title":"Equivalence Principle Approach"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Premiums/#portfolio-percentile-approach","text":"If there are a large number of policies within the insurer's portfolio, then the insurer can consider the breakeven on a portfolio of policies rather than a single policy. Let \\(Y\\) be the random variable denoting the aggregate loss on a portfolio of \\(n\\) iid policies: \\[ \\begin{aligned} Y & \\sim iid \\\\ E(Y) &= n \\cdot E(L) \\\\ Var(Y) &= n \\cdot Var(L) \\\\ \\end{aligned} \\] If the insurer has a large portfolio of policies ( \\(n\\) is sufficiently large), then through Central Limit Theorem , \\[ Y \\sim N(E(Y), Var(Y)) \\] The premium is then set such the probability of profit is set a pre-determined level close to 1: \\[ \\begin{aligned} P(Y < 0) &= \\alpha \\\\ P\\left(\\frac{Y - E(Y)}{\\sqrt{Var (Y)}} \\le \\frac{0 - E(Y)}{\\sqrt{Var (Y)}}\\right) &= \\alpha \\\\ P\\left(Z \\le \\frac{- E(Y)}{\\sqrt{Var (Y)}}\\right) &= \\alpha \\\\ P\\left(Z \\le \\frac{- n \\cdot E(L)}{n \\cdot \\sqrt{Var (L)}}\\right) &= \\alpha \\\\ P\\left(Z \\le \\frac{- E(L)}{\\sqrt{Var (L)}}\\right) &= \\alpha \\end{aligned} \\] Thus, only the expectation and variance of an individual policy is required. For net premiums only, \\[ \\begin{aligned} L_x &= B \\cdot \\text{WL} - P \\cdot \\text{WL}_{Due} \\\\ &= B \\cdot \\text{WL} - P \\cdot \\frac{1- \\text{WL}}{d} \\\\ &= B \\cdot \\text{WL} - P \\left(\\frac{1}{d}-\\frac{\\text{WL}}{d} \\right) \\\\ &= B \\cdot \\text{WL} - P \\left[\\left(\\frac{1}{d}\\right) + P\\left(\\frac{\\text{WL}}{d}\\right) \\right] \\\\ &= \\text{WL} \\left(B + \\frac{P}{d} \\right) - \\frac{P}{d} \\\\ \\\\ \\therefore E(L_x) &= E \\left[\\text{WL} \\left(B + \\frac{P}{d} \\right) - \\frac{P}{d} \\right] \\\\ &= A_x \\left(B + \\frac{P}{d} \\right) - \\frac{P}{d} \\\\ \\\\ \\therefore Var(L_x) &= Var\\left[\\text{WL} \\left(B + \\frac{P}{d} \\right) - \\frac{P}{d} \\right] \\\\ &= Var \\left[\\text{WL} \\left(B + \\frac{P}{d} \\right) \\right] \\\\ &= \\left(B + \\frac{P}{d} \\right)^2 \\cdot Var(\\text{WL}) \\\\ &= \\left(B + \\frac{P}{d} \\right)^2 \\cdot \\left[{}^{2}A_x - (A_x)^2 \\right] \\end{aligned} \\] The premiums determined this way will always be larger than the premiums determined through the equivalence principle: \\[ \\begin{aligned} P_{\\text{Portfolio Percentile}} &\\ge P_{\\text{Equivalence Principle}} \\\\ \\\\ \\text{As n} &\\to \\infty \\\\ P_{\\text{Portfolio Percentile}} &\\to P_{\\text{Equivalence Principle}} \\end{aligned} \\]","title":"Portfolio Percentile Approach"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Premiums/#gross-premiums","text":"What makes gross premiums so much trickier than net premiums is that there are many different types of expenses , where every question can ask for a unique combination of them. Expenses can be differentiated in terms of when they are charged: Acquisition Expenses Renewal Expenses Termination Expense At policy inception During policy lifetime At policy termination Single payment at time 0 Recurring payments from time 1 onwards Single payment at unknown time No transformation needed Corresponding Annuity Functions Corresponding Assurance Function EG. Commissions expenses EG. Administrative Expenses EG. Claims expenses Renewal Expenses can also inflate over time, where they increase by a factpr of \\((1+r)\\) each year. In this case, they are modelled using a geometrically increasing annuity instead. IMPORTANT : Acquisition expenses are usually higher than renewal ones as it is costly to set up a policy. They can also be differentiated in terms of how much they charge: Overhead Expenses Direct Expenses Shared among all policies Borne by a specific policy Fixed amount Percentage of policy premium or benefit Spread out among all policies Borne only by that policy EG. Office Rental EG. Sales Commission or Underwriting Expenses Note that gross premiums can also include other cashflows, such as a pre-determined profit margin . Since these are rare and highly dependent on the situation, they will not be covered in this section.","title":"Gross Premiums"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Premiums/#expense-calculations","text":"Consider a policy that only has acquisition and renewal expenses , denoted by A and R respectively: The PV of the expenses can be easily calculated by using an annuity immediate: \\[ \\text{EPV Expenses} = A + R \\cdot a_x \\] While this is not challenging per se, it is slightly inconvenient as there is a need to use an annuity immediate while premiums use annuity dues - this would make it harder to simplify or more computationally intensive. This can be worked around by splitting the acquisition expense , such that one part of it forms the first payment for the renewal expenses, allowing an annuity due to be used instead: \\[ \\begin{aligned} \\text{EPV Expenses} &= A + R \\cdot a_x \\\\ &= A - R + R + R \\cdot a_x \\\\ &= A - R + R \\cdot \\ddot{a}_x \\\\ &= A + R \\cdot (\\ddot{a}_x - 1) \\end{aligned} \\] However, note that the simplification does NOT hold true if expense inflation is taken into account - the level annuity immediate CANNOT simply be replaced with the geometric annuity. This is because the expenses only start to inflate from the third payment onwards:","title":"Expense Calculations"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/6.%20Premiums/#new-business-strain","text":"If only expenses are considered for premium calculations (ignoring benefits), then the resulting premium is known as the Expense Premium , \\(E\\) . If the gross premiums are calculated using only benefits and expenses (no profit), then the expense premium is the difference between the gross and net premium: \\[ \\begin{aligned} G &= P + E \\\\ E &= G - P \\end{aligned} \\] The expense premium is always smaller than the acquisition expenses . The insurer does not collect enough premiums to set-up the policy, thus makes an initial loss on the policy, known as the New Business Strain . However, the expense premium is always higher than the renewal expenses , reflecting that the insurer recovers the loss over time .","title":"New Business Strain"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/","text":"Reserves Most life insurance contracts charge level premiums - charging the same amount throughout the lifetime of the policy. When the policyholder is young, the probability of death is small, thus the premiums collected are larger than the expected outflow in the year, leading to a surplus . When the policyholder is older, the probability of death is high, thus the premiums collected are smaller than the expected outflow in the year, leading to a deficit . Given the inevitable deficit, the surplus in the earlier years cannot be recognized as a profit and are instead pooled together and safely invested into an account, from which the deficit in later years will draw from. It is a common misconception to think that each policy has its own account. Following this logic, the insurer would not have enough funds to pay an insured who dies shortly after death. In order to determine the adequacy of the account, insurers will calculate the size of the expected loss of ALL policies and compare it against the (hopefully larger) account value. The expected loss is known as the Reserve of the policy because it represents an amount inside the pooled account that is reserved for paying the benefits of the policy. Retrospective Approach There are two approaches to calculate the reserves: Prospective Approach - Based on Present Values Retrospective Approach - Based on Future Values (Not covered) Similar to premiums, there are also two types of reserves, depending on what cashflows are being considered: Net Premium Reserves - Net Premiums and Benefits Gross Premium Reserves - Gross Premiums, Benefits and Expenses While this looks similar to premium calculation, note that the Gross Reserves do NOT include any other cashflows (EG. Profit Margin). This is intuitive, as reserves should only be to meet essential cashflows . Let \\({}_{t}V\\) represent the Reserve in policy year \\(t\\) : \\[ \\begin{aligned} {}_{t}V &= E(L_{x+t}) \\\\ &= \\text{PV Benefits}_{x+t} - \\text{PV Premiums}_{x+t} \\end{aligned} \\] Similar to premiums, we will only define formulas for the Net Premium Reserves . They can be simplified into an expression involving only the following, which allows it to be easily calculated in situations where limited information is provided: Annuities only Assurances only Annuities and Premiums Assurance and Premiums Case 1: Annuities Only \\[ \\begin{aligned} {}_{t}V &= A_{x+t} - P \\ddot{a}_{x+t} \\\\ &= A_{x+t} - \\frac{A_x}{\\ddot{a}_x} \\cdot \\ddot{a}_{x+t} \\\\ &= (1-\\delta\\ddot{a}_{x+t}) - \\frac{1-\\delta\\ddot{a}_{x}}{\\ddot{a}_{x}} \\cdot \\ddot{a}_{x+t} \\\\ &= 1-\\delta\\ddot{a}_{x+t} - \\left (\\frac{1}{\\ddot{a}_x}-\\delta \\right) \\cdot \\ddot{a}_{x+t} \\\\ &= 1-\\delta\\ddot{a}_{x+t} - \\frac{\\ddot{a}_{x+t}}{\\ddot{a}_x} + \\delta\\ddot{a}_{x+t} \\\\ &= 1 - \\frac{\\ddot{a}_{x+t}}{\\ddot{a}_x} \\end{aligned} \\] Case 2: Assurances Only \\[ \\begin{aligned} {}_{t}V &= A_{x+t} - P \\ddot{a}_{x+t} \\\\ &= A_{x+t} - \\frac{A_x}{\\ddot{a}_x} \\cdot \\ddot{a}_{x+t} \\\\ &= A_{x+t} - \\frac{A_x}{\\frac{1-A_x}{d}} \\cdot \\frac{1-A_{x+t}}{d} \\\\ &= A_{x+t} - \\frac{A_x}{1-A_x} \\cdot (1-A_{x+t}) \\\\ &= \\frac{A_{x+t} \\cdot (1-A_{x}) - A_x(1-A_{x+t})}{1-A_x} \\\\ &= \\frac{A_{x+t} - A_{x}A_{x+t} - A_x + A_x A_{x+t}}{1-A_x} \\\\ &= \\frac{A_{x+t} - A_{x}}{1 - A_{x}} \\end{aligned} \\] Case 3: Annuities & Premiums \\[ \\begin{aligned} {}_{t}V &= A_{x+t} - P_x \\cdot \\ddot{a}_{x+t} \\\\ &= \\ddot{a}_{x+t} \\cdot \\left(\\frac{A_{x+t}}{\\ddot{a}_{x+t}} - P_{x} \\right) \\\\ &= \\ddot{a}_{x+t} \\cdot (P_{x+t} - P_{x}) \\end{aligned} \\] Case 4: Assurances & Premiums \\[ \\begin{aligned} {}_{t}V &= A_{x+t} - P_x \\cdot \\ddot{a}_{x+t} \\\\ &= A_{x+t} (1 - P_x \\cdot \\frac{\\ddot{a}_{x+t}}{A_{x+t}}) \\\\ &= A_{x+t} (1 - P_x \\cdot \\frac{1}{\\frac{A_{x+t}}{\\ddot{a}_{x+t}}}) \\\\ &= A_{x+t} (1 - \\frac{P_x}{P_{x+t}}) \\end{aligned} \\] Reserves & Premiums The calculation of reserves and premiums both involve the expectation of the random variable \\(L\\) : Premiums Reserves \\(E(L)\\) given as 0 \\(E(L)\\) is the target \\(P\\) is the target \\(P\\) is given Calculated at age \\(x\\) Calculated at age \\(x+t\\) If net premium reserves are calculated at the start of the policy ( \\(t=0\\) ), then following the equivalence principle, the reserves will be 0 . This is intuitive, as the premiums are set such that they expect to exactly cover the losses of the policy. However, this is not the case for gross premiums. This is because gross premiums include provisions for other cashflows like profit margins which are not considered during the reserve calculation. Thus, gross premiums collect more than enough to cover the expected losses, resulting in a negative reserve at time 0 . Technically speaking, not all gross premiums include profit margins. If they do not, then the gross premium reserve will also be 0. \\[ \\begin{aligned} {}_{0}V^{Net} &= 0 \\\\ {}_{0}V^{Gross} &\\le 0 \\end{aligned} \\] Note that a newly issued policy for another individual aged \\(x+t\\) uses the exact same \\(L\\) as the reserve, \\(L_{x+t}\\) . This is why some of the expressions previously involve \\(P_{x+t}\\) , which is the premium of this newly issued policy. Red Herrings Premium and Reserve calculations are similar, so it is easy to get them mixed up with each other. The confusing part is that some questions requires us to compute the premiums ourselves before calculating the reserves. Given how similar the calculations are, this may mistakenly cause us to use the same cashflows for both, when the reserves should be different. Even if the gross premiums are provided for us, information exclusively used to compute gross premiums may still be provided as a red herring to confuse us. Pay special attention to the following cashflows: Acquisition Expenses - Reserves are usually calculated after inception Profit Margin - Reserves do not consider profit margin Premium Frequency - Reserves use annual premiums; monthly premiums may be provided Recursions Following the same logic as assurances and annuities, net premium reserves can also be recursively expressed as a function of itself: If the policyholder dies, the insurer would have to pay a benefit of 1 at the end of the year If the policyholder survives, the insurer would have to keep a reserve for future payments Premiums are received REGARDLESS of death or survival as it was paid at the start of the year \\[ \\begin{aligned} {}_{t}V^{\\text{Net}} &= - P + \\begin{cases} v, & q_{x+t} \\\\ v \\cdot {}_{t+1}V, & p_{x+t} \\end{cases} \\\\ \\\\ \\therefore {}_{t}V^{\\text{Net}} &= - P + q_{x+t} \\cdot v + p_{x+t} \\cdot v \\cdot {}_{t+1}V \\end{aligned} \\] Gross premium reserves are similar as any expenses are always paid at the beginning of the year with premiums: \\[ {}_{t}V^{\\text{Gross}} = (E-G) + q_{x+t} \\cdot v + p_{x+t} \\cdot v \\cdot {}_{t+1}V \\] Full Preliminary Term Reserves The method and basis for calculating the reserve is based on the prevailing regulation at that time and region. Regardless of region, reserves are typically calculated in a conservative manner such that the insurer has enough reserves for situations that are worse than their estimates - higher than what is expected. However, insurers do not want to be too conservative such that they hold too much reserves . This is because reserves can typically only be invested in safe assets, which typically yield lower returns relative to what they would usually invest in. Thus, if too much reserves are held, the insurer incurs an opportunity cost in investment income . In the US, reserves are calculated using Net Premiums due to its simplicity. However, this comes at the cost of accuracy, as net premium reserves tend to be too conservative : \\[ \\begin{aligned} {}_{t}v^{\\text{Gross}} &= {}_{t}v^{\\text{Net}} + {}_{t}v^{\\text{Expense}} \\\\ {}_{t}v^{\\text{Net}} &= {}_{t}v^{\\text{Gross}} - \\underbrace{{}_{t}v^{\\text{Expense}}}_{\\text{-ve}} \\\\ {}_{t}v^{\\text{Net}} &\\gt {}_{t}v^{\\text{Gross}} \\end{aligned} \\] In order to overcome this difference, the net premium approach is modified to better reflect actual experience while still retaining the benefits of using net premiums, known as the Full Preliminary Term Reserves . Negative Expense Reserve Based on the equivalence principle approach, on average , premiums are expected to cover all benefits and expenses. However, acquisition expenses in the first year are often much higher than the renewal expenses, resulting in a positive loss in the first year. Thus, in all subsequent years, there is a negative loss to gradually recover the positive loss made in the first year. This results in a negative reserve at the end of the first year because the loss in all future years are negative. A negative reserve implies that expected inflows are larger than outflows, thus the insurer can treat the policy as an asset and thus hold less capital overall to cover other liabilities. To be conservative, regulators do not allow insurers to recognize a negative reserve. Thus, any negative reserves will be floored at 0 . Modified Approach Assume that the policy has non-level premiums : Acquisition Premium, \\(\\alpha\\) Renewal Premium, \\(\\beta\\) The goal is to calculate a \\(\\beta\\) such that the reserve at time 1 is 0, replicating the result under gross premium: \\[ \\begin{aligned} {}_{1}V^{\\text{FPT}} &= 0 \\\\ A_{x+1} - \\beta \\ddot{a}_{x+1} &= 0 \\\\ \\beta &= \\frac{A_{x+1}}{\\ddot{a}_{x+1}} \\end{aligned} \\] Note that for a TA, the duration of the policy would have to decrease by 1 year as well. Following the equivalence principle, the reserve at time 0 must still be 0, thus \\(\\alpha\\) is calculated as the Single Premium equal to the expected loss at inception: \\[ \\begin{aligned} {}_{0}V^{\\text{FPT}} &= 0 \\\\ \\alpha &= A_{x} \\end{aligned} \\] Thus, the modified reserves at all later times follows the same formula as before, simply using \\(\\beta\\) instead: \\[ {}_{t}V^{\\text{FPT}} = A_{x+t} - \\beta \\ddot{x+t} \\] Note that \\(\\beta\\) can also be thought of as the net premium for a policy that is: Issued one year later; \\(x+1\\) One less year passed; \\(t-1, n-1\\) $$ \\therefore {} {t}V^{\\text{FPT}} = {} V^{\\text{Net}}_{\\text{One year later, one less year}} $$ Jerram, [20/3/2023 4:53 pm] nEGATIVE Expense reserve is known as Dac Jerram, [20/3/2023 4:54 pm] Recover it in the future","title":"Reserves"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#reserves","text":"Most life insurance contracts charge level premiums - charging the same amount throughout the lifetime of the policy. When the policyholder is young, the probability of death is small, thus the premiums collected are larger than the expected outflow in the year, leading to a surplus . When the policyholder is older, the probability of death is high, thus the premiums collected are smaller than the expected outflow in the year, leading to a deficit . Given the inevitable deficit, the surplus in the earlier years cannot be recognized as a profit and are instead pooled together and safely invested into an account, from which the deficit in later years will draw from. It is a common misconception to think that each policy has its own account. Following this logic, the insurer would not have enough funds to pay an insured who dies shortly after death. In order to determine the adequacy of the account, insurers will calculate the size of the expected loss of ALL policies and compare it against the (hopefully larger) account value. The expected loss is known as the Reserve of the policy because it represents an amount inside the pooled account that is reserved for paying the benefits of the policy.","title":"Reserves"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#retrospective-approach","text":"There are two approaches to calculate the reserves: Prospective Approach - Based on Present Values Retrospective Approach - Based on Future Values (Not covered) Similar to premiums, there are also two types of reserves, depending on what cashflows are being considered: Net Premium Reserves - Net Premiums and Benefits Gross Premium Reserves - Gross Premiums, Benefits and Expenses While this looks similar to premium calculation, note that the Gross Reserves do NOT include any other cashflows (EG. Profit Margin). This is intuitive, as reserves should only be to meet essential cashflows . Let \\({}_{t}V\\) represent the Reserve in policy year \\(t\\) : \\[ \\begin{aligned} {}_{t}V &= E(L_{x+t}) \\\\ &= \\text{PV Benefits}_{x+t} - \\text{PV Premiums}_{x+t} \\end{aligned} \\] Similar to premiums, we will only define formulas for the Net Premium Reserves . They can be simplified into an expression involving only the following, which allows it to be easily calculated in situations where limited information is provided: Annuities only Assurances only Annuities and Premiums Assurance and Premiums","title":"Retrospective Approach"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#case-1-annuities-only","text":"\\[ \\begin{aligned} {}_{t}V &= A_{x+t} - P \\ddot{a}_{x+t} \\\\ &= A_{x+t} - \\frac{A_x}{\\ddot{a}_x} \\cdot \\ddot{a}_{x+t} \\\\ &= (1-\\delta\\ddot{a}_{x+t}) - \\frac{1-\\delta\\ddot{a}_{x}}{\\ddot{a}_{x}} \\cdot \\ddot{a}_{x+t} \\\\ &= 1-\\delta\\ddot{a}_{x+t} - \\left (\\frac{1}{\\ddot{a}_x}-\\delta \\right) \\cdot \\ddot{a}_{x+t} \\\\ &= 1-\\delta\\ddot{a}_{x+t} - \\frac{\\ddot{a}_{x+t}}{\\ddot{a}_x} + \\delta\\ddot{a}_{x+t} \\\\ &= 1 - \\frac{\\ddot{a}_{x+t}}{\\ddot{a}_x} \\end{aligned} \\]","title":"Case 1: Annuities Only"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#case-2-assurances-only","text":"\\[ \\begin{aligned} {}_{t}V &= A_{x+t} - P \\ddot{a}_{x+t} \\\\ &= A_{x+t} - \\frac{A_x}{\\ddot{a}_x} \\cdot \\ddot{a}_{x+t} \\\\ &= A_{x+t} - \\frac{A_x}{\\frac{1-A_x}{d}} \\cdot \\frac{1-A_{x+t}}{d} \\\\ &= A_{x+t} - \\frac{A_x}{1-A_x} \\cdot (1-A_{x+t}) \\\\ &= \\frac{A_{x+t} \\cdot (1-A_{x}) - A_x(1-A_{x+t})}{1-A_x} \\\\ &= \\frac{A_{x+t} - A_{x}A_{x+t} - A_x + A_x A_{x+t}}{1-A_x} \\\\ &= \\frac{A_{x+t} - A_{x}}{1 - A_{x}} \\end{aligned} \\]","title":"Case 2: Assurances Only"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#case-3-annuities-premiums","text":"\\[ \\begin{aligned} {}_{t}V &= A_{x+t} - P_x \\cdot \\ddot{a}_{x+t} \\\\ &= \\ddot{a}_{x+t} \\cdot \\left(\\frac{A_{x+t}}{\\ddot{a}_{x+t}} - P_{x} \\right) \\\\ &= \\ddot{a}_{x+t} \\cdot (P_{x+t} - P_{x}) \\end{aligned} \\]","title":"Case 3: Annuities &amp; Premiums"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#case-4-assurances-premiums","text":"\\[ \\begin{aligned} {}_{t}V &= A_{x+t} - P_x \\cdot \\ddot{a}_{x+t} \\\\ &= A_{x+t} (1 - P_x \\cdot \\frac{\\ddot{a}_{x+t}}{A_{x+t}}) \\\\ &= A_{x+t} (1 - P_x \\cdot \\frac{1}{\\frac{A_{x+t}}{\\ddot{a}_{x+t}}}) \\\\ &= A_{x+t} (1 - \\frac{P_x}{P_{x+t}}) \\end{aligned} \\]","title":"Case 4: Assurances &amp; Premiums"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#reserves-premiums","text":"The calculation of reserves and premiums both involve the expectation of the random variable \\(L\\) : Premiums Reserves \\(E(L)\\) given as 0 \\(E(L)\\) is the target \\(P\\) is the target \\(P\\) is given Calculated at age \\(x\\) Calculated at age \\(x+t\\) If net premium reserves are calculated at the start of the policy ( \\(t=0\\) ), then following the equivalence principle, the reserves will be 0 . This is intuitive, as the premiums are set such that they expect to exactly cover the losses of the policy. However, this is not the case for gross premiums. This is because gross premiums include provisions for other cashflows like profit margins which are not considered during the reserve calculation. Thus, gross premiums collect more than enough to cover the expected losses, resulting in a negative reserve at time 0 . Technically speaking, not all gross premiums include profit margins. If they do not, then the gross premium reserve will also be 0. \\[ \\begin{aligned} {}_{0}V^{Net} &= 0 \\\\ {}_{0}V^{Gross} &\\le 0 \\end{aligned} \\] Note that a newly issued policy for another individual aged \\(x+t\\) uses the exact same \\(L\\) as the reserve, \\(L_{x+t}\\) . This is why some of the expressions previously involve \\(P_{x+t}\\) , which is the premium of this newly issued policy.","title":"Reserves &amp; Premiums"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#red-herrings","text":"Premium and Reserve calculations are similar, so it is easy to get them mixed up with each other. The confusing part is that some questions requires us to compute the premiums ourselves before calculating the reserves. Given how similar the calculations are, this may mistakenly cause us to use the same cashflows for both, when the reserves should be different. Even if the gross premiums are provided for us, information exclusively used to compute gross premiums may still be provided as a red herring to confuse us. Pay special attention to the following cashflows: Acquisition Expenses - Reserves are usually calculated after inception Profit Margin - Reserves do not consider profit margin Premium Frequency - Reserves use annual premiums; monthly premiums may be provided","title":"Red Herrings"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#recursions","text":"Following the same logic as assurances and annuities, net premium reserves can also be recursively expressed as a function of itself: If the policyholder dies, the insurer would have to pay a benefit of 1 at the end of the year If the policyholder survives, the insurer would have to keep a reserve for future payments Premiums are received REGARDLESS of death or survival as it was paid at the start of the year \\[ \\begin{aligned} {}_{t}V^{\\text{Net}} &= - P + \\begin{cases} v, & q_{x+t} \\\\ v \\cdot {}_{t+1}V, & p_{x+t} \\end{cases} \\\\ \\\\ \\therefore {}_{t}V^{\\text{Net}} &= - P + q_{x+t} \\cdot v + p_{x+t} \\cdot v \\cdot {}_{t+1}V \\end{aligned} \\] Gross premium reserves are similar as any expenses are always paid at the beginning of the year with premiums: \\[ {}_{t}V^{\\text{Gross}} = (E-G) + q_{x+t} \\cdot v + p_{x+t} \\cdot v \\cdot {}_{t+1}V \\]","title":"Recursions"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#full-preliminary-term-reserves","text":"The method and basis for calculating the reserve is based on the prevailing regulation at that time and region. Regardless of region, reserves are typically calculated in a conservative manner such that the insurer has enough reserves for situations that are worse than their estimates - higher than what is expected. However, insurers do not want to be too conservative such that they hold too much reserves . This is because reserves can typically only be invested in safe assets, which typically yield lower returns relative to what they would usually invest in. Thus, if too much reserves are held, the insurer incurs an opportunity cost in investment income . In the US, reserves are calculated using Net Premiums due to its simplicity. However, this comes at the cost of accuracy, as net premium reserves tend to be too conservative : \\[ \\begin{aligned} {}_{t}v^{\\text{Gross}} &= {}_{t}v^{\\text{Net}} + {}_{t}v^{\\text{Expense}} \\\\ {}_{t}v^{\\text{Net}} &= {}_{t}v^{\\text{Gross}} - \\underbrace{{}_{t}v^{\\text{Expense}}}_{\\text{-ve}} \\\\ {}_{t}v^{\\text{Net}} &\\gt {}_{t}v^{\\text{Gross}} \\end{aligned} \\] In order to overcome this difference, the net premium approach is modified to better reflect actual experience while still retaining the benefits of using net premiums, known as the Full Preliminary Term Reserves .","title":"Full Preliminary Term Reserves"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#negative-expense-reserve","text":"Based on the equivalence principle approach, on average , premiums are expected to cover all benefits and expenses. However, acquisition expenses in the first year are often much higher than the renewal expenses, resulting in a positive loss in the first year. Thus, in all subsequent years, there is a negative loss to gradually recover the positive loss made in the first year. This results in a negative reserve at the end of the first year because the loss in all future years are negative. A negative reserve implies that expected inflows are larger than outflows, thus the insurer can treat the policy as an asset and thus hold less capital overall to cover other liabilities. To be conservative, regulators do not allow insurers to recognize a negative reserve. Thus, any negative reserves will be floored at 0 .","title":"Negative Expense Reserve"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/7.%20Reserves/#modified-approach","text":"Assume that the policy has non-level premiums : Acquisition Premium, \\(\\alpha\\) Renewal Premium, \\(\\beta\\) The goal is to calculate a \\(\\beta\\) such that the reserve at time 1 is 0, replicating the result under gross premium: \\[ \\begin{aligned} {}_{1}V^{\\text{FPT}} &= 0 \\\\ A_{x+1} - \\beta \\ddot{a}_{x+1} &= 0 \\\\ \\beta &= \\frac{A_{x+1}}{\\ddot{a}_{x+1}} \\end{aligned} \\] Note that for a TA, the duration of the policy would have to decrease by 1 year as well. Following the equivalence principle, the reserve at time 0 must still be 0, thus \\(\\alpha\\) is calculated as the Single Premium equal to the expected loss at inception: \\[ \\begin{aligned} {}_{0}V^{\\text{FPT}} &= 0 \\\\ \\alpha &= A_{x} \\end{aligned} \\] Thus, the modified reserves at all later times follows the same formula as before, simply using \\(\\beta\\) instead: \\[ {}_{t}V^{\\text{FPT}} = A_{x+t} - \\beta \\ddot{x+t} \\] Note that \\(\\beta\\) can also be thought of as the net premium for a policy that is: Issued one year later; \\(x+1\\) One less year passed; \\(t-1, n-1\\) $$ \\therefore {} {t}V^{\\text{FPT}} = {} V^{\\text{Net}}_{\\text{One year later, one less year}} $$ Jerram, [20/3/2023 4:53 pm] nEGATIVE Expense reserve is known as Dac Jerram, [20/3/2023 4:54 pm] Recover it in the future","title":"Modified Approach"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/","text":"Model Estimation Complete Data Individual Data Grouped Data Confidence Intervals Non Linear Confidence Interval Incomplete Data Truncation and Censoring Kaplan Meier Estimation Nelson Aelen Estimation Tail Correction Greenwood Approximation Alive Dead Model","title":"Model Estimation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#model-estimation","text":"","title":"Model Estimation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#complete-data","text":"","title":"Complete Data"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#individual-data","text":"","title":"Individual Data"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#grouped-data","text":"","title":"Grouped Data"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#confidence-intervals","text":"Non Linear Confidence Interval","title":"Confidence Intervals"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#incomplete-data","text":"Truncation and Censoring","title":"Incomplete Data"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#kaplan-meier-estimation","text":"","title":"Kaplan Meier Estimation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#nelson-aelen-estimation","text":"","title":"Nelson Aelen Estimation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#tail-correction","text":"","title":"Tail Correction"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#greenwood-approximation","text":"","title":"Greenwood Approximation"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/8.%20Model%20Estimation/#alive-dead-model","text":"","title":"Alive Dead Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/","text":"Linear Regression Population Regression Model Regression is a statistical model that relates a Dependent Variable (DV) to one or more Independent Variables (IV). The dependent variable is regressed on to the independent variable. They are fundamentally a function of the independent variables and several Regression Parameters , \\(\\beta\\) . The functional form of the regression is based on the relationship between the variables. The goal is to use a model that best captures the relationship between the variables. \\[ Y = f(X, \\beta) \\] Independent Variable(s) Dependent Variables Variable used to make predictions Variable being predicted Free to change the value Depends on the value of indepenent variable Deterministic Random Variable Denoted as \\(X\\) Denoted as \\(Y\\) To be precise, for every set of IVs, the DV has a Conditional Distribution dependent on the given IV. For instance, the the \\(Y\\) could take any possible value (Marginal Distribution), but given these set of \\(X\\) , the possible values can be narrowed down to a certain range (conditional distribution). \\[ \\displaylines{ Y \\sim Distribution \\\\ Y|X \\sim Conditional~Distribution} \\] Thus, the output of the regression model is actually the Expected Value of the conditional distribution, \\(E(Y|X)\\) , for every possible \\(X\\) . \\[ E(Y|X) = f(X, \\beta) \\] The actual observations are unlikely to be exactly equal to its expectation, thus there is a difference between an observation and the corresponding regression output. It known as the Random Error Term which accounts for all other factors that affect the DV that are not captured in the regression. This means that the relationship between the \\(Y\\) and \\(X\\) is only approximate , as the true relationship is probably different due to the possibility of unaccounted variables. Note that the sign of the errors are significant - positive implies the actual value lies above the regression output while negative implies it lies below. \\[ \\varepsilon_i = y_i - f(x_i, \\beta) \\] This means that \\(Y\\) (not its expectation!) can be expressed as a sum of the regression model and the error terms: \\[ y_i = f(x_i,\\beta) + \\varepsilon_i \\] The Regression is known as the Systematic component as it is shared among all observations The Error is known as the Non-Systematic component as it is unique to each observation Sample Regression Model In practice, the population is unobservable hence it is impossible to construct the population regression model. Instead, a regression model is constructed from a sample instead, which aims to estimate the population model. \\[ \\hat{y} = f(X,\\hat{\\beta}) \\] Similarly, the output of this model can be compared to the actual observations. However, the resulting difference is known as the Residual of the model, which like all the other components, is an estimate for the Error term. \\[ \\hat{\\varepsilon_i} = y_i - \\hat{y_i} \\] There are several different methods to estimate the regression parameters, but they usually involve minimizing the residuals of the model, such that the resulting model best fits the given sample, which is why it is also known as the Fitted Regression Model . Hypothesis Testing Once a regression model has been fit, the next step is to determine if the relationship found in the sample is indicative of a relationship in the population. This can be determined through the following two-sided hypothesis test : Null Hypothesis: \\(\\beta_1 = 0\\) Alternative Hypothesis: \\(\\beta_1 \\ne 0\\) Under the null, the regression parameters are assumed to be 0, implying that there is no relationship between \\(Y\\) and \\(X\\) . The test should reject the null , proving that there IS a relationship between the DV and IVs. Prediction Once the best model has been determined, it can be used to make Predictions about future unobserved values. Let these future values be denoted by the subscript \\(*\\) . These unobserved DVs come from the population, thus can be expressed as a function of the population model: \\[ y_* = f(x_*, \\beta) + \\varepsilon_* \\] The corresponding values from the sample regression model is an estimate for this unobserved value: \\[ \\hat{y}_* = f(x_*, \\hat{\\beta}) \\] Like before, the predicted value is unlikely to be exactly equal to the actual value. Thus, the difference between both values can be measured as the Prediction Error : \\[ y_* - \\hat{y_*} = \\varepsilon_* + [f(x_*, \\beta) - f(x_*, \\hat{\\beta})] \\] The prediction error is thus made up of two components : Inherent error present in the DV ( \\(\\varepsilon_*\\) ) Error in estimating the population model ( \\(f(x_*, \\beta) - f(x_*, \\hat{\\beta})\\) ) Based on the distribution of the prediction error, a Prediction Interval at a given confidence level can be calculated to accompany the regression estimate, which is essentially a confidence interval for the predicted value . Note that the prediction intervals will always be wider than confidence intervals . This is because CIs only takes into the account the error in estimating the population model/parameters while PIs take into account the inherent error of the DV as well.","title":"Regression Overview"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#linear-regression","text":"","title":"Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#population-regression-model","text":"Regression is a statistical model that relates a Dependent Variable (DV) to one or more Independent Variables (IV). The dependent variable is regressed on to the independent variable. They are fundamentally a function of the independent variables and several Regression Parameters , \\(\\beta\\) . The functional form of the regression is based on the relationship between the variables. The goal is to use a model that best captures the relationship between the variables. \\[ Y = f(X, \\beta) \\] Independent Variable(s) Dependent Variables Variable used to make predictions Variable being predicted Free to change the value Depends on the value of indepenent variable Deterministic Random Variable Denoted as \\(X\\) Denoted as \\(Y\\) To be precise, for every set of IVs, the DV has a Conditional Distribution dependent on the given IV. For instance, the the \\(Y\\) could take any possible value (Marginal Distribution), but given these set of \\(X\\) , the possible values can be narrowed down to a certain range (conditional distribution). \\[ \\displaylines{ Y \\sim Distribution \\\\ Y|X \\sim Conditional~Distribution} \\] Thus, the output of the regression model is actually the Expected Value of the conditional distribution, \\(E(Y|X)\\) , for every possible \\(X\\) . \\[ E(Y|X) = f(X, \\beta) \\] The actual observations are unlikely to be exactly equal to its expectation, thus there is a difference between an observation and the corresponding regression output. It known as the Random Error Term which accounts for all other factors that affect the DV that are not captured in the regression. This means that the relationship between the \\(Y\\) and \\(X\\) is only approximate , as the true relationship is probably different due to the possibility of unaccounted variables. Note that the sign of the errors are significant - positive implies the actual value lies above the regression output while negative implies it lies below. \\[ \\varepsilon_i = y_i - f(x_i, \\beta) \\] This means that \\(Y\\) (not its expectation!) can be expressed as a sum of the regression model and the error terms: \\[ y_i = f(x_i,\\beta) + \\varepsilon_i \\] The Regression is known as the Systematic component as it is shared among all observations The Error is known as the Non-Systematic component as it is unique to each observation","title":"Population Regression Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#sample-regression-model","text":"In practice, the population is unobservable hence it is impossible to construct the population regression model. Instead, a regression model is constructed from a sample instead, which aims to estimate the population model. \\[ \\hat{y} = f(X,\\hat{\\beta}) \\] Similarly, the output of this model can be compared to the actual observations. However, the resulting difference is known as the Residual of the model, which like all the other components, is an estimate for the Error term. \\[ \\hat{\\varepsilon_i} = y_i - \\hat{y_i} \\] There are several different methods to estimate the regression parameters, but they usually involve minimizing the residuals of the model, such that the resulting model best fits the given sample, which is why it is also known as the Fitted Regression Model .","title":"Sample Regression Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#hypothesis-testing","text":"Once a regression model has been fit, the next step is to determine if the relationship found in the sample is indicative of a relationship in the population. This can be determined through the following two-sided hypothesis test : Null Hypothesis: \\(\\beta_1 = 0\\) Alternative Hypothesis: \\(\\beta_1 \\ne 0\\) Under the null, the regression parameters are assumed to be 0, implying that there is no relationship between \\(Y\\) and \\(X\\) . The test should reject the null , proving that there IS a relationship between the DV and IVs.","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#prediction","text":"Once the best model has been determined, it can be used to make Predictions about future unobserved values. Let these future values be denoted by the subscript \\(*\\) . These unobserved DVs come from the population, thus can be expressed as a function of the population model: \\[ y_* = f(x_*, \\beta) + \\varepsilon_* \\] The corresponding values from the sample regression model is an estimate for this unobserved value: \\[ \\hat{y}_* = f(x_*, \\hat{\\beta}) \\] Like before, the predicted value is unlikely to be exactly equal to the actual value. Thus, the difference between both values can be measured as the Prediction Error : \\[ y_* - \\hat{y_*} = \\varepsilon_* + [f(x_*, \\beta) - f(x_*, \\hat{\\beta})] \\] The prediction error is thus made up of two components : Inherent error present in the DV ( \\(\\varepsilon_*\\) ) Error in estimating the population model ( \\(f(x_*, \\beta) - f(x_*, \\hat{\\beta})\\) ) Based on the distribution of the prediction error, a Prediction Interval at a given confidence level can be calculated to accompany the regression estimate, which is essentially a confidence interval for the predicted value . Note that the prediction intervals will always be wider than confidence intervals . This is because CIs only takes into the account the error in estimating the population model/parameters while PIs take into account the inherent error of the DV as well.","title":"Prediction"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/10.%20Clustering/","text":"","title":"10. Clustering"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/","text":"Simple Linear Regression Simple Linear Regression (SLR) assumes a Linear Relationship between a Numeric DV and single continuous quantitative IV. The model is considered to be simple because it only contains a single independent variable. \\[ E(Y|X) = \\beta_0 + \\beta_1 X \\] \\(\\beta_0\\) \\(\\beta_1\\) Expected value of \\(Y\\) when \\(X = 0\\) Change in the expected value of \\(Y\\) given a one unit increase in \\(X\\) Intercept Parameter Slope Parameter Each observation can also be expressed as sum of the regression and its error term: \\[ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\] Ordinary Least Squares SLR parameters are estimated using the Ordinary Least Squares method, which minimizes the Sum of Squared Residuals of the fitted model. It is commonly referred to as the Residual Sum Squared (RSS). \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 \\] The minimization is solved through calculus by setting the partial derivatives of the RSS to 0: For the intercept parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_0} RSS &= 0 \\\\ -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum y_i - \\sum \\hat{\\beta}_0 - \\sum \\hat{\\beta}_1 x &= 0 \\\\ n\\bar{y} -n\\hat{\\beta}_0 - n\\hat{\\beta}_1 \\bar{x} &= 0 \\\\ \\end{aligned} \\] \\[\\therefore \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\] For the slope parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_1} RSS &= 0 \\\\ -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum (y_i x_i) - \\hat{\\beta}_0 \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} + n\\hat{\\beta}_1 \\bar{x}^2 - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} &= \\hat{\\beta}_1 \\sum (x^2_i) - n\\bar{x}^2 \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 = \\frac{\\sum (x_i y_i) - n\\bar{x}\\bar{y}}{\\sum (x_i^2) - n\\bar{x}^2} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} = r * \\frac{s_y}{s_x} \\] This results in the following fitted regression model, which can be graphically expressed as a Regression Line : \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] Note that it should \\(\\hat{\\varepsilon}\\) in the above image, not \\(e_i\\) . OLS Properties By re-arranging the formula for \\(\\hat{\\beta}_0\\) , we can show that \\((\\bar{x}, \\bar{y})\\) always lies on the fitted regression model: \\[ \\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x} \\] Additionally, since the parameters are estimated through minimization, the resulting model must always fulfil the two first order conditions . The model thus has \\(n-2\\) degrees of freedom to reflect these \"constraints\". \\(\\beta_0\\) FOC \\(\\beta_1\\) FOC \\(\\frac{\\partial}{\\partial \\beta_0} = -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\frac{\\partial}{\\partial \\beta_1} = -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\sum \\hat{\\varepsilon_i} = 0\\) \\(\\sum x_i \\hat{\\varepsilon_i} = 0\\) Residuals are negatively correlated Residuals and Independendent variables are uncorrelated Using the above results, we can also show the following that the mean of the regression outputs is equal to the mean of the population: \\[ \\begin{aligned} \\hat{\\varepsilon_i} &= y_i - \\hat{y_i} \\\\ \\sum \\hat{\\varepsilon_i} &= \\sum y_i - \\hat{y_i} \\\\ 0 &= n\\bar{y} - n\\bar{\\hat{y}} \\\\ \\bar{\\hat{y}} &= \\bar{y} \\\\ \\end{aligned} \\] Goodness of Fit Ideally, the regression should fit the sample closely, having as small as residuals as possible . The size of all the residuals in the model can be summarized through the RSS. The lower the RSS, the better the fit of the model. Recall that the residuals naturally sum to 0 under OLS - the residuals are thus squared to remove the sign so that they can be summed together. \\[ RSS = \\sum (y_i - \\hat{y})^2 \\] However, the SSR on its own is hard to intepret as there is no indication of how low or high it actually is. Thus, the Total Sum of Squares (TSS) can be used as a benchmark for the RSS as it is at least equal to or higher than the RSS. The TSS represents the RSS for a Null Regression - a model with containing only the intercept parameter . The output of this regression is always the sample mean \\(\\bar{y}\\) , which is used for the computation of its residuals. It represents the worst possible model which thus has the highest possible RSS . The lower the RSS compared to the TSS, the better the fit of the model. \\[ TSS = \\sum (y_i - \\bar{y}) \\] Null Model Consider a regression with only the intercept; \\(\\beta_1 = 0\\) . It is known as the Null Model as there are no independent variables used. \\[ y = \\beta_0 \\] We can estimate \\(\\hat{\\beta_0}\\) using OLS, which results in the following result: \\[ \\begin{aligned} -2 \\sum (y_i - \\hat{\\beta_0}) &= 0 \\\\ n \\bar{y} - n \\hat{\\beta_0} &= 0 \\\\ \\hat{\\beta_0} &= \\bar{y} \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{y} = \\bar{y} \\] Sum of Squares The TSS can be further decomposed into two more parts for analysis: \\[ \\begin{aligned} TSS &= \\sum (y_i - \\bar{y})^2 \\\\ TSS &= \\sum[(y_i - \\hat{y}) + (\\hat{y}-\\bar{y})]^2 \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 2 \\sum((y_i - \\hat{y})(\\hat{y}-\\bar{y})) \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 0 \\\\ TSS &= RSS + RegSS \\end{aligned} \\] Residual SS (RSS) Regression SS (RegSS) \\(\\sum(\\hat{y}-\\bar{y})^2\\) \\(\\sum(y_i - \\hat{y})^2\\) Variation of the observed values about the regression Variation of the regression output about the sample mean Variation explained by the regression Variation unexplained by the regression Note that it can also be expressed in terms of the Slope Parameter : \\[ \\begin{align} RegSS &= \\sum(\\hat{y}-\\bar{y})^2 \\\\ &= \\sum(\\hat{\\beta}_0 + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum(\\bar{y} - \\beta_1 \\bar{x} + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum[\\hat{\\beta}_1 (x_i - \\bar{x})]^2 \\\\ &= \\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2 \\\\ \\end{align} \\] The Coefficient of Determination \\(R^2\\) can also be used to demonstrate goodness of fit. It measures the proportion of variation explained by the regression model: \\[ R^2 = \\frac{RegSS}{TSS} = 1 - \\frac{RSS}{TSS} \\] Building off the above expression, it can also be expressed in terms of the Sample Correlation : \\[ \\begin{align} R^2 &= \\frac{\\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\\right)^2 \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^4} \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x}) \\sum (y_i - \\bar{y})}\\right)^2 \\\\ &= r_{y,x}^2 \\end{align} \\] Degrees of Freedom The TSS is based on the naive model with only the intercept parameter, thus, it is subject to the single contraint of all residuals summing to 0. The TSS thus has \\(n-1\\) degrees of freedom . The RSS is based on the SLR with both the intercept and slope parameter, thus it is subject to an additional constraint of the sumproduct of all residuals and independent variables being 0. The RSS thus has \\(n-2\\) degrees of freedom. The sum of the RSS and RegSS is equal to the TSS, thus the sum of their degrees of freedom must also be equal to that of the TSS. By working backwards, the RegSS thus has only \\(1\\) degree of freedom . Mean Squared The division of any Sum of Square (TSS, RSS, RegSS) by its Degrees of Freedom is known as the Mean Squared (MS), which is a measure of its average variance . The MS of the TSS is the Unbiased Estimator for Population Variance , which is why this process is known as the Analysis of Variance , as it decomposes the variance of \\(Y\\) into its constituent components: \\[ s = \\frac{TSS}{n-1} \\] The MS of the RSS is known the Mean Squared Residuals , often also referred to as the Mean Squared Error , as it is an estimate for the population variance of the error \\(\\sigma^2\\) : \\[ MS_{\\text{Residuals}} = \\frac{RSS}{n-2} \\] The MS of the RegSS is known as the Mean Squared Regression , which represents the proportion of variance explained per \\(X\\) used. \\[ MS_{\\text{Regression}} = \\frac{RegSS}{1} \\] F Statistic The ANOVA parameters can be used to conduct a hypothesis test to determine if there is in fact a relationship between \\(X\\) and \\(Y\\) : \\(H_0\\) : \\(\\beta_1 = 0\\) \\(H_1\\) : \\(\\beta_1 \\ne 0\\) Under the null hypothesis, there should be no difference between the assumed model and a null model as both only contain the intercept parameter, thus \\(TSS = RSS\\) , where \\(RegSS = 0\\) . Thus, the F-statistic is testing for the equality of variance between the TSS and RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_1 \\ne 0\\) . The F-statistic can be constructed using the sum of squares: \\[ \\begin{aligned} F &= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &= \\frac{RegSS/1}{RSS/(n-2)} \\\\ &= \\frac{(TSS - RSS)/1}{RSS/(n-2)} \\\\ &= (n-2) * \\frac{R^2}{1-R^2}, \\text{divide both by TSS} \\end{aligned} \\] Similar to the variance test, it can be shown that under the null, this test-statistic follows an F distribution with \\(1\\) and \\(n-2\\) degrees of freedom: \\[ \\begin{aligned} T &= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &= \\frac{\\sigma^2_{RegSS} \\frac{MS_{Reg}}{\\sigma^2_{RegSS}}}{\\sigma^2_{RSS} \\frac{MS_{RSS}}{\\sigma^2_{RSS}}} \\\\ &= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\frac{1 * MS_{Reg}}{\\sigma^2_{RegSS}} * \\frac{1}{1}}{\\frac{(n-2) * MS_{RegSS}}{\\sigma^2_{RegSS}}* \\frac{1}{n-2}} \\\\ &= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\chi_1}{\\chi_{n-2}} * (n-2) \\\\ &= 1 * F_{1, n-2} * (n-2) \\\\ &= F_{1, n-2} * (n-2) \\end{aligned} \\] \\[ \\therefore F \\sim F_{1, n-2} \\] ANOVA Table All the above information is then summarized in a table for convenience, known as the ANOVA Table : Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(1\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-2\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) - Statistical Inference Sampling Distributions Since the errors are assumed to be normally distributed, then \\(Y\\) is assumed to be normally distributed as well. Since \\(Y\\) is a linear combination of the regression parameters, then the parameters (& their estimates) are normally distributed as well. Both estimates can be expressed in another form that makes it more convenient to find their expectation & variances. \\[ \\begin{aligned} \\hat{\\beta}_1 &= \\frac{\\sum [(x_i - \\bar{x})(y_i - \\bar{y})]}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})y_i}{\\sum (x^2_i - \\bar{x})} - \\frac{\\bar{y} \\sum (x_i - \\bar{x})}{\\sum (x^2_i - \\bar{x})} \\\\ &= \\sum \\frac{(x_i - \\bar{x})}{(x^2_i - \\bar{x})}* y_i - 0 \\\\ &= \\sum w_i * y_i \\\\ \\\\ \\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\ &= \\frac{1}{n} \\sum y_i - \\bar{x} \\sum w_i * y_i \\\\ &= \\sum y_i (\\frac{1}{n} - \\bar{x}w_i) \\end{aligned} \\] \\(w_i\\) is a sort of \"weight\" parameter of the sum of squares. It has three interesting properties that makes it useful: \\[ \\begin{aligned} \\sum w_i &= \\frac{\\sum (x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{n\\bar{x}-n\\bar{x}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{0}{\\sum (x^2_i - \\bar{x})} \\\\ &= 0 \\\\ \\\\ \\sum w_i x_i &= \\frac{\\sum x_i(x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x^2_i - \\bar{x} \\sum x_i)}{\\sum x^2_i - 2\\bar{x}\\sum x_i + \\sum \\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - \\bar{x}(n \\bar{x})}{\\sum x^2_i - 2\\bar{x}(n\\bar{x}) + n\\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - 2n\\bar{x}^2 + n\\bar{x}^2} \\\\ &=\\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - n\\bar{x}^2} \\\\ &= 1 \\\\ \\\\ \\sum w_i^2 &= \\frac{\\sum (x_i - \\bar{x})^2}{\\sum (x_i - \\bar{x})^4} \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i) &= n(\\frac{1}{n}) - \\bar{x} \\sum w_i \\\\ &= 1 - 0 \\\\ &= 1 \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i &= \\frac{1}{n} \\sum x_i - \\bar{x} \\sum w_i x_i \\\\ &= \\frac{1}{n} (n\\bar{x}) - \\bar{x} (1) \\\\ &= \\bar{x} - \\bar{x} \\\\ &= 0 \\end{aligned} \\] Using this, the Expectation & Variance can be determined: \\[ \\begin{aligned} E(\\hat{\\beta}_1) &= \\sum w_i E(y_i) \\\\ &= \\sum w_i E(\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum w_i + \\beta_1 \\sum w_i x_i \\\\ &= \\beta_0 (0) + \\beta_1 (1) \\\\ &= \\beta_1\\\\ \\\\ Var(\\hat{\\beta}_1) &= Var(\\sum w_i y_i) \\\\ &= \\sum w_i^2 Var (y_i) \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2} \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 \\sim N(\\beta_1, \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2}) \\] \\[ \\begin{aligned} E(\\hat{\\beta}_0) &= \\sum (\\frac{1}{n} - \\bar{x}w_i) E(y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i) (\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum (\\frac{1}{n} - \\bar{x}w_i) + \\beta_1 \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i \\\\ &= \\beta_0 (1) + \\beta_1 (0) \\\\ &= \\beta_0 \\\\ \\\\ Var(\\hat{\\beta}_0) &= Var(\\sum (\\frac{1}{n} - \\bar{x}w_i)y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i)^2 Var (y_i) \\\\ &= \\sigma^2 \\sum (\\frac{1}{n^2} -\\frac{2\\bar{x}w_i}{n} + \\bar{x}^2 w_i^2) \\\\ &= \\sigma^2 (\\sum \\frac{1}{n^2} - \\frac{2\\bar{x}}{n} \\sum w_i + \\bar{x}^2 \\sum w_i^2) \\\\ &= \\sigma^2 [n(\\frac{1}{n^2}) - \\frac{2\\bar{x}}{n} (0) + \\bar{x}^2 (\\frac{1}{\\sum (x_i - \\bar{x})^2})] \\\\ &= \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2}) \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_0 \\sim N(\\beta_0, \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2})) \\] Hypothesis Testing Since the regression parameters are normally distributed, a z-statistic can also be used to conduct the tests. However, since the population variance is not known, a t-statistic is used instead: \\[ \\begin{aligned} t &= \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\end{aligned} \\] Since the population variance is estimated by the MSE which has \\(n-2\\) degrees of freedom, the corresponding chi-squared and hence t-distribution has \\(n-2\\) degrees of freedom as well. \\[ \\begin{aligned} \\hat{Var}(\\hat{\\beta_1}) &= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} * \\frac{\\frac{1}{n-1}}{\\frac{1}{n-1}} \\\\ &= \\frac{MS_{RSS}}{(n-1) s^2} \\end{aligned} \\] \\[ t \\sim t_{n-2} \\] Since the square of the t-statistic is the F-statistic, both are equivalent ways of doing so and will always lead to the same conclusions. \\[ t^2 \\sim F_{1, n-2} \\] Confidence Intervals Since the distribution of t-statistic is known, it can be used to determine the confidence interval of the estimate: \\[ P\\left(\\text{Margin of Error} < \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} < \\text{Margin of Error}\\right) = 1 - \\alpha \\] \\[ \\text{Confidence Interval} = \\hat{\\beta}_1 \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_1}} \\] Prediction Intervals Consider the Prediction Error of the SLR model: \\[ y_* - \\hat{y_*} = \\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\] Since both \\(y_*\\) and \\(\\hat{y_*}\\) are normally distributed, the prediction errors are normally distributed as well: \\[ \\begin{aligned} E(y_* - \\hat{y_*}) &= E[\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)]] \\\\ &= 0 + \\beta_0 + \\beta_1 E(x_*) - \\beta_0 - \\beta_1 E(x_*) \\\\ &= 0 \\\\ \\\\ Var(y_* - \\hat{y_*}) &= Var(\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= Var(\\varepsilon_*) + Var[(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= ... \\\\ &= \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}] \\end{aligned} \\] \\[ \\therefore y_* - \\hat{y_*} \\sim N\\left(0, \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}]\\right) \\] Similar to before, since the population variance is unknown, it can be approximated using the MSE. Thus, a t-statistic constructed from this sample will have a t-distribution , allowing the following prediction interval to be calculated: \\[ \\text{Prediction Interval} = \\hat{y}_* \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\] Notice that the standard error of the prediction interval increases as \\(x_*\\) moves further away \\(\\bar{x}\\) , indicating that the predictions become less accurate for those values.","title":"Simple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#simple-linear-regression","text":"Simple Linear Regression (SLR) assumes a Linear Relationship between a Numeric DV and single continuous quantitative IV. The model is considered to be simple because it only contains a single independent variable. \\[ E(Y|X) = \\beta_0 + \\beta_1 X \\] \\(\\beta_0\\) \\(\\beta_1\\) Expected value of \\(Y\\) when \\(X = 0\\) Change in the expected value of \\(Y\\) given a one unit increase in \\(X\\) Intercept Parameter Slope Parameter Each observation can also be expressed as sum of the regression and its error term: \\[ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\]","title":"Simple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#ordinary-least-squares","text":"SLR parameters are estimated using the Ordinary Least Squares method, which minimizes the Sum of Squared Residuals of the fitted model. It is commonly referred to as the Residual Sum Squared (RSS). \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 \\] The minimization is solved through calculus by setting the partial derivatives of the RSS to 0: For the intercept parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_0} RSS &= 0 \\\\ -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum y_i - \\sum \\hat{\\beta}_0 - \\sum \\hat{\\beta}_1 x &= 0 \\\\ n\\bar{y} -n\\hat{\\beta}_0 - n\\hat{\\beta}_1 \\bar{x} &= 0 \\\\ \\end{aligned} \\] \\[\\therefore \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\] For the slope parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_1} RSS &= 0 \\\\ -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum (y_i x_i) - \\hat{\\beta}_0 \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} + n\\hat{\\beta}_1 \\bar{x}^2 - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} &= \\hat{\\beta}_1 \\sum (x^2_i) - n\\bar{x}^2 \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 = \\frac{\\sum (x_i y_i) - n\\bar{x}\\bar{y}}{\\sum (x_i^2) - n\\bar{x}^2} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} = r * \\frac{s_y}{s_x} \\] This results in the following fitted regression model, which can be graphically expressed as a Regression Line : \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] Note that it should \\(\\hat{\\varepsilon}\\) in the above image, not \\(e_i\\) .","title":"Ordinary Least Squares"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#ols-properties","text":"By re-arranging the formula for \\(\\hat{\\beta}_0\\) , we can show that \\((\\bar{x}, \\bar{y})\\) always lies on the fitted regression model: \\[ \\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x} \\] Additionally, since the parameters are estimated through minimization, the resulting model must always fulfil the two first order conditions . The model thus has \\(n-2\\) degrees of freedom to reflect these \"constraints\". \\(\\beta_0\\) FOC \\(\\beta_1\\) FOC \\(\\frac{\\partial}{\\partial \\beta_0} = -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\frac{\\partial}{\\partial \\beta_1} = -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\sum \\hat{\\varepsilon_i} = 0\\) \\(\\sum x_i \\hat{\\varepsilon_i} = 0\\) Residuals are negatively correlated Residuals and Independendent variables are uncorrelated Using the above results, we can also show the following that the mean of the regression outputs is equal to the mean of the population: \\[ \\begin{aligned} \\hat{\\varepsilon_i} &= y_i - \\hat{y_i} \\\\ \\sum \\hat{\\varepsilon_i} &= \\sum y_i - \\hat{y_i} \\\\ 0 &= n\\bar{y} - n\\bar{\\hat{y}} \\\\ \\bar{\\hat{y}} &= \\bar{y} \\\\ \\end{aligned} \\]","title":"OLS Properties"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#goodness-of-fit","text":"Ideally, the regression should fit the sample closely, having as small as residuals as possible . The size of all the residuals in the model can be summarized through the RSS. The lower the RSS, the better the fit of the model. Recall that the residuals naturally sum to 0 under OLS - the residuals are thus squared to remove the sign so that they can be summed together. \\[ RSS = \\sum (y_i - \\hat{y})^2 \\] However, the SSR on its own is hard to intepret as there is no indication of how low or high it actually is. Thus, the Total Sum of Squares (TSS) can be used as a benchmark for the RSS as it is at least equal to or higher than the RSS. The TSS represents the RSS for a Null Regression - a model with containing only the intercept parameter . The output of this regression is always the sample mean \\(\\bar{y}\\) , which is used for the computation of its residuals. It represents the worst possible model which thus has the highest possible RSS . The lower the RSS compared to the TSS, the better the fit of the model. \\[ TSS = \\sum (y_i - \\bar{y}) \\]","title":"Goodness of Fit"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#null-model","text":"Consider a regression with only the intercept; \\(\\beta_1 = 0\\) . It is known as the Null Model as there are no independent variables used. \\[ y = \\beta_0 \\] We can estimate \\(\\hat{\\beta_0}\\) using OLS, which results in the following result: \\[ \\begin{aligned} -2 \\sum (y_i - \\hat{\\beta_0}) &= 0 \\\\ n \\bar{y} - n \\hat{\\beta_0} &= 0 \\\\ \\hat{\\beta_0} &= \\bar{y} \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{y} = \\bar{y} \\]","title":"Null Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#sum-of-squares","text":"The TSS can be further decomposed into two more parts for analysis: \\[ \\begin{aligned} TSS &= \\sum (y_i - \\bar{y})^2 \\\\ TSS &= \\sum[(y_i - \\hat{y}) + (\\hat{y}-\\bar{y})]^2 \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 2 \\sum((y_i - \\hat{y})(\\hat{y}-\\bar{y})) \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 0 \\\\ TSS &= RSS + RegSS \\end{aligned} \\] Residual SS (RSS) Regression SS (RegSS) \\(\\sum(\\hat{y}-\\bar{y})^2\\) \\(\\sum(y_i - \\hat{y})^2\\) Variation of the observed values about the regression Variation of the regression output about the sample mean Variation explained by the regression Variation unexplained by the regression Note that it can also be expressed in terms of the Slope Parameter : \\[ \\begin{align} RegSS &= \\sum(\\hat{y}-\\bar{y})^2 \\\\ &= \\sum(\\hat{\\beta}_0 + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum(\\bar{y} - \\beta_1 \\bar{x} + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum[\\hat{\\beta}_1 (x_i - \\bar{x})]^2 \\\\ &= \\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2 \\\\ \\end{align} \\] The Coefficient of Determination \\(R^2\\) can also be used to demonstrate goodness of fit. It measures the proportion of variation explained by the regression model: \\[ R^2 = \\frac{RegSS}{TSS} = 1 - \\frac{RSS}{TSS} \\] Building off the above expression, it can also be expressed in terms of the Sample Correlation : \\[ \\begin{align} R^2 &= \\frac{\\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\\right)^2 \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^4} \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x}) \\sum (y_i - \\bar{y})}\\right)^2 \\\\ &= r_{y,x}^2 \\end{align} \\]","title":"Sum of Squares"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#degrees-of-freedom","text":"The TSS is based on the naive model with only the intercept parameter, thus, it is subject to the single contraint of all residuals summing to 0. The TSS thus has \\(n-1\\) degrees of freedom . The RSS is based on the SLR with both the intercept and slope parameter, thus it is subject to an additional constraint of the sumproduct of all residuals and independent variables being 0. The RSS thus has \\(n-2\\) degrees of freedom. The sum of the RSS and RegSS is equal to the TSS, thus the sum of their degrees of freedom must also be equal to that of the TSS. By working backwards, the RegSS thus has only \\(1\\) degree of freedom .","title":"Degrees of Freedom"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#mean-squared","text":"The division of any Sum of Square (TSS, RSS, RegSS) by its Degrees of Freedom is known as the Mean Squared (MS), which is a measure of its average variance . The MS of the TSS is the Unbiased Estimator for Population Variance , which is why this process is known as the Analysis of Variance , as it decomposes the variance of \\(Y\\) into its constituent components: \\[ s = \\frac{TSS}{n-1} \\] The MS of the RSS is known the Mean Squared Residuals , often also referred to as the Mean Squared Error , as it is an estimate for the population variance of the error \\(\\sigma^2\\) : \\[ MS_{\\text{Residuals}} = \\frac{RSS}{n-2} \\] The MS of the RegSS is known as the Mean Squared Regression , which represents the proportion of variance explained per \\(X\\) used. \\[ MS_{\\text{Regression}} = \\frac{RegSS}{1} \\]","title":"Mean Squared"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#f-statistic","text":"The ANOVA parameters can be used to conduct a hypothesis test to determine if there is in fact a relationship between \\(X\\) and \\(Y\\) : \\(H_0\\) : \\(\\beta_1 = 0\\) \\(H_1\\) : \\(\\beta_1 \\ne 0\\) Under the null hypothesis, there should be no difference between the assumed model and a null model as both only contain the intercept parameter, thus \\(TSS = RSS\\) , where \\(RegSS = 0\\) . Thus, the F-statistic is testing for the equality of variance between the TSS and RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_1 \\ne 0\\) . The F-statistic can be constructed using the sum of squares: \\[ \\begin{aligned} F &= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &= \\frac{RegSS/1}{RSS/(n-2)} \\\\ &= \\frac{(TSS - RSS)/1}{RSS/(n-2)} \\\\ &= (n-2) * \\frac{R^2}{1-R^2}, \\text{divide both by TSS} \\end{aligned} \\] Similar to the variance test, it can be shown that under the null, this test-statistic follows an F distribution with \\(1\\) and \\(n-2\\) degrees of freedom: \\[ \\begin{aligned} T &= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &= \\frac{\\sigma^2_{RegSS} \\frac{MS_{Reg}}{\\sigma^2_{RegSS}}}{\\sigma^2_{RSS} \\frac{MS_{RSS}}{\\sigma^2_{RSS}}} \\\\ &= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\frac{1 * MS_{Reg}}{\\sigma^2_{RegSS}} * \\frac{1}{1}}{\\frac{(n-2) * MS_{RegSS}}{\\sigma^2_{RegSS}}* \\frac{1}{n-2}} \\\\ &= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\chi_1}{\\chi_{n-2}} * (n-2) \\\\ &= 1 * F_{1, n-2} * (n-2) \\\\ &= F_{1, n-2} * (n-2) \\end{aligned} \\] \\[ \\therefore F \\sim F_{1, n-2} \\]","title":"F Statistic"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#anova-table","text":"All the above information is then summarized in a table for convenience, known as the ANOVA Table : Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(1\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-2\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) -","title":"ANOVA Table"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#statistical-inference","text":"","title":"Statistical Inference"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#sampling-distributions","text":"Since the errors are assumed to be normally distributed, then \\(Y\\) is assumed to be normally distributed as well. Since \\(Y\\) is a linear combination of the regression parameters, then the parameters (& their estimates) are normally distributed as well. Both estimates can be expressed in another form that makes it more convenient to find their expectation & variances. \\[ \\begin{aligned} \\hat{\\beta}_1 &= \\frac{\\sum [(x_i - \\bar{x})(y_i - \\bar{y})]}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})y_i}{\\sum (x^2_i - \\bar{x})} - \\frac{\\bar{y} \\sum (x_i - \\bar{x})}{\\sum (x^2_i - \\bar{x})} \\\\ &= \\sum \\frac{(x_i - \\bar{x})}{(x^2_i - \\bar{x})}* y_i - 0 \\\\ &= \\sum w_i * y_i \\\\ \\\\ \\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\ &= \\frac{1}{n} \\sum y_i - \\bar{x} \\sum w_i * y_i \\\\ &= \\sum y_i (\\frac{1}{n} - \\bar{x}w_i) \\end{aligned} \\] \\(w_i\\) is a sort of \"weight\" parameter of the sum of squares. It has three interesting properties that makes it useful: \\[ \\begin{aligned} \\sum w_i &= \\frac{\\sum (x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{n\\bar{x}-n\\bar{x}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{0}{\\sum (x^2_i - \\bar{x})} \\\\ &= 0 \\\\ \\\\ \\sum w_i x_i &= \\frac{\\sum x_i(x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x^2_i - \\bar{x} \\sum x_i)}{\\sum x^2_i - 2\\bar{x}\\sum x_i + \\sum \\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - \\bar{x}(n \\bar{x})}{\\sum x^2_i - 2\\bar{x}(n\\bar{x}) + n\\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - 2n\\bar{x}^2 + n\\bar{x}^2} \\\\ &=\\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - n\\bar{x}^2} \\\\ &= 1 \\\\ \\\\ \\sum w_i^2 &= \\frac{\\sum (x_i - \\bar{x})^2}{\\sum (x_i - \\bar{x})^4} \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i) &= n(\\frac{1}{n}) - \\bar{x} \\sum w_i \\\\ &= 1 - 0 \\\\ &= 1 \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i &= \\frac{1}{n} \\sum x_i - \\bar{x} \\sum w_i x_i \\\\ &= \\frac{1}{n} (n\\bar{x}) - \\bar{x} (1) \\\\ &= \\bar{x} - \\bar{x} \\\\ &= 0 \\end{aligned} \\] Using this, the Expectation & Variance can be determined: \\[ \\begin{aligned} E(\\hat{\\beta}_1) &= \\sum w_i E(y_i) \\\\ &= \\sum w_i E(\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum w_i + \\beta_1 \\sum w_i x_i \\\\ &= \\beta_0 (0) + \\beta_1 (1) \\\\ &= \\beta_1\\\\ \\\\ Var(\\hat{\\beta}_1) &= Var(\\sum w_i y_i) \\\\ &= \\sum w_i^2 Var (y_i) \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2} \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 \\sim N(\\beta_1, \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2}) \\] \\[ \\begin{aligned} E(\\hat{\\beta}_0) &= \\sum (\\frac{1}{n} - \\bar{x}w_i) E(y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i) (\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum (\\frac{1}{n} - \\bar{x}w_i) + \\beta_1 \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i \\\\ &= \\beta_0 (1) + \\beta_1 (0) \\\\ &= \\beta_0 \\\\ \\\\ Var(\\hat{\\beta}_0) &= Var(\\sum (\\frac{1}{n} - \\bar{x}w_i)y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i)^2 Var (y_i) \\\\ &= \\sigma^2 \\sum (\\frac{1}{n^2} -\\frac{2\\bar{x}w_i}{n} + \\bar{x}^2 w_i^2) \\\\ &= \\sigma^2 (\\sum \\frac{1}{n^2} - \\frac{2\\bar{x}}{n} \\sum w_i + \\bar{x}^2 \\sum w_i^2) \\\\ &= \\sigma^2 [n(\\frac{1}{n^2}) - \\frac{2\\bar{x}}{n} (0) + \\bar{x}^2 (\\frac{1}{\\sum (x_i - \\bar{x})^2})] \\\\ &= \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2}) \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_0 \\sim N(\\beta_0, \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2})) \\]","title":"Sampling Distributions"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#hypothesis-testing","text":"Since the regression parameters are normally distributed, a z-statistic can also be used to conduct the tests. However, since the population variance is not known, a t-statistic is used instead: \\[ \\begin{aligned} t &= \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\end{aligned} \\] Since the population variance is estimated by the MSE which has \\(n-2\\) degrees of freedom, the corresponding chi-squared and hence t-distribution has \\(n-2\\) degrees of freedom as well. \\[ \\begin{aligned} \\hat{Var}(\\hat{\\beta_1}) &= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} * \\frac{\\frac{1}{n-1}}{\\frac{1}{n-1}} \\\\ &= \\frac{MS_{RSS}}{(n-1) s^2} \\end{aligned} \\] \\[ t \\sim t_{n-2} \\] Since the square of the t-statistic is the F-statistic, both are equivalent ways of doing so and will always lead to the same conclusions. \\[ t^2 \\sim F_{1, n-2} \\]","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#confidence-intervals","text":"Since the distribution of t-statistic is known, it can be used to determine the confidence interval of the estimate: \\[ P\\left(\\text{Margin of Error} < \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} < \\text{Margin of Error}\\right) = 1 - \\alpha \\] \\[ \\text{Confidence Interval} = \\hat{\\beta}_1 \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_1}} \\]","title":"Confidence Intervals"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#prediction-intervals","text":"Consider the Prediction Error of the SLR model: \\[ y_* - \\hat{y_*} = \\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\] Since both \\(y_*\\) and \\(\\hat{y_*}\\) are normally distributed, the prediction errors are normally distributed as well: \\[ \\begin{aligned} E(y_* - \\hat{y_*}) &= E[\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)]] \\\\ &= 0 + \\beta_0 + \\beta_1 E(x_*) - \\beta_0 - \\beta_1 E(x_*) \\\\ &= 0 \\\\ \\\\ Var(y_* - \\hat{y_*}) &= Var(\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= Var(\\varepsilon_*) + Var[(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= ... \\\\ &= \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}] \\end{aligned} \\] \\[ \\therefore y_* - \\hat{y_*} \\sim N\\left(0, \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}]\\right) \\] Similar to before, since the population variance is unknown, it can be approximated using the MSE. Thus, a t-statistic constructed from this sample will have a t-distribution , allowing the following prediction interval to be calculated: \\[ \\text{Prediction Interval} = \\hat{y}_* \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\] Notice that the standard error of the prediction interval increases as \\(x_*\\) moves further away \\(\\bar{x}\\) , indicating that the predictions become less accurate for those values.","title":"Prediction Intervals"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/","text":"Multiple Linear Regression The natural extension of the SLR model is to include more than one independent variable , which thus results in the more generalized Multiple Linear Regression (MLR) model. \\[ E(Y|X_1, ... X_p) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_{p}X_{p} \\] Unlike the SLR which studies how each individual IV influences the DV, the goal of the MLR model is to study how all the IVs operate together to influence the DV. \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) when \\(X_1 = X_2 = ... = 0\\) Change in \\(E(Y)\\) given a one unit increase in \\(X_j\\) , holding all other \\(X\\) 's constant Intercept Parameter \"Slope\" Parameter For avoidance of doubt, the subscript \\(i\\) will be used to denote observations while \\(j\\) will be used to denote independent variables. Every observation can also be expressed as the sum of the regression and the error term. However, due to the multi-dimensional nature of the model, it is commonly expressed in matrix notation: \\[ \\begin{aligned} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} &= \\begin{pmatrix} 1 & x_{11} & x_{12} & ... & x_{1p} \\\\ 1 & x_{21} & x_{22} & ... & x_{2p} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & ... & x_{np} \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix} + \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix} \\\\ \\boldsymbol{y} &= \\boldsymbol{X\\beta + \\varepsilon} \\end{aligned} \\] Ordinary Least Squares Similar to the SLR model, the regression parameters can be found by minimizing the sum of squared residuals: \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_p x_ip)]^2 \\] There are \\(p+1\\) FOC equations to solve through the minimization, with the additional one reflecting the intercept parameter. It is difficult to algebraically solve this system of equations, thus there is no closed form tsolution for each individual paramater. Instead, here is a vector solution for all of the parameters: \\[ \\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\hat{\\beta_0} \\\\ \\hat{\\beta_1} \\\\ \\vdots \\\\ \\hat{\\beta_p} \\end{pmatrix} = (\\boldsymbol{X'X})^{-1}\\boldsymbol{X'y} \\] Note that since there are \\(p+1\\) equations that must be solved, the model has \\(n-p+1\\) degrees of freedom. Following the same logic, there must be at least \\(p+1\\) observations in order to solve the equations and hence construct the model. This results in the following fitted regression model, which can be graphically expressed as a Regression Plane : \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_P x_ip \\] Manual Computation The tricky part is that \\((\\boldsymbol{X'X})^{-1}\\) is hard to compute by manually , except for the special case where \\(p=1\\) (SLR). Thus, it is likely that the parameters will be provided by the question. If required to compute them manually, then \\((\\boldsymbol{X'X})^{-1}\\) is likely to be provided. The remaining \\(\\boldsymbol{X'y}\\) still needs to be computed and put together to obtain the regression parameters. However, if the model has \\(p=2\\) but no intercept , then \\((\\boldsymbol{X'X})^{-1}\\) is a 2 x 2 Matrix whose inverse can be easily calculated. Similarly, if \\((\\boldsymbol{X'X})^{-1}\\) is a Diagonal Matrix , its inverse can be easily calculated as well. Goodness of Fit The ANOVA for MLR follows the same intuition as the SLR version, adjusted for the new degrees of freedom: Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(p\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-(p+1)\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) - The coefficient of determination still represents the proportion of variance explained by the regression, but has a slightly different formula: \\[ R^2 = \\frac{RegSS}{TSS} = r_{y,\\hat{y}}^2 \\] The multiple IVs of the model are now captured through \\(\\hat{y}\\) instead of \\(x\\) directly. However, the hypothesis test under the MLR is vastly different from the SLR version. Instead of testing if an individual IV is useful, it tests if all the IVs are collectively useful in helping to explain the DV. \\[ \\begin{aligned} H_0 &: \\beta_1 = \\beta_2 = ... = \\beta_p = 0 \\\\ H_1 &: \\text{At least one } \\beta_j \\text{ is non-zero} \\end{aligned} \\] Thus, rejecting the null hypothesis implies that at least one of the IVs used is useful, but does not provide much insight into which of them are useful. Partial F Test A partial F-test can be used to precisely determine which of the IVs are useful in explaining the DV. A regular F test compares the null model with no IVs to the desired model with all the IVs. If the sum of squares are significantly different, then it implies that the additional IVs are jointly useful in explaining \\(Y\\) . The partial F test generalizes this idea. Instead of considering a null model with no IVs, a Reduced Model with a limited number IVs ( \\(q\\) ) is considered instead. Consequently, the desired model is known as the Full Model with all \\(p\\) IVs, where \\(q < p\\) . \\[ \\begin{aligned} H_0: \\beta_{p-q+1} = ... = \\beta_p = 0 \\\\ H_1: \\beta_{p-q+1} = ... = \\beta_p \\ne 0 \\end{aligned} \\] The test is also commonly referred to as the Generalized F test, where the models are referred to as the Restricted and Unrestricted Models. The difference in the RSS between the Full and Reduced Model is known as the Extra Sum of Squares (ExtraSS) . It represents the contribution of the missing variables in explaining the variance of \\(Y\\) . Under the null hypothesis, there should be no difference between the two RSS, and thus \\(ExtraSS = 0\\) . \\[ ExtraSS = RSS_{Reduced} - RSS_{Full} \\] Thus, the Partial F-statistic is testing for the equality of variance between the two RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_{p-q+1} = ... = \\beta_p \\ne 0\\) . \\[ \\begin{aligned} F &= \\frac{MS_{ExtraSS}}{MS_{RSS_{Full}}} \\\\ &= \\frac{ExtraSS/q}{RSS/(n-2)} \\\\ &= \\frac{(RSS_{Reduced} - RSS_{Full})/q}{RSS/(n-2)} \\\\ &= (n-2) * \\frac{(1- R_{Reduced}^2) - (1 - R_{Full}^2)}{1-RSS_{Full}^2}, \\text{divide both by TSS} \\\\ &= (n-2) * \\frac{R_{Full}^2 - R_{Reduced}^2}{1-RSS_{Full}^2} \\end{aligned} \\] Statistical Inference Sampling Distributions Similar to SLR, the regression parameters are normally distributed as well. However, since there are multiple regression parameters, they collectively follow a multivariate normal distribution . \\[ \\hat{\\beta} \\sim N_{p+1}(\\beta, \\sigma^2 (\\boldsymbol{X'X})^{-1}) \\] The variance of the distribution is known as the Variance Covariance Matrix , which provides the covariances between every possible pair of regression parameters. Since the covariance of a variable with itself is its variance, the diagonals are the respective variances of the parameters. Note that the first element of the diagonal is the intercept, thus the variance of the jth IV is the (j+1)th element of the diagonal . \\[ Var(\\hat{\\beta}) = \\begin{pmatrix} Var(\\hat{\\beta}_0) & Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) & ... & Cov(\\hat{\\beta}_0, \\hat{\\beta}_p) \\\\ Cov(\\hat{\\beta}_1, \\hat{\\beta}_0) & Var(\\hat{\\beta}_1) & ... & Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ Cov(\\hat{\\beta}_p, \\hat{\\beta}_0) & Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) & ... & Var(\\hat{\\beta}_p) \\end{pmatrix} \\] Note that the covariances are symmetrical about the diagonal - \\(Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = Cov(\\hat{\\beta}_1, \\hat{\\beta}_0)\\) . Hypothesis Testing Similar to SLR, the t-test can be used to test for the significance of an individual IV, but the intepretation of the test is different from the SLR case. It tests the usefulness of an individual IV in the presence of the other predictors . \\[ \\begin{aligned} H_0: \\beta_j = 0 \\\\ H_1: \\beta_j \\ne 0 \\end{aligned} \\] \\[ t(\\beta_j) = \\frac{\\hat{\\beta_j} - \\beta_j}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\] Recall that the variance of the jth IV si the (j+1)th element of the variance covariance matrix. \\[ t(\\beta_j) \\sim t_{n-p-1} \\] However, this leads to several odd results which needs to be accounted for: Predictor is not significant individually but significant when taken alone . Predictors are not significant individually but significant when taken together . There are now two possible ways to test for the significance of IVs: Conduct a single F-test to test for the joint significance of all IVs Conduct multiple t-tests to test for the joint significance of each IV The problem with the multiple t-test approach lies with the type I error of the tests. For \\(\\alpha = 0.05\\) , the probability of correctly rejecting the null is \\(0.95\\) . Assuming that all the tests are independent, the probability of correctly rejecting all the nulls is \\(0.95^p\\) , which drastically decreases with the number of tests conducted. Given enough predictors, this means that the probability drops to approximately 0, which means that there is bound to be a wrongly rejected null; a type I error is guaranteed even though it was supposed to be limited at a 0.05 chance. The Bonferroni Correction is a method of adjusting \\(\\alpha\\) of each hypothesis test such that the overall type I error is kept at its desired level. However, this has the consequence of increasing the probability of type II errors, which is why it is not popular. The F-test has the advantage of controlling the type I error regardless of the number of predictors , which is why it is preferred for hypothesis testing in the MLR. Confidence Intervals Similar to SLR, the confidence intervals can be constructed using the distribution of the test-statistic: \\[ \\text{Confidence Interval} = \\hat{\\beta}_j \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_j}} \\] Prediction Intervals Unlike in SLR, it is difficult to determine the distribution of the prediction error. Thus, the final result can be found below: \\[ \\begin{aligned} \\text{Prediction Interval} &= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\\\ &= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\sqrt{s^2 [1 + x'_* (\\boldsymbol{X'X})^-1 * x_*]} \\end{aligned} \\] Despite the result looking more complicated, the key takeaway remains the same - the further away \\(x_*\\) is from \\(\\bar{x}\\) , the greater Variations of MLR Qualitative IV The discussion so far has mostly focused on Quantitative IVs, thus this section will explore Qualitative IVs. They can only take values from a list of pre-defined values, known as the Levels of the variable. In a regression context, most qualitative IVs are represented in the form of a Dummy Variable which can only take two possible levels - Yes (1) or No (0). Note that there are other ways of encoding a dummy variable (-1/0/1 etc), but the principles stay large the same. \\[ \\text{Dummy Variable} = \\begin{cases} 1, & \\text{First Level} \\\\ 0, & \\text{Second Level} \\end{cases} \\] In general, \\(n-1\\) dummy variables are needed to represent a qualitative variable with \\(n\\) levels. This is because the status of the last level can be deduced from the other dummy variables . Thus, including a seperate dummy variable for this last level is redundant and will lead to the problem of Collinearity , which will be explored in a later section. For instance, consider four levels (North, East, South & West), represented by the three dummy variables ( \\(N, E , S\\) ). If any of the variables are 1, they represent their respective direction (North, East & South). If all of them are 0, then the direction is the remaining level (West). The last remaining level is often referred to as the Baseline Level as it is the default level of the variable when all other dummies are 0. Any level can be used as the baseline, but the parameters will differ across models with different baselines. \\[ E(Y|X) = \\beta_0 + \\beta_{1, North} x_{North} + \\beta_{2, East} * x_{East} + \\beta_{3, South} * x_{South} \\] \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) at the baseline level Change in \\(E(Y)\\) from the baseline to the chosen level \\(X_1 = X_2 = ... = X_j = 0\\) \\(X_1 = X_2 = ... = 0; X_j = 1\\) Dummy variables are usually used in conjunction with quantitative ones . This essentially creates a \"seperate\" regression model for each of the levels. For the simplest case of one quantiative and one dummy, \\(\\beta_2\\) is the difference in the intercept of the two resulting SLR models. \\[ E(Y|X) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\] \\[ E(Y|X) = \\begin{cases} (\\beta_0 + \\beta_2) + \\beta_1 x_1, & x_2 = 1 \\\\ \\beta_0 + \\beta_1 x_1, & x_2 = 0 \\end{cases} \\] Interaction Model So far, it was assumed that each IV had an independent effect on the DV. However, IVs may interact to produce a joint effect on the DV, where the effect of one IV depends on the value of another IV. For instance, the production of a factory may depend on the number of Machines and Workers. However, the more machines there are, the greater the effect of an additional worker . Thus, this interaction effect can be captured through an Interaction Variable , which is the product of both IVs: \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 \\\\ &= \\beta_0 + (\\beta_1 + \\beta_3 x_2)x_1 + \\beta_2 x_2 \\\\ &= \\beta_0 + \\beta_1 x_1 + (\\beta_2 + \\beta_3 x_1) x_2 \\\\ \\end{aligned} \\] A one unit increase in \\(x_1\\) will increase E(Y) by \\(\\beta_1 + \\beta_3 x_2\\) , which depends on the value of \\(x_2\\) as well, which is is why they interact with each another. Phrased another way, for every one unit increase in \\(x_2\\) , the change in E(Y) from a unit increase in \\(x_1\\) increases by \\(\\beta_3\\) . Something unusual to take note of is that the Interaction Variable tests significant but the constituent variables do not. In this case, it is common practice to retain both the interaction and the consistuent variables in the model. This is practice is known as the Hierarchical Principle . Models containing dummy variables can also have an interaction effect. Building off the example from the previous section, \\(\\beta_2\\) is still the difference in the intercept but with the new \\(\\beta_3\\) being the difference in slopes of the two resulting SLR models. \\[ E(Y|X) = \\begin{cases} (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3)x_1, & x_2 = 1 \\\\ \\beta_0 + \\beta_1 x_1, & x_2 = 0 \\end{cases} \\] Piecewise Model If the DV has an abrupt change in behaviour over different values of the IVs, it can be accounted for the through the use of a Piecewise Regression . The first method of creating a piecewise regression involves the use of an Indicator Function . It is essentially a dummy variable which depends on the value of the other IVs. \\[ z_{\\{x>=c\\}} = \\begin{cases} 0, & x < c \\\\ 1, & x \\ge c \\end{cases} \\] \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 z(x-c) \\\\ &= \\begin{cases} \\beta_0 + \\beta_1 x_1, & x < c \\\\ (\\beta_0 - \\beta_2c) + (\\beta_1 + \\beta_2) x_1, & x \\ge c \\end{cases} \\end{aligned} \\] Note that \\(z(x-c)\\) is treated as a distinct IV and hence can be equivalently expressed as \\(x_2\\) ; the full notation is used here for clarity. \\(c\\) is the value at which the DV abruptly changes in behaviour, known as a Kink in the graph, which continuously connects the two regression lines. The other method is to use an interaction variable instead. Similar to how the interaction variables resulted in the model to \"split\", the model now splits at \\(x = c\\) , resulting in a non-continuous gap. \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2z + \\beta_3 zx \\\\ &= \\begin{cases} \\beta_0 + \\beta_1 x_1, & x < c \\\\ (\\beta_0 - \\beta_2) + (\\beta_1 + \\beta_3) x_1, & x \\ge c \\end{cases} \\end{aligned} \\] Polynomial Model If the relationship between the DV and IV is complex (non-linear), then a Polynomial Regression can be used to better model the relationship between the two. \\[ E(Y|X) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 \\] Note that it is the same IV used in the regression, just with additional powers. Although a polynomial regression may capture the true relationship better, the regression parameters become hard to intepret. The partial derivatives can no longer be intepreted as holding other IVs constant as each IV is dependent on the same quantity, just to different powers. \\[ \\frac{\\partial E(Y|X)}{\\partial x} = \\beta_1 + 2\\beta_2 x + ... + m \\beta_m x^{m-1} \\]","title":"Multiple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#multiple-linear-regression","text":"The natural extension of the SLR model is to include more than one independent variable , which thus results in the more generalized Multiple Linear Regression (MLR) model. \\[ E(Y|X_1, ... X_p) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_{p}X_{p} \\] Unlike the SLR which studies how each individual IV influences the DV, the goal of the MLR model is to study how all the IVs operate together to influence the DV. \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) when \\(X_1 = X_2 = ... = 0\\) Change in \\(E(Y)\\) given a one unit increase in \\(X_j\\) , holding all other \\(X\\) 's constant Intercept Parameter \"Slope\" Parameter For avoidance of doubt, the subscript \\(i\\) will be used to denote observations while \\(j\\) will be used to denote independent variables. Every observation can also be expressed as the sum of the regression and the error term. However, due to the multi-dimensional nature of the model, it is commonly expressed in matrix notation: \\[ \\begin{aligned} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} &= \\begin{pmatrix} 1 & x_{11} & x_{12} & ... & x_{1p} \\\\ 1 & x_{21} & x_{22} & ... & x_{2p} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & ... & x_{np} \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix} + \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix} \\\\ \\boldsymbol{y} &= \\boldsymbol{X\\beta + \\varepsilon} \\end{aligned} \\]","title":"Multiple Linear Regression"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#ordinary-least-squares","text":"Similar to the SLR model, the regression parameters can be found by minimizing the sum of squared residuals: \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_p x_ip)]^2 \\] There are \\(p+1\\) FOC equations to solve through the minimization, with the additional one reflecting the intercept parameter. It is difficult to algebraically solve this system of equations, thus there is no closed form tsolution for each individual paramater. Instead, here is a vector solution for all of the parameters: \\[ \\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\hat{\\beta_0} \\\\ \\hat{\\beta_1} \\\\ \\vdots \\\\ \\hat{\\beta_p} \\end{pmatrix} = (\\boldsymbol{X'X})^{-1}\\boldsymbol{X'y} \\] Note that since there are \\(p+1\\) equations that must be solved, the model has \\(n-p+1\\) degrees of freedom. Following the same logic, there must be at least \\(p+1\\) observations in order to solve the equations and hence construct the model. This results in the following fitted regression model, which can be graphically expressed as a Regression Plane : \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_P x_ip \\]","title":"Ordinary Least Squares"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#manual-computation","text":"The tricky part is that \\((\\boldsymbol{X'X})^{-1}\\) is hard to compute by manually , except for the special case where \\(p=1\\) (SLR). Thus, it is likely that the parameters will be provided by the question. If required to compute them manually, then \\((\\boldsymbol{X'X})^{-1}\\) is likely to be provided. The remaining \\(\\boldsymbol{X'y}\\) still needs to be computed and put together to obtain the regression parameters. However, if the model has \\(p=2\\) but no intercept , then \\((\\boldsymbol{X'X})^{-1}\\) is a 2 x 2 Matrix whose inverse can be easily calculated. Similarly, if \\((\\boldsymbol{X'X})^{-1}\\) is a Diagonal Matrix , its inverse can be easily calculated as well.","title":"Manual Computation"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#goodness-of-fit","text":"The ANOVA for MLR follows the same intuition as the SLR version, adjusted for the new degrees of freedom: Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(p\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-(p+1)\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) - The coefficient of determination still represents the proportion of variance explained by the regression, but has a slightly different formula: \\[ R^2 = \\frac{RegSS}{TSS} = r_{y,\\hat{y}}^2 \\] The multiple IVs of the model are now captured through \\(\\hat{y}\\) instead of \\(x\\) directly. However, the hypothesis test under the MLR is vastly different from the SLR version. Instead of testing if an individual IV is useful, it tests if all the IVs are collectively useful in helping to explain the DV. \\[ \\begin{aligned} H_0 &: \\beta_1 = \\beta_2 = ... = \\beta_p = 0 \\\\ H_1 &: \\text{At least one } \\beta_j \\text{ is non-zero} \\end{aligned} \\] Thus, rejecting the null hypothesis implies that at least one of the IVs used is useful, but does not provide much insight into which of them are useful.","title":"Goodness of Fit"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#partial-f-test","text":"A partial F-test can be used to precisely determine which of the IVs are useful in explaining the DV. A regular F test compares the null model with no IVs to the desired model with all the IVs. If the sum of squares are significantly different, then it implies that the additional IVs are jointly useful in explaining \\(Y\\) . The partial F test generalizes this idea. Instead of considering a null model with no IVs, a Reduced Model with a limited number IVs ( \\(q\\) ) is considered instead. Consequently, the desired model is known as the Full Model with all \\(p\\) IVs, where \\(q < p\\) . \\[ \\begin{aligned} H_0: \\beta_{p-q+1} = ... = \\beta_p = 0 \\\\ H_1: \\beta_{p-q+1} = ... = \\beta_p \\ne 0 \\end{aligned} \\] The test is also commonly referred to as the Generalized F test, where the models are referred to as the Restricted and Unrestricted Models. The difference in the RSS between the Full and Reduced Model is known as the Extra Sum of Squares (ExtraSS) . It represents the contribution of the missing variables in explaining the variance of \\(Y\\) . Under the null hypothesis, there should be no difference between the two RSS, and thus \\(ExtraSS = 0\\) . \\[ ExtraSS = RSS_{Reduced} - RSS_{Full} \\] Thus, the Partial F-statistic is testing for the equality of variance between the two RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_{p-q+1} = ... = \\beta_p \\ne 0\\) . \\[ \\begin{aligned} F &= \\frac{MS_{ExtraSS}}{MS_{RSS_{Full}}} \\\\ &= \\frac{ExtraSS/q}{RSS/(n-2)} \\\\ &= \\frac{(RSS_{Reduced} - RSS_{Full})/q}{RSS/(n-2)} \\\\ &= (n-2) * \\frac{(1- R_{Reduced}^2) - (1 - R_{Full}^2)}{1-RSS_{Full}^2}, \\text{divide both by TSS} \\\\ &= (n-2) * \\frac{R_{Full}^2 - R_{Reduced}^2}{1-RSS_{Full}^2} \\end{aligned} \\]","title":"Partial F Test"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#statistical-inference","text":"","title":"Statistical Inference"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#sampling-distributions","text":"Similar to SLR, the regression parameters are normally distributed as well. However, since there are multiple regression parameters, they collectively follow a multivariate normal distribution . \\[ \\hat{\\beta} \\sim N_{p+1}(\\beta, \\sigma^2 (\\boldsymbol{X'X})^{-1}) \\] The variance of the distribution is known as the Variance Covariance Matrix , which provides the covariances between every possible pair of regression parameters. Since the covariance of a variable with itself is its variance, the diagonals are the respective variances of the parameters. Note that the first element of the diagonal is the intercept, thus the variance of the jth IV is the (j+1)th element of the diagonal . \\[ Var(\\hat{\\beta}) = \\begin{pmatrix} Var(\\hat{\\beta}_0) & Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) & ... & Cov(\\hat{\\beta}_0, \\hat{\\beta}_p) \\\\ Cov(\\hat{\\beta}_1, \\hat{\\beta}_0) & Var(\\hat{\\beta}_1) & ... & Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ Cov(\\hat{\\beta}_p, \\hat{\\beta}_0) & Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) & ... & Var(\\hat{\\beta}_p) \\end{pmatrix} \\] Note that the covariances are symmetrical about the diagonal - \\(Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = Cov(\\hat{\\beta}_1, \\hat{\\beta}_0)\\) .","title":"Sampling Distributions"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#hypothesis-testing","text":"Similar to SLR, the t-test can be used to test for the significance of an individual IV, but the intepretation of the test is different from the SLR case. It tests the usefulness of an individual IV in the presence of the other predictors . \\[ \\begin{aligned} H_0: \\beta_j = 0 \\\\ H_1: \\beta_j \\ne 0 \\end{aligned} \\] \\[ t(\\beta_j) = \\frac{\\hat{\\beta_j} - \\beta_j}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\] Recall that the variance of the jth IV si the (j+1)th element of the variance covariance matrix. \\[ t(\\beta_j) \\sim t_{n-p-1} \\] However, this leads to several odd results which needs to be accounted for: Predictor is not significant individually but significant when taken alone . Predictors are not significant individually but significant when taken together . There are now two possible ways to test for the significance of IVs: Conduct a single F-test to test for the joint significance of all IVs Conduct multiple t-tests to test for the joint significance of each IV The problem with the multiple t-test approach lies with the type I error of the tests. For \\(\\alpha = 0.05\\) , the probability of correctly rejecting the null is \\(0.95\\) . Assuming that all the tests are independent, the probability of correctly rejecting all the nulls is \\(0.95^p\\) , which drastically decreases with the number of tests conducted. Given enough predictors, this means that the probability drops to approximately 0, which means that there is bound to be a wrongly rejected null; a type I error is guaranteed even though it was supposed to be limited at a 0.05 chance. The Bonferroni Correction is a method of adjusting \\(\\alpha\\) of each hypothesis test such that the overall type I error is kept at its desired level. However, this has the consequence of increasing the probability of type II errors, which is why it is not popular. The F-test has the advantage of controlling the type I error regardless of the number of predictors , which is why it is preferred for hypothesis testing in the MLR.","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#confidence-intervals","text":"Similar to SLR, the confidence intervals can be constructed using the distribution of the test-statistic: \\[ \\text{Confidence Interval} = \\hat{\\beta}_j \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_j}} \\]","title":"Confidence Intervals"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#prediction-intervals","text":"Unlike in SLR, it is difficult to determine the distribution of the prediction error. Thus, the final result can be found below: \\[ \\begin{aligned} \\text{Prediction Interval} &= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\\\ &= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\sqrt{s^2 [1 + x'_* (\\boldsymbol{X'X})^-1 * x_*]} \\end{aligned} \\] Despite the result looking more complicated, the key takeaway remains the same - the further away \\(x_*\\) is from \\(\\bar{x}\\) , the greater","title":"Prediction Intervals"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#variations-of-mlr","text":"","title":"Variations of MLR"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#qualitative-iv","text":"The discussion so far has mostly focused on Quantitative IVs, thus this section will explore Qualitative IVs. They can only take values from a list of pre-defined values, known as the Levels of the variable. In a regression context, most qualitative IVs are represented in the form of a Dummy Variable which can only take two possible levels - Yes (1) or No (0). Note that there are other ways of encoding a dummy variable (-1/0/1 etc), but the principles stay large the same. \\[ \\text{Dummy Variable} = \\begin{cases} 1, & \\text{First Level} \\\\ 0, & \\text{Second Level} \\end{cases} \\] In general, \\(n-1\\) dummy variables are needed to represent a qualitative variable with \\(n\\) levels. This is because the status of the last level can be deduced from the other dummy variables . Thus, including a seperate dummy variable for this last level is redundant and will lead to the problem of Collinearity , which will be explored in a later section. For instance, consider four levels (North, East, South & West), represented by the three dummy variables ( \\(N, E , S\\) ). If any of the variables are 1, they represent their respective direction (North, East & South). If all of them are 0, then the direction is the remaining level (West). The last remaining level is often referred to as the Baseline Level as it is the default level of the variable when all other dummies are 0. Any level can be used as the baseline, but the parameters will differ across models with different baselines. \\[ E(Y|X) = \\beta_0 + \\beta_{1, North} x_{North} + \\beta_{2, East} * x_{East} + \\beta_{3, South} * x_{South} \\] \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) at the baseline level Change in \\(E(Y)\\) from the baseline to the chosen level \\(X_1 = X_2 = ... = X_j = 0\\) \\(X_1 = X_2 = ... = 0; X_j = 1\\) Dummy variables are usually used in conjunction with quantitative ones . This essentially creates a \"seperate\" regression model for each of the levels. For the simplest case of one quantiative and one dummy, \\(\\beta_2\\) is the difference in the intercept of the two resulting SLR models. \\[ E(Y|X) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\] \\[ E(Y|X) = \\begin{cases} (\\beta_0 + \\beta_2) + \\beta_1 x_1, & x_2 = 1 \\\\ \\beta_0 + \\beta_1 x_1, & x_2 = 0 \\end{cases} \\]","title":"Qualitative IV"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#interaction-model","text":"So far, it was assumed that each IV had an independent effect on the DV. However, IVs may interact to produce a joint effect on the DV, where the effect of one IV depends on the value of another IV. For instance, the production of a factory may depend on the number of Machines and Workers. However, the more machines there are, the greater the effect of an additional worker . Thus, this interaction effect can be captured through an Interaction Variable , which is the product of both IVs: \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 \\\\ &= \\beta_0 + (\\beta_1 + \\beta_3 x_2)x_1 + \\beta_2 x_2 \\\\ &= \\beta_0 + \\beta_1 x_1 + (\\beta_2 + \\beta_3 x_1) x_2 \\\\ \\end{aligned} \\] A one unit increase in \\(x_1\\) will increase E(Y) by \\(\\beta_1 + \\beta_3 x_2\\) , which depends on the value of \\(x_2\\) as well, which is is why they interact with each another. Phrased another way, for every one unit increase in \\(x_2\\) , the change in E(Y) from a unit increase in \\(x_1\\) increases by \\(\\beta_3\\) . Something unusual to take note of is that the Interaction Variable tests significant but the constituent variables do not. In this case, it is common practice to retain both the interaction and the consistuent variables in the model. This is practice is known as the Hierarchical Principle . Models containing dummy variables can also have an interaction effect. Building off the example from the previous section, \\(\\beta_2\\) is still the difference in the intercept but with the new \\(\\beta_3\\) being the difference in slopes of the two resulting SLR models. \\[ E(Y|X) = \\begin{cases} (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3)x_1, & x_2 = 1 \\\\ \\beta_0 + \\beta_1 x_1, & x_2 = 0 \\end{cases} \\]","title":"Interaction Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#piecewise-model","text":"If the DV has an abrupt change in behaviour over different values of the IVs, it can be accounted for the through the use of a Piecewise Regression . The first method of creating a piecewise regression involves the use of an Indicator Function . It is essentially a dummy variable which depends on the value of the other IVs. \\[ z_{\\{x>=c\\}} = \\begin{cases} 0, & x < c \\\\ 1, & x \\ge c \\end{cases} \\] \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 z(x-c) \\\\ &= \\begin{cases} \\beta_0 + \\beta_1 x_1, & x < c \\\\ (\\beta_0 - \\beta_2c) + (\\beta_1 + \\beta_2) x_1, & x \\ge c \\end{cases} \\end{aligned} \\] Note that \\(z(x-c)\\) is treated as a distinct IV and hence can be equivalently expressed as \\(x_2\\) ; the full notation is used here for clarity. \\(c\\) is the value at which the DV abruptly changes in behaviour, known as a Kink in the graph, which continuously connects the two regression lines. The other method is to use an interaction variable instead. Similar to how the interaction variables resulted in the model to \"split\", the model now splits at \\(x = c\\) , resulting in a non-continuous gap. \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2z + \\beta_3 zx \\\\ &= \\begin{cases} \\beta_0 + \\beta_1 x_1, & x < c \\\\ (\\beta_0 - \\beta_2) + (\\beta_1 + \\beta_3) x_1, & x \\ge c \\end{cases} \\end{aligned} \\]","title":"Piecewise Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#polynomial-model","text":"If the relationship between the DV and IV is complex (non-linear), then a Polynomial Regression can be used to better model the relationship between the two. \\[ E(Y|X) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 \\] Note that it is the same IV used in the regression, just with additional powers. Although a polynomial regression may capture the true relationship better, the regression parameters become hard to intepret. The partial derivatives can no longer be intepreted as holding other IVs constant as each IV is dependent on the same quantity, just to different powers. \\[ \\frac{\\partial E(Y|X)}{\\partial x} = \\beta_1 + 2\\beta_2 x + ... + m \\beta_m x^{m-1} \\]","title":"Polynomial Model"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/","text":"Gauss Markov Theorem Using OLS, the estimated regression parameters will always be unbiased under certain assumptions. The Gauss Markov Theorem extends this, which states that under certain assumptions, the OLS estimators will have the lowest variance among all possible linear unbiased estimators. In a statistics context, they are said to be the most efficient among all other linear unbiased estimators. In a regression context, the OLS estimators are said to be the Best Linear Unbiased Estimators (BLUE). This section will go over the various assumptions for both OLS and Gauss Markov Theorem. It will also cover the diagnostics to determine if the assumptions have been violated. The assumptions needed for OLS and the Gauss Markov theorem are often mixed up with each other as the assumptions needed for OLS are also needed for the theorem. This set of notes makes a clear distinction between the two. OLS Assumptions #1: Linearity Linear regression is a model where the relationship between the DV and IVs are linear. Thus, the regression parameters must be linear , but NOT the DV or IV. This means that the model is still considered a \"Linear Regression\" even after a transformation of the DV and/or IV. \\[ \\begin{aligned} y_i &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\\\ y_i &= \\beta_0 + \\beta_1 x^2 + \\beta_2 x^3 \\\\ \\ln y_i &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\end{aligned} \\] #2: Exogenity Exogenity refers to how a variable comes from outside the model and is thus independent of any other variables within the model . In a regression context, this comes in the form of the errors having a conditional mean of 0 , ensuring that the errors are random and thus not related to the IVs. \\[ E(\\varepsilon_i | x_{ij}) = 0 \\\\ \\] \"Endo\" and \"Exo\" in Greek means \"In\" and \"Out\" respectively, which is how the meaning of the words were derived. There are two key implications of Exogenity: By the Law of Total Expectations, the unconditional expectation of the error is also 0. By the Linearity of Conditional Expectations, the expectation of the product of the Error and IVs is 0. \\[ \\begin{aligned} E(\\varepsilon_i) = 0 \\\\ E(\\varepsilon_i x_{ij}) = 0 \\end{aligned} \\] Following these two implications, it can be shown that the Covariance between the Error and IVs are also 0, which is another consequence of independence (NOT the other way around). \\[ \\begin{aligned} Cov (\\varepsilon_i, x_{ij}) &= E(\\varepsilon_i x_{ij}) - E(\\varepsilon_i) * E(x_{ij}) \\\\ &= 0 - 0 * E(x_{ij}) \\\\ &= 0 \\end{aligned} \\] Without exogenity, the regression parameters would reflect the effect of both the IV and the unmodelled variable within the error term. This causes the OLS estimate to be biased , known as the Omitted Variable Bias . Since the unmodelled variable confounds the results of the regression, it is known as a Confounding Variable . Residual Analysis Since the errors are unobservable, the residuals are used to estimate the errors. If the fitted model is adequate - all relavent IVs are included in the right form, then the residuals should closely resemble the errors and therefore be structureless (random). However, if there are patterns in the residuals , it indicates that there is additional information that can be used to improve the model and thus should be included. Due to the OLS, the correlation between residuals and existing IVs will always be 0 indicating no linear relationship . To check for unmodelled non-linear relationships , a Residual Plot of the IVs against the Residuals can be used. For instance, if the residual plot shows a quadractic pattern (curve), then a quadractic IV should be added into the model. Mathematically, it can be expressed as a function of the existing estimates: \\[ \\begin{aligned} \\hat{y_i} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\varepsilon}_i \\\\ \\hat{\\varepsilon_i} &= \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\text{ (From residual plot)} \\\\ \\hat{y_i} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &= (\\hat{\\beta}_0 + \\hat{\\gamma}_0) + (\\hat{\\beta}_1 + \\hat{\\gamma}_1)x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &= \\hat{\\beta}_0' + \\hat{\\beta}_1'x + \\hat{\\beta}_2' x^2 \\end{aligned} \\] #3: No Perfect Collinearity Collinearity refers to when an IV can be expressed as a linear combination of one or more other IVs . Perfect collinearity is an extreme case where an IV can be perfectly expressed as a combination of another. Technically speaking, Collinearity refers to one to one variable relationship, while Multicollinearity refers to one to many variable relationship, hence \"Multi\". Variable is a multiple of another: \\(x_1 = cx_2\\) Variable differs by a constant from another: \\(x_1 = x_2 \\pm c\\) Variable is an affine transformation of another: Sum of several variables is fixed: Dummy Variable Trap The issue with perfect collinearity is that it affects the linear algebra used to solve for the coefficients (EG. Two equations to solve for three unknowns). There will be no unique solutions - many different values for the coefficients could work equally well. Imperfect Collinearity Imperfect Collinearity is a less extreme case where an IV is highly (but not perfectly) correlated with one or more other IVs. Recall that correlation refers to the extent of a Linear relationship. Non-linear relationships between variables are fine (EG. Polynomial Regression). This means that including the IV does not bring much additional predctive power into the model as its effects are already captured through related predictors and thus can be removed from the model. Unlike in the perfect case, high collinearity does not prevent OLS from finding a solution. However, the intepretation of the variables become complicated: The original intepretation of coefficients \"holding other variables constant\" is no longer true as highly correlated variables tend to move together. Thus, it is hard to seperate the effects of an individual variable . Consequently, OLS has difficulty estimating these coefficients, which could result in weird meaningless estimates ; EG. Large positive coefficient but large negative for its correlated counterpart. This also results in higher standard errors for the coefficients of correlated variables. This reduces the magnitude of t-statistic , which results in more false negatives , failing to reject the null when it should. This results in important variables being omitted from the regression. Technically speaking, there is nothing wrong with collinearity if the purpose of the model is solely for prediction. However, if the purpose of the model was to establish causality, then collinearity poses a problem as it interferes with statistical inference. Detecting Collinearity The simplest way to detect collinearity is through a Scatterplot or Correlation Matrix , which shows the correlations between pairs of variables . A correlation of 0.8 and higher is typically considered high enough where the collinearity becomes problematic. However, the issue is that this method can only detect collinearity between pairs of variable at a time. In order to detect collinearity among three or more variables ( multicollinearity ), then the Variance Inflation Factor (VIF) should be used instead. \\[ VIF = \\frac{1}{1-R^2_j} \\] The VIF is derived from the variance of the regression coefficient. As mentioned previously, under the presence of collinearity, the standard error and hence variance of the coefficient increases (\"inflated\"). The extent of the increase is known as the VIF. \\[ \\hat{Var}(\\hat{\\beta_1}) = \\frac{MS_{RSS}}{(n-1) s^2} * \\frac{1}{1-R^2_j} \\] The \\(R^2_j\\) in the VIF is the coefficient of determination of a model where the jth IV is regressed against all other IVs . A high \\(R^2_j\\) means that the IV is well explained by the other IVs (high correlation), which indicates the presence of collinearity. Generally, a \\(VIF > 10\\) is deemed to have severe collinearity . #4: No Extreme Outliers Outliers are observations with unusual values of the DV relative to fitted regression model. The last OLS assumption is that there are no extreme outliers in the dataset used to create the regression model. Generally, as long as the DV and IVs have a positive and finite Kurtosis , then the probability of such observations occuring are low. Outliers are problematic as OLS is sensitive to outliers . Extreme outliers have large residuals which receive more weight in the optimization process, which causes the resulting model to accomodate it (when it should not), causing the resulting coefficients to be biased. Identifying Outliers By definition, Outliers have unusually large residuals . In order to gauge what is considered a \"large residual\", the residuals are standardized for comparison. Standardization requires knowledge of the sampling distribution of the residuals . Given that the errors have a constant variance of \\(\\sigma^2\\) , the variance of the residuals can be shown to be: \\[ var(\\hat{\\varepsilon}_i) = \\sigma^2 (1-h_{ii}) \\] \\(h_ii\\) is known as the Leverage of the observation, which will be covered in the following section. The sampling distribution can then be determined: \\[ \\hat{\\varepsilon}_i ~ N(0, \\sigma^2 (1-h_{ii})) \\] Thus, the standardized residuals are the raw residuals scaled by their standard error: \\[ \\hat{\\varepsilon}_i^{\\text{standardized}} = \\frac{\\hat{\\varepsilon}_i}{\\sqrt{\\sigma^2 (1-h_{ii})}} \\] In practice, the variance of the errors are unknown, thus it is estimated using the Sample Variance of the residuals instead. Residuals with standardized values of larger than 2 or 3 are considered large and thus can be considered as an outlier. High Leverage Points While outliers are unusual points of the DV, High Leverage observations have unusual values of the IV relative to the majority of the values. It is easy to identify high leverage points when there is only one IV through a scatterplot - simply find the observation that is away from the rest. It becomes much more complicated when there are multiple IVs. The observation's values for each of the IVs could be in the common range of each IV, but unusual when taken collectively : The leverage of the observation can be determined from the MLR: \\[ \\begin{aligned} \\hat{\\boldsymbol{y}} &= \\boldsymbol{X}\\boldsymbol{\\beta} \\\\ &= \\boldsymbol{X}[(X'X)^{-1}y] \\\\ &= \\boldsymbol{H}\\boldsymbol{y} \\end{aligned} \\] \\(\\boldsymbol{H}\\) is known as the Hat Matrix as it puts a hat on y in the notation. The leverage of the \\(ith\\) observation is the \\(ith\\) diagonal element of the matrix, \\(h_{ii}\\) . An observation is considered to have high leverage if its leverage is greater than three times the average leverage: \\[ h_{ii} \\gt 3 \\left(\\frac{p+1}{n}\\right) \\] Influential Points The effect of Outliers and High leverage points can both be summarized into a concept known as Influence . An observation is influential if the exclusion of the observation from the regression leads to significantly differently results. In general the process involves three steps: Fit the original model with all \\(n\\) observations; determine the \\(j-th\\) fitted value \\(\\hat{y}_j\\) Fit an adjusted model with omitting the \\(i-th\\) observation, determine the \\(j-th\\) fitted value \\(\\hat{y}_{j(i)}\\) Calculate the change in the \\(j-th\\) fitted value This process has to be repeated for all fitted values for all observations . Cook's Distance summarizes the effect of the \\(i-th\\) observation on the whole model: \\[ D_i = \\frac{\\sum^n_{j=1} (\\hat{y}_j - \\hat{y}_{j(i)})^2}{(p+1) \\cdot MS_{Residuals}} \\] This method of computation requires \\(n+1\\) datasets - 1 dataset with all the observation and \\(n\\) datasets with the \\(i-th\\) observation omitted. It is also extremely time consuming to have to fit a model to each dataset. An alternative method of determining Cook's Distance is to make use of both the Outliers and Leverage: \\[ D_i = \\frac{1}{p+1} \\cdot (e^{\\text{standardized}})^2 \\cdot \\frac{h_{ii}}{1-h_{ii}} \\] Thus, an observation must be unusual in BOTH the DV and IV in order to be considered influential. Outliers and High leverage points are necessary but not sufficient conditions to be influential. Gauss Markov Assumptions #1: Conditional Homoscedasticity Homoscedasticity refers to the error terms having constant variance while Heteroscedasticity refers to having non-constant variance. \\[ var(\\varepsilon_i | x_i) = \\sigma^2 \\] Under homoscedasticity, the sampling distribution of the estimates are easily derived and thus it can be shown that they are the most efficient estimators. The same cannot be proven under heteroscedasticity. Note that that the OLS estimates are still unbiased; they are just not the most efficient. Identifying Heteroscedasticity Similar to before, if the model is adequate, then the residuals should resemble the errors and have constant variance . Thus, this can be easily determined through a residual plot of the Residuals against the fitted values. If the points are equally spread out about the mean (0) and show no pattern , then homoscedasticity is present. However, if the points show an increasing or decreasing variance (typically in the shape of a funnel ), then heteroscedasticity is present. Alternatively, a hypothesis test can be conducted to determine if heteroscedasticity is present, known as the Bresuch Pagan Test . \\[ \\begin{aligned} H_0 &: \\sigma^2 \\\\ H_1 &: \\sigma^2 + \\boldsymbol{Z\\gamma} \\end{aligned} \\] The test-statistic is computed as follows: Compute the squared standardized residuals from the original model Regress them onto the variables in Z (LOL need to change this part) Compute the RegSS of the new regression \\[ T = \\frac{RegSS}{2} \\] The test-statistic follows a chi-square distribution with \\(q\\) degrees of freedom, where \\(q\\) is the number of variables in \\(\\boldsymbol{Z}\\) . \\[ T \\sim \\chi^2_q \\] Dealing with Heteroscedasticity If prior information is known about the structure of the data, then the most intuitive method would be to incorporate that information into the data. \\[ Var(\\varepsilon_i) = \\frac{\\sigma^2}{w_i} \\] If no prior information is known, then the Heteroscedasticity can be reduced by using a Variance Stabilizing Transformation , such as the Log or Squareroot . Note that since they require positive data, a constant can be added to each term before the transformation to ensure that the values are positive. It is out of the scope for this set of notes to show why these help to stabilize variance. Alternatively, if there is only mild heteroscedasticity in the data, then OLS can be used but with an adjustment to the standard errors of the coefficients, known as heteroscedastic-robust standard errors . Due to complexity of the computations, it will not be covered in this set of notes. However, the general idea is that an weighted estimate of the variance covariance matrix is computed and the standard errors are computed from there. #2: No Serial Correlation If errors are correlated with one another, it is known as Serial Correlation or Autocorrelation . It implies that there are other unmodelled factors that can be used for prediction, which would imply the current model specification to be wrong. Thus, for the SLR model to be true, the errors must be independent of one another . \\[ Cov(\\varepsilon_i,\\varepsilon_j) = 0 \\] Confidence Intervals and PI are narrower than it should be > 95% PI is actually < 95%> P values lower > Appear statisticlaly significant when they shld not be Time series tends to have errors that are positively correlated, which is why it has its own dedicated section No Serial Correlation > Outcome of zero conditional mean, but most likely in time series data Error Distribution Although not needed for OLS estimation or Guass Markov, the errors of the regression are usually assumed to be normally distributed . \\[ \\varepsilon \\sim N(0, \\sigma^2) \\] If the errors are normally distributed, then it follows that \\(\\beta\\) is normally distributed as well since they are linear and additive. This greatly eases the computation needed to determine the sampling distribution for statistical inference. Q-Q Plots Since the errors are normally distributed, the residuals should be normally distributed as well. This can be verified using a Quantile-Quantile Plot (QQ Plot) , which compares the quantiles of two distributions. The first distribution is plotted on the x-axis while the second on the y-axis. If the quantiles are the same (same distribution), then the points should lie on \\(y = x\\) , the 45 degree line.","title":"Gauss Markov Theorem"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#gauss-markov-theorem","text":"Using OLS, the estimated regression parameters will always be unbiased under certain assumptions. The Gauss Markov Theorem extends this, which states that under certain assumptions, the OLS estimators will have the lowest variance among all possible linear unbiased estimators. In a statistics context, they are said to be the most efficient among all other linear unbiased estimators. In a regression context, the OLS estimators are said to be the Best Linear Unbiased Estimators (BLUE). This section will go over the various assumptions for both OLS and Gauss Markov Theorem. It will also cover the diagnostics to determine if the assumptions have been violated. The assumptions needed for OLS and the Gauss Markov theorem are often mixed up with each other as the assumptions needed for OLS are also needed for the theorem. This set of notes makes a clear distinction between the two.","title":"Gauss Markov Theorem"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#ols-assumptions","text":"","title":"OLS Assumptions"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#1-linearity","text":"Linear regression is a model where the relationship between the DV and IVs are linear. Thus, the regression parameters must be linear , but NOT the DV or IV. This means that the model is still considered a \"Linear Regression\" even after a transformation of the DV and/or IV. \\[ \\begin{aligned} y_i &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\\\ y_i &= \\beta_0 + \\beta_1 x^2 + \\beta_2 x^3 \\\\ \\ln y_i &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\end{aligned} \\]","title":"#1: Linearity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#2-exogenity","text":"Exogenity refers to how a variable comes from outside the model and is thus independent of any other variables within the model . In a regression context, this comes in the form of the errors having a conditional mean of 0 , ensuring that the errors are random and thus not related to the IVs. \\[ E(\\varepsilon_i | x_{ij}) = 0 \\\\ \\] \"Endo\" and \"Exo\" in Greek means \"In\" and \"Out\" respectively, which is how the meaning of the words were derived. There are two key implications of Exogenity: By the Law of Total Expectations, the unconditional expectation of the error is also 0. By the Linearity of Conditional Expectations, the expectation of the product of the Error and IVs is 0. \\[ \\begin{aligned} E(\\varepsilon_i) = 0 \\\\ E(\\varepsilon_i x_{ij}) = 0 \\end{aligned} \\] Following these two implications, it can be shown that the Covariance between the Error and IVs are also 0, which is another consequence of independence (NOT the other way around). \\[ \\begin{aligned} Cov (\\varepsilon_i, x_{ij}) &= E(\\varepsilon_i x_{ij}) - E(\\varepsilon_i) * E(x_{ij}) \\\\ &= 0 - 0 * E(x_{ij}) \\\\ &= 0 \\end{aligned} \\] Without exogenity, the regression parameters would reflect the effect of both the IV and the unmodelled variable within the error term. This causes the OLS estimate to be biased , known as the Omitted Variable Bias . Since the unmodelled variable confounds the results of the regression, it is known as a Confounding Variable .","title":"#2: Exogenity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#residual-analysis","text":"Since the errors are unobservable, the residuals are used to estimate the errors. If the fitted model is adequate - all relavent IVs are included in the right form, then the residuals should closely resemble the errors and therefore be structureless (random). However, if there are patterns in the residuals , it indicates that there is additional information that can be used to improve the model and thus should be included. Due to the OLS, the correlation between residuals and existing IVs will always be 0 indicating no linear relationship . To check for unmodelled non-linear relationships , a Residual Plot of the IVs against the Residuals can be used. For instance, if the residual plot shows a quadractic pattern (curve), then a quadractic IV should be added into the model. Mathematically, it can be expressed as a function of the existing estimates: \\[ \\begin{aligned} \\hat{y_i} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\varepsilon}_i \\\\ \\hat{\\varepsilon_i} &= \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\text{ (From residual plot)} \\\\ \\hat{y_i} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &= (\\hat{\\beta}_0 + \\hat{\\gamma}_0) + (\\hat{\\beta}_1 + \\hat{\\gamma}_1)x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &= \\hat{\\beta}_0' + \\hat{\\beta}_1'x + \\hat{\\beta}_2' x^2 \\end{aligned} \\]","title":"Residual Analysis"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#3-no-perfect-collinearity","text":"Collinearity refers to when an IV can be expressed as a linear combination of one or more other IVs . Perfect collinearity is an extreme case where an IV can be perfectly expressed as a combination of another. Technically speaking, Collinearity refers to one to one variable relationship, while Multicollinearity refers to one to many variable relationship, hence \"Multi\". Variable is a multiple of another: \\(x_1 = cx_2\\) Variable differs by a constant from another: \\(x_1 = x_2 \\pm c\\) Variable is an affine transformation of another: Sum of several variables is fixed: Dummy Variable Trap The issue with perfect collinearity is that it affects the linear algebra used to solve for the coefficients (EG. Two equations to solve for three unknowns). There will be no unique solutions - many different values for the coefficients could work equally well.","title":"#3: No Perfect Collinearity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#imperfect-collinearity","text":"Imperfect Collinearity is a less extreme case where an IV is highly (but not perfectly) correlated with one or more other IVs. Recall that correlation refers to the extent of a Linear relationship. Non-linear relationships between variables are fine (EG. Polynomial Regression). This means that including the IV does not bring much additional predctive power into the model as its effects are already captured through related predictors and thus can be removed from the model. Unlike in the perfect case, high collinearity does not prevent OLS from finding a solution. However, the intepretation of the variables become complicated: The original intepretation of coefficients \"holding other variables constant\" is no longer true as highly correlated variables tend to move together. Thus, it is hard to seperate the effects of an individual variable . Consequently, OLS has difficulty estimating these coefficients, which could result in weird meaningless estimates ; EG. Large positive coefficient but large negative for its correlated counterpart. This also results in higher standard errors for the coefficients of correlated variables. This reduces the magnitude of t-statistic , which results in more false negatives , failing to reject the null when it should. This results in important variables being omitted from the regression. Technically speaking, there is nothing wrong with collinearity if the purpose of the model is solely for prediction. However, if the purpose of the model was to establish causality, then collinearity poses a problem as it interferes with statistical inference.","title":"Imperfect Collinearity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#detecting-collinearity","text":"The simplest way to detect collinearity is through a Scatterplot or Correlation Matrix , which shows the correlations between pairs of variables . A correlation of 0.8 and higher is typically considered high enough where the collinearity becomes problematic. However, the issue is that this method can only detect collinearity between pairs of variable at a time. In order to detect collinearity among three or more variables ( multicollinearity ), then the Variance Inflation Factor (VIF) should be used instead. \\[ VIF = \\frac{1}{1-R^2_j} \\] The VIF is derived from the variance of the regression coefficient. As mentioned previously, under the presence of collinearity, the standard error and hence variance of the coefficient increases (\"inflated\"). The extent of the increase is known as the VIF. \\[ \\hat{Var}(\\hat{\\beta_1}) = \\frac{MS_{RSS}}{(n-1) s^2} * \\frac{1}{1-R^2_j} \\] The \\(R^2_j\\) in the VIF is the coefficient of determination of a model where the jth IV is regressed against all other IVs . A high \\(R^2_j\\) means that the IV is well explained by the other IVs (high correlation), which indicates the presence of collinearity. Generally, a \\(VIF > 10\\) is deemed to have severe collinearity .","title":"Detecting Collinearity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#4-no-extreme-outliers","text":"Outliers are observations with unusual values of the DV relative to fitted regression model. The last OLS assumption is that there are no extreme outliers in the dataset used to create the regression model. Generally, as long as the DV and IVs have a positive and finite Kurtosis , then the probability of such observations occuring are low. Outliers are problematic as OLS is sensitive to outliers . Extreme outliers have large residuals which receive more weight in the optimization process, which causes the resulting model to accomodate it (when it should not), causing the resulting coefficients to be biased.","title":"#4: No Extreme Outliers"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#identifying-outliers","text":"By definition, Outliers have unusually large residuals . In order to gauge what is considered a \"large residual\", the residuals are standardized for comparison. Standardization requires knowledge of the sampling distribution of the residuals . Given that the errors have a constant variance of \\(\\sigma^2\\) , the variance of the residuals can be shown to be: \\[ var(\\hat{\\varepsilon}_i) = \\sigma^2 (1-h_{ii}) \\] \\(h_ii\\) is known as the Leverage of the observation, which will be covered in the following section. The sampling distribution can then be determined: \\[ \\hat{\\varepsilon}_i ~ N(0, \\sigma^2 (1-h_{ii})) \\] Thus, the standardized residuals are the raw residuals scaled by their standard error: \\[ \\hat{\\varepsilon}_i^{\\text{standardized}} = \\frac{\\hat{\\varepsilon}_i}{\\sqrt{\\sigma^2 (1-h_{ii})}} \\] In practice, the variance of the errors are unknown, thus it is estimated using the Sample Variance of the residuals instead. Residuals with standardized values of larger than 2 or 3 are considered large and thus can be considered as an outlier.","title":"Identifying Outliers"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#high-leverage-points","text":"While outliers are unusual points of the DV, High Leverage observations have unusual values of the IV relative to the majority of the values. It is easy to identify high leverage points when there is only one IV through a scatterplot - simply find the observation that is away from the rest. It becomes much more complicated when there are multiple IVs. The observation's values for each of the IVs could be in the common range of each IV, but unusual when taken collectively : The leverage of the observation can be determined from the MLR: \\[ \\begin{aligned} \\hat{\\boldsymbol{y}} &= \\boldsymbol{X}\\boldsymbol{\\beta} \\\\ &= \\boldsymbol{X}[(X'X)^{-1}y] \\\\ &= \\boldsymbol{H}\\boldsymbol{y} \\end{aligned} \\] \\(\\boldsymbol{H}\\) is known as the Hat Matrix as it puts a hat on y in the notation. The leverage of the \\(ith\\) observation is the \\(ith\\) diagonal element of the matrix, \\(h_{ii}\\) . An observation is considered to have high leverage if its leverage is greater than three times the average leverage: \\[ h_{ii} \\gt 3 \\left(\\frac{p+1}{n}\\right) \\]","title":"High Leverage Points"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#influential-points","text":"The effect of Outliers and High leverage points can both be summarized into a concept known as Influence . An observation is influential if the exclusion of the observation from the regression leads to significantly differently results. In general the process involves three steps: Fit the original model with all \\(n\\) observations; determine the \\(j-th\\) fitted value \\(\\hat{y}_j\\) Fit an adjusted model with omitting the \\(i-th\\) observation, determine the \\(j-th\\) fitted value \\(\\hat{y}_{j(i)}\\) Calculate the change in the \\(j-th\\) fitted value This process has to be repeated for all fitted values for all observations . Cook's Distance summarizes the effect of the \\(i-th\\) observation on the whole model: \\[ D_i = \\frac{\\sum^n_{j=1} (\\hat{y}_j - \\hat{y}_{j(i)})^2}{(p+1) \\cdot MS_{Residuals}} \\] This method of computation requires \\(n+1\\) datasets - 1 dataset with all the observation and \\(n\\) datasets with the \\(i-th\\) observation omitted. It is also extremely time consuming to have to fit a model to each dataset. An alternative method of determining Cook's Distance is to make use of both the Outliers and Leverage: \\[ D_i = \\frac{1}{p+1} \\cdot (e^{\\text{standardized}})^2 \\cdot \\frac{h_{ii}}{1-h_{ii}} \\] Thus, an observation must be unusual in BOTH the DV and IV in order to be considered influential. Outliers and High leverage points are necessary but not sufficient conditions to be influential.","title":"Influential Points"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#gauss-markov-assumptions","text":"","title":"Gauss Markov Assumptions"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#1-conditional-homoscedasticity","text":"Homoscedasticity refers to the error terms having constant variance while Heteroscedasticity refers to having non-constant variance. \\[ var(\\varepsilon_i | x_i) = \\sigma^2 \\] Under homoscedasticity, the sampling distribution of the estimates are easily derived and thus it can be shown that they are the most efficient estimators. The same cannot be proven under heteroscedasticity. Note that that the OLS estimates are still unbiased; they are just not the most efficient.","title":"#1: Conditional Homoscedasticity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#identifying-heteroscedasticity","text":"Similar to before, if the model is adequate, then the residuals should resemble the errors and have constant variance . Thus, this can be easily determined through a residual plot of the Residuals against the fitted values. If the points are equally spread out about the mean (0) and show no pattern , then homoscedasticity is present. However, if the points show an increasing or decreasing variance (typically in the shape of a funnel ), then heteroscedasticity is present. Alternatively, a hypothesis test can be conducted to determine if heteroscedasticity is present, known as the Bresuch Pagan Test . \\[ \\begin{aligned} H_0 &: \\sigma^2 \\\\ H_1 &: \\sigma^2 + \\boldsymbol{Z\\gamma} \\end{aligned} \\] The test-statistic is computed as follows: Compute the squared standardized residuals from the original model Regress them onto the variables in Z (LOL need to change this part) Compute the RegSS of the new regression \\[ T = \\frac{RegSS}{2} \\] The test-statistic follows a chi-square distribution with \\(q\\) degrees of freedom, where \\(q\\) is the number of variables in \\(\\boldsymbol{Z}\\) . \\[ T \\sim \\chi^2_q \\]","title":"Identifying Heteroscedasticity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#dealing-with-heteroscedasticity","text":"If prior information is known about the structure of the data, then the most intuitive method would be to incorporate that information into the data. \\[ Var(\\varepsilon_i) = \\frac{\\sigma^2}{w_i} \\] If no prior information is known, then the Heteroscedasticity can be reduced by using a Variance Stabilizing Transformation , such as the Log or Squareroot . Note that since they require positive data, a constant can be added to each term before the transformation to ensure that the values are positive. It is out of the scope for this set of notes to show why these help to stabilize variance. Alternatively, if there is only mild heteroscedasticity in the data, then OLS can be used but with an adjustment to the standard errors of the coefficients, known as heteroscedastic-robust standard errors . Due to complexity of the computations, it will not be covered in this set of notes. However, the general idea is that an weighted estimate of the variance covariance matrix is computed and the standard errors are computed from there.","title":"Dealing with Heteroscedasticity"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#2-no-serial-correlation","text":"If errors are correlated with one another, it is known as Serial Correlation or Autocorrelation . It implies that there are other unmodelled factors that can be used for prediction, which would imply the current model specification to be wrong. Thus, for the SLR model to be true, the errors must be independent of one another . \\[ Cov(\\varepsilon_i,\\varepsilon_j) = 0 \\] Confidence Intervals and PI are narrower than it should be > 95% PI is actually < 95%> P values lower > Appear statisticlaly significant when they shld not be Time series tends to have errors that are positively correlated, which is why it has its own dedicated section No Serial Correlation > Outcome of zero conditional mean, but most likely in time series data","title":"#2: No Serial Correlation"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#error-distribution","text":"Although not needed for OLS estimation or Guass Markov, the errors of the regression are usually assumed to be normally distributed . \\[ \\varepsilon \\sim N(0, \\sigma^2) \\] If the errors are normally distributed, then it follows that \\(\\beta\\) is normally distributed as well since they are linear and additive. This greatly eases the computation needed to determine the sampling distribution for statistical inference.","title":"Error Distribution"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#q-q-plots","text":"Since the errors are normally distributed, the residuals should be normally distributed as well. This can be verified using a Quantile-Quantile Plot (QQ Plot) , which compares the quantiles of two distributions. The first distribution is plotted on the x-axis while the second on the y-axis. If the quantiles are the same (same distribution), then the points should lie on \\(y = x\\) , the 45 degree line.","title":"Q-Q Plots"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/","text":"Statistical Learning The regression concepts covered in the previous sections are widely considered to be traditional applied statistics . In a contemporary context, regression is just one of the many methods that fall under Statistical Learning . It is a framework of harnessing data to gain an understanding of how the data is related to one another and/or how a group of variables can be used to accurately predict another. Similar to regression, the relationship between variables can be expressed as a combination of a Signal Function and a Noise term: \\[ y = f(x) + \\varepsilon \\] The two terms originate from Engineering, where Signal refers to the meaningful component of the data while Noise refers to the random variation that inteferes with the signal. Statistical learning models are distinguished based on their signal function. There are two main kinds of models: Parametric Models Non-Parametric Assumes DV follows a specific functional form Does not assume any functional form Function parameters determined from the data - Does not require a large amount of data Requires a large amount of data to work well EG. Linear Regression EG. Clustering | May not fit the data well | Fits the data well | | Described by parameters | No parameters used | | Simple to implement | Requires large amount of data to implement | | Risk that assumed function is wrong | Does not make any assumption about data | They can also be further split according to: Supervised Learning Unsupervised Learning Specified DV to supervise the learning No specific variable chosen Inference/Prediction with respect to the DV Inference/Prediction for all variables EG. Linear Regression EG. Clustering Regression vs classification classification - classifying the observations to a certian level Model Accuracy Since the end goal of the model is to make predictions on new unobserved data, the quality of the model should be evaluated against its performance on unobserved data as well. The observed data used to create the model is known as the Training Data as it helps train the signal function to identify relationships between variables, while the unobserved data used to evaluate the model is known as the Test Data . The quality of the model can then be quantified by the extent to which the model predictions match the data . Similar to regression, this quantity is known as the Error of the model and is summarized through the Mean Square Error (MSE) statistic. The MSE is the average of the sum of squared errors and can be calculated for both the training and test data: \\[ \\begin{aligned} \\text{Training MSE} &= \\frac{[y_i - \\hat{f}(x_0)]^2}{n_{training}} \\\\ \\text{Test MSE} &= \\frac{[y_0 - \\hat{f}(x_0)]^2}{n_{test}} \\end{aligned} \\] Note that this is different from the MSE defined in ANOVA, where the MS is divded by its degrees of freedom rather than the number of observations. The MSE defined in this section is a general concept while the ANOVA MSE is a purely regression concept. The Training MSE reflects the goodness of fit of the model while the Test MSE reflects its prediction accuracy. As alluded to earlier, the goal of statistical learning is to choose the model with the lowest Test MSE . In general, the training MSE should always be smaller than the testing MSE, which is why they are not interchangeable. This is because all models are trained to match the training data to various extents, thus they should naturally have relatively smaller errors. On the flipside, test MSE should always be higher because the model is likely to have mistakenly captured some of the noise in the training data that do not generalize to the test data, resulting in a higher testing error. The extent of the difference is dependent on how well the model fits the training data; the extent to which it learns from it: High Flexibility/Complexity - Tends to overfit the training data; matches data too much; learns too much Low Flexibility/Complexity - Tends to underfit the training data; matches data too little; learns too little This is not to say that low flexibility models are better. In fact, some level of flexibility is needed for the model to pick up most of the signals in the training data, but not too much such that the noise is captured as well. Thus, the test MSE generally decreases with flexibility up till a certain point, following which it increases, forming a U shaped curve : Bias Variance Tradeoff The test MSE can be better understood by decomposing it into its consistuent commponents: \\[ \\text{MSE} = \\text{Bias}[\\hat{f}(x_0)]^2 + \\text{Variance}[\\hat{f}(x_0)] \\] The Bias of the model (also known as the Accuracy ) is the difference in expected value of the estimated signal function and the true signal function. More complex models are better able to capture the signal in the data, thus tends to have a lower bias. The Variance of the model (also known as Precision ) is the change in the estimated signal function across different datasets. Ideally, the model should have low variance such that it would be relatively stable across different training data. While more complex models are better able to capture signals, this makes them prone to overfitting and hence more sensitive to differences in the training data , leading to higher variance. Note that although Precision & Accuracy are both synonyms in English, they have distinct meanings in statistics. Ideally, a model should have both low bias and low variance. However, as explained above, there is an inherent tension between Bias and Variance due to the complexity of the model, known as the Bias-Variance Tradeoff . A relatively simple model (underfitted) tends to have a high bias but low variance . As the complexity increases, the bias initially decreases more than the variance increases , causing the test MSE to fall. At some point, the model becomes too complex (overfitted), where the increase in variance outweighs the fall in bias , resulting in the U-shaped curve as seen previously. Thus, the goal is to find an optimal balance in between Bias and Variance where the test MSE is minimized. Resampling Methods Validation Set LOO Cross Validation K fold Cross Validation Model Selection Feature Selection Forward Stepwise Selection Backward Stepwise Selection Stepwise Selection Shrinkage Methods","title":"Statistical Learning"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#statistical-learning","text":"The regression concepts covered in the previous sections are widely considered to be traditional applied statistics . In a contemporary context, regression is just one of the many methods that fall under Statistical Learning . It is a framework of harnessing data to gain an understanding of how the data is related to one another and/or how a group of variables can be used to accurately predict another. Similar to regression, the relationship between variables can be expressed as a combination of a Signal Function and a Noise term: \\[ y = f(x) + \\varepsilon \\] The two terms originate from Engineering, where Signal refers to the meaningful component of the data while Noise refers to the random variation that inteferes with the signal. Statistical learning models are distinguished based on their signal function. There are two main kinds of models: Parametric Models Non-Parametric Assumes DV follows a specific functional form Does not assume any functional form Function parameters determined from the data - Does not require a large amount of data Requires a large amount of data to work well EG. Linear Regression EG. Clustering | May not fit the data well | Fits the data well | | Described by parameters | No parameters used | | Simple to implement | Requires large amount of data to implement | | Risk that assumed function is wrong | Does not make any assumption about data | They can also be further split according to: Supervised Learning Unsupervised Learning Specified DV to supervise the learning No specific variable chosen Inference/Prediction with respect to the DV Inference/Prediction for all variables EG. Linear Regression EG. Clustering Regression vs classification classification - classifying the observations to a certian level","title":"Statistical Learning"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#model-accuracy","text":"Since the end goal of the model is to make predictions on new unobserved data, the quality of the model should be evaluated against its performance on unobserved data as well. The observed data used to create the model is known as the Training Data as it helps train the signal function to identify relationships between variables, while the unobserved data used to evaluate the model is known as the Test Data . The quality of the model can then be quantified by the extent to which the model predictions match the data . Similar to regression, this quantity is known as the Error of the model and is summarized through the Mean Square Error (MSE) statistic. The MSE is the average of the sum of squared errors and can be calculated for both the training and test data: \\[ \\begin{aligned} \\text{Training MSE} &= \\frac{[y_i - \\hat{f}(x_0)]^2}{n_{training}} \\\\ \\text{Test MSE} &= \\frac{[y_0 - \\hat{f}(x_0)]^2}{n_{test}} \\end{aligned} \\] Note that this is different from the MSE defined in ANOVA, where the MS is divded by its degrees of freedom rather than the number of observations. The MSE defined in this section is a general concept while the ANOVA MSE is a purely regression concept. The Training MSE reflects the goodness of fit of the model while the Test MSE reflects its prediction accuracy. As alluded to earlier, the goal of statistical learning is to choose the model with the lowest Test MSE . In general, the training MSE should always be smaller than the testing MSE, which is why they are not interchangeable. This is because all models are trained to match the training data to various extents, thus they should naturally have relatively smaller errors. On the flipside, test MSE should always be higher because the model is likely to have mistakenly captured some of the noise in the training data that do not generalize to the test data, resulting in a higher testing error. The extent of the difference is dependent on how well the model fits the training data; the extent to which it learns from it: High Flexibility/Complexity - Tends to overfit the training data; matches data too much; learns too much Low Flexibility/Complexity - Tends to underfit the training data; matches data too little; learns too little This is not to say that low flexibility models are better. In fact, some level of flexibility is needed for the model to pick up most of the signals in the training data, but not too much such that the noise is captured as well. Thus, the test MSE generally decreases with flexibility up till a certain point, following which it increases, forming a U shaped curve :","title":"Model Accuracy"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#bias-variance-tradeoff","text":"The test MSE can be better understood by decomposing it into its consistuent commponents: \\[ \\text{MSE} = \\text{Bias}[\\hat{f}(x_0)]^2 + \\text{Variance}[\\hat{f}(x_0)] \\] The Bias of the model (also known as the Accuracy ) is the difference in expected value of the estimated signal function and the true signal function. More complex models are better able to capture the signal in the data, thus tends to have a lower bias. The Variance of the model (also known as Precision ) is the change in the estimated signal function across different datasets. Ideally, the model should have low variance such that it would be relatively stable across different training data. While more complex models are better able to capture signals, this makes them prone to overfitting and hence more sensitive to differences in the training data , leading to higher variance. Note that although Precision & Accuracy are both synonyms in English, they have distinct meanings in statistics. Ideally, a model should have both low bias and low variance. However, as explained above, there is an inherent tension between Bias and Variance due to the complexity of the model, known as the Bias-Variance Tradeoff . A relatively simple model (underfitted) tends to have a high bias but low variance . As the complexity increases, the bias initially decreases more than the variance increases , causing the test MSE to fall. At some point, the model becomes too complex (overfitted), where the increase in variance outweighs the fall in bias , resulting in the U-shaped curve as seen previously. Thus, the goal is to find an optimal balance in between Bias and Variance where the test MSE is minimized.","title":"Bias Variance Tradeoff"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#resampling-methods","text":"","title":"Resampling Methods"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#validation-set","text":"","title":"Validation Set"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#loo-cross-validation","text":"","title":"LOO Cross Validation"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#k-fold-cross-validation","text":"","title":"K fold Cross Validation"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#model-selection","text":"","title":"Model Selection"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#feature-selection","text":"","title":"Feature Selection"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#forward-stepwise-selection","text":"","title":"Forward Stepwise Selection"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#backward-stepwise-selection","text":"","title":"Backward Stepwise Selection"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#stepwise-selection","text":"","title":"Stepwise Selection"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/#shrinkage-methods","text":"","title":"Shrinkage Methods"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/6.%20Generalized%20Linear%20Models/","text":"","title":"6. Generalized Linear Models"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/7.%20Time%20Series%20Models/","text":"","title":"7. Time Series Models"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/8.%20Tree%20Models/","text":"","title":"8. Tree Models"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/9.%20Principal%20Component%20Analysis/","text":"","title":"9. Principal Component Analysis"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/","text":"Review of Statistical Theory This is meant to be a quick review of the basic statistical concepts of VEE: Mathematical Statistics and other undergrad statitics courses that will be relevant for this exam. Overview of Statistics Statistics is a discipline revolving around data. A Population refers to the theoretical set of all possible data of the event of interest. The goal of statistics is to determine certain attributes that summarizes or describes the population, known as Parameters . However, it is impossible to study the entire population at once, thus a subset of the population is studied instead, known as the Sample . Attributes that summarize or describe the sample are known as Statistics . Ideally, the sample is representative of the population, which means that findings from the sample can be applied to the population as a whole. This means that the sample statistics can be used estimate population parameters. We distinguish between the two (when they have the same notation) through the Hat accent (^) - Population Parameters are their written without the hat ( \\(x\\) ) while their corresponding sample statistics are written with the hat ( \\(\\hat{x}\\) ). Common Sample Statistics The Mean is the average of the population. Population Mean Sample Mean \\(\\mu = \\sum\\limits_{i=i}^n x_i * p(x_i)\\) \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=i}^n x_i\\) The Variance measures the spread of values about the mean. However, the units of Variance are unintuitive, thus we consider the Standard Deviation for more practical purposes, which is the square root of the variance . Population Variance Sample Variance \\(\\sigma^2 = \\sum\\limits_{i=i}^n (x_i - \\mu)^2 * p(x_i)\\) \\(s^2 = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x})^2\\) Covariance is a measure of the linear relationship between two variables: Positive Covariance - Variables move in the same direction Negative Covariance - Variables move in opposite directions Population Covariance Sample Covariance \\(\\sigma_{x, y} = \\mu_{xy} - \\mu_x \\mu_y\\) \\(\\hat{\\sigma}_{x, y} = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x}) * (y_i - \\bar{y})\\) However, there are two issues with Covariance - the units are unintuitive (similar to variance) and there is no benchmark as to what constitutes a strong/weak relationship. Thus, the Correlation is an adjusted measure of the relationship between -1 and 1 . Population Correlation Sample Correlation \\(\\rho = \\frac{\\sigma_{x,y}}{\\sigma_x * \\sigma_y}\\) \\(r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x} \\sum(y_i - \\bar{y})}\\) Properties of Estimators Biased Consistent Efficient Degrees of freedom Sampling Distribution Whenever a sample is drawn from a population and a statistic is calculated, it is known as a Point Estimate . Due to measurement error, a different sample would be drawn each time, thus leading to a different point estimate . If this process were to be repeated a large number times, the probability distribution of the resulting point estimates is known as the Sampling Distribution of the statistic. There is no rule surrounding the sampling distribution - it depends on the distribution of the population, statistic being measured, sampling method etc. The standard deviation of this sampling distribution is known as the Standard Error of the statistic: \\[ \\sigma_{\\theta} = \\sqrt {\\sigma^2_{\\theta}} \\] A special case is the Sample Mean . If the population is normally distributed, then it is normally distributed as well. Regardless of the population distribution, it is also approximately normally distributed through the Central Limit Theorem or Law of Large Numbers . \\[ \\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\] Following that, we can compute the standard error: \\[ \\sigma_{\\bar{x}} = \\sqrt \\frac{\\sigma^2_{\\bar{x}}}{n} = \\frac{\\sigma}{\\sqrt{n}} \\] However, the population variance is usually unknown. Thus, it can be approximated using the Sample Variance, which is an unbiased estimator for it. The result is known as an Estimate for the Standard Error : \\[ \\hat{\\sigma}_{\\bar{x}} = \\sqrt \\frac{s^2}{n} = \\frac{s}{\\sqrt{n}} \\] Note that only the sample variance is an unbiased estimator for the population variance. Although it may look like it, the sample SD is NOT an unbiased estimator for the population SD. Confidence Interval Given that there is only one true value for the population parameter and a whole distribution of estimators, it is unlikely that a point estimate will be equal to the population parameter. Thus, instead of a point estimate, a range of estimates is used, known as a Confidence Interval . The interval is made a chosen Confidence Level which represents the proportion of confidence intervals that will contain the true value . In other words, if a large number of confidence intervals constructed in the same manner were to be made, \\((1-\\alpha)%\\) of them would contain the true value. \\[ P(- \\text{Margin of Error} < \\theta < \\text{Margin of Error}) = 1 - \\alpha \\] The Margin of Error represents the range of values on either side of the point estimate that the true value could lie. For instance, for a 95% confidence interval, the confidence lies within the 0.025 and 0.975 percentile of the sampling distribution. The margin of error can be calculated by finding the corresponding values of the sampling distribution at these percentiles. Consider the 95% confidence interval for the Sample Mean , which is normally distributed. For convenience, it is usually normalized such that it will become a Standard Normal Distribution : \\[ \\begin{aligned} P(-1.96 < \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt n}} < 1.96) &= 0.95 \\\\ P(\\bar{x} - 1.96 \\frac{\\sigma}{\\sqrt n} < \\mu < \\bar{x} + 1.96 \\frac{\\sigma}{\\sqrt n}) &= 0.95 \\\\ \\end{aligned} \\] \\[ \\therefore \\text{Margin of Error} = \\bar{x} + Z_{\\frac{\\alpha}{2}} * \\frac{\\sigma}{\\sqrt n} \\] Hypothesis Testing Hypothesis Testing is a formal method of making inferences about the population parameters based on the sample statistic. It starts with a Hypothesis which is a conjecture about the population parameters: Null Hypothesis - What is currently believed to be true Alternative Hypothesis - What is to be proven A Test Statistic is then calculated, which would quantify the behaviour of population such that it would distinguish the null and alternative hypothesis. Assuming the Null Hypothesis is true , the sampling distribution of the test statistic is determined. From the sampling distribution, the p-value is calculated, which is the probability of observing the calculated sample statistic or more extreme . If the p-value is smaller than a pre-determined level of Statistical Significance ( \\(\\alpha\\) ), then the test is \"successful\" and the null hypothesis is rejected. If not, then the test \"fails\" and the null is not rejected. Note that it rejecting the null hypothesis does NOT mean that the alternative hypothesis is accepted - EG. Rejected then hypothesis of a large paw print being from a bear does not mean that it is from Bigfoot. Thus, the hypotheses are usually constructed such that the two hypothesis are complementary , such that rejecting the null allows acceptance of the alternative, leading to a definitive insight. In layman terms, a hypothesis test is a test of extremeness . Assuming the null hypothesis is true, how extreme (low probability) is the observed sample? If the p-value is sufficiently low, it means that the sample is rare - it could be due to pure chance or that it is actually not rare because the null is not true. We distinguish between the two mathematically through \\(\\alpha\\) . It is the probability of a False Positive - that the result obtained was by pure chance. It is typically set at 5%, which means that 5% of all statistics calculated are expected to be extreme under the null. Thus, if the p-value of the sample is smaller than \\(\\alpha\\) , then it is likely that the observation was not due to chance but instead because the null was false . Alternatively, instead of comparing p-value to \\(alpha\\) , the test-statistic and the corresponding value of \\(\\alpha\\) on the sampling distribution can be used. It is known as the Critical Value , which represents the boundary of Reject Null Do not Reject Null p-value smaller than \\(\\alpha\\) p-value smaller than \\(\\alpha\\) test-statistic larger than critical value test-statistic smaller than critical value Let the random variable \\(T\\) denote the test statistics. There are many different kinds of test statistics depending on the distribution and what is being investigated. Z-statistic The most simple test statistic involve the Sample Mean , which is normally distributed: \\[ T = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] If the population variance is known , then the test statistic has a Standard Normal Distribution and thus the test-statistic is known as a Z-Statistic . \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\sqrt n \\\\ &= Z * \\sqrt n \\\\ \\therefore T &\\sim N(0,1) \\end{aligned} \\] t-statistic If the population variance is unknown, then it will be approximated by the Sample Variance . Through algebraic manipulation, the test statistic can still be expressed in the form of a Z variable, but with an additional term: \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}} \\\\ &= \\frac{\\bar{x} - \\mu}{s} * \\sqrt n \\\\ &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\frac{\\sigma}{s} * \\sqrt n \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{s^2}{\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{(n-1)s^2}{(n-1)\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt {\\frac{(n-1)s^2}{\\sigma^2} * \\frac{1}{n-1}} \\\\ \\end{aligned} \\] The additional term can be shown to have a Chi-Squared Distribution of \\(n-1\\) degrees of freedom, which by definition is the sum of \\(n-1\\) independent standard normal variables: \\[ \\begin{aligned} \\chi^2_n &= Z^2 \\\\ &= \\sum \\left(\\frac {\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &= \\sum \\left(\\frac {(x_i - \\bar{x}) + (\\bar{x} - \\mu)}{\\sigma}\\right)^2 \\\\ &= \\sum \\left(\\frac{x_i - \\bar{x}}{\\sigma}\\right)^2 + \\sum \\left(\\frac{\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &= \\frac{1}{\\sigma^2} \\sum (x_i - \\bar{x})^2 + \\sum Z^2 + 0 \\\\ &= \\frac{(n-1)}{\\sigma^2} \\frac{\\sum (x_i - \\bar{x})^2}{n-1} + \\chi^2_1 \\\\ &= \\frac{(n-1)s^2}{\\sigma^2} + \\chi^2_1 \\\\ \\end{aligned} \\] \\[\\therefore \\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\] Thus, the test statistic is the ratio of a Standard Normal Variable to the squareroot of a Chi Squared Variable (divided by its degrees of freedom). By definition, this test statistic has a t-distribution with the same degrees of freedom and is known as the t-statistic : \\[ \\begin{align*} T &= \\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\\\ &= t_{n-1} * \\sqrt{n} \\\\ \\end{align*} \\] \\[\\therefore T \\sim t_{n-1}\\] Despite the slightly convoluted proof, the t-distribution is simply a standard normal distribution with heavier tails . This means that extreme values are slightly more likely, which is meant to account for the increase in variability due to the use of the sample variance rather than the population variance. F-statistic The square of t-statisic has an F-Distribution , which is defined as the ratio of two independent chi-square variables (divided by their respective degrees of freedom). It has two dimensions for its degree of freedom, reflecting the two chi-square variables. \\[ F_{m,n} = \\frac{\\frac{\\chi_m}{m}}{\\frac{\\chi_n}{n}} \\] The square of a t-statistic follows an F-distribution because: The square of the standard normal variable in the numerator becomes a \\(\\chi_1\\) variable The squareroot is removed in the denominator, becoming a \\(\\chi_{n-1}\\) over its degree of freedom \\[ \\begin{aligned} t_{n-1}^2 &= \\left(\\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\right)^2 \\\\ &= \\frac{\\chi_1}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &= \\frac{\\frac{\\chi_1}{1}}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &= F_{1,n-1} * n \\end{aligned} \\] \\[ \\therefore t_{n-1}^2 \\sim F_{1, n-1} \\] Thus, the square of the t-statistic is known as the F-statistic , which is usually used to test for the equality of variance . Maximum Likelihood Estimation If the population distribution is known, there is an alternative method of estimating the parameters apart from calculating the corresponding sample statistics. We model a set of observations as a random sample from an unknown joint probability distribution which is expressed in terms of a set of parameters. The goal of maximum likelihood estimation is to determine the parameters for which the observed data have the highest joint probability. The goal of maximum likelihood estimation is to find the values of the model parameters that maximize the likelihood function over the parameter space","title":"**Review of Statistical Theory**"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#review-of-statistical-theory","text":"This is meant to be a quick review of the basic statistical concepts of VEE: Mathematical Statistics and other undergrad statitics courses that will be relevant for this exam.","title":"Review of Statistical Theory"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#overview-of-statistics","text":"Statistics is a discipline revolving around data. A Population refers to the theoretical set of all possible data of the event of interest. The goal of statistics is to determine certain attributes that summarizes or describes the population, known as Parameters . However, it is impossible to study the entire population at once, thus a subset of the population is studied instead, known as the Sample . Attributes that summarize or describe the sample are known as Statistics . Ideally, the sample is representative of the population, which means that findings from the sample can be applied to the population as a whole. This means that the sample statistics can be used estimate population parameters. We distinguish between the two (when they have the same notation) through the Hat accent (^) - Population Parameters are their written without the hat ( \\(x\\) ) while their corresponding sample statistics are written with the hat ( \\(\\hat{x}\\) ).","title":"Overview of Statistics"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#common-sample-statistics","text":"The Mean is the average of the population. Population Mean Sample Mean \\(\\mu = \\sum\\limits_{i=i}^n x_i * p(x_i)\\) \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=i}^n x_i\\) The Variance measures the spread of values about the mean. However, the units of Variance are unintuitive, thus we consider the Standard Deviation for more practical purposes, which is the square root of the variance . Population Variance Sample Variance \\(\\sigma^2 = \\sum\\limits_{i=i}^n (x_i - \\mu)^2 * p(x_i)\\) \\(s^2 = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x})^2\\) Covariance is a measure of the linear relationship between two variables: Positive Covariance - Variables move in the same direction Negative Covariance - Variables move in opposite directions Population Covariance Sample Covariance \\(\\sigma_{x, y} = \\mu_{xy} - \\mu_x \\mu_y\\) \\(\\hat{\\sigma}_{x, y} = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x}) * (y_i - \\bar{y})\\) However, there are two issues with Covariance - the units are unintuitive (similar to variance) and there is no benchmark as to what constitutes a strong/weak relationship. Thus, the Correlation is an adjusted measure of the relationship between -1 and 1 . Population Correlation Sample Correlation \\(\\rho = \\frac{\\sigma_{x,y}}{\\sigma_x * \\sigma_y}\\) \\(r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x} \\sum(y_i - \\bar{y})}\\) Properties of Estimators Biased Consistent Efficient Degrees of freedom","title":"Common Sample Statistics"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#sampling-distribution","text":"Whenever a sample is drawn from a population and a statistic is calculated, it is known as a Point Estimate . Due to measurement error, a different sample would be drawn each time, thus leading to a different point estimate . If this process were to be repeated a large number times, the probability distribution of the resulting point estimates is known as the Sampling Distribution of the statistic. There is no rule surrounding the sampling distribution - it depends on the distribution of the population, statistic being measured, sampling method etc. The standard deviation of this sampling distribution is known as the Standard Error of the statistic: \\[ \\sigma_{\\theta} = \\sqrt {\\sigma^2_{\\theta}} \\] A special case is the Sample Mean . If the population is normally distributed, then it is normally distributed as well. Regardless of the population distribution, it is also approximately normally distributed through the Central Limit Theorem or Law of Large Numbers . \\[ \\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\] Following that, we can compute the standard error: \\[ \\sigma_{\\bar{x}} = \\sqrt \\frac{\\sigma^2_{\\bar{x}}}{n} = \\frac{\\sigma}{\\sqrt{n}} \\] However, the population variance is usually unknown. Thus, it can be approximated using the Sample Variance, which is an unbiased estimator for it. The result is known as an Estimate for the Standard Error : \\[ \\hat{\\sigma}_{\\bar{x}} = \\sqrt \\frac{s^2}{n} = \\frac{s}{\\sqrt{n}} \\] Note that only the sample variance is an unbiased estimator for the population variance. Although it may look like it, the sample SD is NOT an unbiased estimator for the population SD.","title":"Sampling Distribution"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#confidence-interval","text":"Given that there is only one true value for the population parameter and a whole distribution of estimators, it is unlikely that a point estimate will be equal to the population parameter. Thus, instead of a point estimate, a range of estimates is used, known as a Confidence Interval . The interval is made a chosen Confidence Level which represents the proportion of confidence intervals that will contain the true value . In other words, if a large number of confidence intervals constructed in the same manner were to be made, \\((1-\\alpha)%\\) of them would contain the true value. \\[ P(- \\text{Margin of Error} < \\theta < \\text{Margin of Error}) = 1 - \\alpha \\] The Margin of Error represents the range of values on either side of the point estimate that the true value could lie. For instance, for a 95% confidence interval, the confidence lies within the 0.025 and 0.975 percentile of the sampling distribution. The margin of error can be calculated by finding the corresponding values of the sampling distribution at these percentiles. Consider the 95% confidence interval for the Sample Mean , which is normally distributed. For convenience, it is usually normalized such that it will become a Standard Normal Distribution : \\[ \\begin{aligned} P(-1.96 < \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt n}} < 1.96) &= 0.95 \\\\ P(\\bar{x} - 1.96 \\frac{\\sigma}{\\sqrt n} < \\mu < \\bar{x} + 1.96 \\frac{\\sigma}{\\sqrt n}) &= 0.95 \\\\ \\end{aligned} \\] \\[ \\therefore \\text{Margin of Error} = \\bar{x} + Z_{\\frac{\\alpha}{2}} * \\frac{\\sigma}{\\sqrt n} \\]","title":"Confidence Interval"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#hypothesis-testing","text":"Hypothesis Testing is a formal method of making inferences about the population parameters based on the sample statistic. It starts with a Hypothesis which is a conjecture about the population parameters: Null Hypothesis - What is currently believed to be true Alternative Hypothesis - What is to be proven A Test Statistic is then calculated, which would quantify the behaviour of population such that it would distinguish the null and alternative hypothesis. Assuming the Null Hypothesis is true , the sampling distribution of the test statistic is determined. From the sampling distribution, the p-value is calculated, which is the probability of observing the calculated sample statistic or more extreme . If the p-value is smaller than a pre-determined level of Statistical Significance ( \\(\\alpha\\) ), then the test is \"successful\" and the null hypothesis is rejected. If not, then the test \"fails\" and the null is not rejected. Note that it rejecting the null hypothesis does NOT mean that the alternative hypothesis is accepted - EG. Rejected then hypothesis of a large paw print being from a bear does not mean that it is from Bigfoot. Thus, the hypotheses are usually constructed such that the two hypothesis are complementary , such that rejecting the null allows acceptance of the alternative, leading to a definitive insight. In layman terms, a hypothesis test is a test of extremeness . Assuming the null hypothesis is true, how extreme (low probability) is the observed sample? If the p-value is sufficiently low, it means that the sample is rare - it could be due to pure chance or that it is actually not rare because the null is not true. We distinguish between the two mathematically through \\(\\alpha\\) . It is the probability of a False Positive - that the result obtained was by pure chance. It is typically set at 5%, which means that 5% of all statistics calculated are expected to be extreme under the null. Thus, if the p-value of the sample is smaller than \\(\\alpha\\) , then it is likely that the observation was not due to chance but instead because the null was false . Alternatively, instead of comparing p-value to \\(alpha\\) , the test-statistic and the corresponding value of \\(\\alpha\\) on the sampling distribution can be used. It is known as the Critical Value , which represents the boundary of Reject Null Do not Reject Null p-value smaller than \\(\\alpha\\) p-value smaller than \\(\\alpha\\) test-statistic larger than critical value test-statistic smaller than critical value Let the random variable \\(T\\) denote the test statistics. There are many different kinds of test statistics depending on the distribution and what is being investigated.","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#z-statistic","text":"The most simple test statistic involve the Sample Mean , which is normally distributed: \\[ T = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] If the population variance is known , then the test statistic has a Standard Normal Distribution and thus the test-statistic is known as a Z-Statistic . \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\sqrt n \\\\ &= Z * \\sqrt n \\\\ \\therefore T &\\sim N(0,1) \\end{aligned} \\]","title":"Z-statistic"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#t-statistic","text":"If the population variance is unknown, then it will be approximated by the Sample Variance . Through algebraic manipulation, the test statistic can still be expressed in the form of a Z variable, but with an additional term: \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}} \\\\ &= \\frac{\\bar{x} - \\mu}{s} * \\sqrt n \\\\ &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\frac{\\sigma}{s} * \\sqrt n \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{s^2}{\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{(n-1)s^2}{(n-1)\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt {\\frac{(n-1)s^2}{\\sigma^2} * \\frac{1}{n-1}} \\\\ \\end{aligned} \\] The additional term can be shown to have a Chi-Squared Distribution of \\(n-1\\) degrees of freedom, which by definition is the sum of \\(n-1\\) independent standard normal variables: \\[ \\begin{aligned} \\chi^2_n &= Z^2 \\\\ &= \\sum \\left(\\frac {\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &= \\sum \\left(\\frac {(x_i - \\bar{x}) + (\\bar{x} - \\mu)}{\\sigma}\\right)^2 \\\\ &= \\sum \\left(\\frac{x_i - \\bar{x}}{\\sigma}\\right)^2 + \\sum \\left(\\frac{\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &= \\frac{1}{\\sigma^2} \\sum (x_i - \\bar{x})^2 + \\sum Z^2 + 0 \\\\ &= \\frac{(n-1)}{\\sigma^2} \\frac{\\sum (x_i - \\bar{x})^2}{n-1} + \\chi^2_1 \\\\ &= \\frac{(n-1)s^2}{\\sigma^2} + \\chi^2_1 \\\\ \\end{aligned} \\] \\[\\therefore \\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\] Thus, the test statistic is the ratio of a Standard Normal Variable to the squareroot of a Chi Squared Variable (divided by its degrees of freedom). By definition, this test statistic has a t-distribution with the same degrees of freedom and is known as the t-statistic : \\[ \\begin{align*} T &= \\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\\\ &= t_{n-1} * \\sqrt{n} \\\\ \\end{align*} \\] \\[\\therefore T \\sim t_{n-1}\\] Despite the slightly convoluted proof, the t-distribution is simply a standard normal distribution with heavier tails . This means that extreme values are slightly more likely, which is meant to account for the increase in variability due to the use of the sample variance rather than the population variance.","title":"t-statistic"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#f-statistic","text":"The square of t-statisic has an F-Distribution , which is defined as the ratio of two independent chi-square variables (divided by their respective degrees of freedom). It has two dimensions for its degree of freedom, reflecting the two chi-square variables. \\[ F_{m,n} = \\frac{\\frac{\\chi_m}{m}}{\\frac{\\chi_n}{n}} \\] The square of a t-statistic follows an F-distribution because: The square of the standard normal variable in the numerator becomes a \\(\\chi_1\\) variable The squareroot is removed in the denominator, becoming a \\(\\chi_{n-1}\\) over its degree of freedom \\[ \\begin{aligned} t_{n-1}^2 &= \\left(\\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\right)^2 \\\\ &= \\frac{\\chi_1}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &= \\frac{\\frac{\\chi_1}{1}}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &= F_{1,n-1} * n \\end{aligned} \\] \\[ \\therefore t_{n-1}^2 \\sim F_{1, n-1} \\] Thus, the square of the t-statistic is known as the F-statistic , which is usually used to test for the equality of variance .","title":"F-statistic"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/Review%20of%20Statistical%20Theory/#maximum-likelihood-estimation","text":"If the population distribution is known, there is an alternative method of estimating the parameters apart from calculating the corresponding sample statistics. We model a set of observations as a random sample from an unknown joint probability distribution which is expressed in terms of a set of parameters. The goal of maximum likelihood estimation is to determine the parameters for which the observed data have the highest joint probability. The goal of maximum likelihood estimation is to find the values of the model parameters that maximize the likelihood function over the parameter space","title":"Maximum Likelihood Estimation"}]}