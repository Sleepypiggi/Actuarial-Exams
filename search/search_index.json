{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Actuarial Exam Notes","text":"<p>Test</p>"},{"location":"Placeholder/","title":"Work in progress!","text":""},{"location":"0.%20Foundational/1.%20Review%20of%20Linear%20Algebra/","title":"Review of Linear Algebra","text":"<p>Vector Matrix System of equations</p>"},{"location":"0.%20Foundational/2.%20Review%20of%20Calculus/","title":"Review of Calculus","text":"<p>Differentiation Application of differentiation - Optimization</p> <p>Integration Application  - Area/Volume</p>"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/","title":"Review of Statistical Theory","text":"<p>This is meant to be a quick review of the basic statistical concepts of VEE: Mathematical Statistics and other undergrad statitics courses that will be relevant for this exam.</p>"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#overview-of-statistics","title":"Overview of Statistics","text":"<p>Statistics is a discipline revolving around data.</p> <p>A Population refers to the theoretical set of all possible data of the event of interest. The goal of statistics is to determine certain attributes that summarizes or describes the population, known as Parameters.</p> <p>However, it is impossible to study the entire population at once, thus a subset of the population is studied instead, known as the Sample. Attributes that summarize or describe the sample are known as Statistics.</p> <p>Ideally, the sample is representative of the population, which means that findings from the sample can be applied to the population as a whole. This means that the sample statistics can be used estimate population parameters.</p> <p>We distinguish between the two (when they have the same notation) through the Hat accent (^) - Population Parameters are their written without the hat (\\(x\\)) while their corresponding sample statistics are written with the hat (\\(\\hat{x}\\)).</p>"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#common-sample-statistics","title":"Common Sample Statistics","text":"<p>The Mean is the average of the population.</p> <p> Population Mean Sample Mean \\(\\mu = \\sum\\limits_{i=i}^n x_i * p(x_i)\\) \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=i}^n x_i\\) <p></p> <p>The Variance measures the spread of values about the mean. However, the units of Variance are unintuitive, thus we consider the Standard Deviation for more practical purposes, which is the square root of the variance.</p> <p> Population Variance Sample Variance \\(\\sigma^2 = \\sum\\limits_{i=i}^n (x_i - \\mu)^2 * p(x_i)\\) \\(s^2 = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x})^2\\) <p></p> <p>Covariance is a measure of the linear relationship between two variables:</p> <ul> <li>Positive Covariance - Variables move in the same direction</li> <li>Negative Covariance - Variables move in opposite directions</li> </ul> <p> Population Covariance Sample Covariance \\(\\sigma_{x, y} = \\mu_{xy} - \\mu_x \\mu_y\\) \\(\\hat{\\sigma}_{x, y} = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x}) * (y_i - \\bar{y})\\) <p></p> <p>However, there are two issues with Covariance - the units are unintuitive (similar to variance) and there is no benchmark as to what constitutes a strong/weak relationship. Thus, the Correlation is an adjusted measure of the relationship between -1 and 1.</p> <p> Population Correlation Sample Correlation \\(\\rho = \\frac{\\sigma_{x,y}}{\\sigma_x * \\sigma_y}\\) \\(r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x} \\sum(y_i - \\bar{y})}\\) <p></p> <p>Properties of Estimators Biased Consistent Efficient Degrees of freedom</p>"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#sampling-distribution","title":"Sampling Distribution","text":"<p>Whenever a sample is drawn from a population and a statistic is calculated, it is known as a Point Estimate.</p> <p>Due to measurement error, a different sample would be drawn each time, thus leading to a different point estimate. If this process were to be repeated a large number times, the probability distribution of the resulting point estimates is known as the Sampling Distribution of the statistic.</p> <p>There is no rule surrounding the sampling distribution - it depends on the distribution of the population, statistic being measured, sampling method etc.</p> <p>The standard deviation of this sampling distribution is known as the Standard Error of the statistic:</p> \\[ \\sigma_{\\theta} = \\sqrt {\\sigma^2_{\\theta}} \\] <p>A special case is the Sample Mean. If the population is normally distributed, then it is normally distributed as well. Regardless of the population distribution, it is also approximately normally distributed through the Central Limit Theorem or Law of Large Numbers.</p> \\[ \\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\] <p>Following that, we can compute the standard error:</p> \\[ \\sigma_{\\bar{x}} = \\sqrt \\frac{\\sigma^2_{\\bar{x}}}{n} = \\frac{\\sigma}{\\sqrt{n}} \\] <p>However, the population variance is usually unknown. Thus, it can be approximated using the Sample Variance, which is an unbiased estimator for it. The result is known as an Estimate for the Standard Error:</p> \\[ \\hat{\\sigma}_{\\bar{x}} = \\sqrt \\frac{s^2}{n} = \\frac{s}{\\sqrt{n}} \\] <p>Note that only the sample variance is an unbiased estimator for the population variance. Although it may look like it, the sample SD is NOT an unbiased estimator for the population SD.</p>"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#confidence-interval","title":"Confidence Interval","text":"<p>Given that there is only one true value for the population parameter and a whole distribution of estimators, it is unlikely that a point estimate will be equal to the population parameter. Thus, instead of a point estimate, a range of estimates is used, known as a Confidence Interval.</p> <p>The interval is made a chosen Confidence Level which represents the proportion of confidence intervals that will contain the true value. In other words, if a large number of confidence intervals constructed in the same manner were to be made, \\((1-\\alpha)%\\) of them would contain the true value.</p> \\[ P(- \\text{Margin of Error} &lt; \\theta &lt; \\text{Margin of Error}) = 1 - \\alpha \\] <p>The Margin of Error represents the range of values on either side of the point estimate that the true value could lie. For instance, for a 95% confidence interval, the confidence lies within the 0.025 and 0.975 percentile of the sampling distribution.</p> <p>The margin of error can be calculated by finding the corresponding values of the sampling distribution at these percentiles.</p> <p>Consider the 95% confidence interval for the Sample Mean, which is normally distributed. For convenience, it is usually normalized such that it will become a Standard Normal Distribution:</p> \\[ \\begin{aligned} P(-1.96 &lt; \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt n}} &lt; 1.96) &amp;= 0.95 \\\\ P(\\bar{x} - 1.96 \\frac{\\sigma}{\\sqrt n} &lt; \\mu &lt; \\bar{x} + 1.96 \\frac{\\sigma}{\\sqrt n}) &amp;= 0.95 \\\\ \\end{aligned} \\] \\[ \\therefore \\text{Margin of Error} = \\bar{x} + Z_{\\frac{\\alpha}{2}} * \\frac{\\sigma}{\\sqrt n} \\] <p></p>"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#hypothesis-testing","title":"Hypothesis Testing","text":"<p>Hypothesis Testing is a formal method of making inferences about the population parameters based on the sample statistic.</p> <p>It starts with a Hypothesis which is a conjecture about the population parameters:</p> <ul> <li>Null Hypothesis - What is currently believed to be true</li> <li>Alternative Hypothesis - What is to be proven</li> </ul> <p>A Test Statistic is then calculated, which would quantify the behaviour of population such that it would distinguish the null and alternative hypothesis.</p> <p>Assuming the Null Hypothesis is true, the sampling distribution of the test statistic is determined. From the sampling distribution, the p-value is calculated, which is the probability of observing the calculated sample statistic or more extreme.</p> <p>If the p-value is smaller than a pre-determined level of Statistical Significance (\\(\\alpha\\)), then the test is \"successful\" and the null hypothesis is rejected. If not, then the test \"fails\" and the null is not rejected.</p> <p>Note that it rejecting the null hypothesis does NOT mean that the alternative hypothesis is accepted - EG. Rejected then hypothesis of a large paw print being from a bear does not mean that it is from Bigfoot.</p> <p>Thus, the hypotheses are usually constructed such that the two hypothesis are complementary, such that rejecting the null allows acceptance of the alternative, leading to a definitive insight.</p> <p>In layman terms, a hypothesis test is a test of extremeness. Assuming the null hypothesis is true, how extreme (low probability) is the observed sample? If the p-value is sufficiently low, it means that the sample is rare - it could be due to pure chance or that it is actually not rare because the null is not true.</p> <p>We distinguish between the two mathematically through \\(\\alpha\\). It is the probability of a False Positive - that the result obtained was by pure chance. It is typically set at 5%, which means that 5% of all statistics calculated are expected to be extreme under the null. Thus, if the p-value of the sample is smaller than \\(\\alpha\\), then it is likely that the observation was not due to chance but instead because the null was false.</p> <p></p> <p>Alternatively, instead of comparing p-value to \\(alpha\\), the test-statistic and the corresponding value of \\(\\alpha\\) on the sampling distribution can be used. It is known as the Critical Value, which represents the boundary of </p> <p> Reject Null Do not Reject Null p-value smaller than \\(\\alpha\\) p-value smaller than \\(\\alpha\\) test-statistic larger than critical value test-statistic smaller than critical value <p></p> <p>Let the random variable \\(T\\) denote the test statistics. There are many different kinds of test statistics depending on the distribution and what is being investigated.</p>"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#z-statistic","title":"Z-statistic","text":"<p>The most simple test statistic involve the Sample Mean, which is normally distributed:</p> \\[ T = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] <p>If the population variance is known, then the test statistic has a Standard Normal Distribution and thus the test-statistic is known as a Z-Statistic.</p> \\[ \\begin{aligned} T &amp;= \\frac{\\bar{x} - \\mu}{\\sigma} * \\sqrt n \\\\ &amp;= Z * \\sqrt n \\\\ \\therefore T &amp;\\sim N(0,1) \\end{aligned} \\]"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#t-statistic","title":"t-statistic","text":"<p>If the population variance is unknown, then it will be approximated by the Sample Variance. Through algebraic manipulation, the test statistic can still be expressed in the form of a Z variable, but with an additional term:</p> \\[ \\begin{aligned} T &amp;= \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}} \\\\ &amp;= \\frac{\\bar{x} - \\mu}{s} * \\sqrt n \\\\ &amp;= \\frac{\\bar{x} - \\mu}{\\sigma} * \\frac{\\sigma}{s} * \\sqrt n \\\\ &amp;= Z\\sqrt n \\div \\sqrt \\frac{s^2}{\\sigma^2} \\\\ &amp;= Z\\sqrt n \\div \\sqrt \\frac{(n-1)s^2}{(n-1)\\sigma^2} \\\\ &amp;= Z\\sqrt n \\div \\sqrt {\\frac{(n-1)s^2}{\\sigma^2} * \\frac{1}{n-1}} \\\\ \\end{aligned} \\] <p>The additional term can be shown to have a Chi-Squared Distribution of \\(n-1\\) degrees of freedom, which by definition is the sum of \\(n-1\\) independent standard normal variables:</p> \\[ \\begin{aligned} \\chi^2_n &amp;= Z^2 \\\\ &amp;= \\sum \\left(\\frac {\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &amp;= \\sum \\left(\\frac {(x_i - \\bar{x}) + (\\bar{x} - \\mu)}{\\sigma}\\right)^2 \\\\ &amp;= \\sum \\left(\\frac{x_i - \\bar{x}}{\\sigma}\\right)^2 + \\sum \\left(\\frac{\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &amp;= \\frac{1}{\\sigma^2} \\sum (x_i - \\bar{x})^2 + \\sum Z^2 + 0 \\\\ &amp;= \\frac{(n-1)}{\\sigma^2} \\frac{\\sum (x_i - \\bar{x})^2}{n-1} + \\chi^2_1 \\\\ &amp;= \\frac{(n-1)s^2}{\\sigma^2} + \\chi^2_1 \\\\ \\end{aligned} \\] \\[\\therefore \\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\] <p>Thus, the test statistic is the ratio of a Standard Normal Variable to the squareroot of a Chi Squared Variable (divided by its degrees of freedom). By definition, this test statistic has a t-distribution with the same degrees of freedom and is known as the t-statistic:</p> \\[ \\begin{align*} T &amp;= \\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\\\ &amp;= t_{n-1} * \\sqrt{n} \\\\ \\end{align*} \\] \\[\\therefore T \\sim t_{n-1}\\] <p>Despite the slightly convoluted proof, the t-distribution is simply a standard normal distribution with heavier tails. This means that extreme values are slightly more likely, which is meant to account for the increase in variability due to the use of the sample variance rather than the population variance.</p> <p></p>"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#f-statistic","title":"F-statistic","text":"<p>The square of t-statisic has an F-Distribution, which is defined as the ratio of two independent chi-square variables (divided by their respective degrees of freedom). It has two dimensions for its degree of freedom, reflecting the two chi-square variables.</p> \\[ F_{m,n} = \\frac{\\frac{\\chi_m}{m}}{\\frac{\\chi_n}{n}} \\] <p>The square of a t-statistic follows an F-distribution because:</p> <ol> <li>The square of the standard normal variable in the numerator becomes a \\(\\chi_1\\) variable</li> <li>The squareroot is removed in the denominator, becoming a \\(\\chi_{n-1}\\) over its degree of freedom</li> </ol> \\[ \\begin{aligned} t_{n-1}^2 &amp;= \\left(\\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\right)^2 \\\\ &amp;= \\frac{\\chi_1}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &amp;= \\frac{\\frac{\\chi_1}{1}}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &amp;= F_{1,n-1} * n \\end{aligned} \\] \\[ \\therefore t_{n-1}^2 \\sim F_{1, n-1} \\] <p>Thus, the square of the t-statistic is known as the F-statistic, which is usually used to test for the equality of variance.</p>"},{"location":"0.%20Foundational/3.%20Review%20of%20Statistical%20Theory/#maximum-likelihood-estimation","title":"Maximum Likelihood Estimation","text":"<p>If the population distribution is known, there is an alternative method of estimating the parameters apart from calculating the corresponding sample statistics.</p> <p>We model a set of observations as a random sample from an unknown joint probability distribution which is expressed in terms of a set of parameters. The goal of maximum likelihood estimation is to determine the parameters for which the observed data have the highest joint probability.</p> <p>The goal of maximum likelihood estimation is to find the values of the model parameters that maximize the likelihood function over the parameter space</p>"},{"location":"2.%20Actuarial%20Mathematics/Overview/","title":"Overview of Actuarial Mathematics","text":"<p>These set of exams cover the main concepts needed to evaluate insurance related risks. They are split into two main exams:</p> <ol> <li>Fundamentals of Actuarial Mathematics (FAM)</li> <li>Advanced Long Term or Short Term Actuarial Mathematics (ALTAM or ASTAM)</li> </ol> <p>This set of notes splits the FAM component into two seperate components for clarity, as the components are not related to one another:</p> <ol> <li>FAM-S (Short Term)</li> <li>FAM-L (Long Term)</li> </ol> <p>However, the actual FAM exam tests both components together in an MCQ format. It is regarded as one of the tougher examinations due to its wide breadth (due to the unrelated components).</p>"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/","title":"Survival Models","text":"<p>Survival Models are probability distributions that measure the time to failure of an entity, or phrased another way, the future lifetime of an entity. In an Actuarial context, it measures the time to death of a person.</p> <p>Since survival models measure time, they are denoted by the continuous random variable \\(T_i\\), where the subscript represents the age of the person being studied.</p>"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#newborn-lifetime","title":"Newborn Lifetime","text":"<p>The base survival model measures the future lifetime of a person aged 0 (newborn), denoted by \\(T_0\\).</p> <p>The CDF thus represents the probability that the newborn dies before age \\(t\\):</p> \\[ F_0(t) = P(T_0 \\le t) \\] <p>The complement of the CDF is the probability that the newborn survives past a certain age \\(t\\). From a different perspective, it can also be seen as the newborn dying after age \\(t\\). This is known as the Survival Function.</p> \\[ S_0(t) = P(T_0 \\ge t) = 1 - P(T_0 \\le t) = 1 - F_0(t) \\]"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#continuous-lifetime","title":"Continuous Lifetime","text":"<p>The above can be generalized for a person aged \\(x\\), denoted by \\(T_x\\). However, the intepretation of the functions have a slightly different meaning.</p> <p>The CDF instead represents the probability that the person dies within a certain number of years \\(t\\). Given how often this expression is used, it can be simplified to the notation \\(q\\).</p> \\[ F_x(t) = P(T_x \\le t) = {}_{t}q_{x} \\] <p>For convenience, \\({}_{1}q_{x}\\) is often written as just \\(q_{x}\\). In other words, \\(t\\) is often omitted if it is 1.</p> <p>It can also be written in terms of the newborn distribution, whereby the newborn dies by age \\(x+t\\), given that it survives till age \\(x\\):</p> \\[ P(T_0 \\le x + t | T_0 &gt; x) = \\frac{P(x &lt; T_0 &lt; x +t)}{P(T_0 &gt; x)} = \\frac{P(T_0 &lt; x +t)-P(T_0 &lt; x)}{P(T_0 &gt; x)} = \\frac{F_0(x+t) - F_0(x)}{S_0(x)} \\] <p></p> <p>Conversely, the survival function is the probability that the person survives another certain number of years \\(t\\) or dies after a certain number of years \\(t\\). It can be simplified to the notation \\(p\\).</p> \\[ S_x(t) = P(T_x \\ge t) = {}_{t}p_{x} \\] <p>Similarly, \\({}_{1}p_{x}\\) is often written as just \\(p_{x}\\).</p> <p>It can also be written in terms of the newborn distribution, whereby the newborn survives till age \\(x+t\\), given that it survives till age \\(x\\).</p> \\[ P(T_x \\ge t) = P(T_0 ge x + t | T_0 &gt; x) = \\frac{P(T_0 &gt; x + t)}{P(T_0 &gt; x)} = \\frac{S_0(x+t)}{S_0(x)} \\] <p>Note that since the surviving till age \\(x\\) is a subset of surviving till age \\(x+t\\), the conditional probability simply uses \\(x+t\\) in the numerator.</p> <p></p> <p>Recall that the two are complements of one another - the person will inevitably die.</p> \\[ \\begin{aligned} F_x(t) + S_x(t) &amp;= 1 \\\\ {}_{t}q_{x} + {}_{t}p_{x} &amp;= 1 \\end{aligned} \\]"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#deferred-death","title":"Deferred Death","text":"<p>The death of the person can also be \"deferred\", whereby the person survives \\(s\\) years then dies within the following \\(t\\) years. Following this definition:</p> \\[ {}_{s|t}q_{x} = {}_{s}p_{x} * {}_{t}q_{x+s} \\] <p>It is important to remember to update the starting age of the second term to the \"new age\" after the first term.</p> <p>From another perspective, it calculates the probability of the person dying within a specified range of time in the future:</p> \\[ {}_{s|t}q_{x} = {}_{s+t}q_{x} - {}_{s}q_{x} \\] <p></p>"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#force-of-mortality","title":"Force of Mortality","text":"<p>The force of mortality is the instantanous rate of death at a specific age.</p> <p>Similar to gradient, it is defined as the limit of probability of death in a given a period over the period itself, as the period tends to 0:</p> \\[ \\mu_x = \\lim_{h \\to 0} \\frac{{}_{h}q_{x}}{h} \\] <p>For very small periods of time, the force of mortality can be used to approximate the change in probability of death:</p> \\[ {}_{h}q_{x} = {}_{0}q_{x} + h * \\mu_x = h * \\mu_x \\]"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#probability-density-function","title":"Probability Density Function","text":"<p>The PDF of \\(T_x\\) is the differential of the CDF:</p> \\[ \\begin{aligned} f_x(t) &amp;= \\frac{d}{dt} F_x(t) \\\\ &amp;= \\frac{d}{dt} P(T_x \\le t) \\\\ &amp;= \\lim_{h \\to 0} \\frac{P(T_x \\le t+h) - P(T_x \\le t)}{h} \\\\ &amp;= \\lim_{h \\to 0} \\frac{P(x + t &lt; T_0 &lt; x + t +h)}{h * P(T_0 &gt; t)} \\end{aligned} \\] <p>The denominator of the limit can be adjusted to make it into a smoother conditional probability:</p> \\[ \\begin{aligned} f_x(t) &amp;= \\lim_{h \\to 0} \\frac{P(x + t &lt; T_0 &lt; x + t +h)}{h * P(T_0 &gt; t)} \\\\ &amp;= \\frac{P(T_0 &gt; t)}{P(T_0 &gt; x + t)} \\lim_{h \\to 0} \\frac{P(x + t &lt; T_0 &lt; x + t +h)}{h * P(T_0 &gt; x + t)} \\\\ &amp;= \\frac{P(T_0 &gt; t)}{P(T_0 &gt; x + t)} \\lim_{h \\to 0} \\frac{P(x + t &lt; T_0 &lt; x + t +h | T_0 &gt; x + t)}{h} \\\\ &amp;= {}_{t}p_{x} * \\mu_{x+t} \\end{aligned} \\]"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#cdf-and-survival-function","title":"CDF and Survival Function","text":"<p>The CDF can also be expressed as a function of the force of mortality, making use of the PDF derived above:</p> \\[ \\begin{aligned} F_x(t) &amp;= \\int_{0}^{t} f_x(t) \\\\ F_x(t) &amp;= \\int_{0}^{t} {}_{t}p_{x} * \\mu_{x+t} \\\\ {}_{t}q_{x} &amp;= \\int_{0}^{t} {}_{t}p_{x} * \\mu_{x+t} \\end{aligned} \\] <p>Similarly for the Survival Function,</p> \\[ \\begin{aligned} f_x(t) &amp;= {}_{t}p_{x} * \\mu_{x + t} \\\\ \\mu_{x + t} &amp;= \\frac{f_x(t)}{S_x(t)} \\end{aligned} \\] <p>Consider another expression for the PDF:</p> \\[ \\begin{aligned} f_x(t) &amp;= \\frac{d}{dt} F_x(t) \\\\ f_x(t) &amp;= \\frac{d}{dt} (1 - S_x(t)) \\\\ f_x(t) &amp;= -S'_x(t) \\end{aligned} \\] <p>Combining both the above,</p> \\[ \\begin{aligned} \\mu_{x + t} &amp;= \\frac{-S'_x(t)}{S_x(t)} \\\\ \\mu_{x + t} &amp;= - \\frac{d}{dt} (\\ln S_x(t)) \\\\ \\ln S_x(t) &amp;= - \\int_{0}^{t} \\mu_{x + t} \\\\ S_x(t) &amp;= e ^ {- \\int_{0}^{t} \\mu_{x + t}} \\\\ {}_{t}p_{x} &amp;= e ^ {- \\int_{0}^{t} \\mu_{x + t}} \\end{aligned} \\]"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#expectation-and-variance","title":"Expectation and Variance","text":"<p>The Expectation of the Future Lifetime is known as the Life Expectancy of the individual.</p> \\[ \\begin{aligned} \\mathring{e}_x &amp;= E(T_x) \\\\ &amp;= \\int_{0}^{t} t * {}_{t}p_{x} * \\mu_{x+t} \\\\ &amp;= \\int_{0}^{t} t * -S'_x(t) \\\\ &amp;= [-t * {}_{t}p_{x}]^{\\infty}_0 + \\int_{0}^{t} {}_{t}p_{x} \\\\ &amp;= \\int_{0}^{t} {}_{t}p_{x} \\end{aligned} \\] <p>Note that \\([-t * {}_{t}p_{x}]^{\\infty}_0\\) is always 0 as the probability of living forever is 0.</p> <p>Unfortunately, there is no simplification to be made for the Variance of the future lifetime:</p> \\[ \\begin{aligned} Var(T_x) &amp;= E(T^2_x) - [E(T_x)]^2 \\\\ &amp;= \\int_{0}^{t} t^2 * {}_{t}p_{x} * \\mu_{x+t} - [\\mathring{e}_x]^2 \\end{aligned} \\]"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#discrete-lifetime","title":"Discrete Lifetime","text":"<p>If the lifetime of the person is discrete instead, then it is known as the Curtate Future Lifetime of the person. It is denoted by \\(K_x\\).</p> <p>The intepretation is slightly different - \\(P(K_x = k)\\) represents the probability of the dying between age k (inclusive) and k+1 (Exclusive).</p> <p>This can be expressed in continuous terms as:</p> \\[ P(K_x = k) = P(k &lt; T_x &lt; K + 1) = {}_{k}p_{x} * q_{x+k} \\]"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#expectation-variance","title":"Expectation &amp; Variance","text":"<p>Similarly, the expected future lifetime can be calcualated:</p> \\[ \\begin{aligned} e_x &amp;=(K_x) \\\\ &amp;= \\sum_{k=0}^{\\infty} k * {}_{k}p_{x} * q_{x+k} \\\\ &amp;= {}_{1}p_{x} * q_{x+1} + 2{}_{2}p_{x} * q_{x+2} + 3{}_{3}p_{x} * q_{x+3} + ... \\\\ &amp;= ({}_{1}p_{x} * q_{x+1} + {}_{2}p_{x} * q_{x+2} + ...) + ({}_{2}p_{x} * q_{x+2} + {}_{3}p_{x} * q_{x+3} + ...) + ({}_{3}p_{x} * q_{x+3} + {}_{4}p_{x} * q_{x+4} + ...) \\\\ &amp;= \\sum_{k=1}^{\\infty} \\sum_{j=k}^{\\infty} {}_{j}p_{x}* q_{x+j} \\\\ &amp;= \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\end{aligned} \\] <p>Unfortunately, there is no simplification to be made for the Variance:</p> \\[ \\begin{aligned} Var(K_x) &amp;= E(K_x^2) - [E(K_x)]^2 \\\\ &amp;= \\sum_{k=0}^{\\infty} (k^2 {}_{k}p_{x} * q_{x+k}) - (e_x)^2 \\end{aligned} \\]"},{"location":"2.%20Actuarial%20Mathematics/ASA-FAML/1.%20Survival%20Models/#life-tables","title":"Life Tables","text":"<p>The Discrete Survival Model above can be expressed in tabular form as well, known as a Life Table. Instead of providing the probabilities directly, it provides the expected number of people alive at a given age \\(l_x\\). From these values, the probability rates can be calculated.</p> <p>The lowest age of the life table is known as the Starting Age, denoted by \\(\\alpha\\). Conversely, the highest age is known as the Terminal Age, denoted by \\(\\omega\\).</p> <p>The number of people alive at the starting age is known as the Radix of the life table, which is usually a large round number; \\(l_{\\alpha} = 100,000\\). All members of the poulation are expected to gradually die by the terminal age; \\(l_{\\omega} = 0\\).</p> \\[ l_x &gt; l_{x+t} \\] <p></p> <p>An additional column for \\(d_x\\) can also be calculated, which represents the expected number of people who die within a year; from age \\(x\\) to age \\(x+1\\). Note that because the life table is decreasing, it is written from the perspective of \\(x\\), not \\(x+1\\).</p> \\[ d_x = l_x - l_{x+1} \\] <p>If the life table is incomplete, \\(d_x\\) can be used to fill up the remaining information:</p> \\[ l_x - l_{x+n} = d_x + d_{x+1} + ... d_{x+n-1} \\] <p></p> <p>The probability of death/survival over a period can be calculated as the proportion of people who died/survived in that period:</p> \\[ \\begin{aligned} {}_{t}p_{x} &amp;= \\frac{l_{x+t}}{l_x} \\\\ {}_{t}q_{x} &amp;= \\frac{l_x - l_{x+t}}{l_x} \\\\ {}_{s|t}q_{x} &amp;= \\frac{l_{x+s} - l_{x+s+t}}{l_{x}} \\\\ \\end{aligned} \\] <p>If considering only the probabilities for a single period, then \\(d_x\\) can be used as well:</p> \\[ \\begin{aligned} p_{x} &amp;= \\frac{l_x - d_x}{l_x} \\\\ q_{x} &amp;= \\frac{d_x}{l_x} \\\\ \\end{aligned} \\] <p></p> <p>If the life table only includes the yearly probabilities, then the probability over a specific period can be calculated via first principles using a probability tree:</p> \\[ {}_{t}q_{x} = q_x + p_x * q_{x+1} + p_x * p_{x+1} * q_{x+2} + ... p_{x} * p _{x+1} * ... p_{x+n-2} * q_{x+n-1} \\] <p></p>"},{"location":"3.%20Predictive%20Analytics/Exam%20Overview/","title":"Overview of Predictive Analytics","text":"<p>Predictive Analytics is the usage of statistical models to analyze historical or current data to make predictions about the future or unknown events.</p> <p>Due to the growing relevance of Predictive Analytics, the SOA has added a significant amount of material on the topic into the credentialling process:</p> <ol> <li>Exam Statistics for Risk Modelling (SRM)</li> <li>Exam Predictive Analytics (PA)</li> <li>Exam Advanced Topics for Predictive Analytics (ATPA)</li> </ol> <p>Exam ATPA was added midway through 2022 to replace Exam IFM. Credit for IFM also counts towards credit for ATPA, thus this set of notes will NOT be covering ATPA.</p> <p>All of them share the same theme of working with statistical models:</p> <ol> <li>Constructing statistical models</li> <li>Intepreting their outputs</li> <li>Evaluating their performance</li> </ol> <p>Exam SRM covers the theory about the various types of models, tested in the typical MCQ format. Unlike the other exams where most of the questions are quantitative, most of SRM's questions are qualitative.</p> <p>Exam PA tests the same concepts in an applied manner, providing real data and a business problem to navigate through in a written format. Exam ATPA builds on both of these exams, covering more advanced concepts in a business context.</p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/","title":"Regression Overview","text":""},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#population-regression-model","title":"Population Regression Model","text":"<p>Regression is a statistical model that relates a Dependent Variable (DV) to one or more Independent Variables (IV). The dependent variable is regressed on to the independent variable.</p> <p>They are fundamentally a function of the independent variables and several Regression Parameters, \\(\\beta\\). The functional form of the regression is based on the relationship between the variables. The goal is to use a model that best captures the relationship between the variables.</p> \\[ Y = f(X, \\beta) \\] <p> Independent Variable(s) Dependent Variables Variable used to make predictions Variable being predicted Free to change the value Depends on the value of indepenent variable Deterministic Random Variable Denoted as \\(X\\) Denoted as \\(Y\\) <p></p> <p>To be precise, for every set of IVs, the DV has a Conditional Distribution dependent on the given IV.</p> <p>For instance, the the \\(Y\\) could take any possible value (Marginal Distribution), but given these set of \\(X\\), the possible values can be narrowed down to a certain range (conditional distribution).</p> \\[ \\displaylines{ Y \\sim Distribution \\\\ Y|X \\sim Conditional~Distribution} \\] <p>Thus, the output of the regression model is actually the Expected Value of the conditional distribution, \\(E(Y|X)\\), for every possible \\(X\\).</p> \\[ E(Y|X) = f(X, \\beta) \\] <p></p> <p>The actual observations are unlikely to be exactly equal to its expectation, thus there is a difference between an observation and the corresponding regression output. It known as the Random Error Term which accounts for all other factors that affect the DV that are not captured in the regression.</p> <p>This means that the relationship between the \\(Y\\) and \\(X\\) is only approximate, as the true relationship is probably different due to the possibility of unaccounted variables.</p> <p>Note that the sign of the errors are significant - positive implies the actual value lies above the regression output while negative implies it lies below.</p> \\[ \\varepsilon_i = y_i - f(x_i, \\beta) \\] <p></p> <p>This means that \\(Y\\) (not its expectation!) can be expressed as a sum of the regression model and the error terms:</p> \\[ y_i = f(x_i,\\beta) + \\varepsilon_i \\] <ul> <li>The Regression is known as the Systematic component as it is shared among all observations</li> <li>The Error is known as the Non-Systematic component as it is unique to each observation</li> </ul>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#sample-regression-model","title":"Sample Regression Model","text":"<p>In practice, the population is unobservable hence it is impossible to construct the population regression model. Instead, a regression model is constructed from a sample instead, which aims to estimate the population model.</p> \\[ \\hat{y} = f(X,\\hat{\\beta}) \\] <p>Similarly, the output of this model can be compared to the actual observations. However, the resulting difference is known as the Residual of the model, which like all the other components, is an estimate for the Error term.</p> \\[ \\hat{\\varepsilon_i} = y_i - \\hat{y_i} \\] <p>There are several different methods to estimate the regression parameters, but they usually involve minimizing the residuals of the model, such that the resulting model best fits the given sample, which is why it is also known as the Fitted Regression Model.</p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#hypothesis-testing","title":"Hypothesis Testing","text":"<p>Once a regression model has been fit, the next step is to determine if the relationship found in the sample is indicative of a relationship in the population.</p> <p>This can be determined through the following two-sided hypothesis test:</p> <ul> <li>Null Hypothesis: \\(\\beta_1 = 0\\)</li> <li>Alternative Hypothesis: \\(\\beta_1 \\ne 0\\)</li> </ul> <p>Under the null, the regression parameters are assumed to be 0, implying that there is no relationship between \\(Y\\) and \\(X\\). The test should reject the null, proving that there IS a relationship between the DV and IVs.</p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/1.%20Regression%20Overview/#prediction","title":"Prediction","text":"<p>Once the best model has been determined, it can be used to make Predictions about future unobserved values. Let these future values be denoted by the subscript \\(*\\).</p> <p></p> <p>These unobserved DVs come from the population, thus can be expressed as a function of the population model:</p> \\[ y_* = f(x_*, \\beta) + \\varepsilon_* \\] <p>The corresponding values from the sample regression model is an estimate for this unobserved value:</p> \\[ \\hat{y}_* = f(x_*, \\hat{\\beta}) \\] <p>Like before, the predicted value is unlikely to be exactly equal to the actual value. Thus, the difference between both values can be measured as the Prediction Error:</p> \\[ y_* - \\hat{y_*} = \\varepsilon_* + [f(x_*, \\beta) - f(x_*, \\hat{\\beta})] \\] <p>The prediction error is thus made up of two components:</p> <ol> <li>Inherent error present in the DV (\\(\\varepsilon_*\\))</li> <li>Error in estimating the population model (\\(f(x_*, \\beta) - f(x_*, \\hat{\\beta})\\))</li> </ol> <p>Based on the distribution of the prediction error, a Prediction Interval at a given confidence level can be calculated to accompany the regression estimate, which is essentially a confidence interval for the predicted value.</p> <p>Note that the prediction intervals will always be wider than confidence intervals. This is because CIs only takes into the account the error in estimating the population model/parameters while PIs take into account the inherent error of the DV as well.</p> <p></p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/","title":"Simple Linear Regression","text":"<p>Simple Linear Regression (SLR) assumes a Linear Relationship between a Numeric DV and single continuous quantitative IV. The model is considered to be simple because it only contains a single independent variable.</p> \\[ E(Y|X) = \\beta_0 + \\beta_1 X \\] <p> \\(\\beta_0\\) \\(\\beta_1\\) Expected value of \\(Y\\) when \\(X = 0\\) Change in the expected value of \\(Y\\) given a one unit increase in \\(X\\) Intercept Parameter Slope Parameter <p></p> <p>Each observation can also be expressed as sum of the regression and its error term:</p> \\[ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\]"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#ordinary-least-squares","title":"Ordinary Least Squares","text":"<p>SLR parameters are estimated using the Ordinary Least Squares method, which minimizes the Sum of Squared Residuals of the fitted model. It is commonly referred to as the Residual Sum Squared (RSS).</p> \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 \\] <p>The minimization is solved through calculus by setting the partial derivatives of the RSS to 0:</p> <p>For the intercept parameter,</p> \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_0} RSS &amp;= 0 \\\\ -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &amp;= 0 \\\\ \\sum y_i - \\sum \\hat{\\beta}_0 - \\sum \\hat{\\beta}_1 x &amp;= 0 \\\\ n\\bar{y} -n\\hat{\\beta}_0 - n\\hat{\\beta}_1 \\bar{x} &amp;= 0 \\\\ \\end{aligned} \\] \\[\\therefore \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\] <p>For the slope parameter,</p> \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_1} RSS &amp;= 0 \\\\ -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &amp;= 0 \\\\ \\sum (y_i x_i) - \\hat{\\beta}_0 \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &amp;= 0 \\\\ \\sum (y_i x_i) - (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &amp;= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} + n\\hat{\\beta}_1 \\bar{x}^2 - \\hat{\\beta}_1 \\sum x^2_i &amp;= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} &amp;= \\hat{\\beta}_1 \\sum (x^2_i) - n\\bar{x}^2 \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 = \\frac{\\sum (x_i y_i) - n\\bar{x}\\bar{y}}{\\sum (x_i^2) - n\\bar{x}^2} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} = r * \\frac{s_y}{s_x} \\] <p>This results in the following fitted regression model, which can be graphically expressed as a Regression Line:</p> \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] <p></p> <p>Note that it should \\(\\hat{\\varepsilon}\\) in the above image, not \\(e_i\\).</p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#ols-properties","title":"OLS Properties","text":"<p>By re-arranging the formula for \\(\\hat{\\beta}_0\\), we can show that \\((\\bar{x}, \\bar{y})\\) always lies on the fitted regression model:</p> \\[ \\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x} \\] <p>Additionally, since the parameters are estimated through minimization, the resulting model must always fulfil the two first order conditions. The model thus has \\(n-2\\) degrees of freedom to reflect these \"constraints\".</p> <p> \\(\\beta_0\\) FOC \\(\\beta_1\\) FOC \\(\\frac{\\partial}{\\partial \\beta_0} = -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\frac{\\partial}{\\partial \\beta_1} = -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\sum \\hat{\\varepsilon_i} = 0\\) \\(\\sum x_i \\hat{\\varepsilon_i} = 0\\) Residuals are negatively correlated Residuals and Independendent variables are uncorrelated <p></p> <p>Using the above results, we can also show the following that the mean of the regression outputs is equal to the mean of the population:</p> \\[ \\begin{aligned} \\hat{\\varepsilon_i} &amp;= y_i - \\hat{y_i} \\\\ \\sum \\hat{\\varepsilon_i} &amp;= \\sum y_i - \\hat{y_i} \\\\ 0 &amp;= n\\bar{y} - n\\bar{\\hat{y}} \\\\ \\bar{\\hat{y}} &amp;= \\bar{y} \\\\ \\end{aligned} \\]"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#goodness-of-fit","title":"Goodness of Fit","text":"<p>Ideally, the regression should fit the sample closely, having as small as residuals as possible. The size of all the residuals in the model can be summarized through the RSS. The lower the RSS, the better the fit of the model.</p> <p>Recall that the residuals naturally sum to 0 under OLS - the residuals are thus squared to remove the sign so that they can be summed together.</p> \\[ RSS = \\sum (y_i - \\hat{y})^2 \\] <p>However, the SSR on its own is hard to intepret as there is no indication of how low or high it actually is. Thus, the Total Sum of Squares (TSS) can be used as a benchmark for the RSS as it is at least equal to or higher than the RSS.</p> <p>The TSS represents the RSS for a Null Regression - a model with containing only the intercept parameter. The output of this regression is always the sample mean \\(\\bar{y}\\), which is used for the computation of its residuals.</p> <p>It represents the worst possible model which thus has the highest possible RSS. The lower the RSS compared to the TSS, the better the fit of the model.</p> \\[ TSS = \\sum (y_i - \\bar{y}) \\]"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#null-model","title":"Null Model","text":"<p>Consider a regression with only the intercept; \\(\\beta_1 = 0\\). It is known as the Null Model as there are no independent variables used.</p> \\[ y = \\beta_0 \\] <p>We can estimate \\(\\hat{\\beta_0}\\) using OLS, which results in the following result:</p> \\[ \\begin{aligned} -2 \\sum (y_i - \\hat{\\beta_0}) &amp;= 0 \\\\ n \\bar{y} - n \\hat{\\beta_0} &amp;= 0 \\\\ \\hat{\\beta_0} &amp;= \\bar{y} \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{y} = \\bar{y} \\]"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#sum-of-squares","title":"Sum of Squares","text":"<p>The TSS can be further decomposed into two more parts for analysis:</p> \\[ \\begin{aligned} TSS &amp;= \\sum (y_i - \\bar{y})^2 \\\\ TSS &amp;= \\sum[(y_i - \\hat{y}) + (\\hat{y}-\\bar{y})]^2 \\\\ TSS &amp;= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 2 \\sum((y_i - \\hat{y})(\\hat{y}-\\bar{y})) \\\\ TSS &amp;= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 0 \\\\ TSS &amp;= RSS + RegSS \\end{aligned} \\] Residual SS (RSS) Regression SS (RegSS) \\(\\sum(\\hat{y}-\\bar{y})^2\\) \\(\\sum(y_i - \\hat{y})^2\\) Variation of the observed values about the regression Variation of the regression output about the sample mean Variation explained by the regression Variation unexplained by the regression <p></p> <p>Note that it can also be expressed in terms of the Slope Parameter:</p> \\[ \\begin{align} RegSS &amp;= \\sum(\\hat{y}-\\bar{y})^2 \\\\ &amp;= \\sum(\\hat{\\beta}_0 + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &amp;= \\sum(\\bar{y} - \\beta_1 \\bar{x} + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &amp;= \\sum[\\hat{\\beta}_1 (x_i - \\bar{x})]^2 \\\\ &amp;= \\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2 \\\\ \\end{align} \\] <p>The Coefficient of Determination \\(R^2\\) can also be used to demonstrate goodness of fit. It measures the proportion of variation explained by the regression model:</p> \\[ R^2 = \\frac{RegSS}{TSS} = 1 - \\frac{RSS}{TSS} \\] <p>Building off the above expression, it can also be expressed in terms of the Sample Correlation:</p> \\[ \\begin{align} R^2 &amp;= \\frac{\\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &amp;= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\\right)^2 \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &amp;= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^4} \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &amp;= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2} \\\\ &amp;= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x}) \\sum (y_i - \\bar{y})}\\right)^2 \\\\ &amp;= r_{y,x}^2 \\end{align} \\]"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#degrees-of-freedom","title":"Degrees of Freedom","text":"<p>The TSS is based on the naive model with only the intercept parameter, thus, it is subject to the single contraint of all residuals summing to 0. The TSS thus has \\(n-1\\) degrees of freedom.</p> <p>The RSS is based on the SLR with both the intercept and slope parameter, thus it is subject to an additional constraint of the sumproduct of all residuals and independent variables being 0. The RSS thus has \\(n-2\\) degrees of freedom.</p> <p>The sum of the RSS and RegSS is equal to the TSS, thus the sum of their degrees of freedom must also be equal to that of the TSS. By working backwards, the RegSS thus has only \\(1\\) degree of freedom.</p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#mean-squared","title":"Mean Squared","text":"<p>The division of any Sum of Square (TSS, RSS, RegSS) by its Degrees of Freedom is known as the Mean Squared (MS), which is a measure of its average variance.</p> <p>The MS of the TSS is the Unbiased Estimator for Population Variance, which is why this process is known as the Analysis of Variance, as it decomposes the variance of \\(Y\\) into its constituent components:</p> \\[ s = \\frac{TSS}{n-1} \\] <p>The MS of the RSS is known the Mean Squared Residuals, often also referred to as the Mean Squared Error, as it is an estimate for the population variance of the error \\(\\sigma^2\\):</p> \\[ MS_{\\text{Residuals}} = \\frac{RSS}{n-2} \\] <p>The MS of the RegSS is known as the Mean Squared Regression, which represents the proportion of variance explained per \\(X\\) used.</p> \\[ MS_{\\text{Regression}} = \\frac{RegSS}{1} \\]"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#f-statistic","title":"F Statistic","text":"<p>The ANOVA parameters can be used to conduct a hypothesis test to determine if there is in fact a relationship between \\(X\\) and \\(Y\\):</p> <ul> <li>\\(H_0\\): \\(\\beta_1 = 0\\)</li> <li>\\(H_1\\): \\(\\beta_1 \\ne 0\\)</li> </ul> <p>Under the null hypothesis, there should be no difference between the assumed model and a null model as both only contain the intercept parameter, thus \\(TSS = RSS\\), where \\(RegSS = 0\\).</p> <p>Thus, the F-statistic is testing for the equality of variance between the TSS and RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_1 \\ne 0\\).</p> <p>The F-statistic can be constructed using the sum of squares:</p> \\[ \\begin{aligned} F &amp;= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &amp;= \\frac{RegSS/1}{RSS/(n-2)} \\\\ &amp;= \\frac{(TSS - RSS)/1}{RSS/(n-2)} \\\\ &amp;= (n-2) * \\frac{R^2}{1-R^2}, \\text{divide both by TSS} \\end{aligned} \\] <p>Similar to the variance test, it can be shown that under the null, this test-statistic follows an F distribution with \\(1\\) and \\(n-2\\) degrees of freedom:</p> \\[ \\begin{aligned} T &amp;= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &amp;= \\frac{\\sigma^2_{RegSS} \\frac{MS_{Reg}}{\\sigma^2_{RegSS}}}{\\sigma^2_{RSS} \\frac{MS_{RSS}}{\\sigma^2_{RSS}}} \\\\ &amp;= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\frac{1 * MS_{Reg}}{\\sigma^2_{RegSS}} * \\frac{1}{1}}{\\frac{(n-2) * MS_{RegSS}}{\\sigma^2_{RegSS}}* \\frac{1}{n-2}} \\\\ &amp;= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\chi_1}{\\chi_{n-2}} * (n-2) \\\\ &amp;= 1 * F_{1, n-2} * (n-2) \\\\ &amp;= F_{1, n-2} * (n-2) \\end{aligned} \\] \\[ \\therefore F \\sim F_{1, n-2} \\]"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#anova-table","title":"ANOVA Table","text":"<p>All the above information is then summarized in a table for convenience, known as the ANOVA Table:</p> <p> Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(1\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-2\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) - <p></p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#statistical-inference","title":"Statistical Inference","text":""},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#sampling-distributions","title":"Sampling Distributions","text":"<p>Since the errors are assumed to be normally distributed, then \\(Y\\) is assumed to be normally distributed as well. Since \\(Y\\) is a linear combination of the regression parameters, then the parameters (&amp; their estimates) are normally distributed as well.</p> <p>Both estimates can be expressed in another form that makes it more convenient to find their expectation &amp; variances.</p> \\[ \\begin{aligned} \\hat{\\beta}_1 &amp;= \\frac{\\sum [(x_i - \\bar{x})(y_i - \\bar{y})]}{\\sum (x_i - \\bar{x})^2} \\\\ &amp;= \\frac{\\sum (x_i - \\bar{x})y_i}{\\sum (x^2_i - \\bar{x})} - \\frac{\\bar{y} \\sum (x_i - \\bar{x})}{\\sum (x^2_i - \\bar{x})} \\\\ &amp;= \\sum \\frac{(x_i - \\bar{x})}{(x^2_i - \\bar{x})}* y_i - 0 \\\\ &amp;= \\sum w_i * y_i \\\\ \\\\ \\hat{\\beta}_0 &amp;= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\ &amp;= \\frac{1}{n} \\sum y_i - \\bar{x} \\sum w_i * y_i \\\\ &amp;= \\sum y_i (\\frac{1}{n} - \\bar{x}w_i) \\end{aligned} \\] <p>\\(w_i\\) is a sort of \"weight\" parameter of the sum of squares. It has three interesting properties that makes it useful:</p> \\[ \\begin{aligned} \\sum w_i &amp;= \\frac{\\sum (x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &amp;= \\frac{n\\bar{x}-n\\bar{x}}{\\sum (x_i - \\bar{x})^2} \\\\ &amp;= \\frac{0}{\\sum (x^2_i - \\bar{x})} \\\\ &amp;= 0 \\\\ \\\\ \\sum w_i x_i &amp;= \\frac{\\sum x_i(x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &amp;= \\frac{\\sum (x^2_i - \\bar{x} \\sum x_i)}{\\sum x^2_i - 2\\bar{x}\\sum x_i + \\sum \\bar{x}^2} \\\\ &amp;= \\frac{\\sum x^2_i - \\bar{x}(n \\bar{x})}{\\sum x^2_i - 2\\bar{x}(n\\bar{x}) + n\\bar{x}^2} \\\\ &amp;= \\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - 2n\\bar{x}^2 + n\\bar{x}^2} \\\\ &amp;=\\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - n\\bar{x}^2} \\\\ &amp;= 1 \\\\ \\\\ \\sum w_i^2 &amp;= \\frac{\\sum (x_i - \\bar{x})^2}{\\sum (x_i - \\bar{x})^4} \\\\ &amp;= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i) &amp;= n(\\frac{1}{n}) - \\bar{x} \\sum w_i \\\\ &amp;= 1 - 0 \\\\ &amp;= 1 \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i &amp;= \\frac{1}{n} \\sum x_i - \\bar{x} \\sum w_i x_i \\\\ &amp;= \\frac{1}{n} (n\\bar{x}) - \\bar{x} (1) \\\\ &amp;= \\bar{x} - \\bar{x} \\\\ &amp;= 0 \\end{aligned} \\] <p>Using this, the Expectation &amp; Variance can be determined:</p> \\[ \\begin{aligned} E(\\hat{\\beta}_1) &amp;= \\sum w_i E(y_i) \\\\ &amp;= \\sum w_i E(\\beta_0 + \\beta_1 x_i) \\\\ &amp;= \\beta_0 \\sum w_i + \\beta_1 \\sum w_i x_i \\\\ &amp;= \\beta_0 (0) + \\beta_1 (1) \\\\ &amp;= \\beta_1\\\\ \\\\ Var(\\hat{\\beta}_1) &amp;= Var(\\sum w_i y_i) \\\\ &amp;= \\sum w_i^2 Var (y_i) \\\\ &amp;= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ &amp;= \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2} \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 \\sim N(\\beta_1, \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2}) \\] \\[ \\begin{aligned} E(\\hat{\\beta}_0) &amp;= \\sum (\\frac{1}{n} - \\bar{x}w_i) E(y_i) \\\\ &amp;= \\sum (\\frac{1}{n} - \\bar{x}w_i) (\\beta_0 + \\beta_1 x_i) \\\\ &amp;= \\beta_0 \\sum (\\frac{1}{n} - \\bar{x}w_i) + \\beta_1 \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i \\\\ &amp;= \\beta_0 (1) + \\beta_1 (0) \\\\ &amp;= \\beta_0 \\\\ \\\\ Var(\\hat{\\beta}_0) &amp;= Var(\\sum (\\frac{1}{n} - \\bar{x}w_i)y_i) \\\\ &amp;= \\sum (\\frac{1}{n} - \\bar{x}w_i)^2 Var (y_i) \\\\ &amp;= \\sigma^2 \\sum (\\frac{1}{n^2} -\\frac{2\\bar{x}w_i}{n} + \\bar{x}^2 w_i^2) \\\\ &amp;= \\sigma^2 (\\sum \\frac{1}{n^2} - \\frac{2\\bar{x}}{n} \\sum w_i + \\bar{x}^2 \\sum w_i^2) \\\\ &amp;= \\sigma^2 [n(\\frac{1}{n^2}) - \\frac{2\\bar{x}}{n} (0) + \\bar{x}^2 (\\frac{1}{\\sum (x_i - \\bar{x})^2})] \\\\ &amp;= \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2}) \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_0 \\sim N(\\beta_0, \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2})) \\]"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#hypothesis-testing","title":"Hypothesis Testing","text":"<p>Since the regression parameters are normally distributed, a z-statistic can also be used to conduct the tests. However, since the population variance is not known, a t-statistic is used instead:</p> \\[ \\begin{aligned} t &amp;= \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\end{aligned} \\] <p>Since the population variance is estimated by the MSE which has \\(n-2\\) degrees of freedom, the corresponding chi-squared and hence t-distribution has \\(n-2\\) degrees of freedom as well.</p> \\[ \\begin{aligned} \\hat{Var}(\\hat{\\beta_1}) &amp;= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} \\\\ &amp;= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} * \\frac{\\frac{1}{n-1}}{\\frac{1}{n-1}} \\\\ &amp;= \\frac{MS_{RSS}}{(n-1) s^2} \\end{aligned} \\] \\[ t \\sim t_{n-2} \\] <p>Since the square of the t-statistic is the F-statistic, both are equivalent ways of doing so and will always lead to the same conclusions.</p> \\[ t^2 \\sim F_{1, n-2} \\]"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#confidence-intervals","title":"Confidence Intervals","text":"<p>Since the distribution of t-statistic is known, it can be used to determine the confidence interval of the estimate:</p> \\[ P\\left(\\text{Margin of Error} &lt; \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} &lt; \\text{Margin of Error}\\right) = 1 - \\alpha \\] \\[ \\text{Confidence Interval} = \\hat{\\beta}_1 \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_1}} \\]"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/2.%20Simple%20Linear%20Regression/#prediction-intervals","title":"Prediction Intervals","text":"<p>Consider the Prediction Error of the SLR model:</p> \\[ y_* - \\hat{y_*} = \\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\] <p>Since both \\(y_*\\) and \\(\\hat{y_*}\\) are normally distributed, the prediction errors are normally distributed as well:</p> \\[ \\begin{aligned} E(y_* - \\hat{y_*}) &amp;= E[\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)]] \\\\ &amp;= 0 + \\beta_0 + \\beta_1 E(x_*) - \\beta_0 - \\beta_1 E(x_*) \\\\ &amp;= 0 \\\\ \\\\ Var(y_* - \\hat{y_*}) &amp;= Var(\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &amp;= Var(\\varepsilon_*) + Var[(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &amp;= ... \\\\ &amp;= \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}] \\end{aligned} \\] \\[ \\therefore y_* - \\hat{y_*} \\sim N\\left(0, \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}]\\right) \\] <p>Similar to before, since the population variance is unknown, it can be approximated using the MSE. Thus, a t-statistic constructed from this sample will have a t-distribution, allowing the following prediction interval to be calculated:</p> \\[ \\text{Prediction Interval} = \\hat{y}_* \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\] <p>Notice that the standard error of the prediction interval increases as \\(x_*\\) moves further away \\(\\bar{x}\\), indicating that the predictions become less accurate for those values.</p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/","title":"Multiple Linear Regression","text":"<p>The natural extension of the SLR model is to include more than one independent variable, which thus results in the more generalized Multiple Linear Regression (MLR) model.</p> \\[ E(Y|X_1, ... X_p) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_{p}X_{p} \\] <p>Unlike the SLR which studies how each individual IV influences the DV, the goal of the MLR model is to study how all the IVs operate together to influence the DV.</p> <p> \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) when \\(X_1 = X_2 = ... = 0\\) Change in \\(E(Y)\\) given a one unit increase in \\(X_j\\), holding all other \\(X\\)'s constant Intercept Parameter \"Slope\" Parameter <p></p> <p>For avoidance of doubt, the subscript \\(i\\) will be used to denote observations while \\(j\\) will be used to denote independent variables.</p> <p>Every observation can also be expressed as the sum of the regression and the error term. However, due to the multi-dimensional nature of the model, it is commonly expressed in matrix notation:</p> \\[ \\begin{aligned}     \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} &amp;=     \\begin{pmatrix}         1 &amp; x_{11} &amp; x_{12} &amp; ... &amp; x_{1p} \\\\         1  &amp; x_{21} &amp; x_{22} &amp; ... &amp; x_{2p} \\\\         \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\         1  &amp; x_{n1} &amp; x_{n2} &amp; ... &amp; x_{np}     \\end{pmatrix}     \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix} +     \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix} \\\\     \\boldsymbol{y} &amp;= \\boldsymbol{X\\beta + \\varepsilon} \\end{aligned} \\]"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#ordinary-least-squares","title":"Ordinary Least Squares","text":"<p>Similar to the SLR model, the regression parameters can be found by minimizing the sum of squared residuals:</p> \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_p x_ip)]^2 \\] <p>There are \\(p+1\\) FOC equations to solve through the minimization, with the additional one reflecting the intercept parameter. It is difficult to algebraically solve this system of equations, thus there is no closed form tsolution for each individual paramater. Instead, here is a vector solution for all of the parameters:</p> \\[ \\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\hat{\\beta_0} \\\\ \\hat{\\beta_1} \\\\ \\vdots \\\\ \\hat{\\beta_p} \\end{pmatrix} = (\\boldsymbol{X'X})^{-1}\\boldsymbol{X'y} \\] <p>Note that since there are \\(p+1\\) equations that must be solved, the model has \\(n-p+1\\) degrees of freedom.</p> <p>Following the same logic, there must be at least \\(p+1\\) observations in order to solve the equations and hence construct the model.</p> <p>This results in the following fitted regression model, which can be graphically expressed as a Regression Plane:</p> \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_P x_ip \\] <p></p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#manual-computation","title":"Manual Computation","text":"<p>The tricky part is that \\((\\boldsymbol{X'X})^{-1}\\) is hard to compute by manually, except for the special case where \\(p=1\\) (SLR). Thus, it is likely that the parameters will be provided by the question.</p> <p>If required to compute them manually, then \\((\\boldsymbol{X'X})^{-1}\\) is likely to be provided. The remaining \\(\\boldsymbol{X'y}\\) still needs to be computed and put together to obtain the regression parameters.</p> <p>However, if the model has \\(p=2\\) but no intercept, then \\((\\boldsymbol{X'X})^{-1}\\) is a 2 x 2 Matrix whose inverse can be easily calculated. Similarly, if \\((\\boldsymbol{X'X})^{-1}\\) is a Diagonal Matrix, its inverse can be easily calculated as well.</p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#goodness-of-fit","title":"Goodness of Fit","text":"<p>The ANOVA for MLR follows the same intuition as the SLR version, adjusted for the new degrees of freedom:</p> <p> Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(p\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-(p+1)\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) - <p></p> <p>The coefficient of determination still represents the proportion of variance explained by the regression, but has a slightly different formula:</p> \\[ R^2 = \\frac{RegSS}{TSS} = r_{y,\\hat{y}}^2 \\] <p>The multiple IVs of the model are now captured through \\(\\hat{y}\\) instead of \\(x\\) directly.</p> <p>However, the hypothesis test under the MLR is vastly different from the SLR version. Instead of testing if an individual IV is useful, it tests if all the IVs are collectively useful in helping to explain the DV.</p> \\[ \\begin{aligned} H_0 &amp;: \\beta_1 = \\beta_2 = ... = \\beta_p = 0 \\\\ H_1 &amp;: \\text{At least one } \\beta_j \\text{ is non-zero} \\end{aligned} \\] <p>Thus, rejecting the null hypothesis implies that at least one of the IVs used is useful, but does not provide much insight into which of them are useful.</p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#partial-f-test","title":"Partial F Test","text":"<p>A partial F-test can be used to precisely determine which of the IVs are useful in explaining the DV.</p> <p>A regular F test compares the null model with no IVs to the desired model with all the IVs. If the sum of squares are significantly different, then it implies that the additional IVs are jointly useful in explaining \\(Y\\).</p> <p>The partial F test generalizes this idea. Instead of considering a null model with no IVs, a Reduced Model with a limited number IVs (\\(q\\)) is considered instead. Consequently, the desired model is known as the Full Model with all \\(p\\) IVs, where \\(q &lt; p\\).</p> \\[ \\begin{aligned} H_0: \\beta_{p-q+1} = ... = \\beta_p = 0 \\\\ H_1: \\beta_{p-q+1} = ... = \\beta_p \\ne 0 \\end{aligned} \\] <p>The test is also commonly referred to as the Generalized F test, where the models are referred to as the Restricted and Unrestricted Models.</p> <p>The difference in the RSS between the Full and Reduced Model is known as the Extra Sum of Squares (ExtraSS). It represents the contribution of the missing variables in explaining the variance of \\(Y\\). Under the null hypothesis, there should be no difference between the two RSS, and thus \\(ExtraSS = 0\\).</p> \\[ ExtraSS = RSS_{Reduced} - RSS_{Full} \\] <p>Thus, the Partial F-statistic is testing for the equality of variance between the two RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_{p-q+1} = ... = \\beta_p \\ne 0\\).</p> \\[ \\begin{aligned} F &amp;= \\frac{MS_{ExtraSS}}{MS_{RSS_{Full}}} \\\\ &amp;= \\frac{ExtraSS/q}{RSS/(n-2)} \\\\ &amp;= \\frac{(RSS_{Reduced} - RSS_{Full})/q}{RSS/(n-2)} \\\\ &amp;= (n-2) * \\frac{(1- R_{Reduced}^2) - (1 - R_{Full}^2)}{1-RSS_{Full}^2}, \\text{divide both by TSS} \\\\ &amp;= (n-2) * \\frac{R_{Full}^2 - R_{Reduced}^2}{1-RSS_{Full}^2} \\end{aligned} \\]"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#statistical-inference","title":"Statistical Inference","text":""},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#sampling-distributions","title":"Sampling Distributions","text":"<p>Similar to SLR, the regression parameters are normally distributed as well. However, since there are multiple regression parameters, they collectively follow a multivariate normal distribution.</p> \\[ \\hat{\\beta} \\sim N_{p+1}(\\beta, \\sigma^2 (\\boldsymbol{X'X})^{-1}) \\] <p>The variance of the distribution is known as the Variance Covariance Matrix, which provides the covariances between every possible pair of regression parameters.</p> <p>Since the covariance of a variable with itself is its variance, the diagonals are the respective variances of the parameters. Note that the first element of the diagonal is the intercept, thus the variance of the jth IV is the (j+1)th element of the diagonal.</p> \\[ Var(\\hat{\\beta}) = \\begin{pmatrix}     Var(\\hat{\\beta}_0) &amp; Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) &amp; ... &amp; Cov(\\hat{\\beta}_0, \\hat{\\beta}_p) \\\\     Cov(\\hat{\\beta}_1, \\hat{\\beta}_0) &amp; Var(\\hat{\\beta}_1) &amp; ... &amp; Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) \\\\     \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\     Cov(\\hat{\\beta}_p, \\hat{\\beta}_0) &amp; Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) &amp; ... &amp; Var(\\hat{\\beta}_p) \\end{pmatrix} \\] <p>Note that the covariances are symmetrical about the diagonal - \\(Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = Cov(\\hat{\\beta}_1, \\hat{\\beta}_0)\\).</p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#hypothesis-testing","title":"Hypothesis Testing","text":"<p>Similar to SLR, the t-test can be used to test for the significance of an individual IV, but the intepretation of the test is different from the SLR case. It tests the usefulness of an individual IV in the presence of the other predictors.</p> \\[ \\begin{aligned} H_0: \\beta_j = 0 \\\\ H_1: \\beta_j \\ne 0 \\end{aligned} \\] \\[ t(\\beta_j) = \\frac{\\hat{\\beta_j} - \\beta_j}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\] <p>Recall that the variance of the jth IV si the (j+1)th element of the variance covariance matrix.</p> \\[ t(\\beta_j) \\sim t_{n-p-1} \\] <p>However, this leads to several odd results which needs to be accounted for:</p> <ol> <li>Predictor is not significant individually but significant when taken alone.</li> <li>Predictors are not significant individually but significant when taken together.</li> </ol> <p>There are now two possible ways to test for the significance of IVs:</p> <ol> <li>Conduct a single F-test to test for the joint significance of all IVs</li> <li>Conduct multiple t-tests to test for the joint significance of each IV</li> </ol> <p>The problem with the multiple t-test approach lies with the type I error of the tests. For \\(\\alpha = 0.05\\), the probability of correctly rejecting the null is \\(0.95\\). Assuming that all the tests are independent, the probability of correctly rejecting all the nulls is \\(0.95^p\\), which drastically decreases with the number of tests conducted.</p> <p>Given enough predictors, this means that the probability drops to approximately 0, which means that there is bound to be a wrongly rejected null; a type I error is guaranteed even though it was supposed to be limited at a 0.05 chance.</p> <p>The Bonferroni Correction is a method of adjusting \\(\\alpha\\) of each hypothesis test such that the overall type I error is kept at its desired level. However, this has the consequence of increasing the probability of type II errors, which is why it is not popular.</p> <p>The F-test has the advantage of controlling the type I error regardless of the number of predictors, which is why it is preferred for hypothesis testing in the MLR.</p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#confidence-intervals","title":"Confidence Intervals","text":"<p>Similar to SLR, the confidence intervals can be constructed using the distribution of the test-statistic:</p> \\[ \\text{Confidence Interval} = \\hat{\\beta}_j \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_j}} \\]"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#prediction-intervals","title":"Prediction Intervals","text":"<p>Unlike in SLR, it is difficult to determine the distribution of the prediction error. Thus, the final result can be found below:</p> \\[ \\begin{aligned} \\text{Prediction Interval} &amp;= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\\\ &amp;= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\sqrt{s^2 [1 + x'_* (\\boldsymbol{X'X})^-1 * x_*]} \\end{aligned} \\] <p>Despite the result looking more complicated, the key takeaway remains the same - the further away \\(x_*\\) is from \\(\\bar{x}\\), the greater</p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#variations-of-mlr","title":"Variations of MLR","text":""},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#qualitative-iv","title":"Qualitative IV","text":"<p>The discussion so far has mostly focused on Quantitative IVs, thus this section will explore Qualitative IVs. They can only take values from a list of pre-defined values, known as the Levels of the variable.</p> <p>In a regression context, most qualitative IVs are represented in the form of a Dummy Variable which can only take two possible levels - Yes (1) or No (0).</p> <p>Note that there are other ways of encoding a dummy variable (-1/0/1 etc), but the principles stay large the same.</p> \\[ \\text{Dummy Variable} = \\begin{cases}     1,  &amp; \\text{First Level} \\\\     0, &amp; \\text{Second Level} \\end{cases} \\] <p>In general, \\(n-1\\) dummy variables are needed to represent a qualitative variable with \\(n\\) levels. This is because the status of the last level can be deduced from the other dummy variables. Thus, including a seperate dummy variable for this last level is redundant and will lead to the problem of Collinearity, which will be explored in a later section.</p> <p>For instance, consider four levels (North, East, South &amp; West), represented by the three dummy variables (\\(N, E , S\\)). If any of the variables are 1, they represent their respective direction (North, East &amp; South). If all of them are 0, then the direction is the remaining level (West).</p> <p>The last remaining level is often referred to as the Baseline Level as it is the default level of the variable when all other dummies are 0. Any level can be used as the baseline, but the parameters will differ across models with different baselines.</p> \\[ E(Y|X) = \\beta_0 + \\beta_{1, North} x_{North} + \\beta_{2, East} * x_{East} + \\beta_{3, South} * x_{South} \\] <p> \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) at the baseline level Change in \\(E(Y)\\) from the baseline to the chosen level \\(X_1 = X_2 = ... = X_j = 0\\) \\(X_1 = X_2 = ... = 0; X_j = 1\\) <p></p> <p>Dummy variables are usually used in conjunction with quantitative ones. This essentially creates a \"seperate\" regression model for each of the levels.</p> <p>For the simplest case of one quantiative and one dummy, \\(\\beta_2\\) is the difference in the intercept of the two resulting SLR models.</p> \\[ E(Y|X) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\] \\[ E(Y|X)  = \\begin{cases}     (\\beta_0 + \\beta_2) + \\beta_1 x_1, &amp; x_2 = 1 \\\\     \\beta_0 + \\beta_1 x_1, &amp; x_2 = 0 \\end{cases} \\] <p></p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#interaction-model","title":"Interaction Model","text":"<p>So far, it was assumed that each IV had an independent effect on the DV. However, IVs may interact to produce a joint effect on the DV, where the effect of one IV depends on the value of another IV.</p> <p>For instance, the production of a factory may depend on the number of Machines and Workers. However, the more machines there are, the greater the effect of an additional worker.</p> <p>Thus, this interaction effect can be captured through an Interaction Variable, which is the product of both IVs:</p> \\[ \\begin{aligned} E(Y|X) &amp;= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 \\\\ &amp;= \\beta_0 + (\\beta_1 + \\beta_3 x_2)x_1 + \\beta_2 x_2 \\\\ &amp;= \\beta_0 + \\beta_1 x_1 + (\\beta_2 + \\beta_3 x_1) x_2 \\\\ \\end{aligned} \\] <p>A one unit increase in \\(x_1\\) will increase E(Y) by \\(\\beta_1 + \\beta_3 x_2\\), which depends on the value of \\(x_2\\) as well, which is is why they interact with each another. Phrased another way, for every one unit increase in \\(x_2\\), the change in E(Y) from a unit increase in \\(x_1\\) increases by \\(\\beta_3\\).</p> <p>Something unusual to take note of is that the Interaction Variable tests significant but the constituent variables do not. In this case, it is common practice to retain both the interaction and the consistuent variables in the model. This is practice is known as the Hierarchical Principle.</p> <p>Models containing dummy variables can also have an interaction effect. Building off the example from the previous section, \\(\\beta_2\\) is still the difference in the intercept but with the new \\(\\beta_3\\) being the difference in slopes of the two resulting SLR models.</p> \\[ E(Y|X)  = \\begin{cases}     (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3)x_1, &amp; x_2 = 1 \\\\     \\beta_0 + \\beta_1 x_1, &amp; x_2 = 0 \\end{cases} \\] <p></p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#piecewise-model","title":"Piecewise Model","text":"<p>If the DV has an abrupt change in behaviour over different values of the IVs, it can be accounted for the through the use of a Piecewise Regression.</p> <p>The first method of creating a piecewise regression involves the use of an Indicator Function. It is essentially a dummy variable which depends on the value of the other IVs.</p> \\[ z_{\\{x&gt;=c\\}} = \\begin{cases}     0, &amp; x &lt; c \\\\     1, &amp; x \\ge c \\end{cases} \\] \\[ \\begin{aligned} E(Y|X) &amp;= \\beta_0 + \\beta_1 x_1 + \\beta_2 z(x-c) \\\\ &amp;= \\begin{cases}     \\beta_0 + \\beta_1 x_1, &amp; x &lt; c \\\\     (\\beta_0 - \\beta_2c) + (\\beta_1 + \\beta_2) x_1, &amp; x \\ge c \\end{cases} \\end{aligned} \\] <p>Note that \\(z(x-c)\\) is treated as a distinct IV and hence can be equivalently expressed as \\(x_2\\); the full notation is used here for clarity.</p> <p>\\(c\\) is the value at which the DV abruptly changes in behaviour, known as a Kink in the graph, which continuously connects the two regression lines.</p> <p></p> <p>The other method is to use an interaction variable instead. Similar to how the interaction variables resulted in the model to \"split\", the model now splits at \\(x = c\\), resulting in a non-continuous gap.</p> \\[ \\begin{aligned} E(Y|X) &amp;= \\beta_0 + \\beta_1 x_1 + \\beta_2z + \\beta_3 zx \\\\ &amp;= \\begin{cases}     \\beta_0 + \\beta_1 x_1, &amp; x &lt; c \\\\     (\\beta_0 - \\beta_2) + (\\beta_1 + \\beta_3) x_1, &amp; x \\ge c \\end{cases} \\end{aligned} \\] <p></p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/3.%20Multiple%20Linear%20Regression/#polynomial-model","title":"Polynomial Model","text":"<p>If the relationship between the DV and IV is complex (non-linear), then a Polynomial Regression can be used to better model the relationship between the two.</p> \\[ E(Y|X) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 \\] <p>Note that it is the same IV used in the regression, just with additional powers.</p> <p>Although a polynomial regression may capture the true relationship better, the regression parameters become hard to intepret. The partial derivatives can no longer be intepreted as holding other IVs constant as each IV is dependent on the same quantity, just to different powers.</p> \\[ \\frac{\\partial E(Y|X)}{\\partial x} = \\beta_1 + 2\\beta_2 x + ... + m \\beta_m x^{m-1} \\] <p></p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/","title":"Gauss Markov Theorem","text":"<p>Using OLS, the estimated regression parameters will always be unbiased under certain assumptions. The Gauss Markov Theorem extends this, which states that under certain assumptions, the OLS estimators will have the lowest variance among all possible linear unbiased estimators.</p> <p>In a statistics context, they are said to be the most efficient among all other linear unbiased estimators. In a regression context, the OLS estimators are said to be the Best Linear Unbiased Estimators (BLUE).</p> <p>This section will go over the various assumptions for both OLS and Gauss Markov Theorem. It will also cover the diagnostics to determine if the assumptions have been violated.</p> <p>The assumptions needed for OLS and the Gauss Markov theorem are often mixed up with each other as the assumptions needed for OLS are also needed for the theorem. This set of notes makes a clear distinction between the two.</p> <p></p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#ols-assumptions","title":"OLS Assumptions","text":""},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#1-linearity","title":"#1: Linearity","text":"<p>Linear regression is a model where the relationship between the DV and IVs are linear. Thus, the regression parameters must be linear, but NOT the DV or IV.</p> <p>This means that the model is still considered a \"Linear Regression\" even after a transformation of the DV and/or IV.</p> \\[ \\begin{aligned} y_i &amp;= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\\\ y_i &amp;= \\beta_0 + \\beta_1 x^2 + \\beta_2 x^3 \\\\ \\ln y_i &amp;= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\end{aligned} \\]"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#2-exogenity","title":"#2: Exogenity","text":"<p>Exogenity refers to how a variable comes from outside the model and is thus independent of any other variables within the model.</p> <p>In a regression context, this comes in the form of the errors having a conditional mean of 0, ensuring that the errors are random and thus not related to the IVs.</p> \\[ E(\\varepsilon_i | x_{ij}) = 0 \\\\ \\] <p>\"Endo\" and \"Exo\" in Greek means \"In\" and \"Out\" respectively, which is how the meaning of the words were derived.</p> <p>There are two key implications of Exogenity:</p> <ol> <li>By the Law of Total Expectations, the unconditional expectation of the error is also 0.</li> <li>By the Linearity of Conditional Expectations, the expectation of the product of the Error and IVs is 0.</li> </ol> \\[ \\begin{aligned} E(\\varepsilon_i) = 0 \\\\ E(\\varepsilon_i x_{ij}) = 0 \\end{aligned} \\] <p>Following these two implications, it can be shown that the Covariance between the Error and IVs are also 0, which is another consequence of independence (NOT the other way around).</p> \\[ \\begin{aligned} Cov (\\varepsilon_i, x_{ij}) &amp;= E(\\varepsilon_i x_{ij}) - E(\\varepsilon_i) * E(x_{ij}) \\\\ &amp;= 0 - 0 * E(x_{ij}) \\\\ &amp;= 0 \\end{aligned} \\] <p>Without exogenity, the regression parameters would reflect the effect of both the IV and the unmodelled variable within the error term. This causes the OLS estimate to be biased, known as the Omitted Variable Bias. Since the unmodelled variable confounds the results of the regression, it is known as a Confounding Variable.</p> <p></p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#residual-analysis","title":"Residual Analysis","text":"<p>Since the errors are unobservable, the residuals are used to estimate the errors.</p> <p>If the fitted model is adequate - all relavent IVs are included in the right form, then the residuals should closely resemble the errors and therefore be structureless (random). However, if there are patterns in the residuals, it indicates that there is additional information that can be used to improve the model and thus should be included.</p> <p>Due to the OLS, the correlation between residuals and existing IVs will always be 0 indicating no linear relationship. To check for unmodelled non-linear relationships, a Residual Plot of the IVs against the Residuals can be used.</p> <p>For instance, if the residual plot shows a quadractic pattern (curve), then a quadractic IV should be added into the model. Mathematically, it can be expressed as a function of the existing estimates:</p> \\[ \\begin{aligned} \\hat{y_i} &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\varepsilon}_i \\\\ \\hat{\\varepsilon_i} &amp;= \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\text{ (From residual plot)} \\\\ \\hat{y_i} &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &amp;= (\\hat{\\beta}_0 + \\hat{\\gamma}_0) + (\\hat{\\beta}_1 + \\hat{\\gamma}_1)x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &amp;= \\hat{\\beta}_0' + \\hat{\\beta}_1'x + \\hat{\\beta}_2' x^2 \\end{aligned} \\] <p></p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#3-no-perfect-collinearity","title":"#3: No Perfect Collinearity","text":"<p>Collinearity refers to when an IV can be expressed as a linear combination of one or more other IVs. Perfect collinearity is an extreme case where an IV can be perfectly expressed as a combination of another.</p> <p>Technically speaking, Collinearity refers to one to one variable relationship, while Multicollinearity refers to one to many variable relationship, hence \"Multi\".</p> <ol> <li>Variable is a multiple of another: \\(x_1 = cx_2\\)</li> <li>Variable differs by a constant from another: \\(x_1 = x_2 \\pm c\\)</li> <li>Variable is an affine transformation of another:</li> <li>Sum of several variables is fixed: Dummy Variable Trap</li> </ol> <p>The issue with perfect collinearity is that it affects the linear algebra used to solve for the coefficients (EG. Two equations to solve for three unknowns). There will be no unique solutions - many different values for the coefficients could work equally well.</p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#imperfect-collinearity","title":"Imperfect Collinearity","text":"<p>Imperfect Collinearity is a less extreme case where an IV is highly (but not perfectly) correlated with one or more other IVs.</p> <p>Recall that correlation refers to the extent of a Linear relationship. Non-linear relationships between variables are fine (EG. Polynomial Regression).</p> <p>This means that including the IV does not bring much additional predctive power into the model as its effects are already captured through related predictors and thus can be removed from the model.</p> <p>Unlike in the perfect case, high collinearity does not prevent OLS from finding a solution. However, the intepretation of the variables become complicated:</p> <ol> <li>The original intepretation of coefficients \"holding other variables constant\" is no longer true as highly correlated variables tend to move together. Thus, it is hard to seperate the effects of an individual variable.</li> <li>Consequently, OLS has difficulty estimating these coefficients, which could result in weird meaningless estimates; EG. Large positive coefficient but large negative for its correlated counterpart.</li> <li>This also results in higher standard errors for the coefficients of correlated variables. This reduces the magnitude of t-statistic, which results in more false negatives, failing to reject the null when it should. This results in important variables being omitted from the regression.</li> </ol> <p>Technically speaking, there is nothing wrong with collinearity if the purpose of the model is solely for prediction. However, if the purpose of the model was to establish causality, then collinearity poses a problem as it interferes with statistical inference.</p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#detecting-collinearity","title":"Detecting Collinearity","text":"<p>The simplest way to detect collinearity is through a Scatterplot or Correlation Matrix, which shows the correlations between pairs of variables. A correlation of 0.8 and higher is typically considered high enough where the collinearity becomes problematic.</p> <p></p> <p>However, the issue is that this method can only detect collinearity between pairs of variable at a time. In order to detect collinearity among three or more variables (multicollinearity), then the Variance Inflation Factor (VIF) should be used instead.</p> \\[ VIF = \\frac{1}{1-R^2_j} \\] <p>The VIF is derived from the variance of the regression coefficient. As mentioned previously, under the presence of collinearity, the standard error and hence variance of the coefficient increases (\"inflated\"). The extent of the increase is known as the VIF.</p> \\[ \\hat{Var}(\\hat{\\beta_1}) = \\frac{MS_{RSS}}{(n-1) s^2} * \\frac{1}{1-R^2_j} \\] <p>The \\(R^2_j\\) in the VIF is the coefficient of determination of a model where the jth IV is regressed against all other IVs. A high \\(R^2_j\\) means that the IV is well explained by the other IVs (high correlation), which indicates the presence of collinearity. Generally, a \\(VIF &gt; 10\\) is deemed to have severe collinearity.</p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#4-no-extreme-outliers","title":"#4: No Extreme Outliers","text":"<p>Outliers are observations with unusual values of the DV relative to majority of the observations.</p> <p>The last OLS assumption is that there are no extreme outliers in the dataset used to create the regression model. Generally, as long as the DV and IVs have a positive and finite Kurtosis, then the probability of such observations occuring are low.</p> <p>Outliers are problematic as OLS is sensitive to outliers. Extreme outliers have large residuals which receive more weight in the optimization process, which causes the resulting model to accomodate it (when it should not), causing the resulting coefficients to be biased.</p> <p> {.center}</p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#identifying-outliers","title":"Identifying Outliers","text":"<p>Outliers Unusual values of Y relative to the predicted values, large residuals Hard to gauge how large is large &gt; Standardize the residuals to compare However, need to know sampling distribution of the residuals Larger than 2 or 3 is considered outlier</p> <p>Example from actex: Not unusual in x1 or x2 but unusual when taken collectively</p> <p>Average leverage Exceed three times the average to be considered large</p> <p>Measure of remoteness</p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#cooks-distance","title":"Cooks Distance","text":"<p>Influence definition Fit the main model Fit the model without the observation want to test the bigger the difference, the more influential that observation</p> <p>Cooks distance takes this for all points on the whole model etc Expected value of D is 1/n, so anything larger than that is large</p> <p>Need n+1 datasets dataset with all observations to get mse and y then n datasets that each exclude one of the observations</p> <p>Can be expressed algebraically as well Product of studentized residual and leverage &gt; Need to be large in both X and Y axis to be influential</p> <p>Unity threshold</p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#identifying-leverage-points","title":"Identifying Leverage Points","text":"<p>Leverage is unusual values of X Visually easy for one variable &gt; far away from the majority in the X axis Visually hard for multiple &gt; could lie in the range of each variable but far away collectively</p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#gauss-markov-assumptions","title":"Gauss Markov Assumptions","text":""},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#1-conditional-homoscedasticity","title":"#1: Conditional Homoscedasticity","text":"<p>Homoscedasticity refers to the error terms having constant variance while Heteroscedasticity refers to having non-constant variance.</p> \\[ var(\\varepsilon_i | x_i) = \\sigma^2 \\] <p>Under homoscedasticity, the sampling distribution of the estimates are easily derived and thus it can be shown that they are the most efficient estimators. The same cannot be proven under heteroscedasticity.</p> <p>Note that that the OLS estimates are still unbiased; they are just not the most efficient.</p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#identifying-heteroscedasticity","title":"Identifying Heteroscedasticity","text":"<p>Similar to before, if the model is adequate, then the residuals should resemble the errors and have constant variance. Thus, this can be easily determined through a residual plot of the Residuals against the fitted values.</p> <p>If the points are equally spread out about the mean (0) and show no pattern, then homoscedasticity is present. However, if the points show an increasing or decreasing variance (typically in the shape of a funnel), then heteroscedasticity is present.</p> <p></p> <p>Alternatively, a hypothesis test can be conducted to determine if heteroscedasticity is present, known as the Bresuch Pagan Test.</p> \\[ \\begin{aligned} H_0: \\sigma^2 H_1: \\sigma^2 + \\boldsymbol{Z\\gamma} \\end{aligned} \\] <p>The test-statistic is computed as follows:</p> <ol> <li>Compute the squared standardized residuals from the original model</li> <li>Regress them onto the variables in Z (LOL need to change this part)</li> <li>Compute the RegSS of the new regression</li> </ol> \\[ T = \\frac{RegSS}{2} \\] <p>The test-statistic follows a chi-square distribution with \\(q\\) degrees of freedom, where \\(q\\) is the number of variables in \\(\\boldsymbol{Z}\\).</p> \\[ T \\sim \\chi^2_q \\]"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#dealing-with-heteroscedasticity","title":"Dealing with Heteroscedasticity","text":"<p>If prior information is known about the structure of the data, then the most intuitive method would be to incorporate that information into the data.</p> \\[ Var(\\varepsilon_i) = \\frac{\\sigma^2}{w_i} \\] <p>If no prior information is known, then the Heteroscedasticity can be reduced by using a Variance Stabilizing Transformation, such as the Log or Squareroot. Note that since they require positive data, a constant can be added to each term before the transformation to ensure that the values are positive.</p> <p>It is out of the scope for this set of notes to show why these help to stabilize variance.</p> <p>Alternatively, if there is only mild heteroscedasticity in the data, then OLS can be used but with an adjustment to the standard errors of the coefficients, known as heteroscedastic-robust standard errors.</p> <p>Due to complexity of the computations, it will not be covered in this set of notes. However, the general idea is that an weighted estimate of the variance covariance matrix is computed and the standard errors are computed from there.</p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#2-no-serial-correlation","title":"#2: No Serial Correlation","text":"<p>If errors are correlated with one another, it is known as Serial Correlation or Autocorrelation.</p> <p>It implies that there are other unmodelled factors that can be used for prediction, which would imply the current model specification to be wrong.</p> <p>Thus, for the SLR model to be true, the errors must be independent of one another.</p> \\[ Cov(\\varepsilon_i,\\varepsilon_j) = 0 \\] <p>Confidence Intervals and PI are narrower than it should be &gt; 95% PI is actually &lt; 95%&gt; P values lower &gt; Appear statisticlaly significant when they shld not be</p> <p>Time series tends to have errors that are positively correlated, which is why it has its own dedicated section No Serial Correlation &gt; Outcome of zero conditional mean, but most likely in time series data</p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#error-distribution","title":"Error Distribution","text":"<p>Although not needed for OLS estimation or Guass Markov, the errors of the regression are usually assumed to be normally distributed.</p> \\[ \\varepsilon \\sim N(0, \\sigma^2) \\] <p>If the errors are normally distributed, then it follows that \\(\\beta\\) is normally distributed as well since they are linear and additive. This greatly eases the computation needed to determine the sampling distribution for statistical inference.</p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/4.%20Gauss%20Markov%20Theorem/#q-q-plots","title":"Q-Q Plots","text":"<p>Since the errors are normally distributed, the residuals should be normally distributed as well. This can be verified using a Quantile-Quantile Plot (QQ Plot), which compares the quantiles of two distributions.</p> <p>The first distribution is plotted on the x-axis while the second on the y-axis. If the quantiles are the same, then the points should lie on \\(y = x\\), the 45 degree line.</p> <p></p>"},{"location":"3.%20Predictive%20Analytics/ASA-SRM/5.%20Statistical%20Learning/","title":"Statistical Learning","text":""}]}