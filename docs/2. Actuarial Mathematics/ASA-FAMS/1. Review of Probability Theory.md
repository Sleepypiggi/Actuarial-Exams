# **Review of Probability Theory**

## **Basic Probability**

Probability is the study of **Experiments** whose results cannot be predicted with certainty. The result of such an experiment is known as its **Outcome**.

The **Sample Space** $\left(\Omega \right)$ is the *set* of ALL possible outcomes from an experiment. The **Event Space** $(E)$ is a *subset* of the sample space, representing only the outcomes that we are interested in studying. Conversely, its **Complement** $(E^c)$ is the set of all OTHER outcomes not inside $E$.

The probability of the event occuring is the ratio of the **number of elements** in the event to the sample space. It is a measure of the *chance* that the outcome of the experiment is inside the event space.

$$
    P(E) = \frac{n(E)}{n\left(\Omega \right)}
$$

Consider the probability of rolling an odd number on a standard dice:

* **Experiment** - Rolling a dice
* **Outcome** - The number showed on the dice
* **Sample Space** - ${1, 2, 3, 4, 5, 6}$
* **Event Space** - ${1, 3, 5}$
* **Complement** - ${2, 4, 6}$
* **Probability of Event** - $\frac{3}{6}$
* **Probability of Complement** - $\frac{3}{6}$

Within the same experiment, there may be multiple events of interest. For any two events A and B, its **Union** $(A \cup B)$ is the set with outcomes that are **either in A or B** while their **Intersection** $(A \cap B)$ is the set with outcomes that are **in BOTH A and B**.

If both A and B have no outcomes in common $(A \cap B = \emptyset)$, then they are said to be **Mutually Exclusive**. Naturally, an event and its complement are always mutually exclusive.

!!! Warning

    The following *seems intuitive*, but is actually a common mistake:

    $$
        (A \cap B)^c \ne A^c \cap B^c
    $$

    This is properly explained through **De-morgans Law**:

    <!-- Obtained from OnlineMathLearning -->
    ![DeMorgan](Assets/1.%20Review%20of%20Probability%20Theory.md/DeMorgan.png){.center}

    It can be easily remembered by applying the complement to all components of the expression, **including the intersection/union symbol**:

    $$
    \begin{aligned}
        \cap^c &= \cup \\
        \cup^c &= \cap
    \end{aligned}
    $$

### **Probability Axioms**

**Axiom 1** states that all probabilities must be **non-negative**:

$$
    P(E) \ge 0
$$

**Axiom 2** states that probability of the Sample Space is exactly equal to 1:

$$
    P(\Omega) = 1
$$

**Axiom 3** states the probability of a union of **mutually exclusive** events is equal to the sum of their probabilities, known as **Countable Additivity**.

$$
\begin{aligned}
    A \cap B &= \emptyset \\
    P(A \cup B) &= P(A) + P(B)
\end{aligned}
$$

Based on these axioms, several other important properties can also be deduced:

* **Monoticity**: $A \subset B \rightarrow P(A) \le P(B)$
* **Empty Set**: $P(\emptyset) = 0$
* **Complement Rule**: $P(E^c) = 1 - P(E)$
* **Numeric Bound**: $0 \le P(E) \le 1$
* **Sum Rule**:  $P(A \cup B) = P(A) + P(B) - P(A \cap B)$

### **Conditional Probability**

Conditional Probabilities are denoted by $P(A|B)$, which is the probability of event A occuring **given that event B has already occurred**.

The intuition is best understood by considering the following - Given that event B has already occured, what is the probability that event A also occurs?

The event space is $A \cap B$, as we are interested in the probability that both A and B occur. However, since event B has already occured, the sample space is no longer all possible outcomes but rather the event space for B!

$$
\begin{aligned}
    P(A|B)
    &= \frac{n(A \cap B)}{n(B)} \\
    &= \frac{\frac{n(A \cap B)}{n(\Omega)}}{\frac{n(B)}{n(\Omega)}} \\
    &= \frac{P(A \cap B)}{P(B)}
\end{aligned}
$$

Following this expression, the **probability of an intersection** of two events is given by:

$$
    P(A \cap B) = P(A|B) \cdot P(B) = P(B|A) \cdot P(A)
$$

<!-- Obtained from Probability Course -->
![Conditional Probability](Assets/1.%20Review%20of%20Probability%20Theory.md/Conditional%20Probability.png){.center}

Most experiments involving conditional probabilities are multi-staged experiments, which are best visualized using **Probability Trees**:

![Probability Trees](Assets/1.%20Review%20of%20Probability%20Theory.md/Probability%20Tree.png){.center}

Instead of calculating conditional probabilities from scratch, some questions provide the conditional probability $P(A|B)$ (or the components to do so!) and ask us to **find the reverse** - $P(B|A)$.

$$
    P(B|A) = \frac{P(B \cap A)}{P(A)}
$$

The formula is the same as before, but the issue is that the unconditional probability of event A is usually not given. This problem is accounted for in **Bayes Theorem**:

$$
\begin{aligned}
    A &= (A \cap B) + (A \cap B^c) \\
    P(A) &= P(A \cap B) + P(A \cap B^c) \\
    P(A) &= P(A|B) \cdot P(B) + P(A|B^c) \cdot P(B^c) \\
    \\
    \therefore P(B|A) &= \frac{P(A|B) \cdot P(B)}{P(A|B) \cdot P(B) + P(A|B^c) \cdot P(B^c)}
\end{aligned}
$$

Note that if the Conditional Probability of A given B is the same as the unconditional probability of A, then **events A and B are independent**; B has no effect on A.

Thus, the **probability of an intersection** of two independent events is simply their product:

$$
\begin{aligned}
    P(A|B) &= \frac{P(A \cap B)}{P(B)} \\
    P(A) &= \frac{P(A \cap B)}{P(B)} \\
    P(A \cap B) &= P(A) \cdot P(B)
\end{aligned}
$$

## **Random Variables**

Unlike rolling a dice, the outcome of most experiments are **non-numeric**, which makes them hard to work with. For instance, the outcomes of a coin toss are "Heads" and "Tails".

A **Random Variable** is a **many to one function** that *maps* each outcome to a single real number. Each outcome must have only one corresponding number, but different outcomes can have the same value.

!!! Note

    Although the mapping is deterministic, the underlying experiment is still random which is why it is still a "random" variable.

<!-- Obtained from Helping With Math -->
![Many to one function](Assets/1.%20Review%20of%20Probability%20Theory.md/Many%20to%20One%20Function.png){.center}

The **range of possible values** that the random variable can take is known as its **Support**. They are broadly categorized based on its support:

<center>

| Discrete | Continuous |
|:-:|:-:|
| Countable Support | Uncountable Support |
| 1, 2, 3, 4, ... | 1, 1.1, 1.01, 1.001, ... |

</center>

Random variables are denoted using **upper case letters** (X, Y, Z) while their corresponding values are denoted using **lower case letters** (x, y, z) and their appropriate **subscripts**.

The notation $X(s) = x_1$ denotes that the random variable $X$ maps the outcome $s$ to the value of $x_1$. Thus, the **corresponding probability** is denoted by $P(X = x_1)$.

<!-- Obtained from DSA201 Notes -->
![Random Variable](Assets/1.%20Review%20of%20Probability%20Theory.md/Random%20Variable.png){.center}

### **Probability Distributions**

Similar to how a random variable maps the outcomes to a real number, a **Probability Distribution** is a function that maps the outcomes to its **probability of occurrence**.

For **Discrete Random Variables**, their distribution is described using a **Probability Mass Function** (PMF). The PMF provides the probability that the random variable is **exactly equal** to some value $(X = x_1)$.

It is typically denoted in **lower case** and sometimes includes a **subscript of the random variable** when working with multiple to distinguish them from one another.

$$
\begin{aligned}
    P(X = a) &= p(a) \\
    \\
    P(X = a) &= p_X(a) \\
    P(Y = a) &= p_Y(a)
\end{aligned}
$$

Since it is a probability measure, the **sum of the PMF** over the support of the random variable **must be equal to 1** (Probability Axiom).

$$
    \sum_{x \in \text{Support}} p(x) = 1
$$

PMFs can be represented in three main ways - Functions, Tables or Histograms.

For **Continuous Random Variables**, their distribution is described using a **Probability Density Function** (PDF). The PDF is a non-negative function where the **area under it** provides the probability that the random variable takes on some **range of values** $(a \le X \le b)$.

Similarly, it is typically denoted in lower case and includes a subscript when working with multiple random variables:

$$
\begin{aligned}
    P(a \le X \le b) = \int^b_a f(x) \\
    \\
    P(a \le X \le b) = \int^b_a f_X(x) \\
    P(a \le Y \le b) = \int^b_a f_Y(y)
\end{aligned}
$$

<!-- Obtained from BYJU -->
![Probability Density Function](Assets/1.%20Review%20of%20Probability%20Theory.md/Probability%20Density%20Function.png){.center}

Similarly, since the area is a probability measure, the **total area** under the graph **must be equal to 1**:

$$
    P(-\infty \le X \le \infty) = \int^{\infty}_{-\infty} f(x) = 1
$$

!!! Note

    $\infty$ is used as a catch all for the upper and lower bound of the random variable. If the actual bounds are known, then using them instead is more appropriate.

Additionally, note that the **probability of a specific value** for a continuous RV is 0. This is because there is an **infinite number of possible values**, thus the probability of a specific value (EG. 1.45679383920) is **infinitely small** such that it is assumed to be 0.

$$
    P(X = a) = \int^{a}_{a} f(x) = 0
$$

The **Cumulative Distribution Function** (CDF) is the probability that the random variable is **less than or equal** to some value $X \le t$.

It is typically denoted in **upper case** to distinguish it from the PDF and includes subscripts as well when working with multiple random variables.

$$
\begin{aligned}
    F(t) &= P(X < t) \\
    \\
    F_X(t) &= P(X < t) \\
    F_Y(t) &= P(Y < t)
\end{aligned}
$$

For discrete variables, the CDF is the **sum of all probabilities before the specified value**. Following this, the **difference of consecutive CDFs** allows us to obtain the PMFs at that value:

$$
\begin{aligned}
    F(t) &= \sum_{x \le t} p(x) \\
    p(x_i) &= F(x_i) - F(x_{i-1})
\end{aligned}
$$

For continuous variables, the CDF is the **integral from the lower bound to the specified value**. However, instead of integrating with respect to an actual value, it is better to integrate with respect to a dummy variable $t$ to obtain a **general expression for the CDF**, allowing it to be easily calculated for any value.

Although the CDF is very useful, it **can only be used to calculate probabilities starting from the lower bound**. When probabilities starting from other ranges are needed, the PDF can be obtained from the CDF by differentiating it and then re-integrating with different limits.

$$
\begin{aligned}
    F(t) &= \int^{t}_{-\infty} f(x) dx \\
    \\
    F(t) &= \int^{t}_{-\infty} f(t) dt \\
    f(t) &= F'(t)
\end{aligned}
$$

### **Moments**

The **Moments** of a distribution are quantities that describe **characteristics of the distribution**.

**Raw Moments** are calculated with respect to the **origin**. The **n-th raw moment** is calculated as the following:

$$
\begin{aligned}
    E(X^n) &= \int x^n \cdot f(x) dx \\
    \mu'_k &= \sum x^n \cdot p(x)
\end{aligned}
$$

The **first raw moment** is known as the **Mean**, which is a measure of the **Centrality** of the distribution. It is commonly denoted as $\mu$, without any super or subscripts.

It has several important properties (more at the section on transformation):

1. $E(c) = c$
2. $E(X + Y) = E(X) + E(Y)$

**Central Moments** are calculated with respect to the **mean**. The **n-th central moment** is calculated as the following:

$$
\begin{aligned}
    E[(X - \mu)^n] &= \int (x - \mu)^n \cdot f(x) dx \\
    \mu_k&= \sum (x - \mu)^n \cdot p(x)
\end{aligned}
$$

The **second central moment** is known as the **Variance**, which is a measure of the **Spread** of the distribution about the mean.

Since calculating central moments directly is complicated, it can be simplified to an expression involving raw moments:

$$
\begin{aligned}
    Var(X)
    &= E[(X - \mu)^2] \\
    &= E(X^2 - 2\mu X + \mu^2) \\
    &= E(X^2) - 2\mu^2 + \mu^2 \\
    &= E(X^2) - \mu^2 \\
    &= E(X^2) - [E(X)]^2
\end{aligned}
$$

It has several important properties (more at the section on transformation):

1. $Var(c) = 0$
2. $Var(X+b) = Var(X) + Var (Y) + 2 \cdot Cov(X,Y)$

<!-- Obtained from Coaching Actuaries -->
![Variance](Assets/1.%20Review%20of%20Probability%20Theory.md/Variance.png){.center}

However, one problem with variance is that it uses squared units, which makes it hard to intepret. Thus, the **squareroot of the variance** is used instead, known as the **Standard Deviation**.

$$
    \sigma = \sqrt{Var(X)}
$$

Similarly, standard deviation cannot be used to compare data with **different units**. Thus, the **Coefficient of Variation** is used instead, which is a **unitless measure** of the spread of the distribution.

$$
    CV(X) = \frac{\sigma}{\mu}
$$

The **third central moment** is **Skewness**, which is a measure of the **symmetry** of distribution about the mean. Being left/right skewed means that the distribution has a "longer tail" on that side, which implies that **values on the opposite side are more likely to occur**.

!!! Note

    Skewness is also sometimes referred to as being **Positively or Negatively Skewed**. An easy way to remember is that positive values occur to the right of the origin, hence is the same as being right skewed; vice-versa.

$$
\begin{aligned}
    \text{Skewness}
    &= \frac{E[(X - \mu)^3]}{\sigma^3} \\
    &= \frac{E[(X - \mu)^3]}{(\sigma^2)^\frac{3}{2}} \\
    &= \frac{E(X^3) - 3 E(X^2) \cdot E(X) + 2 [E(X)]^3}{(E(X^2) - [E(X)]^2)^\frac{3}{2}}
\end{aligned}
$$

<!-- Obtained from Coaching Actuaries -->
![Skewness](Assets/1.%20Review%20of%20Probability%20Theory.md/Skewness.png){.center}

The fourth central moment is **Kurtosis**, which is a measure of the **flatness** of the distribution, typically with respect to the normal distribution. It is indicative of the **likelihood of producing extreme values** (outliers).

The normal distribution has a kurtosis of 3. If a distribution has a kurtosis greater than 3, then it is flatter and hence more likely to produce outliers as compared to the normal distribution.

$$
\begin{aligned}
    \text{Kurtosis}
    &= \frac{E[(X - \mu)^4]}{\sigma^4} \\
    &= \frac{E[(X - \mu)^4]}{(\sigma^2)^2} \\
    &= \frac{E(X^4) - 4 E(X^3) \cdot E(X) + 6 E(X^2) \cdot [E(X)]^2 - 3 [E(X)]^4}{(E(X^2) - [E(X)]^2)^2}
\end{aligned}
$$

<!-- Obtained from AnalystPrep -->
![Kurtosis](Assets/1.%20Review%20of%20Probability%20Theory.md/Kurtosis.png){.center}

### **Statistical Metrics**

Apart from the moment of the distribution, there are some other **Statistical Metrics** that provide useful information.

The **Median** is the value of the random variable that seperates the upper and lower half of the probability distribution.

For discrete variables, the median $M$ is the smallest value such that $P(X \le M) \ge 0.5$ and $P(X \ge M) \ge 0.5$.

For continuous variables, the median $M$ is found by solving $F(M) = 0.5$. The key difference is that the continuous median is the value that **exactly seperates** the distribution while the discrete one approximately splits it, depending on the PMF.

The **Mode** is the value of the random variable that maximises the PMF or PDF. It is the **most likely outcome** of the experiment (loosely speaking for continuous variables).

The **Percentile** is the value of the random variable below which a **certain percentage of observations fall**. For instance, the 85th percentile is the value below which 85% of the observations fall.

Let $p$ be the percentage of observations. The **100p-th percentile** for a discrete variable is the smallest value $a$ such that $P(X \lt a) \le p \lt P(X \le a)$.

Similar as before, for continuous variables, the percentile is found by solving $F(a) = p$.

!!! Note

    100p-th looks strange because p is a percentage. For instance, if $p$ is 0.85, then 100p is 85, representing the 85-th percentile.

    Also, the methods for Percentiles and Medians look similar because the median is simply the 50th percentile!

The 25th, 50th & 75th percentile are known as the first, second & third **Quartiles** $(q)$ respectively. The difference between the 3rd and 1st quartile is known as the **Inter Quartile Range**.

$$
    IQR = q_3 - q_1
$$

### **Shifting, Scaling & Transformation**

An existing random variable $X$ can be adjusted in order to make a new random variable $Y$.

If a constant $a$ has been **multiplied** to the random variable, then it has been **Scaled** by $a$:

$$
\begin{aligned}
    Y &= aX \\
    \\
    F_Y(y)
    &= P(Y \le y) \\
    &= P(aX \le y) \\
    &= P \left(X \le \frac{y}{a} \right) \\
    &= F_X \left(\frac{y}{a} \right) \\
    \\
    f_Y(y)
    &= \frac{d}{dy} F_Y(y) \\
    &= \frac{1}{a} \cdot f_X \left(\frac{y}{a} \right)
\end{aligned}
$$

If a constant $b$ is **added** to the random variable instead, then it has been **Shifted** by $b$:

$$
\begin{aligned}
    Y &= X + b \\
    \\
    F_Y(y)
    &= P(Y \le y) \\
    &= P(X + b \le y) \\
    &= P(X \le y - b) \\
    &= F_X (y - b) \\
    \\
    f_Y(y)
    &= \frac{d}{dy} F_Y(y) \\
    &= f_X (y - b)
\end{aligned}
$$

For both scaling and transformation, the expectation and variance can be easily determined if that of the original is known as well:

$$
\begin{aligned}
    E(cX) &= c \cdot E(X) \\
    E(X+c)
    &= E(X) + E(c) \\
    &= E(X) + c
    \\
    Var(cX) &= c^2 \cdot Var(x) \\
    Var(X+c)
    &= Var(X) + Var(c) \\
    &= Var(X)
\end{aligned}
$$

If the random variable has been **raised by a power** of $\frac{1}{c}$ where $c \ne 1$, then it has been **Power Transformed**:

$$
\begin{aligned}
    Y &= X^{\frac{1}{c}} \\
    \\
    F_Y(y)
    &= P(Y \le y) \\
    &= P(X^{\frac{1}{c}} \le y) \\
    &= P(X \le y^c) \\
    &= F_X (y^c) \\
    \\
    f_Y(y)
    &= \frac{d}{dy} F_Y(y) \\
    &= cy^{c-1}\cdot f_X (y - b)
\end{aligned}
$$

If the random variable has been **exponentiated**, then it has also been **Exponential Transformed**:

$$
\begin{aligned}
    Y &= e^X \\
    \\
    F_Y(y)
    &= P(Y \le y) \\
    &= P(e^X \le y) \\
    &= P(X \le \ln y) \\
    &= F_X (\ln y) \\
    \\
    f_Y(y)
    &= \frac{d}{dy} F_Y(y) \\
    &= \frac{1}{y} \cdot f_X (\ln y)
\end{aligned}
$$

For both types of transformations, there is no simple method of determining the mean and variance. The various raw moments must be **manually determined via integration**.

### **Generating Functions**

Another characteristic
PGF and MGF uniquely identify the distribution

#### **Moment Generating Function**

Both discrete and continuous

$$
\begin{aligned}
    M_x(t)
    &= E(e^{tX}) \\
    &= \sum e^{tx} \cdot p(x)
\end{aligned}
$$

As its name suggests, the MGF can be used to **calculate the raw moments** of the distribution. To obtain the **k-th raw moment**,

1. **Differentiate** the MGF k times
2. **Evaluate** the expression at $t=0$

$$

$$

However, the main benefit of MGFs is that they **uniquely identify** a distribution. If two random variables have the **same MGF**, then they have the **same distribution**.

This becomes especially useful when dealing with **linear combinations of INDEPENDENT random variables**. By determining the MGF of the combination, its exact distribution can be determined.

$$
\begin{aligned}
    Y &= X_1 + X_2 + ... + X_k \\
    \\
    M_Y(t)
    &= E(e^{tY}) \\
    &= E(e^{t(X_1 + X_2 + ... + X_k)}) \\
    &= E(e^{tX_1} \cdot e^{tX_2} \cdot ... \cdot e^{tX_k}) \\
    &= \prod E(e^{tx}) \\
    &= \prod M_x(t)
\end{aligned}
$$

#### **Probability Generating Function**

Discrete only

$$
\begin{aligned}
    P(t)
    &= E(t^x) \\
    &= \sum t^X \cdot p(x)
\end{aligned}
$$

As its name suggests, the PGF can be used to calculate the **individual probabilities** of the distribution. To obtain the **probability of the k-th value**,

1. **Differentiate** the PGF k times
2. **Divide** the expression by k factorial
3. **Evaluate** the expression at $t=0$

$$

$$

Additionally moments

$$
\begin{aligned}

\end{aligned}
$$

Uniquely identifies distribution

### **Conditional Distributions**

The concept of conditional probabilities can be extended to Random Variables as well. In particular, one random variable can be conditional on another random variable, resulting in a **Conditional Distribution** $(X|Y)$.

Most problems require us to find the **Unconditional Distribution** given only the conditional distributions:

* $X$ is the random variable denoting the test grades (A, B, C)
* $Y$ is the random variable denoting the gender (M, F)

The teacher would like to find the overall distribution of test scores $(X)$, but only has the **conditional distribution** of the scores of the students for each gender $(X|Y)$ and the proportion of the Genders $(Y)$.

The **Law of Total Probability** can be used to determine the **unconditional probability** of $X$:

$$
\begin{aligned}
    P(X = a)
    &= E_Y[P(X = a|Y)] \\
    &= \sum_{y} P(X = a | y) \cdot p(Y = y)
\end{aligned}
$$

Note that this is equivalent to adding up the final probabilities from the relevant branches from a probability tree:

<!-- Obtained from Cleanpng -->
![Law of Total Probability](Assets/1.%20Review%20of%20Probability%20Theory.md/Law%20of%20Total%20Probability.png){.center}

Naturally, this also means that the **unconditional CDF** can be obtained in similar fashion:

$$
\begin{aligned}
    F(a)
    &= E_Y[F(a)] \\
    &= \sum_{y} F(a) \cdot p(Y = y)
\end{aligned}
$$

Following the same logic, the **Law of Total Expectation** can be used to determine the **unconditional expectation** of $x$:

$$
\begin{aligned}
    E(X)
    &= E_Y[E_X(X|Y)] \\
    &= \sum_{y} E_X(X|Y) \cdot p(Y = y)
\end{aligned}
$$

The **Law of Total Variance** can be used to determine the **unconditional variance** of $x$. However, unlike the previous two, it is NOT simply the expectation of the conditional variance:

$$
\begin{aligned}
    Var (X) &= E_Y [Var_X(X|Y)] + Var_Y[E_X(X|Y)] \\
    \\
    E_Y [Var_X(X|Y)] &= \sum_{y} Var_X(X|Y) \cdot p(Y = y) \\
    \\
    Var_Y[E_X(X|Y)]
    &= E_Y[E_X(X|Y)^2] - (E_Y[E_X(X|Y)])^2 \\
    &= \sum_{y} E_X(X|Y)^2 \cdot p(Y = y) - \sum_{y} E_X(X|Y) \cdot p(Y = y)
\end{aligned}
$$

Alternatively, the Unconditional Variance can be **directly calculated** using the typical formula of $E(X^2) - [E(X)]^2$, where the **two unconditional expectations** are calculated using the law of total expectation.

Alternatively once more, if the conditional distribution $Y$ only has two outcomes, then the **Bernoulli Shortcut** (covered in a later section) can be used to quickly compute the value of $Var_Y[E_X(X|Y)]$.

!!! Note

    The discrete case was shown in this section due to its simplicity. All the same concepts apply to the continuous variables as well - simply replace the summation & PMFs with integrals and PDFs.

#### **Mixture Distributions**

A mixture distribution is a distribution whose values can be intepreted as being derived from an underlying set of **other random variables**.

In an insurance context, a Homeowners Insurance claim could be from a fire, burglary or liability accident. To model it, we could use a mixture that is made up of the basic distributions used to individually model each type of accident.

If the mixture contains a countable number of other distributions, then it is known as a **Discrete Mixture**. Otherwise, it is known as a **Continuous Mixture**.

!!! Warning

    It is a common mistake to think that a discrete mixture is only made up of discrete distributions, vice-versa.

    Any type of distribution can be included in a mixture; the classification is based on the number of distributions.

The random variable $X$ is a **k-point mixture** if its **probability functions** can be expressed as the **weighted average** of the probability functions of the $k$ distributions $X_1, X_2, ... X_k$:

$$
\begin{aligned}
    F_X(x) &= w_1 \cdot F_{X_1}(x) + w_2 \cdot F_{X_2}(x) + ... + w_k \cdot F_{X_k}(x) \\
    f_X(x) &= w_1 \cdot f_{X_1}(x) + w_2 \cdot f_{X_2}(x) + ... + w_k \cdot f_{X_k}(x)
\end{aligned}
$$

!!! Note

    For this exam, questions will usually only use **2 or 3 point mixtures**.

$w$ represents the **mixing weights**, such that $w_1 + w_2 + ... + w_k = 1$. It can be intepreted that $Y$ **follows the distribution** of $X_1$ $100w_1 \%$ of the time, follows the distribution of $X_2$ $100w_2 \%$ of the time etc.

!!! Warning

    Another common mistake is confusing mixtures with **Linear Combinations** of random variables:

    $$
        X \ne w_1 X_1 + w_2 X_2 + ... + w_k X_k
    $$

    In a linear combination, $X$ **neither follows the distribution** of any of the $X_k$. Furthermore, since $w_k$ are not weights, they can be **any real number and do not need to sum to 1**.

The mixing weights can also be thought of as **Discrete Probabilities** that come from a random variable $Y$ with the support $\set{1, 2, ..., k}$.

Thus, we can think of the overall mixture $X$ as an **unconditional distribution** while each of the underlying distributions are conditional distributions $X|Y$. This allows us to make use of the all the **previous results** from the conditional distributions:

$$
\begin{aligned}
    P(X = a) &= E_Y[P(X = a|Y)] \\
    F(a) &= E_Y[F(a)] \\
    E(X) &= E_Y[E_X(X|Y)] \\
    Var (X) &= E_Y [Var_X(X|Y)] + Var_Y[E_X(X|Y)]
\end{aligned}
$$

<!-- TBC
Independence?
First central moment is 0
Tail Weights
Empirical Distribution
 -->