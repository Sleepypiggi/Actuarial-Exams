# **Frequency Models**

## **Counting Distributions**

Number of claims
P_k notation

PGF is provided for all discrete distributions in case

### **Poisson Distribution**

The **Poisson Distribution** is used to count the number of random events in a **specified unit of space or time**.

It only has **one parameter** $\lambda$, the **mean** number of occurrences in the specified unit of space or time. The key property is that its **mean and variance are both equal** to $\lambda$.

$$
\begin{aligned}
    N &\sim \text{Poisson}(\lambda) \\
    \\
    p_n &= \frac{e^{-\lambda} \cdot \lambda^k}{k!} \\
    \\
    E(N) &= \lambda \\
    Var (N) &= \lambda
\end{aligned}
$$



The sum of $k$ independent poisson random variables is **still poisson**:

$$
\begin{aligned}
    N_n &\sim \text{Poisson}(\lambda_k) \\
    N &= N_1 + N_2 + ... + N_k \\
    \therefore N &\sim \text{Poisson} (\lambda_1 + \lambda_2 + ... + \lambda_k)
\end{aligned}
$$

### **Bernoulli Distribution**

The **Bernoulli Distribution** is used to determine the outcome of a **Bernoulli Trial**. They are experiments with only **two possible outcomes**.

For a **Standard Bernoulli Distribution**, the two outcomes are **Successes and Failures**, denoted by 1 and 0 respectively:

$$
\begin{aligned}
    X &\sim \text{Bernoulli} (q) \\
    \\
    p_x &=
    \begin{cases}
        \text{Success } (1),& \text{Probability} = q, \\
        \text{Failure } (0),& \text{Probability} = 1-q
    \end{cases} \\
    \\
    E(X) &= q \\
    Var (X) &= q(1-q)
\end{aligned}
$$

However, the **Bernoulli Shortcut** generalizes this for **any two mutually exclusive outcomes**, denoted by $a$ and $b$ respectively:

$$
\begin{aligned}
    Y &= (a-b)X + b \\
    \\
    Y &=
    \begin{cases}
        \text{Outcome 1 } (a),& \text{Probability} = q, \\
        \text{Outcome 2 } (b),& \text{Probability} = 1-q
    \end{cases} \\
    \\
    E(Y)
    &= E[(a-b)X + b] \\
    &= (a-b)q + b \\
    \\
    Var (Y)
    &= Var[(a-b)X + b] \\
    &= (a-b)^2 \cdot q(1-q)
\end{aligned}
$$

Note that there is no need to memorize the mean and variance for the bernoulli shortcut - they can be easily derived from the standard distribution.

### **Binomial Distribution**

The **Binomial Distribution** is used to count the **number of successes** out of a **fixed number of independent Standard Bernoulli Trials**.

It has **two parameters**:

1. The **number** of independent trials, $m$
2. The **probability** of success for each trial, $q$

$$
\begin{aligned}
    N &\sim \text{Binomial} (m, q) \\
    \\
    p_n &= {m \choose k} q^k (1-q)^{1-k} \\
    \\
    E(N) &= nq \\
    Var (N) &= nq(1-q)
\end{aligned}
$$

The binomial distribution is actually the **sum of $m$ independent standard bernoulli variables** with the *same probability*:

$$
\begin{aligned}
    X_k &\sim \text{Bernoulli} (q) \\
    N &= X_1 + X_2 + ... + X_m \\
    \therefore N &\sim \text{Binomial} (m, q)
\end{aligned}
$$

!!! Note

    A bernoulli distribution is simply a binomial distribution with $m=1$.

Thus, the sum of $k$ independent binomial variables with the *same probability* will **still binomial**:

$$
\begin{aligned}
    N_k &\sim \text{Binomial}(m, q) \\
    N &= N_1 + N_2 + ... + N_k \\
    \therefore N &\sim \text{Binomial} (m_1 + m_2 + ... + m_k, q)
\end{aligned}
$$

This is intuitive, because the sum of binomial variables is actually the **sum of even more bernoulli variables**, which as shown previously, will follow the binomial distribution.

### **Geometric Distribution**

The **Geometric Distribution** can be understood in one of two ways, with respect to a **sequence of independent bernoulli trials**:

1. **Number of trials** needed to get the **first success**, $N \in \set{1, 2, 3, ...}$
2. **Number of failures** needed to get the **first success**, $F \in \set{0, 1, 2, ...}$

If not explicitly stated, the two intepretations can be distinguished from their supports - there must be at least 1 trial but there can be 0 failures. It is useful to **remember just one intepretation** and understand how they are related:

$$
    F = N-1
$$

The **second intepretation is preferred**, since that is what is provided in the formula sheet. Regardless, the Geometric distribution only has **one parameter**, the probability of success:

$$
\begin{aligned}
    N &\sim \text{Geom} (q) \\
    \\
    p_k &= (1-q)^{k} \cdot q \\
    \\
    E(N) &= \frac{1-p}{p} \\
    Var (N) &= \frac{1-p}{p^2}
\end{aligned}
$$

The key property is that it is **Memoryless**. Intuitively, the geometric distribution is "waiting" for the first success to occur.

However, it **does not matter how many failures have occurred**, the probability of the first success occuring in another $k$ failures is **independent of the history of the process**. The distribution "forgets" (memoryless) the current state that the process is in.

$$
    P(N > m + n | N \ge m) = P(N > n)
$$

### **Negative Binomial Distribution**

The **Negative Binomial Distribution** is used to count the **number of failures** to get a **fixed number of successes**, with respect to a sequence of independent bernoulli trials.

It has two parameters:

1. The number of successes, $r$
2. The probability of success, $q$

$$
\begin{aligned}
    N &\sim \text{Negative Binomial} (r, q) \\
    \\
    p_n &= {{k+r-1} \choose {k}} (1-p)^k p^{r} \\
    \\
    E(N) &= \frac{r(1-p)}{p} \\
    Var (N) &= \frac{r(1-p)}{p^2}
\end{aligned}
$$

!!! Note

    The distribution is also referred to as **Pascals Distribution**, but is most commonly called the "negative" binomial because it uses a **negative integer in the binomial coefficient**:

    $$
    \begin{aligned}
        {n \choose k}
        &= \frac{n!}{k! \cdot (n-k)!} \\
        \\
        {{k+r-1} \choose {k}}
        &= \frac{(k+r-1)!}{k! \cdot (r-1)!} \\
        &= \frac{(k+r-1) \cdot (k+r-2) \cdot \dots \cdot (r)}{k!} \\
        &= (-1)^k \cdot \frac{(-r) \cdot (-r-1) \cdot \dots \cdot (-r-k+1)}{k!} \\
        &= (-1)^k {-r \choose k}
    \end{aligned}
    $$

The negative binomial distribution is actually the **sum of $r$ i.i.d. geometric variables** with the *same probability*:

$$
\begin{aligned}
    X_k &\sim \text{Geom} (q) \\
    N &= X_1 + X_2 + ... + X_r \\
    \therefore N &\sim \text{Negative Binomial} (r, q)
\end{aligned}
$$

!!! Note

    A geometric distribution is simply a negative binomial distribution with $r=1$.

Thus, following the same logic as before, the sum of $k$ independent negative binomial variables with the same probability will **still be negative binomial**:

$$
\begin{aligned}
    N_k &\sim \text{Negative Binomial}(r, q) \\
    N &= N_1 + N_2 + ... + N_k \\
    \therefore N &\sim \text{Negative Binomial} (r_1 + r_2 + ... + r_k, q)
\end{aligned}
$$

### **Poisson Gamma Mixture**

TBC
Re-parameterized, refer to wikipedia
Odds vs Probabiliyu

## **(a, b, 0) Class**

ONLY the distributions discussed above fall into the **(a, b, 0) class** of distributions, as their **PMFs follow the same recursion**:

$$
\begin{aligned}
    \frac{p_{n}}{p_{n-1}} &= a + \frac{b}{n} \\
    p_n &= \left(a + \frac{b}{n} \right) p_{n-1}
\end{aligned}
$$

$a$ and $b$ are constants that are unique to each distribution while the "0" comes from the fact that the recursion starts from $n-1=0$.

<!-- Obtained from MBFinan Exam C -->
![ab0 class](Assets/3.%20Frequency%20Models.md/ab0%20class.png)

Notice that each distribution has a **different signs** for $a$. Thus, given the recursive equation, the **underlying distribution can be determined** based on the sign of $a$.

## **(a, b, 1) Class**

When using the (a, b, 0) class of distributions to model experiments, one common problem is that the **probability at $n=0$ does not match experience**.

Thus, there is a need to **modify** the distribution such that the **probability at 0 is at the desired level**. This modification is known as the **Zero-Modification**.

!!! Note

    If the desired level is 0, then the modification is known as a **Zero-Truncation**, which is a special case of the zero-modification. This is because this truncates (removes) the possibility of 0 from the random variable.

This is done by **directly changing** the probability at 0 to the desired level. However, this modification alone would cause the sum of probabilities to deviate from 1, thus the probabilities at all other levels of the support must be **scaled such that they sum to 1**.

Let $N^M$ be the zero-modified distribution and $p^{M}_{n}$ be its PMF.

$$
    p^{M}_{n} = \frac{1-p^{M}_{0}}{1-p_{0}} \cdot p_n
$$

If $N$ is a poisson random variable, then $N^M$ is known as a **Zero-Modified Poisson Random Variable**. However, it is important to note that $N^M$ does NOT follow a poisson distribution - it has its own unique distribution.

Thus, by applying the previous result, the moments of the new distribution can be calculated:

$$
\begin{aligned}
    E \left(N^M \right)
    &= 0 \cdot p^{M}_{0} + 1 \cdot p^{M}_{1} + 2 \cdot p^{M}_{2} + ... \\
    &= 0 \cdot \frac{1-p^{M}_{0}}{1-p_{0}} \cdot p_0 + 1 \cdot \frac{1-p^{M}_{0}}{1-p_{0}} \cdot p_1 + 2 \cdot \frac{1-p^{M}_{0}}{1-p_{0}} \cdot p_2 + ... \\
    &= \frac{1-p^{M}_{0}}{1-p_{0}} (0 \cdot p_0 + 1 \cdot p_1 + 2 \cdot p_2 + ...) \\
    &= \frac{1-p^{M}_{0}}{1-p_{0}} \cdot E(N) \\
    \\
    \therefore E \left[\left(N^M \right)^k \right] &= \frac{1-p^{M}_{0}}{1-p_{0}} \cdot E \left[N^k \right]
\end{aligned}
$$

Note that this result ONLY applies to **raw moments**. In order to calculate the central moments, express them in terms of raw moments and then calculate them from the above conversion.

Even after modification, the PMFs can still follow the same recursion:

$$
\begin{aligned}
    \frac{p^{M}_{n}}{p^{M}_{n-1}}
    &= \frac{\frac{1-p^{M}_{0}}{1-p_{0}} \cdot p_{n}}{\frac{1-p^{M}_{0}}{1-p_{0}} \cdot p_{n-1}} \\
    &= \frac{p_{n}}{p_{n-1}} \\
    &= a + \frac{b}{n}
\end{aligned}
$$

However, since the distribution is zero-modified, the recursion can **only start from $k-1=1$**, which is why they are known as the **(a, b, 1) class** of distributions. The **domain of the recursion** is what allows us to distinguish the two classes of distributions.

!!! Warning

    There are only 4 distributions under the (a, b, 0) class: Poisson, Binomial, Geometric & Negative Binomial.

    It is *seemingly intuitive* to think that their 0-modified versions are the only members of the (a, b, 1) class. However, the **Logarithmic Distribution** is also a member of the (a, b, 1).
    