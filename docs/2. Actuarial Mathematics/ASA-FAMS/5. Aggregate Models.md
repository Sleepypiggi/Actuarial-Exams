# **Aggregate Models**

## **Overview**

The combination of the Frequency Models $(N)$ and the Severity Models $(X)$ discussed previously results in **Aggregate Model** $(S)$, which represents the **total losses** experienced by an insurance company.

$$
\begin{aligned}
    S &= X_1 + X_2 + ... + X_N
\end{aligned}
$$

The main type of aggregate model is known as the **Collective Risk Model**, which makes assumes the following assumptions:

1. **Losses are iid**: $X_1, X_2, ..., X_N$ are iid
2. **Frequency and Severity are independent**: $X$ and $N$ are independent

!!! Tip

    Some questions may not *directly* provide the frequency distribution.

    For instance, we may want to model aggregate revenue of a business. The following two distributions are given:

    * The number customers **per shop** is modelled by a Frequency Distribution
    * The revenue per customer is modelled by a Severity Distribution

    By combining these two distributions, we can model aggregate revenue **per shop**. However, we want the aggregate revenue for the **entire business**.

    If the business has $n$ shops, then we need to **adjust the frequency distribution** accordingly by **scaling it by the number of shops**:

    $$
    \begin{aligned}
        \text{Customers Per Shop} &\sim \text{Poisson}(\mu) \\
        \text{Customers Per Business} &\sim \text{Poisson}(n \cdot \mu)
    \end{aligned}
    $$

    Scaling a random variable by a constant $n$ is simply the **sum of $n$ of those variables**; refer to the frequency distribution section to understand how the sums are distributed. 

## **Collective Risk Model**

The collective risk model is a **sum of an unknown number of random variables**. In probability theory, this is known as a **Compound Distribution** where $N$ is known as the **Primary Distribution** and $X$ is known as the **Secondary Distribution**.

### **Moments**

Another perspective is that the aggregate model follows the **conditional distribution** $S \mid N$. Thus, we can compute the probability, expectation and variance using previously found results.

!!! Note

    It can also be thought of as a mixture distribution where the mixing weights are the probability that the there that number of losses:

    $$
        w_k = P(N=K)
    $$
    
    Thus, **Compound Distributions and Mixture Distributions are essentially the same concept**.

Starting with the **Law of Total Expectation**,

$$
\begin{aligned}
    E(S) &= E_{N} [E_{S}(S \mid N)] \\
    \\
    E_{S}(S \mid N)
    &= E(X_1 + X_2 + \dots + X_N \mid N) \\
    &= E(X_1 \mid N) + E(X_2 \mid N) + \dots + E(X_N \mid N) \\
    &= E(X_1) + E(X_2) + \dots + E(X_N), \ \text{Independence} \\
    &= E(X) + E(X)+ \dots + E(X), \ \text{Identical} \\
    &= N \cdot \underbrace{E(X)}_{\text{Constant}} \\
    \\
    \therefore E(S)
    &= E_{N} [E_{S}(S \mid N)] \\
    &= E_{N} [N \cdot E(X)] \\
    &= E(X)\cdot E(N)
\end{aligned}
$$

<!--
If each policy has a deductible then just E(N) E(X-d)+
-->

!!! Note

    This result is **highly intuitive** as $\text{Risk} = \text{Frequency} \cdot \text{Severity}$.

Moving on to the **Law of Total Variance**:

$$
\begin{aligned}
    Var (S)
    &= E_N [Var (S \mid N)] + Var_{N} [E(S \mid N)] \\
    \\
    Var (S \mid N)
    &= Var (X_1 + X_2 + ... + X_N \mid N) \\
    &= N \cdot \underbrace{Var(X)}_\text{Constant}, \ \text{IID} \\
    \\
    Var (S)
    &= E_N [Var (S \mid N)] + Var_{N} [E(S \mid N)] \\
    &= E [N \cdot Var(X)] + Var_{N} [N \cdot E(X)] \\
    &= Var(X) \cdot E(N) + [E(X)]^2 \cdot Var(N)
\end{aligned}
$$

!!! Tip

    If the **mean and variance of the frequency distribution are equal** (EG. Poisson Distribution), then the Variance can be simplified:

    $$
    \begin{aligned}
        Var (S)
        &= Var(X) \cdot E(N) + [E(X)]^2 \cdot Var(N) \\
        &= E(N) \cdot [Var(X) + [E(X)]^2] \\
        &= E(N) \cdot E(X^2)
    \end{aligned}
    $$

The probability of $s$ is **difficult to derive**. Thus, for simplicity, we will assume a **Discrete Severity** for this portion. By the **Law of Total Probability**:

$$
\begin{aligned}
    P(S = s)
    &= E_{N} [P(S = s \mid N = n)] \\
    &= \sum P(S = s \mid N = n) \cdot P(N = n)
\end{aligned}
$$

!!! Note

    If both the Frequency and Severity models are discrete, then the Aggrgegate Loss is discrete as well.

### **Convolutions**

The term within the above summation is known as a **Convolution**, as it is the probability of a sum of random variables. Since there are $n$ random variables, it is known as the **n-fold Convolution**.

$$
    P(S = s \mid N = n) = P(X_1 + X_2 + \dots + X_n = s)
$$

For simplicity, consider a two fold convolution:

$$
\begin{aligned}
    P(S = s \mid n = 2)
    &= \sum P(X_1 = x, X_2 = s - x_1) \\
    &= \sum P(X_1 = x) \cdot P(X_2 = s - x_1)
\end{aligned}
$$

A convolution is simply the **probability of all possible combinations** of the random variables that result in the specified value (Recall the two dice example).

However, one implicit assumption for this to work is that the **individual severities cannot be 0**. The reason is because it would lead to **endless possibilities** for each case.

Consider $S=1$:

* $1 + 0 + 0 + \dots + 0 = 1$
* $0 + 1 + 0 + \dots + 0 = 1$
* $0 + 0 + 1 + \dots + 0 = 1$
* $0 + 0 + 0 + \dots + 1 = 1$

However, then how is $S=0$ determined, since there are **no sums that result in 0**? There is now **only ONE way** for aggregate losses to be 0 - if there are **no claims at all**.

$$
    P(S = 0) = P(n = 0)
$$

### **Generating Functions**

An alternative method to calculate the moments would be to use the **MGF**:

$$
    M_{S}(t) = M_{N} [\ln M_{X}(t)]
$$

Similarly, if severity is discrete, then an alternative method to calculate its probability would be to use its **PGF**:

$$
    P_{S}(t) = P_{N} [P_{X}(t)]
$$

These methods tend to be more **time consuming**, thus they are preferred only when calculating higher moments or large values of $s$.

### **Approximation**

Another method of determining the probability would be to simply **approximate** it. This is especially useful when the individual losses are continuous, such that the previous methods would not work.

By the **Central Limit Theorem**, as the number of random variables increase, their **sum is approximately normal distributed**:

$$
    S \sim (E[S], Var[S])
$$

Instead of using the normal distribution, the **Lognormal Distribution** can be used as well. However, we need to **first solve for its parameters**:

$$
\begin{aligned}
    e^{\mu + 0.5\sigma^2} &= E(S) \\
    \left(e^{\mu + 0.5\sigma^2} \right)^2 \cdot (e^{\sigma^2}-1) &= Var (S) \\
    \\
    \therefore S \sim \text{Log Normal} (\mu, \sigma^2)
\end{aligned}
$$

!!! Warning

    Although the notation is the same, **do NOT confuse** the lognormal parameters with the normal parameters.

#### **Continuity Correction**

We may still want to use the approximation even when losses are discrete as they may still be **too complicated** to compute.

However, this may lead to some inaccuracies as we are using a continuous distribution to approximate a discrete distribution.

Under a discrete distribution, the following are complements:

$$
    P(X \le n) + P(X \gt n+1) = 1
$$

However, the same cannot be said for a continuous distribution, as there is some density between $n$ and $n+1$ that is not accounted for:

$$
    P(S \le n) + P(S \gt n+1) \ne 1
$$

Thus, **Continuity Correction** attempts to account for this by reducing the missing density. The general idea is to adjust the range such that the **midpoint of the two values**:

$$
\begin{aligned}
    P(S \le n) &= P \left(S \le \frac{n+(n+1)}{2} \right) \\
    P(S \ge n) &= P \left(S \ge \frac{n+(n-1)}{2} \right)
\end{aligned}
$$

Another concept to remember when going from Continuous to Discrete is that a continuous $S \lt n$ is a discrete $S \le (n-1)$ since there are a countable number of values, vice versa.

<!-- Obtained from Coaching Actuaries -->
![Continuity Correction](Assets/5.%20Aggregate%20Models.md/Continuity%20Correction.png){.center}

If required to approximate the **probability at a specified value**, then the above method will not work as a continuous distribution cannot find the probability at a single point.

Thus, it will be converted to a **range around the specified value** instead, with the **midpoint value used on both sides** of the specified value:

<!-- Obtained from Coaching Actuaries -->
![Continuity Correction Special](Assets/5.%20Aggregate%20Models.md/Continuity%20Correction%20Special.png){.center}

!!! Tip

    If the discrete distribution does not support consecutive integers $\set{0, 1, 2, \dots}$, then simply take the midpoint of whatever **consecutive values** in its support $\set{100, 200, 300}$.

### **Aggregate Deductibles**

Insurers can also purchase insurance for themselves, known as **Reinsurance**.

Insurers may want to limit their losses to a certain amount and getting the excess loss insured by a reinsurer. This is known as **Stop Loss Insurance**.

Intuitively, it can be thought of as a **deductible being applied to Aggregate Losses**, as the insurer (policyholder) only pays for losses up the deductible while the reinsurer pays for the rest.

As with regular deductibles, the main quantity of interest is the **reinsurer's expected loss**, as it represents the **Net Premium for the Stop Loss insurance**.

!!! Info

    Recall from FAM-L that Net Premiums refer to premiums that are charged without consideration of expenses or profits, purely based on the insurers loss.

$$
\begin{aligned}
    E(S-d)_{+} &= E(S) - E(S \land d)
\end{aligned}
$$

$E(S)$ can be found using the methods described earlier. However, unlike a normal deductible, the distribution of $S$ is not known, thus $E(S \land d)$ can only be found **via first principles**.

In most cases, the **underlying severity distribution is discrete** for simplicity. What makes this much more challenging than its regular deductible counterpart is that it is *tricky* to obtain all possible values of S and their corresponding probabilities.

It is recommended to consider a **probability tree** to easily see all combinations of X and N that results in $S \le d$ and its corresponding probability.

!!! Tip

    There may be multiple combinations that may lead to the same aggregate loss:

    * $X = 2, \ N = 1, \ \therefore S = 2$
    * $X = 1, \ N = 2, \ \therefore S = 2$

    The probability of $S=2$ must **reflect both these combinations**.

### **Aggregate Payments**

The discussion so far has solely been on Aggregate *Losses*. However, if each policy has a deductible, then there is a need to consider **Aggregate Payments** instead.

Both types of aggregate payments (per loss and per payment) can be used to determine aggregate payments:

If considering **Payments per Losses** $(Y^L)$, then as its name suggests, the appropriate frequency to use is the **Loss Frequency** $(N)$.

$$
\begin{aligned}
    S &= Y^L_{1} + Y^L_{2} + \dots + Y^L_{N} \\
    \\
    E(S) &= E(N) \cdot E(Y^L) \\
    Var (S) &= Var(Y^L) \cdot E(N) + \left[E \left(Y^L \right) \right]^2 \cdot Var (N)
\end{aligned}
$$

If considering **Payments per Payments** $(Y^P)$, then as its name suggests, the appropriate frequency to use is the **Payment Frequency** $(N^{*})$.

$$
\begin{aligned}
    S &= Y^P_{1} + Y^P_{2} + \dots + Y^P_{N^{*}} \\
    \\
    E(S) &= E(N^{*}) \cdot E(Y^P) \\
    Var (S) &= Var(Y^P) \cdot E(N^{*}) + \left[E \left(Y^P \right) \right]^2 \cdot Var (N^{*})
\end{aligned}
$$

The key is to **match the frequency distribution with the severity**: Loss & Loss Frequency; Payments & Payment Frequency.

#### **Payment Frequency**

Imposing a deductible naturally results in **fewer payments than losses** as not all losses result in a payment.

The **Payment Frequency** $(N^{*})$ is thus an adjusted version of the loss distribution that only takes into account the losses that result in a payment.

Consider a **Bernoulli Indicator variable** that takes on the value 1 if a payment will be made and 0 if it will not:

$$
\begin{aligned}
    I
    &=
    \begin{cases}
        1 \ (\text{Paid}),& X \gt d \\
        0 \ (\text{Not Paid}),& X \le d
    \end{cases} \\
    \\
    v &= P(X \gt d) \\
    P_I(t) &= 1 + v(t-1)
\end{aligned}
$$

$N^{*}$ is the **sum of $N$ idendepent indicator variables**:

$$
    N^{*} = I_1 + I_2 + \dots + I_N
$$

It is essentially a **Compound Distribution** with $N$ being the primary distribution and $I$ being the secondary distribution. Thus, the PGF of $N^{*}$ can be determined:

$$
\begin{aligned}
    P_{N^*}
    &= P_{N}[P_{I}(t)] \\
    &= P_{N}[1 + p(t-1)]
\end{aligned}
$$

By expanding simplfying the expression, the distribution of $N^{*}$ can be determined:

$$
\begin{aligned}
    N &\sim \text{Poisson}(\mu) \\
    \\
    P_{N}[1 + p(t-1)]
    &= e^{\mu(1 + v(z-1) -1)} \\
    &= e^{\mu (v(z-1))} \\
    &= e^{v\mu (z-1)} \\
    \\
    \therefore N^{*} &\sim \text{Poisson}(v \cdot \mu)
\end{aligned}
$$

Repeating the process for the other two members of the (a, b, 0) class,

$$
\begin{aligned}
   N &\sim \text{Binomial} (n, p) \\
   \therefore N^{*} &\sim \text{Binomial} (n, vp) \\
   \\
   N &\sim \text{Negative Binomial} (r, \theta) \\
   \therefore N^{*} &\sim \text{Negative Binomial} (r, v \theta)
\end{aligned}
$$
