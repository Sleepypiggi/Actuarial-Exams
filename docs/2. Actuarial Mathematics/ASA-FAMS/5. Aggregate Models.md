# **Aggregate Models**

## **Overview**

The combination of the Frequency Models $(N)$ and the Severity Models $(X)$ discussed previously results in **Aggregate Model** $(S)$, which represents the **total losses** experienced by an insurance company.

$$
\begin{aligned}
    S &= X_1 + X_2 + ... + X_N
\end{aligned}
$$

There are two main types of aggregate models:

The first type of aggregate model is known as the **Collective Risk Model**, which makes assumes the following assumptions:

1. **Losses are iid**: $X_1, X_2, ..., X_N$ are iid
2. **Frequency and Severity are independent**: $X$ and $N$ are independent

The second type is known as the **Individual Risk Model**, which makes similar assumptions:

1. **Losses are independent but not necessarily identical**: $X_1, X_2, \dots, X_N$ are independent but MAY have *different distributions*
2. **Frequency is a constant**: $N = n$
3. Typically used for **Group Policies** where there is a fixed number of poliycholders with varying levels of coverage

Generally speaking, it is much better to model frequency and severity **seperately** and then combine into an aggregate loss rather than directly modelling it as it **more accurate and flexible**.

<!-- Suspect may have some questions on this. To update with information from TB. -->

## **Collective Risk Model**

The collective risk model is a **sum of an unknown number of random variables**. In probability theory, this is known as a **Compound Distribution** where $N$ is known as the **Primary Distribution** and $X$ is known as the **Secondary Distribution**.

Another perspective is that the aggregate model follows the **conditional distribution** $S \mid N$. Thus, we can compute the probability, expectation and variance using previously found results.

Starting with the **Law of Total Expectation**,

$$
\begin{aligned}
    E(S) &= E_{N} [E_{S}(S \mid N)] \\
    \\
    E_{S}(S \mid N)
    &= E(X_1 + X_2 + \dots + X_N \mid N) \\
    &= E(X_1 \mid N) + E(X_2 \mid N) + \dots + E(X_N \mid N) \\
    &= E(X_1) + E(X_2) + \dots + E(X_N), \ \text{Independence} \\
    &= E(X) + E(X)+ \dots + E(X), \ \text{Identical} \\
    &= N \cdot \underbrace{E(X)}_{\text{Constant}} \\
    \\
    \therefore E(S)
    &= E_{N} [E_{S}(S \mid N)] \\
    &= E_{N} [N \cdot E(X)] \\
    &= E(X)\cdot E(N)
\end{aligned}
$$

!!! Note

    This result is **highly intuitive** as $\text{Expected Risk} = \text{Expected Claims} \cdot \text{Expected Severity}$.

Moving on to the **Law of Total Variance**:

$$
\begin{aligned}
    Var (S)
    &= E_N [Var (S \mid N)] + Var_{N} [E(S \mid N)] \\
    \\
    Var (S \mid N)
    &= Var (X_1 + X_2 + ... + X_N \mid N) \\
    &= N \cdot \underbrace{Var(X)}_\text{Constant}, \ \text{IID} \\
    \\
    Var (S)
    &= E_N [Var (S \mid N)] + Var_{N} [E(S \mid N)] \\
    &= E [N \cdot Var(X)] + Var_{N} [N \cdot E(X)] \\
    &= Var(X) \cdot E(N) + [E(X)]^2 \cdot Var(N)
\end{aligned}
$$

The probability of $s$ is **difficult to derive**. Thus, for simplicity, we will assume a **Discrete Severity** for this portion. By the **Law of Total Probability**:

$$
\begin{aligned}
    P(S = s)
    &= E_{N} [P(S = s \mid N = n)] \\
    &= \sum P(S = s \mid N = n) \cdot P(N = n)
\end{aligned}
$$

!!! Note

    If both the Frequency and Severity models are discrete, then the Aggrgegate Loss is discrete as well.

### **Convolutions**

The term within the above summation is known as a **Convolution**, as it is the probability of a sum of random variables. Since there are $n$ random variables, it is known as the **n-fold Convolution**.

$$
    P(S = s \mid N = n) = P(X_1 + X_2 + \dots + X_n = s)
$$

For simplicity, consider a two fold convolution:

$$
\begin{aligned}
    P(S = s \mid n = 2)
    &= \sum P(X_1 = x, X_2 = s - x_1) \\
    &= \sum P(X_1 = x) \cdot P(X_2 = s - x_1)
\end{aligned}
$$

A convolution is simply the **probability of all possible combinations** of the random variables that result in the specified value (Recall the two dice example).

However, one implicit assumption for this to work is that the **individual severities cannot be 0**. The reason is because it would lead to **endless possibilities** for each case.

Consider $S=1$:

* $1 + 0 + 0 + \dots + 0 = 1$
* $0 + 1 + 0 + \dots + 0 = 1$
* $0 + 0 + 1 + \dots + 0 = 1$
* $0 + 0 + 0 + \dots + 1 = 1$

However, then how is $S=0$ determined, since there are **no sums that result in 0**? There is now only ONE way for aggregate losses to be 0 - if there are **no claims at all**.

$$
    P(S = 0) = P(n = 0)
$$

### **Generating Functions**

An alternative method to calculate the moments would be to use the **MGF**:

$$
    M_{S}(t) = M_{N} [\ln M_{X}(t)]
$$

Similarly, if severity is discrete, then an alternative method to calculate its probability would be to use its **PGF**:

$$
    P_{S}(t) = P_{N} [P_{X}(t)]
$$

These methods tend to be more **time consuming**, thus they are preferred only when calculating higher moments or large values of $s$.

### **Approximation**

Another method of determining the probability would be to simply **approximate** it. This is especially useful when the individual losses are continuous, such that the previous methods would not work.

By the **Central Limit Theorem**, as the number of random variables increase, their **sum is approximately normal distributed**:

$$
    S \sim (E[S], Var[S])
$$

Instead of using the normal distribution, the **Lognormal Distribution** can be used as well. However, we need to **first solve for its parameters**:

$$
\begin{aligned}
    e^{\mu + 0.5\sigma^2} &= E(S) \\
    \left(e^{\mu + 0.5\sigma^2} \right)^2 \cdot (e^{\sigma^2}-1) &= Var (S) \\
    \\
    \therefore S \sim \text{Log Normal} (\mu, \sigma^2)
\end{aligned}
$$

!!! Warning

    Although the notation is the same, **do NOT confuse** the lognormal parameters with the normal parameters.

#### **Continuity Correction**

We may still want to use the approximation even when losses are discrete as they may still be **too complicated** to compute.

However, this may lead to some inaccuracies as we are using a continuous distribution to approximate a discrete distribution.

Under a discrete distribution, the following are complements:

$$
    P(X \le n) + P(X \gt n+1) = 1
$$

However, the same cannot be said for a continuous distribution, as there is some density between $n$ and $n+1$ that is not accounted for:

$$
    P(S \le n) + P(S \gt n+1) \ne 1
$$

Thus, **Continuity Correction** attempts to account for this by reducing the missing density. The general idea is to adjust the range such that the **midpoint of the two values**:

$$
\begin{aligned}
    P(S \le n) &= P \left(S \le \frac{n+(n+1)}{2} \right) \\
    P(S \ge n) &= P \left(S \ge \frac{n+(n-1)}{2} \right)
\end{aligned}
$$

Another concept to remember when going from Continuous to Discrete is that a continuous $S \lt n$ is a discrete $S \le (n-1)$ since there are a countable number of values, vice versa.

<!-- Obtained from Coaching Actuaries -->
![Continuity Correction](Assets/5.%20Aggregate%20Models.md/Continuity%20Correction.png){.center}

If required to approximate the **probability at a specified value**, then the above method will not work as a continuous distribution cannot find the probability at a single point.

Thus, it will be converted to a **range around the specified value** instead, with the **midpoint value used on both sides** of the specified value:

<!-- Obtained from Coaching Actuaries -->
![Continuity Correction Special](Assets/5.%20Aggregate%20Models.md/Continuity%20Correction%20Special.png){.center}

!!! Tip

    If the discrete distribution does not support consecutive integers $\set{0, 1, 2, \dots}$, then simply take the midpoint of whatever **consecutive values** in its support $\set{100, 200, 300}$.

<!--
Stop Loss Insurance > Aggregate Deductible, similar
Aggregate Payments, YP or YL but otherwise same concepts
-->

## **Individual Risk Model**
