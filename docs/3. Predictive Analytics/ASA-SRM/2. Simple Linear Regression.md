# **Regression Introduction**

## **Regression Fundamentals**

Regression is a statistical model that relates a **Dependent Variable** to one or more **Independent Variables**, usually with the goal of prediction.

<center>

| Independent Variable(s) | Dependent Variables |
| :-: | :-: |
| Variable used to make predictions | Variable being predicted |
| *Free* to change the value | *Depends* on the value of indepenent variable |
| Denoted as $X$ | Denoted as $Y$ |

</center>

Different regression models assume **different relationships** between the Independent and Dependent variable. The goal is to use the model that **best captures the relationship** between the variables.

All regression models express the dependent variable as some **function of the independent variables**.

are a **mathematical function** that relates the dependent variable to the independent variables.

Every regression is a **mathematical function** that contains one or more **Regression Parameters** that **define the relationship** between the Independent and Dependent variables.

Since the independent variables can be freely changed, they are **Deterministic**; NOT random. Conversely, the dependent variable is a **Random Variable** that follows some unknown **probability distribution**.

To be precise, for every set of independent variables, the dependent variable has a **Conditional Probability Distribution** dependent on the given independent variables. For instance, the the dependent variable could take any possible value (non-conditional distribution), but *given* these set of independent variables, the possible values can be **narrowed down to a certain range** (conditional distribution).

$$
Y|X \sim Distribution
$$

The output of the regression model is the **Expected Value** of the conditional distribution, $E(Y|X)$. It is also commonly denoted as just $y$.

<!-- Obtained from Colorado Uni -->
![Dependent Expectation](Assets/2.%20Simple%20Linear%20Regression.md/Regression%20Expectation.png){ .center}

We can then gauge the "accuracy" of the regression by comparing the actual value to the model's predicted value. The difference between an observed value and the regression is known as the **Error** of the model. It represents **all other factors** affecting the dependent variable that were not captured in the regression.

$$
\varepsilon = y_i - y
$$

<!-- Obtained from Cloudera -->
![Regression Errors](Assets/2.%20Simple%20Linear%20Regression.md/Regression%20Errors.png){ .center}

Every regression is fundamentally a **mathematical function** that relates the dependent to the indpendent variables.




In practice, the prediction of the Sample Model is an **estimate** of the expected value of the conditional distribution. The difference between an observed value and the *sample model's* is known as the **Residual**. It is an **estimate** of the model error.

$$
\hat{\varepsilon} = y_i - \hat{y}
$$

## **Simple Linear Regression**

**Simple Linear Regression** (SLR) assumes a **Linear Relationship** between a pair of **Numeric** independent and dependent variable.

$$
y = \beta_0 + \beta_1 x
$$

fit the regression
