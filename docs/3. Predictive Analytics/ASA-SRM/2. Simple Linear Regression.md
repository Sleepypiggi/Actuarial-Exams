# **Regression Fundamentals**

## **Population Regression Model**

Regression is a statistical model that relates a **Dependent Variable** to one or more **Independent Variables**. The dependent variable is *regressed on to* the independent variable.

They are fundamentally a function of the independent variables and several **Regression Parameters**, $\beta$. The functional form of the regression is based on the relationship between the variables.

$$
y = f(X, \beta)
$$

Different regression models assume **different relationships** between the Independent and Dependent variable. The goal is to use the model that **best captures the relationship** between the variables.

<center>

| Independent Variable(s) | Dependent Variables |
| :-: | :-: |
| Variable used to make predictions | Variable being predicted |
| *Free* to change the value | *Depends* on the value of indepenent variable |
| Deterministic | Random Variable |
| Denoted as $X$ | Denoted as $Y$ |

</center>

To be precise, for every set of independent variables, the dependent variable has a **Conditional Distribution** dependent on the given independent variables.

For instance, the the dependent variable could take any possible value (non-conditional distribution), but *given* these set of independent variables, the possible values can be **narrowed down to a certain range** (conditional distribution).

$$
\displaylines{
Y \sim Distribution \\
Y|X \sim Conditional~Distribution}
$$

The regression model calculates the **Expected Value** of the conditional distribution, $E(Y|X)$, for every possible $X$. It is also commonly denoted as just $y$.

<!-- Obtained from Colorado Uni -->
![Dependent Expectation](Assets/2.%20Regression%20Introduction.md/Regression%20Expectation.png){ .center}

The "accuracy" of the regression can be gauged by comparing the actual value to the model's predicted value. The **difference** between an observed value and the corresponding regression value is known as the **Error** of the model. It represents **all other factors** affecting the dependent variable that were not captured in the regression.

Note that the **sign of the errors** are significant - positive implies the actual value lies above the regression model while negative implies it lies below.

$$
\varepsilon_i = y_i - f(X, \beta)
$$

<!-- Obtained from Cloudera -->
![Regression Errors](Assets/2.%20Regression%20Introduction.md/Regression%20Errors.png){ .center}

## **Sample Regression Model**

Everything discussed up till this point was regarding the **Population Regression Model**. In practice, a **Sample Regression Model** is created from the data that seeks to estimate the population's.

Fit the model

In practice sample model, estimates, residuals sum squared residuals

In practice, the prediction of the Sample Model is an **estimate** of the expected value of the conditional distribution. The difference between an observed value and the *sample model's* is known as the **Residual**. It is an **estimate** of the model error.

$$
\hat{\varepsilon} = y_i - \hat{y}
$$

There are several methods to determine the parameters, but the most common is the **Least Squares Method**, where the parameters are chosen such that they **Minimize the Sum of the Squared Errors**. The errors are **squared to remove the sign** of the errors.

The model thus **best fits** the observations, such that the distance between the regression model and ALL observations are minimized.


## **Simple Linear Regression**

**Simple Linear Regression** (SLR) assumes a **Linear Relationship** between a pair of **Numeric** independent and dependent variable.

$$
y = \beta_0 + \beta_1 x
$$

fit the regression
