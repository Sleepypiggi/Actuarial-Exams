# **Simple Linear Regression**

Every statistical model has two components:

| Independent Variable(s) | Dependent Variables |
| :-: | :-: |
| Variable used to make predictions | Variable being predicted |
| *Free* to change the value | *Depends* on the value of indepenent variable |
| Denoted as $X$ | Denoted as $Y$ |

Different statistical models assume **different relationships** between the Independent and Dependent variable - Linear, Non-Linear, Piecewise Constant etc. The goal is to use the model that **best captures the true relationship** between the variables.

Simple Linear Regression (SLR) assumes a **Linear Relationship** between a pair of **Numeric** independent and dependent *variable*. It is the *simplest* of all forms of statistical models (hence the name), but it is well suited to demonstrate the fundamental concepts.

## **Regression Fundamentals**

Every regression is a **mathematical function** that contains one or more required **Parameters** that **define the relationship** between the Independent and Dependent variables. It is the parameters of the model that *distinguishes* one model (of the same type) from another.

These parameters are **Estimated** through the analysis of the historical/current data. This process is known as **Training** the model.

If the entire population data is used to train the model, then the resulting model is the *Population Model*, which represents the **theoretical** relationship between the variables. If a sample is used to train the model, then the resulting model is a *Sample Model*, which aims to **estimate** the population one.

## **Model Prediction**

Since the independent variables can be freely changed, they are **Deterministic**; NOT random. Naturally, the dependent variable is a **Random Variable** that follows some unknown **probability distribution**.

To be precise, for every set of independent variables, the dependent variable has a **Conditional Probability Distribution** dependent on the those independent variables. For instance, the the dependent variable could take any possible value (non-conditional distribution), but *given* these set of independent variables, the possible values can be **narrowed down to a certain range** (conditional distribution).

$$
Y|X \sim Distribution
$$

The corresponding prediction of the model based on those independent variables is the **Expected Value** of this conditional distribution, $E(Y|X)$. It is also commonly denoted as just $y$. Note that we refer to a random variable using **Large Letters** and values of the variable using **Small Letters**.

<!-- Obtained from Colorado Uni -->
![Dependent Expectation](Assets/1.%20SRM%20Overview.md/Dependent%20Expectation.png)

We can gauge the "accuracy" of the model by comparing the actual value to the model's predicted value. The difference between an observed value and the model's is known as the **Error** of the model. It represents **all other factors** affecting the dependent variable that were not captured in the model.

$$
\varepsilon = y_i - y
$$

In practice, the prediction of the Sample Model is an **estimate** of the expected value of the conditional distribution. The difference between an observed value and the *sample model's* is known as the **Residual**. It is an **estimate** of the model error.

$$
\hat{\varepsilon} = y_i - \hat{y}
$$




$$
y = \hat{\beta}_0 + \hat{\beta}_1 x
$$



Note that Y and X are **vectors** which represent the collection of **all values of the independent variable** and their corresponding dependent variable.

$\beta_0$ and $\beta_1$ are known as the **Parameters** of the model as they **define the relationship** between the variables. 

$\varepsilon$ is the **Error term** which is a **catch-all for all other factors** that the model does not capture.