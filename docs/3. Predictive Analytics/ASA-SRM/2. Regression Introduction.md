# **Regression Introduction**

## **Overview**

Regression is a statistical model that relates a **Dependent Variable** to one or more **Independent Variables**. The dependent variable is *regressed on to* the independent variable.

Different regression models assume **different relationships** between the Independent and Dependent variable. The goal is to use the model that **best captures the relationship** between the variables.

<center>

| Independent Variable(s) | Dependent Variables |
| :-: | :-: |
| Variable used to make predictions | Variable being predicted |
| *Free* to change the value | *Depends* on the value of indepenent variable |
| Deterministic | Random Variable |
| Denoted as $X$ | Denoted as $Y$ |

</center>

To be precise, for every set of independent variables, the dependent variable has a **Conditional Distribution** dependent on the given independent variables. For instance, the the dependent variable could take any possible value (non-conditional distribution), but *given* these set of independent variables, the possible values can be **narrowed down to a certain range** (conditional distribution).

$$
Y|X \sim Distribution
$$

The regression model calculates the **Expected Value** of the conditional distribution, $E(Y|X)$. It is also commonly denoted as just $y$.

<!-- Obtained from Colorado Uni -->
![Dependent Expectation](Assets/2.%20Simple%20Linear%20Regression.md/Regression%20Expectation.png){ .center}

The "accuracy" of the regression can be gauged by comparing the actual value to the model's predicted value. The **difference** between an observed value and the corresponding regression value is known as the **Error** of the model. It represents **all other factors** affecting the dependent variable that were not captured in the regression.

$$
\varepsilon_i = y_i - y
$$

<!-- Obtained from Cloudera -->
![Regression Errors](Assets/2.%20Simple%20Linear%20Regression.md/Regression%20Errors.png){ .center}

Regressions are a function of the independent variables and several **Regression Parameters** ($\beta$). There are several methods to determine the parameters, but the most common is the **Least Squares Method**, where the parameters are chosen such that they **Minimize the Sum of the Squared Errors**.

This creates a regression model that **best fits** the observations - distance between regression model and all observations are minimized.

$$
y = f(X,\beta)
$$

## **Simple Linear Regression**

In practice sample model, estimates, residuals sum squared residuals

In practice, the prediction of the Sample Model is an **estimate** of the expected value of the conditional distribution. The difference between an observed value and the *sample model's* is known as the **Residual**. It is an **estimate** of the model error.

$$
\hat{\varepsilon} = y_i - \hat{y}
$$

## **Simple Linear Regression**

**Simple Linear Regression** (SLR) assumes a **Linear Relationship** between a pair of **Numeric** independent and dependent variable.

$$
y = \beta_0 + \beta_1 x
$$

fit the regression
