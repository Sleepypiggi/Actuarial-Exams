# **Introduction**

## **Statistical Models**

Predictive Analytics is the usage of **statistical models** to analyze historical or current data to make **predictions** about the future or unknown events.

Every statistical model has two components:

| Independent Variable(s) | Dependent Variables |
| :-: | :-: |
| Variable used to make predictions | Variable being predicted |
| *Free* to change the value | *Depends* on the value of indepenent variable |
| Denoted as $x$ | Denoted as $y$ |

> They are also commonly referred to as the **Predictor** and **Response** variable respectively.

Different statistical models assume **different relationships** between the Independent and Dependent variable - Linear, Non-Linear, Piecewise Constant etc. The goal is to use the model that **best captures the true relationship** between the variables.

Every regression is a **mathematical function** that contains one or more required **Parameters** that **define the relationship** between the Independent and Dependent variables. It is the parameters of the model that *distinguishes* one model (of the same type) from another.

These parameters are **Estimated** through the analysis of the historical/current data. This process is known as **Training** the model.

> The **Population** refers to the **theoretical set of all possible values** of the variables. **Samples** refer to a **subset of the population** that has been recorded for analysis. Since it is impossible to observe the entire population at once, the recorded to sample to draw conclusions about the population.

If the entire population data is used to train the model, then the resulting model is the *Population Model*, which represents the theoretical relationship between the variables. If a sample is used to train the model, then the resulting model is a *Sample Model*, which **aims to be representative** of the population one.

The two are distinguished functionally through the Hat notation (^). Population parameters are written without the hat ($x$) while Sample parameters are written with the hat ($\hat{x}$). Since the sample model is the main model of interest, the hat is often dropped for convenience.

## **Prediction**

Since the independent variables can be freely changed, they are **Deterministic**; NOT random. Naturally, the dependent variable is a **Random Variable** that follows some unknown **probability distribution**.

To be precise, for every set of independent variables, the dependent variable has a **Conditional Probability Distribution** dependent on the those independent variables. For instance, the the dependent variable could take any possible value (non-conditional distribution), but *given* these set of independent variables, the possible values can be **narrowed down to a certain range** (conditional distribution).

$$
Y|X \tilde{}
$$

The corresponding prediction of the model based on those independent variable is the **Expected Value** of this conditional distribution, $E(Y|X)$.

<!-- Obtained from Colorado Uni -->
![Dependent Expectation](Assets/1.%20SRM%20Overview.md/Dependent%20Expectation.png)

The actual value of the prediction is often going to be different from the predicted value. Similarly, we can distinguish the two using the Hat notation as well - the actual value does not contain the hat ($y$) while the predicted one does ($\hat{y}$).

The **difference between the actual and predicted value** is known as the **Error** of the model ($\varepsilon$). It is calculated from the *perspective of the actual observation* - a **positive error** implies that the prediction was too low while a **negative error** implies it was too high.

Naturally, since $y$ is a random variable, $\varepsilon$ is a random variable as well following the same distribution.

$$
\varepsilon = y - \hat{y}
$$

<!-- Not useful to think this way? -->
