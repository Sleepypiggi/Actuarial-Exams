# **Simple Linear Regression**

**Simple Linear Regression** (SLR) assumes a **Linear Relationship** between a **Numeric** $Y$ and *single* numeric $X$. The model is considered to be *simple* because it only contains a single $X$ variable.

$$
E(Y|X) = \beta_0 + \beta_1 X
$$

<center>

| $\beta_0$  | $\beta_1$ |
| :-: | :-: |
| **Expected value** of $Y$ when $X = 0$ | **Change in the expected value** of $Y$ given a one unit increase in $X$ |
| Intercept Parameter | Slope Parameter |

</center>

Each observation can also be expressed as sum of the regression and its error term:

$$
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
$$

## **Ordinary Least Squares**

SLR parameters are estimated using the **Ordinary Least Squares** method, which minimizes the **Sum of Squared Residuals** of the fitted model. It is commonly referred to as the **Residual Sum Squared** (RSS).

$$
RSS = \sum y_i - \hat{y} =\sum [y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i)]^2
$$

The minimization is solved through calculus by setting the partial derivatives of the RSS to 0:

For the intercept parameter,

$$
\begin{aligned}
\frac{\partial}{\partial \beta_0} RSS &= 0 \\
-2 \sum [y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i)] &= 0 \\
\sum y_i - \sum \hat{\beta}_0 - \sum \hat{\beta}_1 x &= 0 \\
n\bar{y} -n\hat{\beta}_0 - n\hat{\beta}_1 \bar{x} &= 0 \\
\end{aligned}
$$

$$\therefore \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$$

For the slope parameter,

$$
\begin{aligned}
\frac{\partial}{\partial \beta_1} RSS &= 0 \\
-2 \sum x_i [y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i)] &= 0 \\
\sum (y_i x_i) - \hat{\beta}_0 \sum x_i - \hat{\beta}_1 \sum x^2_i &= 0 \\
\sum (y_i x_i) - (\bar{y} - \hat{\beta}_1 \bar{x}) \sum x_i - \hat{\beta}_1 \sum x^2_i &= 0 \\
\sum (y_i x_i) - n\bar{x}\bar{y} + n\hat{\beta}_1 \bar{x}^2 - \hat{\beta}_1 \sum x^2_i &= 0 \\
\sum (y_i x_i) - n\bar{x}\bar{y} &= \hat{\beta}_1 \sum (x^2_i) - n\bar{x}^2 \\
\end{aligned}
$$

$$
\therefore \hat{\beta}_1
= \frac{\sum (x_i y_i) - n\bar{x}\bar{y}}{\sum (x_i^2) - n\bar{x}^2}
= \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}
= r * \frac{s_y}{s_x}
$$

This results in the following fitted regression model:

$$
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_i \\
$$

<!-- Obtained from ACTEX Manual -->
![Fitted Regression](Assets/3.%20Simple%20Linear%20Regression.md/Fitted%20Regression.png)

> Note that it should $\hat{\varepsilon}$ in the above image, not $e_i$.

### **OLS Properties**

By re-arranging the formula for $\hat{\beta}_0$, we obtain the following:

$$
\bar{y} = \hat{\beta}_0 + \hat{\beta}_1 \bar{x}
$$

This means that **$(\bar{x}, \bar{y})$ always lies** on the fitted regression model.

Additionally, totice that the two estimates will also always satisfy the partial derivatives of the minimization problem, which leads to some interesting results:

<center>

| Intercept Parameter | Slope Parameter |
|:-:|:-:|
| $\frac{\partial}{\partial \beta_0} = -2 \sum [y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i)] = 0$ | $\frac{\partial}{\partial \beta_1} = -2 \sum x_i [y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i)] = 0$ |
| $\sum \hat{\varepsilon_i} = 0$ | $\sum x_i \hat{\varepsilon_i} = 0$ |
| Residuals are negatively correlated | Residuals and Independendent variables are uncorrelated |

</center>

<!-- mean of prediction = mean of sample -->

### **OLS Assumption**

**Assumption 1: Errors are independent of one another.**

If errors are correlated with one another, it is known as **Serial Correlation** or **Autocorrelation**.

It implies that there are other unmodelled factors that can be used for prediction, which would imply the current model specification to be wrong.

Thus, for the SLR model to be true, the errors **must be independent of one another**.

$$
Cov(\varepsilon_i,\varepsilon_j) = 0
$$

**Assumption 2: Errors are independent of $X$.**

Note that the error term is NOT independent of $Y$, as it captures the unmodelled factors that could affect $Y$.

If the errors are NOT independent of $X$ as well, then the errors would influence $X$ which would then influence $Y$. The resulting model would be the effect of **both** $X$ and the unmodelled factors, when the model only specified $X$, leading to **misleading results**.

In this case, the unmodelled factors are known as **Confounding Variables**, as it confounds the results of the regression.

Thus, for the SLR model to be true, the errors must be independent of $X$. Phrased another way, $X$ must be an **Exogenous Variable**.

$$
Cov(\varepsilon_i,X) = 0
$$

<!-- Obtained from Statology -->
![Confounding Variables](Assets/3.%20Simple%20Linear%20Regression.md/Confounding%20Variable.png){.center}

**Assumption 3: Errors have an expectation of 0.**

Building off the previous assumptions, it implies that the errors should be **Random**. Thus, they should have a **conditional expectation of 0**.

$$
E(\varepsilon_i | X) = 0
$$

**Assumption 4: Errors have constant variance.**

Building off the previous assumptions, the errors should not exhibit any particular pattern. Thus, they should have a **constant variance** of $\sigma^2$.

$$
var(\varepsilon_i) = \sigma^2
$$

Non constant then overdominated by outliers

<!-- Obtained from Corporate Finance Institute -->
![Homoscedastacity](Assets/3.%20Simple%20Linear%20Regression.md/Homoscedasticity.png){.center}

**Assumption 5: Errors are normally distributed.**

Following the previous assumptions, the errors are assumed to be normally distributed:

$$
\varepsilon \sim N(0, \sigma^2)
$$

If the errors are normally distributed, then it follows that $y$ and $\beta$ are normally distributed as well. While this assumption is not necessary to create the model, it greatly eases the computation for making statistical inferences.

<!-- To check on specific normal distributions for Y > Does it have the same variance as E? -->

## **Goodness of Fit**

Ideally, the regression should fit the sample closely, having **as small as residuals as possible**. The size of all the residuals in the model can be summarized through the RSS. The lower the RSS, the better the fit of the model.

Recall that the residuals naturally sum to 0 under OLS - the residuals are thus squared to remove the sign so that they can be summed together.

$$
RSS = \sum (y_i - \hat{y})^2
$$

However, the SSR on its own is hard to intepret as there is no indication of how low or high it actually is. Thus, the **Total Sum of Squares** (TSS) can be used as a **benchmark for the RSS** as it is at least equal to or higher than the RSS.

The TSS represents the RSS for a **Null Regression** - a model with containing **only the intercept parameter**. The output of this regression is **always the sample mean $\bar{y}$**, which is used for the computation of its residuals.

It represents the worst possible model which thus has the **highest possible RSS**. The lower the RSS compared to the TSS, the better the fit of the model.

$$
TSS = \sum (y_i - \bar{y})
$$

### **Null Model**

Consider a regression with only the intercept. In other words, $\beta_1 = 0$. Notice that this is the same as the null hypothesis, which is why it the model is known as the **Null Model**.

$$
y = \beta_0
$$

We can estimate $\hat{\beta_0}$ using OLS, which results in the following result:

$$
\begin{aligned}
-2 \sum (y_i - \hat{\beta_0}) &= 0 \\
n \bar{y} - n \hat{\beta_0} &= 0 \\
\hat{\beta_0} &= \bar{y} \\
\end{aligned}
$$

$$
\therefore \hat{y} = \bar{y}
$$

### **Sum of Squares**

The TSS can be further decomposed into two more parts for analysis:

$$
\begin{aligned}
TSS &= \sum (y_i - \bar{y})^2 \\
TSS &= \sum[(y_i - \hat{y}) + (\hat{y}-\bar{y})]^2 \\
TSS &= \sum(y_i - \hat{y})^2 + \sum(\hat{y}-\bar{y})^2 + 2 \sum((y_i - \hat{y})(\hat{y}-\bar{y})) \\
TSS &= \sum(y_i - \hat{y})^2 + \sum(\hat{y}-\bar{y})^2 + 0 \\
TSS &= RSS + RegSS
\end{aligned}
$$

| Residual SS (RSS) | Regression SS (RegSS) |
|:-:|:-:|
| $\sum(\hat{y}-\bar{y})^2$ | $\sum(y_i - \hat{y})^2$ |
| Variation of the observed values about the regression  | Variation of the regression output about the sample mean |
| Variation explained by the regression | Variation unexplained by the regression |

Note that it can also be expressed in terms of the **Slope Parameter**:

$$
\begin{align}
RegSS
&= \sum(\hat{y}-\bar{y})^2 \\
&= \sum(\hat{\beta}_0 + \hat{\beta}_1 x - \bar{y})^2 \\
&= \sum(\bar{y} - \beta_1 \bar{x} + \hat{\beta}_1 x - \bar{y})^2 \\
&= \sum[\hat{\beta}_1 (x_i - \bar{x})]^2 \\
&= \hat{\beta}^2_1 \sum(x_i - \bar{x})^2 \\
\end{align}
$$

The **Coefficient of Determination** $R^2$ can also be used to demonstrate goodness of fit. It measures the **proportion of variation** explained by the regression model.

$$
R^2 = \frac{RegSS}{TSS} = 1 - \frac{RSS}{TSS}
$$

Building off the above expression, it can also be expressed in terms of the **Sample Correlation**:

$$
\begin{align}
R^2
&= \frac{\hat{\beta}^2_1 \sum(x_i - \bar{x})^2}{\sum (y_i - \bar{y}) ^2} \\
&= \left(\frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}\right)^2 \frac{\sum(x_i - \bar{x})^2}{\sum (y_i - \bar{y}) ^2} \\
&= \frac{\sum (x_i - \bar{x})^2 (y_i - \bar{y})^2}{\sum (x_i - \bar{x})^4} \frac{\sum(x_i - \bar{x})^2}{\sum (y_i - \bar{y}) ^2} \\
&= \frac{\sum (x_i - \bar{x})^2 (y_i - \bar{y})^2}{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2} \\
&= \left(\frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x}) \sum (y_i - \bar{y})}\right)^2 \\
&= r^2
\end{align}
$$

### **Degrees of Freedom**

The TSS is based on the naive model with only the intercept parameter, thus, it is subject to the **single contraint** of all residuals summing to 0. The TSS thus has **$n-1$ degrees of freedom**.

The RSS is based on the SLR with both the intercept and slope parameter, thus it is subject to an **additional constraint** of the sumproduct of all residuals and independent variables being 0. The RSS thus has **$n-2$ degrees of freedom.**

The sum of the RSS and RegSS is equal to the TSS, thus the sum of their degrees of freedom must also be equal to that of the TSS. By working backwards, the RegSS thus has **only $1$ degree of freedom**.

### **Mean Squared**

The division of any Sum of Square (TSS, RSS, RegSS) by its Degrees of Freedom is known as the **Mean Squared** (MS), which is a measure of its **average variance**.

The MS of the TSS is the **Unbiased Estimator for Population Variance**, which is why this process is known as the **Analysis of Variance**, as it decomposes the variance of $Y$ into its constituent components:

$$
s = \frac{TSS}{n-1}
$$

The MS of the RSS is known the **Mean Squared Residuals**, often also referred to as the **Mean Squared Error**, as it is an estimate for the population variance of the error $\sigma^2$:

$$
MS_{\text{Residuals}} = \frac{RSS}{n-2}
$$

The MS of the RegSS is known as the **Mean Squared Regression**, which represents the proportion of variance explained per $X$ used.

$$
MS_{\text{Regression}} = \frac{RegSS}{1}
$$

### **F Statistic**

The ANOVA parameters can be used to conduct a hypothesis test to determine if there is in fact a relationship between $X$ and $Y$.

Under the null hypothesis ($\beta_1 = 0$), there should be **no difference** between the assumed model and a null model as both only contain the intercept parameter, $TSS = RSS$, where $RegSS = 0$

 causing $RegSS = RSS$. If the null hypothesis were false, then there should be a **difference** between the two, and thus $TSS > RSS$.

This **comparison of variance** (sum of squares) can be conducted using the following test statistic:

$$
T = \frac{MS_{RegSS}}{MS_{RSS}} = \frac{RegSS/1}{RSS/(n-2)}
$$

It can be shown that under the null, this test-statistic follows an F distribution with $1$ and $n-2$ degrees of freedom:

$$
\begin{aligned}
T
&= \frac{MS_{RegSS}}{MS_{RSS}} \\
&= \frac{\sigma^2_{RegSS} \frac{MS_{Reg}}{\sigma^2_{RegSS}}}{\sigma^2_{RSS} \frac{MS_{RSS}}{\sigma^2_{RSS}}} \\
&= \frac{\sigma^2_{RegSS}}{\sigma^2_{RSS}} * \frac{\frac{1 * MS_{Reg}}{\sigma^2_{RegSS}} * \frac{1}{1}}{\frac{(n-2) * MS_{RegSS}}{\sigma^2_{RegSS}}* \frac{1}{n-2}} \\
&= \frac{\sigma^2_{RegSS}}{\sigma^2_{RSS}} * \frac{\chi_1}{\chi_{n-2}} * (n-2) \\
&= 1 * F_{1, n-2} * (n-2) \\
&= F_{1, n-2} * (n-2)
\end{aligned}
$$

$$
\therefore T \sim F_{1, n-2}
$$

The key to understanding the ANOVA proof lies with the ratio of the \sigma^2_{RegSS} and \sigma^2_{RSS}


It can also be calculated in terms of $R^2$, by dividing the above formula by the TSS in both the numerator and denominator:

$$
F = (n-2) * \frac{RegSS/TSS}{RSS/TSS} = (n-2) * \frac{R^2}{1-R^2}
$$

<!--
F Distribution = Ratio of chi squared over their df > To rewrite the above
 -->

### **ANOVA Table**

All the above information is then summarized in a table for convenience, known as the **ANOVA Table**:

<center>

| Source | Sum of Squares | Degrees of Freedom | Mean Square | F-Statistic |
| :-: | :-: | :-: | :-: | :-: |
| Regression | RegSS | $1$ | RegSS | $\frac{RegSS}{MSE}$ |
| Error | RSS | $n-2$ | $MSE$ | - |
| Total | TSS | $n-1$ | $s^2$ | - |

</center>

## **Statistical Inference**

### **Sampling Distributions**

Since the errors are assumed to be normally distributed, then $Y$ is assumed to be normally distributed as well. Since $Y$ is a linear combination of the regression parameters, then the parameters (& their estimates) are **normally distributed** as well.

Both estimates can be **expressed in another form** that makes it *more convenient* to find their expectation & variances.

$$
\begin{aligned}
\hat{\beta}_1
&= \frac{\sum [(x_i - \bar{x})(y_i - \bar{y})]}{\sum (x_i - \bar{x})^2} \\
&= \frac{\sum (x_i - \bar{x})y_i}{\sum (x^2_i - \bar{x})} - \frac{\bar{y} \sum (x_i - \bar{x})}{\sum (x^2_i - \bar{x})} \\
&= \sum \frac{(x_i - \bar{x})}{(x^2_i - \bar{x})}* y_i - 0 \\
&= \sum w_i * y_i \\
\\
\hat{\beta}_0
&= \bar{y} - \hat{\beta}_1 \bar{x} \\
&= \frac{1}{n} \sum y_i - \bar{x} \sum w_i * y_i \\
&= \sum y_i (\frac{1}{n} - \bar{x}w_i)
\end{aligned}
$$

$w_i$ is a sort of "weight" parameter of the sum of squares. It has three interesting properties that makes it useful:

$$
\begin{aligned}
\sum w_i
&= \frac{\sum (x_i - \bar{x})}{\sum (x_i - \bar{x})^2} \\
&= \frac{n\bar{x}-n\bar{x}}{\sum (x_i - \bar{x})^2} \\
&= \frac{0}{\sum (x^2_i - \bar{x})} \\
&= 0 \\
\\
\sum w_i x_i
&= \frac{\sum x_i(x_i - \bar{x})}{\sum (x_i - \bar{x})^2} \\
&= \frac{\sum (x^2_i - \bar{x} \sum x_i)}{\sum x^2_i - 2\bar{x}\sum x_i + \sum \bar{x}^2} \\
&= \frac{\sum x^2_i - \bar{x}(n \bar{x})}{\sum x^2_i - 2\bar{x}(n\bar{x}) + n\bar{x}^2} \\
&= \frac{\sum x^2_i - n\bar{x}^2}{\sum x^2_i - 2n\bar{x}^2 + n\bar{x}^2} \\
&=\frac{\sum x^2_i - n\bar{x}^2}{\sum x^2_i - n\bar{x}^2} \\
&= 1 \\
\\
\sum w_i^2
&= \frac{\sum (x_i - \bar{x})^2}{\sum (x_i - \bar{x})^4} \\
&= \frac{1}{\sum (x_i - \bar{x})^2} \\
\\
\sum (\frac{1}{n} - \bar{x}w_i)
&= n(\frac{1}{n}) - \bar{x} \sum w_i \\
&= 1 - 0 \\
&= 1 \\
\\
\sum (\frac{1}{n} - \bar{x}w_i)x_i
&= \frac{1}{n} \sum x_i - \bar{x} \sum w_i x_i \\
&= \frac{1}{n} (n\bar{x}) - \bar{x} (1) \\
&= \bar{x} - \bar{x} \\
&= 0
\end{aligned}
$$

Using this, the Expectation & Variance can be determined:

$$
\begin{aligned}
E(\hat{\beta}_1)
&= \sum w_i E(y_i) \\
&= \sum w_i E(\beta_0 + \beta_1 x_i) \\
&= \beta_0 \sum w_i + \beta_1 \sum w_i x_i \\
&= \beta_0 (0) + \beta_1 (1) \\
&= \beta_1\\
\\
Var(\hat{\beta}_1)
&= Var(\sum w_i y_i) \\
&= \sum w_i^2 Var (y_i) \\
&= \frac{1}{\sum (x_i - \bar{x})^2} \\
&= \frac{\sigma^2}{\sum (x_i - \bar{x})^2}
\end{aligned}
$$

$$
\therefore \hat{\beta}_1 \sim N(\beta_1, \frac{\sigma^2}{\sum (x_i - \bar{x})^2})
$$

$$
\begin{aligned}
E(\hat{\beta}_0)
&= \sum (\frac{1}{n} - \bar{x}w_i) E(y_i) \\
&= \sum (\frac{1}{n} - \bar{x}w_i) (\beta_0 + \beta_1 x_i) \\
&= \beta_0 \sum (\frac{1}{n} - \bar{x}w_i) + \beta_1 \sum (\frac{1}{n} - \bar{x}w_i)x_i \\
&= \beta_0 (1) + \beta_1 (0) \\
&= \beta_0 \\
\\
Var(\hat{\beta}_0)
&= Var(\sum (\frac{1}{n} - \bar{x}w_i)y_i) \\
&= \sum (\frac{1}{n} - \bar{x}w_i)^2 Var (y_i) \\
&= \sigma^2 \sum (\frac{1}{n^2} -\frac{2\bar{x}w_i}{n} + \bar{x}^2 w_i^2) \\
&= \sigma^2 (\sum \frac{1}{n^2} - \frac{2\bar{x}}{n} \sum w_i + \bar{x}^2 \sum w_i^2) \\
&= \sigma^2 [n(\frac{1}{n^2}) - \frac{2\bar{x}}{n} (0) + \bar{x}^2 (\frac{1}{\sum (x_i - \bar{x})^2})] \\
&= \sigma^2 (\frac{1}{n} + \frac{\bar{x}^2}{\sum (x_i - \bar{x})^2})
\end{aligned}
$$

$$
\therefore \hat{\beta}_0 \sim N(\beta_0, \sigma^2 (\frac{1}{n} + \frac{\bar{x}^2}{\sum (x_i - \bar{x})^2}))
$$

### **Hypothesis Testing**

As mentioned in the overview section, hypothesis testing can be used to test for the presence of a relationship or its strength.

Since the regression parameters are normally distributed, a z-statistic can be used to conduct the tests. However, since the population variance is not known, a **t-statistic** is used instead:

$$
\begin{aligned}
t &= \frac{\hat{\beta_1} - \beta_1}{\hat{\sigma}_{\hat{\beta_1}}}
\end{aligned}
$$

Since the population variance is estimated by the MSE which has $n-2$ degrees of freedom, the corresponding chi-squared and hence t-distribution has $n-2$ degrees of freedom as well.

$$
t(\hat{\beta_1}) \sim t_{n-2}
$$

When testing for the presence of a relationship, both the F-statistic and t-statistic are **equivalent** ways of doing so and will always lead to the same conclusions. It is advised to use whichever statistic is more convenient to calculate based on the given information.

$$
t(\hat{\beta_1})^2 \sim F_{1, n-2}
$$

### **Confidence Intervals**

Since the distribution of t-statistic is known, it can be used to determine the confidence interval of the estimate:

$$
P\left(\text{Margin of Error} < \frac{\hat{\beta_1} - \beta_1}{\hat{\sigma}_{\hat{\beta_1}}} < \text{Margin of Error}\right) = 1 - \alpha
$$

$$
\text{Confidence Interval} = \hat{\beta}_1 + t_{n-2, \frac{\alpha}{2}} * \hat{\sigma}_{\hat{\beta_1}}
$$

### **Prediction Intervals**

Consider the Prediction Error of the SLR model:

$$
y_* - \hat{y_*} = \varepsilon_* + [(\beta_0 + \beta_1 x_*) - (\hat{\beta_0} + \hat{\beta_1}x_*)]
$$

Since both $y_*$ and $\hat{y_*}$ are normally distributed, the prediction errors are **normally distributed** as well:

$$
\begin{aligned}
E(y_* - \hat{y_*})
&= E[\varepsilon_* + [(\beta_0 + \beta_1 x_*) - (\hat{\beta_0} + \hat{\beta_1}x_*)]] \\
&= 0 + \beta_0 + \beta_1 E(x_*) - \beta_0 - \beta_1 E(x_*) \\
&= 0 \\
\\
Var(y_* - \hat{y_*})
&= Var(\varepsilon_* + [(\beta_0 + \beta_1 x_*) - (\hat{\beta_0} + \hat{\beta_1}x_*)] \\
&= Var(\varepsilon_*) + Var[(\beta_0 + \beta_1 x_*) - (\hat{\beta_0} + \hat{\beta_1}x_*)] \\
&= ... \\
&= \sigma^2 [1 + \frac{1}{n} + \frac{(x_* - \bar{x})^2}{\sum (x_i - \bar{x})^2}]
\end{aligned}
$$

$$
\therefore y_* - \hat{y_*} \sim N(0, \sigma^2 [1 + \frac{1}{n} + \frac{(x_* - \bar{x})^2}{\sum (x_i - \bar{x})^2}])
$$

Similar to before, since the population variance is unknown, it can be approximated using the MSE. Thus, a t-statistic constructed from this sample will have a **t-distribution**, allowing the following prediction interval to be calculated:

$$
\text{Prediction Interval} = \hat{y}_* + t_{n-2, \frac{\alpha}{2}} * \hat{\sigma}_{y_* - \hat{y_*}}
$$

Notice that the standard error of the prediction interval increases as $x_i$ moves further away from the mean, indicating that the predictions become **less accurate** for those values.
