# **Review of Statistical Theory**

This is meant to be a quick review of the key content of **VEE: Mathematical Statistics** that will be used for this exam.

## **Overview of Statistics**

Statistics is a discipline revolving around data.

A **Population** refers to the *theoretical* set of **all possible** data of the event of interest. The goal of statistics is to determine certain attributes that **summarizes or describes** the population, known as **Parameters**.

However, it is impossible to study the entire population at once, thus a **subset of the population** is studied instead, known as the **Sample**. Attributes that summarize or describe the sample are known as **Statistics**.

Ideally, the sample is **representative** of the population, which means that findings from the sample can be applied to the population as a whole. This means that the sample statistics can be used **estimate** population parameters.

We distinguish between the two (when they have the same notation) through the **Hat accent** (^) - Population Parameters are their written without the hat ($x$) while their corresponding sample statistics are written with the hat ($\hat{x}$).

## **Common Sample Statistics**

The **Mean** is the average of the population.

| Population Mean | Sample Mean |
| :-: | :-: |
| $\mu = \sum\limits_{i=i}^n x_i * p(x_i)$ | $\bar{x} = \frac{1}{n} \sum\limits_{i=i}^n x_i$ |

The **Variance** measures the spread of values about the mean. However, the units of Variance are unintuitive, thus we consider the **Standard Deviation** for more practical purposes, which is the **square root of the variance**.

| Population Variance | Sample Variance |
| :-: | :-: |
| $\sigma^2 = \sum\limits_{i=i}^n (x_i - \mu)^2 * p(x_i)$ | $s^2 = \frac{1}{n-1} \sum\limits_{i=i}^n (x_i - \bar{x})^2$ |

<!--Bessel's correction-->

**Covariance** is a measure of the **linear relationship** between two variables:

* **Positive Covariance** - Variables move in the **same direction**
* **Negative Covariance** - Variables move in **opposite directions**

| Population Covariance | Sample Covariance |
| :-: | :-: |
| $\sigma_{x, y}$ | $\hat{\sigma}_{x, y} = \frac{1}{n-1} \sum\limits_{i=i}^n (x_i - \bar{x}) * (y_i - \bar{y})$ |

However, there are two issues with Covariance - the units are unintuitive (similar to variance) and there is no benchmark as to what constitutes a strong/weak relationship. Thus, the **Correlation** is an adjusted measure of the relationship **between -1 and 1**.

| Population Correlation | Sample Correlation |
| :-: | :-: |
| $\rho = \frac{\sigma_{x,y}}{\sigma_x * \sigma_y}$ | $r = \frac{s_{x,y}}{s_x * s_y}$ |

## **Sampling Distribution**

Whenever a sample is drawn from a population and a statistic is calculated, it is known as a **Point Estimate**.

If another sample were to be drawn, it would most likely result in a **different point estimate** as the underlying sample is likely to be different. If this process were to be repeated a large number times, the **probability distribution** of the resulting point estimates is known as the **Sampling Distribution** of the statistic. The standard deviation of this sampling distribution is known as the **Standard Error**.

There is no rule surrounding the sampling distribution - it depends on the distribution of the population, statistic, sampling method etc.

A special case is the **Sample Mean**. If the population is normally distributed, then it is normally distributed as well. However, regardless of the population distribution, it is approximately normally distributed through the **Central Limit Theorem** or **Law of Large Numbers**.

$$
\bar{x} \sim N(\mu, \frac{\sigma^2}{n})
$$

Following that, the standard error is given by the following:

$$
\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}}
$$

However, the population standard deviation is most likely unknown. Thus, we can calculate an **estimate of the standard error** by using the sample standard deviation instead:

$$
\hat{\sigma_{\bar{x}}} = \frac{s}{\sqrt{n}}
$$

## **Confidence Interval**

Given that there is only one true value for the population parameter and a whole distribution of estimators, it is unlikely that a point estimate will be equal to the population parameter. Thus, instead of a point estimate, a range of estimates is used, known as a **Confidence Interval**.

The interval is made a chosen **Confidence Level** which represents the **proportion of confidence intervals that will contain the true value**. In other words, if a large number of confidence intervals constructed in the same manner were to be made, {confidence level}% of them would contain the true value.

$$
CI = \hat{\theta}~\pm~Margin~of~Error
$$

The **Margin of Error** represents the range of values on *both sides* of the point estimate that the true value could lie. The width of the confidence interval is thus the sum of the two margins.

$$
Margin~of~Error = Critical Value * SE(\hat{\theta})
$$

<!-- To change -->

<!-- Obtained from Analyst Prep -->
![95% Confidence Interval](Assets/1.%20Review%20of%20Statistical%20Theory.md/Confidence%20Interval.png)

## **Hypothesis Testing**

**Hypothesis Testing** is a formal method of making inferences about the population parameters based on the sample statistic.

It starts with a **Hypothesis** which is a conjecture about the population parameters:

* **Null Hypothesis** - What is currently believed to be true
* **Alternative Hypothesis** - What is to be proven

A **Test Statistic** is then calculated, which would quantify the behaviour of population such that it would **distinguish** the null and alternative hypothesis.

*Assuming the Null Hypothesis is true*, the sampling distribution of the test statistic is determined. From the sampling distribution, the **p-value** is calculated, which is the **probability** of observing the **calculated sample statistic or more extreme**.

If the **p-value is smaller** than a pre-determined level of **Statistical Significance** ($\alpha$), then the test is "successful" and the null hypothesis is rejected. If not, then the test "fails" and the null is not rejected.

> Note that it rejecting the null hypothesis does NOT mean that the alternative hypothesis is accepted - EG. Rejected then hypothesis of a large paw print being from a bear does not mean that it is from Bigfoot.
>
> Thus, the hypotheses are usually constructed such that the two hypothesis are **complementary**, such that rejecting the null allows acceptance of the alternative, leading to a definitive insight.

In layman terms, a hypothesis test is a **test of extremeness**. Assuming the null hypothesis is true, how extreme (low probability) is the observed sample? If the p-value is sufficiently low, it means that the sample is rare - it could be due to **pure chance** or that it is **actually not rare** because the null is not true.

We distinguish between the two mathematically through $\alpha$. It is the **probability of a False Positive** - that the result obtained was by pure chance. It is typically set at 5%, which means that 5% of all statistics calculated are expected to be extreme under the null. Thus, if the p-value of the sample is smaller than $\alpha$, then it is likely that the observation was **not due to chance but instead because the null was false**.

<!-- Obtained from Zscore geek -->
![Hypothesis Testing](Assets/1.%20Review%20of%20Statistical%20Theory.md/Hypothesis%20Testing.png){.center}

Alternatively, instead of comparing p-value to $alpha$, the test-statistic and the **corresponding value** of $\alpha$ on the sampling distribution can be used. It is known as the **Critical Value**, which represents the boundary of 

<center>

| Reject Null | Do not Reject Null |
| :-: | :-: |
| p-value smaller than $\alpha$ | p-value smaller than $\alpha$ |
| test-statistic *larger* than critical value | test-statistic *smaller* than critical value |

</center>

## **Maximum Likelihood Estimation**

If the population distribution is known, there is an **alternative method of estimating** the parameters apart from calculating the corresponding sample statistics.

We model a set of observations as a random sample from an unknown joint probability distribution which is expressed in terms of a set of parameters. 
The goal of maximum likelihood estimation is to determine the parameters for which the observed data have the highest joint probability.

The goal of maximum likelihood estimation is to find the values of the model parameters that maximize the likelihood function over the parameter space
